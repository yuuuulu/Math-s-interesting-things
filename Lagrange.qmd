---
title: "Extreme Values of Function using not only Lagrange but the same philosophy"
---
# introduction with the relationship between it and lasso
The Lagrange multiplier method can transform constrained problems into unconstrained problems, which is a great wisdom. It can be used to solve lasso, and lasso is an interesting and wonderful method for dimensionality reduction in statistics, which makes me feel that the journey of learning multivariate calculus is a joyï¼š
## Lasso Regression

Lasso regression (Least Absolute Shrinkage and Selection Operator) is a regularization technique for linear regression models that introduces an L1 norm penalty. The goal is to minimize the following objective function:

\[
\text{minimize} \quad \frac{1}{2} \| \mathbf{y} - \mathbf{X}\mathbf{\beta} \|_2^2 + \lambda \|\mathbf{\beta}\|_1
\]

where:
- \(\mathbf{y}\) is the vector of target variables.
- \(\mathbf{X}\) is the feature matrix.
- \(\mathbf{\beta}\) is the vector of regression coefficients.
- \(\lambda\) is the regularization parameter controlling the strength of regularization.
- \(\|\mathbf{\beta}\|_1\) is the L1 norm of \(\mathbf{\beta}\), which is the sum of the absolute values of the coefficients.

## Lagrange Multipliers

The method of Lagrange multipliers is used to solve optimization problems with constraints. It involves introducing Lagrange multipliers to incorporate constraints into the objective function, converting constrained problems into unconstrained problems. Given an objective function \( f(\mathbf{x}) \) with constraints \( g_i(\mathbf{x}) = 0 \), the Lagrange function is:

\[
\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) + \sum_{i} \lambda_i g_i(\mathbf{x})
\]

where \(\lambda_i\) are the Lagrange multipliers. By setting the derivatives of \(\mathcal{L}\) to zero, we can find the optimal solution.

## Relationship Between Lasso and Lagrange Multipliers

In Lasso regression, the L1 norm regularization term \(\|\mathbf{\beta}\|_1\) can be viewed as a constraint. The Lasso problem can be reformulated as a constrained optimization problem:

\[
\text{minimize} \quad \frac{1}{2} \| \mathbf{y} - \mathbf{X}\mathbf{\beta} \|_2^2 \quad \text{subject to} \quad \|\mathbf{\beta}\|_1 \leq t
\]

where \(t\) is a non-negative constant representing the limit on the regularization strength.

### Using Lagrange Multipliers for Lasso

We can use Lagrange multipliers to solve this constrained problem. The Lagrange function is defined as:

\[
\mathcal{L}(\mathbf{\beta}, \lambda) = \frac{1}{2} \| \mathbf{y} - \mathbf{X}\mathbf{\beta} \|_2^2 + \lambda (\|\mathbf{\beta}\|_1 - t)
\]

where \(\lambda\) is the Lagrange multiplier. The optimal solution \(\mathbf{\beta}^*\) satisfies:

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{\beta}} = -\mathbf{X}^\top (\mathbf{y} - \mathbf{X}\mathbf{\beta}) + \lambda \text{sign}(\mathbf{\beta}) = 0
\]

\[
\|\mathbf{\beta}\|_1 \leq t
\]

\[
\lambda (\|\mathbf{\beta}\|_1 - t) = 0
\]

## Summary

- **Lasso Regression** uses L1 regularization to achieve feature selection, and its optimization problem can be transformed into a constrained optimization problem.
- **Lagrange Multipliers** provide a method to handle constraints in optimization problems by converting them into unconstrained problems and introducing multipliers to adjust the regularization strength.

In Lasso regression, the L1 regularization constraint can be handled using Lagrange multipliers, converting the problem into one with multipliers that adjust the strength of regularization.
### Problem

Find extreme values of 

$$ f(x, y) = \cos x + \cos y + \cos (x + y). $$

### Solution

Since cosine is a periodic function, we can consider the region \( 0 \leq x \leq 2\pi \), \( 0 \leq y \leq 2\pi \) (bounded and closed region) to find the maximal and minimal values.

Firstly, we consider the interior of the region to find stationary points:

$$ f_x = -\sin x - \sin (x + y) = 0 $$
$$ f_y = -\sin y - \sin (x + y) = 0 $$

This implies:

$$ \sin x = \sin y $$

Then, inside the region (not on boundary), we have three cases:

1. \( y = x \)
2. \( y = \pi - x \) for \( 0 < x < \pi \)
3. \( y = 3\pi - x \) for \( \pi < x < 2\pi \)

#### Case 1

$$ \sin x + \sin (x + y) = \sin x + 2 \sin x \cos x = 0 $$
$$ \sin x (2 \cos x + 1) = 0 $$

This implies:

$$ x = \pi, y = \pi $$
or
$$ x = \frac{2\pi}{3}, y = \frac{2\pi}{3} $$
or
$$ x = \frac{4\pi}{3}, y = \frac{4\pi}{3} $$

Evaluating the function at these points:

$$ f(\pi, \pi) = -1 $$
$$ f\left( \frac{2\pi}{3}, \frac{2\pi}{3} \right) = -\frac{3}{2} $$
$$ f\left( \frac{4\pi}{3}, \frac{4\pi}{3} \right) = -\frac{3}{2} $$

#### Case 2

$$ \sin x + \sin (x + y) = \sin x + \sin \pi = \sin x = 0 $$

This implies:

$$ x = \pi, y = 0 $$ (on boundary)

#### Case 3

$$ \sin x + \sin (3\pi - x) = \sin x = 0 $$

This implies:

$$ x = \pi, y = 2\pi $$ (also on boundary)

### On the Boundary

Due to periodic property, we consider:

$$ x = 0, 0 \leq y \leq 2\pi $$
and
$$ y = 0, 0 \leq x \leq 2\pi $$

Evaluating the function at these boundaries:

$$ f(0, y) = 1 + 2 \cos y, \min = -1, \max = 3 $$
$$ f(x, 0) = 1 + 2 \cos x, \min = -1, \max = 3 $$

So, 

$$ \max f(x, y) = 3 \text{ at } (2n\pi, 2k\pi) \text{ for any } n, k \in \mathbb{Z} $$
$$ \min f(x, y) = -\frac{3}{2} \text{ at } \left( (2n+1)\pi \pm \frac{\pi}{3}, (2k+1)\pi \pm \frac{\pi}{3} \right) $$

