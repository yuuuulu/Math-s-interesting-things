{
  "hash": "a29cffd65ef914cd0f82e0470ce609c9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"APH101+ MTH113 Intro to probability and statistics+APH003--exploring world through data\"\neditor: visual\n---\n\n\n\n\n\n\n\n```r\ndplyr::filter()\n```\n\n\n\n\n# Simple random samlple\n\nWe know the total population but do not know p of our interests. \n\nBased on hypergeometric probability calculation we can estimate p as $\\hat p = \\text{intrests}/N= X_1+...+X_N/N$ where $X_i$~ Bernoulli(p) and $\\hat p$ is a random variable. Therefore, $E[\\hat p] = p$ (E[$X_i$]=p based on Bernouli distribution).\n\n\n- If $X_i$ are independent then E($X_1+X_2$) = 2E($X_1$) \n\n\nHere, eg. E[$\\hat p^2$]= 1/n E[$X_1^2$]+n-1/nE[$X_1 X_2$]\n\nSince $X_1=1$ $X_1^2=X_1$\n\nand E[$X_1 X_2$] = P[$X_1, X_2$]\n\n\nFinally, we can look at the distribution of $\\hat p$, suppose we know the true p is 0.54, we can use **simulation** to randomly sample $X_1,...X_n$ from Np people who support 1 and ......who support\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nset.seed(111)\n\npopulation_size <- 12141897\n\np <- 0.54\n\nnum_simulations <- 500\n\nsample_size <- 1000\n\n\np_hat_values <- replicate(num_simulations, {\n  # Simulate sampling from the population\n  sample <- sample(c(rep(1,population_size * p), rep(0,population_size*(1-p))), sample_size, replace =FALSE) # replicate(num_simulations, {...})：这个函数会重复执行大括号中的代码num_simulations次，并将每次的结果存储在一个向量中； rep创建一个包含population_size * p个1和population_size * (1-p)个0的向量，模拟总体。\nmean(sample) #calculate the p_hat for each sample\n})\n\n\n  \nhistogram <- ggplot(data.frame(p = p_hat_values), aes(x=p))+\n  geom_histogram(binwidth = 0.01, fill =\"blue\", color = \"black\")+\n  labs(title =\" \",\n       x = \"p_hat\",\n       y= \"Frequency\")+\n  theme_minimal()\n\nprint(histogram)\n```\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\ndoing this 500 times, \n\n\n\n\nbased on the central limit theory\n\n\n\nStatistical inference and Probability distribution\n\n\n- estimation\n\n- condifidence intervals\n\n- hypothesis testing \n\n\n# Sample Space and Probability Measure\n\n\n\n\n$\\sigma$ algebra $F$ is a collection of   satisfying :\n\nfull and null set\n\n\ncompleterment \n\n\ncountably union\n\n\n\n\n\nmeasurble we can find a function that takes the elements of F and output a real number\n\n\nA probability measure is a mapping P:F--> R satisfying the following 3 axioms:\n\n\ncountably for mutually exclusive events $A_1, A_2,...\\in F$\n\n\n\n# interpret the probability\n\n\n- frequentist view\n\n- Bayesian view\n\n\nBorel set is a combination of open set in some spaces\n\n\n\n\n```r\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(magrittr)\n\ncirc <- read_csv(\"Charm_City_Circulator_Ridership.csv\")\n## take just average ridership per day\navg = circ %>% \n  filter(type == \"Average\")\n# keep non-missing data\navg = avg %>% \n  filter(!is.na(number))\n```\n\n\n\n\n# Moment Generating Functions\n\n\n\n$M_x (t)=E(e^{tx})$\n\n## case 1\n\n$M_x$ may not exist. When it exists in a neighborhood of 0, using talor\n\n\n$$e^{tx}=1+tX+(tX)^2/2+...$$\n\n$$M_x(t)=1+t\\mu+t^2 \\mu/2+...$$\n\n$\\mu_j = E(X^i)$ is the j-th moment of X. Therefore,\n\n\n$$E(X^i)=M^{(j)}(0)$$\n\neg. Then we could also get the variance by take the 2nd order derivatives\n\n## case 2\n\n$\\int$\n\neg. normal\n\n- X~N(0,1)\n\n\nidea: try to write an integral of a certain distribution's pdf and we get the result of this integral as 1. here, we get the pdf of the N(t,1) and finally we get $e^{t^2/2}$\n\n\n- X~N($\\mu, \\sigma^2$)\n\n\nThen X=$\\mu+\\sigma Z$ where Z~N(0,1)\n\n\n$$M_x(t)=E[e^{tx}]=e^{\\mu t} E[e^{\\sigma t Z}]=e^{\\mu t} M_Z(\\sigma t)=e^{\\mu t +\\sigma^2 t^2/2}$$\n\n\n### Gamma disteibution\n\nthe family of gamma distributions generalizes the family of exponential distributions. The gamma distribution with shape r and rate $\\lambda$\n\n\n#### addition rule\n\nr +\n\n$\\lambda$ stays\n\n\n\n\n\n##  why MGF is useful? -- to determine if two random variables have the identical CDF / to prove the addition property of distributions\n\n\n**MGF** includes all characteristics of a distribution, from whom we could get pdf, cdf, expectation, variance\n\n- Thm If X and Y are random variables with the same MGF, which is finite on [-t, t ] for some t >0 then X and Y have the same distribution\n\n\nA gamma distribution with shape r =1 is an exponential distribution\n\n\n\n\n\n\nA more general function than MGF is the characteristic function.\n\n$\\phi_X (t) = E(e^{itX})$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Preface\n\nThe first section here is the knowledge that attracts most of my interests on this module, followed by the lecture notes I tapped when I am on the journey of this module.\n\n-   Motivation\n\nThe history of the development of statistics has helped me gain a deeper understanding of this fascinating and practical field of knowledge. Each advancement represents a leap from specific facts to broader, generalized conclusions. What truly captivates me is the remarkable alignment between natural phenomena and statistical principles; there always seems to be a coincidence where reality and theory intersect.\n\nInitially, I only grasped the surface of this knowledge. Then, as I delved deeper, I began to understand the underlying principles through observable phenomena. Eventually, almost miraculously, I realized how incredibly useful these theories are and how they coincide with natural occurrences. This process of moving closer and closer to generalization has profoundly illustrated to me the truth in the saying: \"Mathematics is the art of giving the same name to different things.\"\n\nTheorems come, theorems go. Only examples are lying forever. (Practice to use statistics to interpret real world examples)\n\n# 把公式推导单独放一栏（后）\n\n# error?\n\n-   Independent = Disjoint? NO!\n\n    Independent vs Disjoint: If P(E) \\> 0 and P(F) \\> 0, then E and F can NOT be both independent and disjoint.\n\n    Subsets are dependent. If E ⊂ F and neither P(E) = 0 nor P(F) = 1, then E and F are dependent.\n\n    Complements are dependent. If neither P(E) = 0 nor P(E) = 1, then E and E C are dependent.\n\n# My reading during this course：\n\n## 十堂极简概率课 中信出版 diaconis\n\n## 心理统计\n\n## The lady tasting tea\n\n## 概率论与数理统计（第三版） 峁诗松等老师编著\n\n## Probability....\n\n# bivariate data\n\n# linear regression\n\n![](images/clipboard-1774849890.png)\n\n![](images/clipboard-807464638.png)\n\n## background\n\n-   Regression to the mean (Galton's thinking)\n\n![](images/clipboard-3333660003.png)\n\n![](images/clipboard-1881050695.png)\n\n## it is not stable to predict the data outside our data sample'\n\n## Residual plot should have no pattern\n\nacross the whole range, it could not be showing a certain trend or a specific shape.\n\npositive and negative points sperate averagely .\n\n![](images/clipboard-2419630434.png)\n\n![](images/clipboard-3592387892.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 数据\nx <- c(50, 55, 50, 79, 44, 37, 70, 45, 49)  # Rock surface area\ny <- c(152, 48, 22, 35, 38, 171, 13, 185, 25)  # Algae colony density\n\n# (a) 计算最小二乘回归方程\nmodel <- lm(y ~ x)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-65.53 -63.91 -14.47  46.99  84.39 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  232.258     92.390   2.514   0.0402 *\nx             -2.926      1.690  -1.731   0.1271  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63.32 on 7 degrees of freedom\nMultiple R-squared:  0.2998,\tAdjusted R-squared:  0.1997 \nF-statistic: 2.997 on 1 and 7 DF,  p-value: 0.1271\n```\n\n\n:::\n\n```{.r .cell-code}\n# 获取回归系数\nintercept <- coef(model)[1]\nslope <- coef(model)[2]\ncat(\"最小二乘回归方程: y =\", intercept, \"+\", slope, \"* x\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n最小二乘回归方程: y = 232.2575 + -2.925507 * x\n```\n\n\n:::\n\n```{.r .cell-code}\n# (b) 计算 R^2 值并解释\nr_squared <- summary(model)$r.squared\ncat(\"R^2 值:\", r_squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR^2 值: 0.2997552 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"解释: R^2 表示了\", round(r_squared * 100, 2), \"% 的 y 的变异可以通过 x 来解释。\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n解释: R^2 表示了 29.98 % 的 y 的变异可以通过 x 来解释。\n```\n\n\n:::\n\n```{.r .cell-code}\n# (c) 计算残差标准误差 s_e\nse <- summary(model)$sigma\ncat(\"残差标准误差 s_e:\", se, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n残差标准误差 s_e: 63.31527 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\n```\n\n\n:::\n\n```{.r .cell-code}\n# (d) 判断线性关系的方向和强度\ncorrelation <- cor(x, y)\ncat(\"相关系数 r:\", correlation, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n相关系数 r: -0.547499 \n```\n\n\n:::\n\n```{.r .cell-code}\nif (correlation > 0) {\n  direction <- \"正相关\"\n} else {\n  direction <- \"负相关\"\n}\n\nif (abs(correlation) > 0.7) {\n  strength <- \"强相关\"\n} else if (abs(correlation) > 0.3) {\n  strength <- \"中等相关\"\n} else {\n  strength <- \"弱相关\"\n}\n\ncat(\"线性关系:\", direction, \"且为\", strength, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n线性关系: 负相关 且为 中等相关 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 数据\nquality_rating <- c(111, 113, 93, 130, 170, 87, 83, 117, 135, 109)\nsatisfaction_rating <- c(832, 845, 794, 854, 836, 842, 877, 745, 797, 795)\n\n# 计算相关系数\ncorrelation_coefficient <- cor(quality_rating, satisfaction_rating)\nprint(paste(\"相关系数 r:\", correlation_coefficient))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"相关系数 r: -0.115403519735578\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# 绘制散点图\nplot(quality_rating, satisfaction_rating,\n     main = \"Scatterplot of Quality Rating vs. Satisfaction Rating\",\n     xlab = \"Quality Rating\",\n     ylab = \"Satisfaction Rating\",\n     pch = 19, col = \"blue\")\nabline(lm(satisfaction_rating ~ quality_rating), col = \"red\")  # 添加回归线\n```\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# skew\n\nData skewed to the left(negatively skewed) have a longer left tail, and the mean and media are to the left of the mode.\n\nData skewed to the right(positively skewed) have a longer right tail, and the mean and media are to the right of the mode.\n\nmean is much sensitive than the median to the extreame value(not a resistant measure of center) so the outlier drags the mean to the skew...... ![](images/clipboard-2849357671.png)\n\nCompare $Q_2-Q_1$ and $Q_3-Q_2$ to decide left/right skew or symmetry.\n\nps: the **trimmed mean** is more resistant.\n\n![](images/clipboard-2111278063.png)\n\n![](images/clipboard-2144359450.png)\n\n![](images/clipboard-1854557292.png)\n\nAs long as we know the mean and standard deviation of the data, we can determine how far a certain proportion of the data falls, without knowing the specific distribution shape of the data.\n\n# outlier (eg of normal distribution)\n\na mild outlier if it lies more than 1.5(iqr) away from the nearest quartile (the nearest end of the box);\n\nan extreme outlier if it lies more than 3(iqr) away from the nearest quartile.\n\n(These definitions and distances are based on the hypothetical Normal distribution (bell shaped, symmetric, normal tails). When there is a reason to suspect that the distribution is skewed, the bounds should be changed.)\n\n# quartile\n\n$Q_1$(First quartile): at least 25% of the sorted values are less than or equal to $Q_1$ and at least 75% of the values are greater than or equal to $Q_1$\n\n$Q_3$(Third quartile): ..\n\n# modified boxplot\n\nA modified boxplot is a box plot where the whiskers only extend to the largest (or smallest) observation that is not an outlier and the outliers are plotted using a full circle (mild) or empty circle (extreme).\n\nIf there are no outliers, then the the whiskers end at the maximum (or minimum)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 数据\nratios <- c(0.553, 0.570, 0.576, 0.601, 0.606, 0.606, 0.609, 0.611, \n            0.615, 0.628, 0.654, 0.662, 0.668, 0.670, 0.672, 0.690, \n            0.693, 0.749, 0.844, 0.933)\n\n# 计算四分位数、IQR、温和和极端异常值的界限\nQ1 <- quantile(ratios, 0.25)\nQ3 <- quantile(ratios, 0.75)\nmedian_val <- median(ratios)\niqr <- Q3 - Q1\nmild_outlier_limit <- 1.5 * iqr\nextreme_outlier_limit <- 3 * iqr\n\nlower_mild <- Q1 - mild_outlier_limit\nlower_extreme <- Q1 - extreme_outlier_limit\nupper_mild <- Q3 + mild_outlier_limit\nupper_extreme <- Q3 + extreme_outlier_limit\n\n# 标记温和和极端异常值\nmild_outliers <- ratios[ratios > upper_mild & ratios <= upper_extreme | ratios < lower_mild & ratios >= lower_extreme]\nextreme_outliers <- ratios[ratios > upper_extreme | ratios < lower_extreme]\n\n# 绘制箱线图，并标记温和和极端异常值\nboxplot(ratios, main = \"Modified Boxplot of Width-to-Length Ratios\", ylim = c(0.3, 1))\npoints(which(ratios %in% mild_outliers), mild_outliers, pch = 16, col = \"blue\")    # 实心圆表示温和异常值\npoints(which(ratios %in% extreme_outliers), extreme_outliers, pch = 1, col = \"red\") # 空心圆表示极端异常值\n```\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# 输出四分位数、中位数和IQR结果\n\n#确定第一四分位数（Q1）：这是数据从小到大排列后，位于下四分之一位置的值，表示前25%的数据范围的最大值。\n#确定第三四分位数（Q3）：这是数据从小到大排列后，位于上四分之一位置的值，表示后75%的数据范围的最小值。\n\ncat(\"Q1:\", Q1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ1: 0.606 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Median:\", median_val, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMedian: 0.641 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Q3:\", Q3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ3: 0.6765 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"IQR:\", iqr, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIQR: 0.0705 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Lower mild outlier limit:\", lower_mild, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLower mild outlier limit: 0.50025 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Lower extreme outlier limit:\", lower_extreme, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLower extreme outlier limit: 0.3945 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Upper mild outlier limit:\", upper_mild, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUpper mild outlier limit: 0.78225 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Upper extreme outlier limit:\", upper_extreme, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUpper extreme outlier limit: 0.888 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 加载ggplot2包\nif (!require(ggplot2)) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# 数据\nratios <- c(0.553, 0.570, 0.576, 0.601, 0.606, 0.606, 0.609, 0.611, \n            0.615, 0.628, 0.654, 0.662, 0.668, 0.670, 0.672, 0.690, \n            0.693, 0.749, 0.844, 0.933)\n\n# 计算四分位数、IQR、温和和极端异常值的界限\nQ1 <- quantile(ratios, 0.25)\nQ3 <- quantile(ratios, 0.75)\nmedian_val <- median(ratios)\niqr <- Q3 - Q1\nmild_outlier_limit <- 1.5 * iqr\nextreme_outlier_limit <- 3 * iqr\n\nlower_mild <- Q1 - mild_outlier_limit\nlower_extreme <- Q1 - extreme_outlier_limit\nupper_mild <- Q3 + mild_outlier_limit\nupper_extreme <- Q3 + extreme_outlier_limit\n\n# 标记温和和极端异常值\nmild_outliers <- ratios[ratios > upper_mild & ratios <= upper_extreme | ratios < lower_mild & ratios >= lower_extreme]\nextreme_outliers <- ratios[ratios > upper_extreme | ratios < lower_extreme]\n\n# 创建数据框\ndata <- data.frame(ratios = ratios)\ndata$outlier_type <- ifelse(data$ratios %in% mild_outliers, \"Mild Outlier\",\n                            ifelse(data$ratios %in% extreme_outliers, \"Extreme Outlier\", \"Normal\"))\n\n# 使用ggplot2绘制\nggplot(data, aes(x = \"\", y = ratios)) +\n  geom_boxplot(outlier.shape = NA, fill = \"lightblue\") +  # 不显示默认的异常值\n  geom_point(data = subset(data, outlier_type == \"Mild Outlier\"), aes(y = ratios), color = \"blue\", size = 3, shape = 16) + # 温和异常值，实心圆\n  geom_point(data = subset(data, outlier_type == \"Extreme Outlier\"), aes(y = ratios), color = \"red\", size = 3, shape = 1) + # 极端异常值，空心圆\n  labs(title = \"Modified Boxplot of Width-to-Length Ratios\", y = \"Width-to-Length Ratios\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank()) + # 移除x轴标签\n  coord_cartesian(ylim = c(0.3, 1))      # 设置y轴范围\n```\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# Measures of relative standing\n\n## z Scores\n\neg. height(among women or men (come from different populations)instead of just comparing the height itself)\n\n# tutorial\n\ndotplot没有纵轴in r(vs scallar plot)\n\nprice of a textbook is discrete\n\nzip code is categorical\n\ndotplot and scallar plot\n\ndo not manipulate data---experimental\n\ntable 2.1\n\n11\n\nhuge data for 顺序的 shuffle is ok\n\nbut ..\n\nthe likelihood is totally different\n\n# The role of statistics and the Data Analysis Process\n\n## Intro\n\nstat is a large field in math involving the collection, organization, analysis,interpretation, and presentation of data(a collection of observations on one or more variables(A characteristic whose value may change from one observation to another))\n\nStatistics is the scientific discipline that provides methods to help us make sense of data.\n\nIt is important to be able to:\n\n1 Extract information from tables, charts, and graphs.\n\n2 Follow numerical arguments.\n\n3 Understand the basics of how data should be gathered, summarized, and analysed to draw statistical conclusions.\n\nThe Data Analysis Process\n\n1 Understanding the nature of the research problem or goals.\n\n2 Deciding what to measure and how.\n\n3 Collecting data.\n\n4 Data summarization and preliminary analysis.\n\n5 Formal Data Analysis (Statistical Methods).\n\n6 Interpretation of the results.\n\n## populations and samples\n\npopulation: The entire collection of individuals or objects about which information is desired\n\nsample: A sample is a subset of the population, selected for study.\n\nthen select the sample\n\nthen we could summarize it using 2 branches of stat.--- Decriptive stat.(methods for organizing and summarizing data.) or inferential stat.(generalizing from a sample(incomplete information) to the population from which the sample was selected and assessing the reliability of such generalizations.So we run the risk(An important aspect of statistics and making statistics inferences involves quantifying the chance of making an incorrect conclusions.))\n\n### descriptive stat\n\n### inferential stat\n\nsample\n\n## Types of data\n\n### uni data set and bivariate and multivariate\n\n### categorical and numerical(discrete and continuous) with plot using excel (data analysis) or rstudio plot (ggplot2)\n\nfor categorial data we could use a bar chart which is a graph of a frequency distribution for categorical data.\n\nfor a small numerical data we could use dotplot\n\n-   discrete\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# creat data：Wechat number\ndiscrete_data <- data.frame(value = c(30, 15, 20,30,60))\n\n# plot\nggplot(discrete_data, aes(x = value)) +\n  geom_dotplot(binwidth = 1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Discrete Data (Number of Wechats)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n-   continuous\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_athletes <- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball <- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 设置画布的高度，以便将两个图绘制在同一页面上\nplot.new()\nplot.window(xlim = c(0, 100), ylim = c(0.5, 2.5))\n\n# 绘制 Basketball 数据的 dotplot\nstripchart(basketball, method = \"stack\", at = 2, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 绘制 All Athletes 数据的 dotplot\nstripchart(all_athletes, method = \"stack\", at = 1, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 添加 X 轴\naxis(1, at = seq(10, 100, by = 10), labels = seq(10, 100, by = 10))\n\n# 添加标签\ntext(-5, 2, \"Basketball\", xpd = TRUE, adj = 1)\ntext(-5, 1, \"All Athletes\", xpd = TRUE, adj = 1)\n\n# 添加横线\nabline(h = 1.5, col = \"black\", lwd = 2)\n\n# 添加 X 轴标签\ntitle(xlab = \"Graduation rates (%)\")\n```\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# creat data--time spent in minutes\ncontinuous_data <- data.frame(value = c(6, 5.25, 3.62,1,2,3.1,3.2,4,5,6,7,4,10))\n\n# dotplot\nggplot(continuous_data, aes(x = value)) +\n  geom_dotplot(binwidth = 0.1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Continuous Data (Time Spent in Minutes)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# 毕业率数据\nschool <- 33:68\nall_athletes <- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball <- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 创建数据框\ndata <- data.frame(school, all_athletes, basketball)\n\n# 画图\nggplot() +\n  geom_dotplot(data = data, aes(x = all_athletes, y = \"All Athletes\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5) +\n  geom_dotplot(data = data, aes(x = basketball, y = \"Basketball\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5, color = \"red\") +\n  xlab(\"Graduation rates (%)\") +\n  ylab(\"\") +\n  theme_minimal() +\n  ggtitle(\"Dotplot of Graduation Rates for All Athletes and Basketball Players\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](mth1113_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n![histogram excel plot](images/clipboard-3026116502.png)\n\n## collect data sensibly\n\n# chapter 2\n\n## Two types of studies: Observational studies and Experiments.\n\n### Observational\n\nA study in which the investigator observes characteristics of a sample selected from one or more existing populations. The goal is to draw conclusions about the corresponding population or about differences between two or more populations.\n\nIn an observational study, it is impossible to draw clear cause-and-effect conclusions\n\n### Experiments\n\nA study in which the investigator observes how a response variable behaves when one or more explanatory variables, also called factors, are manipulated.\n\nA well-designed experiment can result in data that provide evidence for a cause-and-effect relationship.\n\n![](images/clipboard-858121668.png)\n\n-   Experimental conditions: Any particular combination of values for the explanatory variables, which are also called treatments.\n\n### comparison\n\n-   Both observational studies and experiments can be used to compare groups, but in an experiment the researcher controls who is in which group, whereas this is not the case in an observational study.\n\n-   In an observational study, it is impossible to draw clear cause-and\u0002effect conclusions\n\n### confounding vars\n\nA variable that is related to both how the experimental groups were formed and the response variable of interest.\n\n-   Two methods for data collection: Sampling and Experimentation.\n\n-   distinguish between selection bias, measurement or response bias, and non-response bias.\n\n-   select a simple random sample from a given population.\n\n-   distinguish between simple random sampling, stratified random sampling, cluster sampling, systematic sampling, and convenience sampling\n\n## variable\n\n### response variable--y\n\nThe response variable is the focus of a question in a study or experiment.\n\n### explanotory variable--x\n\nAn explanatory variable is one that explains for changes in the response variable.\n\n### experiments and obeservational study\n\n### bias\n\nselection bias：When the way the sample is selected systematically excludes some part of the population of interest.\n\nmeasurement or response bias\n\neg: survey question/scale(The scale or a machine used for measurements is not calibrated properly)\n\nNon-response Bias:When responses are not obtained from all individuals selected for inclusion in the sample.\n\nnon-response bias can distort results if those who respond differ in important ways from those who do not respond (e.g. laziness a confounding vari\u0002able).\n\n### random sampling\n\ndef: A sample that is selected from a population in a way that ensures that every different possible sample of size n has the same chance of being selected.\n\nthe same chance to be selected\n\ncounter eg:\n\nConsider 100 students in a classroom, 60 females and 40 males. If we randomly sample 6 females, and 4 males, then each female has a 6/60 = 0.1 chance of being selected. Same for males, 4/40=0.1. However, not every group of 10 students is equally likely to be selected. This is not simple random sampling\n\nThe random selection process allows us to be confident that the sample adequately reflects the population, even when the sample consists of only a small fraction of the population.\n\neg.Voting Sample Size in a country\n\n### stratified and cluster\n\n-   stratified random sampling:In stratified random sampling, separate simple random samples are independently selected from each subgroup. Each subgroup is called a strata.\n\nIn general, it is much easier to produce relatively accurate es\u0002timates of characteristics of a homogeneous group than of a heterogeneous group.\n\nstratified: according to certain characteristic\n\neg.Even with a small sample, it is possible to obtain an accurate estimate of the average grade point average (GPA) of students graduating with high honours from a university (Similar high grades, homogenous, thus only sample a few students). On the other hand, producing a reasonably accurate estimate of the average GPA of all seniors at the university, a much more diverse group of GPAs, is a more difficult task. **Not only does this ensure that students at each GPA level are represented, it also allows for a more accurate estimate of the overall average GPA.**\n\n-   cluster reflect general characteristic about the whole entire population\n\ncluster: randomly groups\n\nCluster sampling involves dividing the population of interest into non-overlapping subgroups, called clusters. Clusters are then selected at random, and then all individuals in the selected clusters are included in the sample.\n\n### systematic sampling\n\nA value k is specified (e.g. k = 50 or k = 200). Then one of the first k individuals is selected at random, after which every k-th individual in the sequence is included in the sample. A sample selected in this way is called a 1 in k systematic sample.\n\nIn the case of large samples, it can ensure that the sample is evenly distributed in the population.\n\n# Random Variable\n\n## Random Variable (R.V.)\n\nA numerical variable whose value depends on the outcome of a chance experiment. A random variable associates a numerical value with each outcome of a chance experiment. (Think of it as a rule that translates each result of a chance event into a number.)\n\nIn shorta: random variables convert random events into numbers.\n\nA real-valued random variable X is a function: X : S → $\\mathbb R$, where S is the sample space of a chance experiment.\n\nContinuous random variable X: S--\\> R is continuous if its set of possible values includes an entire interval on the number line(measurement), which could not be count.\n\nDiscrete random variable: X: S--\\> R if its set of possible value is a collection of isolated points along the number line.(counting)\n\n### eg\n\nExamples: Coin Tossing (Discrete Random Variable) If we flip a coin 5 times, let X be the number of heads we get. Possible values of X {0,1,2,3,4,5} (where 0 means no heads, and 5 means all heads). Here, X turns each outcome of multiple coin tosses into a count of heads.\n\nDeparture Time (Continuous Random Variable) Imagine tracking when people leave a subway station between 10 PM and 11 PM. Let Y represent the time (in hours) someone leaves, so Y can be any number from 10 to 11. Here, Y assigns each departure time to a point in the range \\[10,11\\].\n\n## sample space of multivariables\n\nG:gender; F: year---corresponding to a certain student:S\n\n(G, H) : S → $\\mathbb R^2$, S={1,2,3,4}\n\n...\n\n## Probability Mass Function and Cumulative Distribution Function for Discrete Random Variables\n\nProbability Mass Function (PMF): $p_x (x) := P(X = x) ,\\forall x$\n\nCumulative Distribution Function (CDF): $F_x (x) := P(X \\leq x) ,\\forall x$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 定义每个事件的概率和对应的 X 值\noutcomes <- c(\"GGGG\", \"EGGG\", \"GEGG\", \"GGEG\", \"GGGE\", \n              \"EEGG\", \"EGEG\", \"EGGE\", \"GEEG\", \"GEGE\", \n              \"GGEE\", \"GEEE\", \"EEEG\", \"EEGE\", \"EEEE\")\nprobabilities <- c(0.1296, 0.0864, 0.0864, 0.0864, 0.0864, \n                   0.0576, 0.0576, 0.0576, 0.0576, 0.0576, \n                   0.0384, 0.0384, 0.0384, 0.0384, 0.0256)\nX_values <- c(0, 1, 1, 1, 1, \n              2, 2, 2, 2, 2,\n              3, 3, 3, 3, 4)\n\n# 计算每个 X 值的 PMF 通过分组和求和\npmf <- tapply(probabilities, X_values, sum)\n\n# 定义可能的 X 值\nX_values_unique <- sort(unique(X_values))\n\n# 计算 CDF\ncdf <- cumsum(pmf)\n\n# 确保 CDF 在 x > 4 时为 1\ncdf <- c(cdf, 1)\n\n# 更新 X 值以包括 x > 4 的情况\nX_values_unique <- c(X_values_unique, \">4\")\n\n# 创建数据框显示 PMF 和 CDF\ntable <- data.frame(\n  X = X_values_unique,\n  `PMF P(X=x)` = c(pmf, 1-0.1296-0.3456-0.2880-0.1536-0.0256),  # PMF 没有对应的 x > 4 值，填 NA\n  `CDF F(X<=x)` = cdf\n)\n\n# 打印表格\nprint(table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   X PMF.P.X.x. CDF.F.X..x.\n0  0     0.1296      0.1296\n1  1     0.3456      0.4752\n2  2     0.2880      0.7632\n3  3     0.1536      0.9168\n4  4     0.0256      0.9424\n  >4     0.0576      1.0000\n```\n\n\n:::\n:::\n\n\n-   Note that the domain of a cdf is (−∞, ∞)\n\n$\\mathrm{pmf} \\Longrightarrow \\mathrm{cdf}$ $$\nF_x(x)=\\sum_{y \\leq x} p_x(y)\n$$ cdf $\\Longrightarrow$ pmf Suppose $X$ takes ordered values $x_1, x_2, x_3, \\cdots$, then $$\n\\begin{aligned}\np_X\\left(x_i\\right) & =P\\left(X=x_i\\right)=P\\left(x_{i-1}<X \\leq x_i\\right) \\\\\n& =P\\left(X \\leq x_i\\right)-P\\left(X \\leq x_{i-1}\\right) \\\\\n& =F\\left(x_i\\right)-F\\left(x_{i-1}\\right)\n\\end{aligned}\n$$\n\n-   remark: The probability of a discrete distribution varies depending on the inclusion and exclusion of the boundary values.\n\n## Expectation and Variance for Discrete Random Variables\n\n$$\n\\frac{0 \\cdot f_0+1 \\cdot f_1+2 \\cdot f_2+\\cdots+n \\cdot f_n}{N}=\\frac{1}{N} \\sum_{i=0}^n i \\cdot f_i\n$$\n\nNote that in $\\frac{1}{N} \\sum_{i=0}^n i \\cdot f_i$, $$\n\\lim _{N \\rightarrow \\infty} \\frac{f_i}{N}=P(X=i)\n$$\n\nSo the average number will be $$\n\\sum_{i=0}^n i \\cdot P(X=i)\n$$\n\n-   Definition: Expectation Given a discrete random variable $X$, the expectation of $X$ is $$\n    E[X]=\\sum_x x \\cdot p_X(x)\n    $$\n\n-   Properties of Expectation\n\n-   If $c$ is a constant, then $E[c]=c$.\n\n-   If $X \\geq 0$ then $E[X] \\geq 0$.\n\n-   If $a \\leq X \\leq b$ then $a \\leq E[X] \\leq b$.\n\n-   Proof of 3: First show $E[X] \\geq a$, then show $E[X] \\leq b$, $$\n    \\begin{aligned}\n    E[X] & =\\sum_x x p_X(x) \\geq \\sum_x a p_X(x), \\\\\n    & =a \\sum_x p_x(x)=a .\n    \\end{aligned}\n    $$\n\nSimilarly, $E[X] \\leq b$.\n\n-   Suppose $X$ is a discrete random variable and $Y=g(X)$, then $$\n    \\begin{aligned}\n    E[Y] & =\\sum_y y p_Y(y)=\\sum_y y P(Y=y) \\\\\n    & =\\sum_y y \\sum_{\\{x: g(x)=y\\}} P(X=x) \\\\\n    & =\\sum_y \\sum_{\\{x: g(x)=y\\}} y P(X=x) \\\\\n    & =\\sum_y \\sum_{\\{x: g(x)=y\\}} g(x) P(X=x) \\\\\n    & =\\sum_x g(x) P(X=x)\n    \\end{aligned}\n    $$\n\n-   First moment of $X$ (mean): $$\n    E[X]=\\sum_x x p_X(x) .\n    $$\n\n-   Second moment of $X$ : $$\n    E\\left[X^2\\right]=\\sum_x x^2 p_x(x) .\n    $$\n\n-   In general, $E[g(X)] \\neq g(E[X])$. For example, let $g(x)=x^2$, and consider $X$ such that $$\n    p_X(x)= \\begin{cases}0.5, & \\text { for } x=-1 \\\\ 0.5, & \\text { for } x=1\\end{cases}\n    $$\n\nThen clearly $E\\left[X^2\\right]=1 \\neq 0=(E[X])^2$.\n\n-   There are exceptions (e.g. when g is linear)!\n\n-   Linearity of Expectation\n\nE\\[aX + b\\] = aE\\[X\\] + b\n\nproof:\n\nSuppose $g(x)=a x+b$. Then $$\n\\begin{aligned}\nE[g(X)] & =\\sum_x g(x) p_X(x), \\\\\n& =\\sum_x(a x+b) p_x(x), \\\\\n& =\\sum_x a x p_x(x)+\\sum_x b p_x(x), \\\\\n& =a \\sum_x x p_x(x)+b \\sum_x p_x(x), \\\\\n& =a E[X]+b=g(E[X]),\n\\end{aligned}\n$$ this implies $$\nE[a X+b]=a E[X]+b\n$$\n\n-   Remark: Apart from this case, always assume $E[g(X)] \\neq g(E[X])$.\n\n# joint distribution of X and Y\n\nThe joint probability mass function (i.e., joint pmf) of X and Y for discrete random variables is defined as $p_{X,Y} (x, y) := P(X = x \\text{ and } Y = y)$\n\nP\\[(X,Y) ∈ A\\] =$\\sum_{(x,y)\\in A}p_{X,Y}(x,y)$,where A belongs to a subset of the $\\mathbb R^2$ where X and Y taking values.\n\n# Marginal Probability Mass Function\n\nLet $X$ and $Y$ have the joint probability mass function $p_{X, Y}(x, y)$ with space $\\mathcal{S}$. The probability mass function of $X$ (or $Y$ ) alone, is called the marginal probability mass function of $X$ (or $Y$ ) and defined by: $$\n\\begin{aligned}\n& p_X(x)=P(X=x)=\\sum_y p_{X, Y}(x, y) \\\\\n& p_Y(y)=P(Y=y)=\\sum_x p_{X, Y}(x, y)\n\\end{aligned}\n$$\n\nFor the marginal of X, we sum over all values of y.\n\nFor the marginal of Y, we sum over all values of x.\n\neg.--helping understand both sample space, event and Marginal probability mass function\n\n![](images/clipboard-206208503.png)\n\n# Functions of multiple random variables\n\nit has the same philosophy as one random variable things, such as expectation:\n\n$$E[g(X,Y)]=\\sum_x \\sum_y g(x,y)p_{X,Y}(x,y)$$, also the linearity of expectation\n\neg. Z = X + 2Y.\n\n![](images/clipboard-369824916.png)\n\n# independence\n\n-   P($A\\cap B$)=P(A)P(B) or P(A\\|B)=P(A)\n\n-   Two discrete random variables X and Y are independent if P(X = x and Y = y) = P(X = x)P(Y = y):\n\n$p_{X,Y}(x,y)=p_X(x)p_Y(y), \\forall x,y$\n\n$p_{X|Y}(x|y)=p_X(x), \\forall x,y$\n\nit is easy to detect the dependent as long as we find one example, eg: $P_{X|Y}(1|1)$ is not equal to $P(X=1)$\n\n## independence, expectations(mean) and variance\n\nIndependence and Expectations If $X$ and $Y$ are independent, then $$\nE[X Y]=E[X] E[Y] .\n$$\n\nProof: We use $E[g(X, Y)]$ where $g(x, y)=x y$. $$\n\\begin{aligned}\nE[X Y] & =\\sum_x \\sum_y x y p_{X, Y}(x, y) \\\\\n& =\\sum_x \\sum_y x y p_X(x) p_Y(y), \\quad(\\text { by inde } \\\\\n& =\\left(\\sum_x x p_X(x)\\right)\\left(\\sum_y y p_Y(y)\\right)=E[X] E[Y] .\n\\end{aligned}\n$$ (by independence)\n\nSimilarly, if $X$ and $Y$ are independent, then $$\nE[g(X) h(Y)]=E[g(X)] E[h(Y)]\n$$\n\n-   Independence and Variances\n\nIt is always true that $$\n\\operatorname{Var}(a X)=a^2 \\operatorname{Var}(X), \\quad \\text { and } \\quad \\operatorname{Var}(X+a)=\\operatorname{Var}(X)\n$$\n\nIn general, when we have a sum of random variables $X$ and $Y$ $$\n\\operatorname{Var}(X+Y) \\neq \\operatorname{Var}(X)+\\operatorname{Var}(Y) .\n$$\n\nIt is only true if $X$ and $Y$ are independent\n\n-   Sum of Variance for independent R.V. If two random variables $X$ and $Y$ are independent then $$\n    \\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)\n    $$\n\n-   Sum of Variance for independent R.V.\n\nIf two random variables $X$ and $Y$ are independent then $$\n\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y) .\n$$\n\nProof: Independence implies $E[X Y]=E[X] E[Y]$. Thus $$\n\\begin{aligned}\n& \\operatorname{Var}(X+Y)=E\\left[(X+Y-(E[X]+E[Y]))^2\\right], \\\\\n& =E\\left[(X-E[X])^2+2(X-E[X])(Y-E[Y])+(Y-E[Y])^2\\right], \\\\\n& =\\operatorname{Var}(X)+\\operatorname{Var}(Y)+2 E[(X-E[X])(Y-E[Y])]\n\\end{aligned}\n$$\n\nAs $X$ is indep to $Y$, then $X-\\mu_X$ is indep to $Y-\\mu_Y$ so $$\nE[(X-E[X])(Y-E[Y])]=E[X-E[X]] E[Y-E[Y]]=0\n$$\n\nExample: Assume independence, $\\operatorname{Var}(3 X-5 Y)=$ $$\n\\operatorname{Var}(3 X)+\\operatorname{Var}(-5 Y)=9 \\operatorname{Var}(X)+25 \\operatorname{Var}(Y)\n$$\n\n-   Example. Independence, mean and variance\n\nLet $Y$ be the random variable denoting the total number of heads by tossing a coin $n$ times. Find the mean and variance of $Y$.\n\nLet $$\nY_i= \\begin{cases}1, & \\text { if the } i^{\\text {th }} \\text { toss gets a head } \\\\ 0, & \\text { otherwise. }\\end{cases}\n$$\n\nThen $Y=Y_1+Y_2+\\cdots+Y_n$ where $Y_1, Y_2, \\cdots, Y_n$ are independent. For any $i=1,2, \\cdots, n$, we have $$\n\\begin{array}{ccc}\ny & 0 & 1 \\\\\nP_{Y_i}(y) & 1 / 2 & 1 / 2\n\\end{array}\n$$\n\nAs $E\\left[Y_i\\right]=\\frac{1}{2}$ and $\\operatorname{Var}\\left(Y_i\\right)=\\frac{1}{4}$ for $i=1,2, \\cdots, n$ $$\n\\begin{gathered}\nE[Y]=E\\left[Y_1\\right]+E\\left[Y_2\\right]+\\cdots+E\\left[Y_n\\right]=\\frac{n}{2} \\\\\n\\operatorname{Var}(Y)=\\operatorname{Var}\\left(Y_1\\right)+\\operatorname{Var}\\left(Y_2\\right)+\\cdots+\\operatorname{Var}\\left(Y_n\\right)=\\frac{n}{4}\n\\end{gathered}\n$$ (for not independent cases we have the same result as E but not Var because Var is not linear)\n\n# Continuous Random Variables\n\nWe have to consider intervals instead of one when referring to continuous random variables or the probability will equals to 0, which has no meaning.\n\neg. $P(a\\leq Y\\leq b)=P(a<Y<b)$ if Y is a continuous variable.\n\n## Definition. Probability Density Function (pdf)\n\nFor a continuous random variable X, the Probability Density Function (PDF) of X is f(x) where P(X = x) = 0 for all x and for any a ≤ b\n\n## discrete and continuous\n\n![](images/clipboard-1910128728.png)\n\nDensity function's value could be greater than 1 because the integral of it in a very tiny interval could be very small, which represents the probability(which could not be greater than 1 and since the interval could be very tiny this condition is satisfied!).(This reminder notices that the value of pdf is not the probability since the integration is the probability which is totally 1)\n\n# Covariance\n\n![](images/clipboard-4061434485.png)\n\n## wait for reviewing.....\n\n![](images/clipboard-2808960987.png)\n\n# Descrete distributions\n\nA probability distribution is a graph, table of formula that gives the probability for each value of the random variable.\n\n## Bernoulli is composed by binomial\n\n## Hypergeometric distribution could be approximated by Binomial when samples are large\n\n## Poisson distribution as the limit of Binomial distribution when the number of trials is large and the probabiliy of success of each trial is inverse-proportional to the number of trials\n\n-   The Poisson distribution is a discrete probability distribution that applies to occurrences of some event over a specified interval. The random variable x is the number of occurrences of the event in an interval.The probability of the event occurring x times over an interval is given by $$P(x)=\\frac{u^x\\cdot e^{-\\mu}}{x!}$$ where the random variable x is the number of occurences of an event over some interval and the ovvurrences must be random and independent of each other.\n\n中心极限定理：当泊松分布的参数 λ 较大时，泊松分布的形状会接近正态分布。这是因为中心极限定理指出，大量独立随机变量的和趋向于正态分布，而泊松分布可以看作是大量伯努利试验成功次数的分布，当试验次数足够多时，其和可以用正态分布来近似\n\n### ???The occurrences must be uniformly distributed over the interval being used\n\nThe mean is $\\mu$\n\nThe standard deviation is $\\sigma= \\sqrt \\mu$\n\n-   eg of Poisson distribution: (describing the behavior of rare events(with small probabilities). radioactive decay, arrivals of people in a line, eagles nesting in a region, patients arriving at an emergency room(the local hospital experiences a mean of 2.3 patients arriving at the emergency room during 10-11 P.M. on Fri. is known, we can find the probability that for a randomly selected Fri. between 10-11 P.M., exactly four patients arrive), Internet users logging onto a Web site )\n\n-   Comparison between Binomial:\n\nBinomal distribution is affected by yhe smaple size n and the probability p, whereas the Poisson distribution is affected only by mean $\\mu$\n\nA binomial distribution has a limit of possible values but a Poisson distribution has a possible values x without upper bound.\n\nSuppose the $X_n,n\\geq1$ is a sequence of random variables such that $X_n$\\~Bin($n,p_n$), where $p_n$\\~$\\lambda/n$ as $n->\\infty$ $lim_{n->\\infty}(np_0)=\\lambda$, **intuitively we can observe that** $\\lambda$ is the mean of\n\ngiven the well-known limit $\\lim _{n \\rightarrow \\infty}\\left(1-\\frac{\\lambda}{n}\\right)^n=e^{-\\lambda}$, and $$\n\\begin{aligned}\n& \\frac{n}{n} \\frac{n-1}{n} \\ldots \\frac{n-k+1}{n}=\\prod_{i=0}^{k-1}\\left(1-\\frac{i}{n}\\right) \\\\\n& \\lim _{n \\rightarrow \\infty} \\prod_{i=0}^{k-1}\\left(1-\\frac{i}{n}\\right)=1 \\\\\n& P\\left(X_n=k\\right)=\\binom{n}{k} p_n^k\\left(1-p_n\\right)^{n-k} \\\\\n&= \\frac{n \\cdots(n-k+1)}{k!} p_n^k\\left(1-p_n\\right)^n\\left(1-p_n\\right)^{-k} \\\\\n&= \\frac{1}{k!}(\\underbrace{n p_n}_{\\rightarrow \\lambda})^k \\underbrace{\\frac{n}{n} \\frac{n-1}{n} \\cdots \\frac{n-k+1}{n}}_{\\rightarrow 1} \\underbrace{\\left(1-p_n\\right)^n}_{\\rightarrow \\mathrm{e}^{-\\lambda}} \\underbrace{\\left(1-p_n\\right)^{-k}}_{\\rightarrow 1} \\\\\n& \\rightarrow \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda} \\underbrace{}_{\\text {as } n \\rightarrow \\infty .}\n\\end{aligned}\n$$\n\nCheck that $p_x(k), k=0,1,2, \\cdots$ defines a probability mass function: given the Taylor Series of $e^\\lambda$ around $\\lambda=0$ is given by $f(x)=$ $\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!} x^n$ and $e^\\lambda=\\sum_{n=0}^{\\infty} \\frac{\\lambda^n}{n!}$. Hence, $$\n\\begin{aligned}\n\\sum_{k=0}^{\\infty} p_X(k) & =\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda} \\\\\n& =\\mathrm{e}^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\\\\n& =\\mathrm{e}^{-\\lambda} \\cdot \\mathrm{e}^\\lambda \\\\\n& =1 .\n\\end{aligned}\n$$ \\## Geometric distribution (and Geometric series--powerful!fantastic series)\n\n-   The probability of the first happening\n\n-   well-defined $\\begin{aligned} \\sum_{k=1}^{\\infty} p_X(k) & =\\sum_{k=1}^{\\infty}(1-p)^{k-1} p \\\\ & =p \\sum_{k=0}^{\\infty}(1-p)^k \\\\ & =p \\cdot \\frac{1}{1-(1-p)} \\\\ & =1 .\\end{aligned}$\n\n-   Tail probability of the Geometric distribution\n\nLet $X \\sim \\operatorname{Geom}(p)$ then $$\n\\begin{aligned}\nP(X>n) & =P(X=n+1)+P(X=n+2)+P(X=n+3)+\\cdots \\\\\n& =(1-p)^n p+(1-p)^{n+1} p+(1-p)^{n+2} p+\\cdots \\\\\n& =(1-p)^n p\\left(1+(1-p)+(1-p)^2+\\cdots\\right) \\\\\n& =(1-p)^n p \\frac{1}{1-(1-p)} \\\\\n& =(1-p)^n\n\\end{aligned}\n$$ for any $n=0,1,2, \\ldots$\n\n-   Memoryless property of Geometric distribution\n\nSuppose that $X \\sim \\operatorname{Geom}(p)$ and $n \\in\\{1,2,3, \\cdots\\}$. Then $$\nP(X-n=k \\mid X>n)=P(X=k) \\quad, k=1,2,3, \\cdots\n$$\n\nThat is, the distribution of $X-n$ under the probability function $P(\\cdot \\mid X>n)$ is the same as the distribution of $X$\n\nthe memoryless property is saying that given the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success (independent)\n\n-   Expectation\n\nSuppose that $X \\sim \\operatorname{Geom}(p)$. Then $$\n\\begin{aligned}\nE(X)=\\sum_{k=1}^{\\infty} k P(X=k) & =\\sum_{k=1}^{\\infty} k(1-p)^{k-1} p \\\\\n& =p \\sum_{k=1}^{\\infty}\\left[-\\frac{\\mathrm{d}}{\\mathrm{~d} p}(1-p)^k\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\sum_{k=0}^{\\infty}(1-p)^k\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\frac{1}{1-(1-p)}\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\frac{1}{p}\\right] \\\\\n& =\\frac{1}{p}\n\\end{aligned}\n$$ — variance\n\nLikewise, $$\n\\begin{aligned}\nE(X(X-1)) & =\\sum_{k=1}^{\\infty} k(k-1) P(X=k) \\\\\n& =\\sum_{k=2}^{\\infty} k(k-1)(1-p)^{k-1} p \\\\\n& =p(1-p) \\sum_{k=2}^{\\infty} k(k-1)(1-p)^{k-2} \\\\\n& =p(1-p) \\frac{\\mathrm{d}^2}{\\mathrm{~d} p^2}\\left[\\sum_{k=0}^{\\infty}(1-p)^k\\right] \\\\\n& =p(1-p) \\frac{\\mathrm{d}^2}{\\mathrm{~d} p^2} \\frac{1}{p} \\\\\n& =p(1-p) \\cdot \\frac{2}{p^3} \\\\\n& =\\frac{2(1-p)}{p^2} .\n\\end{aligned}\n$$ $$\\begin{aligned} \\operatorname{Var}(X) & =E(X(X-1))+E(X)-(E(X))^2 \\\\ & =\\frac{2(1-p)}{p^2}+\\frac{1}{p}-\\frac{1}{p^2} \\\\ & =\\frac{1-p}{p^2}\\end{aligned}$$\n\n## Geometric distribution\n\n### The irrelevance of past events to the probability of future independent events\n\nGiven the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success.\n\nX\\~Geom(p) and n\\$\\in \\${1,2,3,....}\n\nP(X-n=k\\|X\\>n)=P(X=k), k=1,2,3,...\n\nTail probability for the probability calculation of more/higher than….\n\n## Addition\n\nIndependent random variable with the same distribution allows the addition law\n\n# Normal distribution\n\n## interpretation:\n\n-   Standard normally distributed sth. z=1.58(corresponding to 0.9429):\n\n    the probability of randomly selecting sth. with a value less than 1.58(unit) is equal to the area(probability) of 0.9429.\n\n    (Or: 94.29% of sth, will have a value below 1.58(unit))\n\n## Sampling distributions and Estimators\n\nWe are beginning to embark a jourfa d dney that allows us to learn about populations by obtaining data from samples since it is rare that we know all values in an entire population.\n\n**Sampling distribution of a statistic** is the probability distribution of a sample statistics (such as mean/proportion which tend to target the population mean/proportion), with all samples having the same sample size. This concept is important to understand. The behavior of a statistic can be known by understanding its distribution( (The random variable in this case is the value of that sample statistics)). Under certain condition, the distribution of sampling mean/proportion approximates a normal distribution.\n\n**Though statistics does not depend on unknown parameters, the distribution of it depend on unknown parameters.(eg. Normal distribution of sample means depends on population mean(an unknow parameter) and standard deviation)**\n\n(ps: the advantage of sampling with replacement:\n\nwhen selecting a relatively small sample from a large population, it makes no significant difference whether we sample with or without replacement.\n\nSampling with replacement results in independent events that are unaffected by previous outcomes, and independent events are easier to analyze and they result in simpler formulas.)\n\n**For a fixed sample size, the mean of all possible sample means is equal to the mean of population though sample means vary(sampling variability)**\n\n## Unbiased estimators and biased estimators\n\nStatistics that target population parameters: Mean, variance, proportion\n\nStatistics that do target population parameters: Median, Range, Standard Deviation\n\n(the bias is relatively small then sampling standard deviation in large samples so s is ofent used to estimate $\\sigma$)\n\n# The Central Limit Thm\n\nIt is the foundation for estimation population parameters and hypothesis testing\n\n(Recall: A random variable is a variable that has a single numerical value(eg. x=1,x=2), determined by chance, for each outcome of a procedure随机变量是一种变量，对于一个过程的每个结果都有一个随机确定的单一数值(可以理解为，现象作用其上\n\nA probability distribution is a graph, table or formula that gives the probability for each value of a random variable\n\nThe sampling distribution of the mean is the probability fistribution of sample means, with all sample having the same sample size n)\n\nAs the sample size increases, the corresponding sample means tend to vary less. The central limit thm tells us that if the sample size is large engough, the distribution of sample means can be approximated by normal distribution\n\nConclusion of CLT: (it is so important since it allows us to use the basic normal distribution methods in a wide variety of different circumstances)\n\nThe distribution of sample means will, as the sample size increases, approach a normal distribution\n\nThe mean of all sample means is the population mean $\\mu.$. $$\\mu_{\\bar x}=\\mu$$\n\nThe standard deviation of all sample means is $\\sigma/ \\sqrt n$ (i,e, the normal distribution from conclusion\"The distribution of sample means will, as the sample size increases, approach a normal distribution\" has standard deviation $\\sigma/ \\sqrt n$ ) $$\\sigma_{\\bar x}=\\frac{\\sigma}{\\sqrt n}$$ $\\sigma_{\\bar x}$ is often called the standard error of the mean.\n\n30\n\nNotice: Be careful to look at if it is from a normally ditributed population or a mean for some sample/group.\n\nIf given the population mean is a and a sample(c subjects) mean is b which has so small probbility under the sampling distribution with this populatio mean, if the mean is really a, then there is an extremely small probability of getting a sample mean of b or lower when c subjects are randomly selected. we can interpret it in such 2 ways:\n\n1)  population mean is true and their sample represents a chance event that is extreamely rare\n\n2)  population mean is not true and sample is typical.\n\nBecause the probability is too low, it seems more reasonable to conclude that the population mean is lower than a---hypothesis testing's thinking\n\n### Optional: Correction for a finite population\n\nWhen sampling without replacement and the sample size n is greater than 5% of the finite populaiton size B(i.e. n\\>0.05N), ...\n\n## Using the Normal distribution as an approximation to the binomial distribution\n\nrequirement: np, n(1-p) $\\geq$ 5\n\nBe careful: adjust x for continuity by + or - 0.5(eg. at least 99, choose 98.5)\n\n## to hypothesis testing\n\na question (eg):\n\nIn a test of a gender-selection technique assume that 100 couples using a particular treatment give birth to 52 girls (and 48 boys). If the technique has no effect, then the probability of a girl is approximately 0.5. If the probability of a girl is 0.5, find the probability that among 100 newborn babies, exactly 52 are girls. Based on the result, is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?\n\nit is a binomial distribution with np=nq=100\\*0.5=50$\\geq 5$\n\nso we use the normal distribution with mean of 50 and $\\sigma = \\sqrt {npq} = 5$as an approximation to the binomial distribution\n\nto answer\"is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?\", we need to calculate more than 52(x successes among n trials is an unusually high number of successes if P($x\\geq a$) is very small)\n\n原因是因为如果只看52这个数字肯定概率很小，因为任何单个数字发生的可能性概率都很小\n\nSo if the answer of P($x\\geq 52$) is small we could conclude that the gender selection is useful\n\n(总结：如果是0。5概率来看的话52以上本来就不是难事，as indicated by the such large probability of P($x\\geq 52$) 所以我们没有充分证据拒绝“not effective”这个假设前提)\n\n此处还没有引入假设检验所以都是用using probability to determine when results are unusual这个思想来思考问题的：\n\n-   Unusually low： x successes among n trails is an unually low number of successes if P(x or fewer) is very small\n\n![](images/clipboard-804415.png)\n\nInterpretation for another example of gender selection(using only unusual results to explain): Because the probability of more than 13 girls, which is 0.001 is so low, we conclude that it is unusual to get 13 girls among 14 babies (using binomial to calculate it). This suggests that the technique of gender selection appears to be effective since it is highly unlikely that the result of 13 girls among 14 births happened by chance.\n\n## Normal distribution\n\nIf a variable is the superposition result of a large number of small independent random factors, then the variable must obey the normal distribution of variables\n\ne.g. measure error\n\n### Assessing Normality\n\nIn general, quantile plots can be used to assess any probability distribution.\n\nFor a normal quantile plot(or normal probability plot), it is a graph of points(x,y) where each x vaue is from the original set of sample data and each y value is the cooresponding z score that is a quantile value expected from the standard normal distribution\n\n#### Procedures\n\n-   Histogram(not helpful for small data set)\n\n-   outliers: reject normality if there is more than 1 outlier present(not helpful for small data set)\n\n-   normal quantile plot:\n\nsort data from lowest to highest\n\n-   Sampling distribution of pˆ when population is infinite\n\n![](images/clipboard-1400086523.png)\n\n## ![](images/clipboard-2929822579.png)inferences from 2 samples introduces the differences between two populaton means using matched pairs but correlaition and regression analyze the association between the 2 variables and if such an association exists we wnat to describe it with an equation that can be used for predictions\n\npaired sampled data(or called bivariate data)\n\n-   a correlation exists between two variables when one of them is related to the other in some way.\n\n-   the linear correlation coefficient r measures the strength of the linear association between the paired x- and y-quantitative values in a sample. Its value is computed by using the formula(Pearson(1857-1937) product moment correlation coefficient)\n\n......otherwise there is not sufficient evidence to support the conclusion of a significant linear equation\n\n!!!: interpreting r: explained variation: the value of $r^2$ is the proportion of the variation in y that is explained by the linear association between x and y.(and the other percentage is explained by factors other thanx such as characteristics not included in the study)\n\n-   correlation does not imply causality,just the association\n\n-   average suppress individual variation and may inflate the correlation coefficient(the linear correlation coefficient became higher when reginal averages were used)\n\n# point estimation and confident interval\n\n-   Tomorrow is the exam Good luck! 12.29\n\n-   12.30 Everything on MTH113 exam was fine\n\nAll in all, the exam did not decide anything. I think there is a long way to go and MTH113 just include not many about statistics, more than a half has been teached during high school......I will keep exploring the charismatic statistics world since we could not only understand it using the very basic life common sense but also prove them in a rigorous way, while during the proof, such as regression, we also could enjoy its explanation by linear algebra which I also enjoy.....\n\n-   I love you MTH113!!!\n\n![](images/clipboard-1345728232.png)\n",
    "supporting": [
      "mth1113_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}