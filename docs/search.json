[
  {
    "objectID": "Lagrange.html",
    "href": "Lagrange.html",
    "title": "Extreme Values of Function using not only Lagrange but the same philosophy",
    "section": "",
    "text": "The Lagrange multiplier method can transform constrained problems into unconstrained problems, which is a great wisdom. It can be used to solve lasso, and lasso is an interesting and wonderful method for dimensionality reduction in statistics, which makes me feel that the journey of learning multivariate calculus is a joy： ## Lasso Regression\nLasso regression (Least Absolute Shrinkage and Selection Operator) is a regularization technique for linear regression models that introduces an L1 norm penalty. The goal is to minimize the following objective function:\n[ | - |_2^2 + ||_1 ]\nwhere: - () is the vector of target variables. - () is the feature matrix. - () is the vector of regression coefficients. - () is the regularization parameter controlling the strength of regularization. - (||_1) is the L1 norm of (), which is the sum of the absolute values of the coefficients.\n\n\nThe method of Lagrange multipliers is used to solve optimization problems with constraints. It involves introducing Lagrange multipliers to incorporate constraints into the objective function, converting constrained problems into unconstrained problems. Given an objective function ( f() ) with constraints ( g_i() = 0 ), the Lagrange function is:\n[ (, ) = f() + _{i} _i g_i() ]\nwhere (_i) are the Lagrange multipliers. By setting the derivatives of () to zero, we can find the optimal solution.\n\n\n\nIn Lasso regression, the L1 norm regularization term (||_1) can be viewed as a constraint. The Lasso problem can be reformulated as a constrained optimization problem:\n[ | - |_2^2 ||_1 t ]\nwhere (t) is a non-negative constant representing the limit on the regularization strength.\n\n\nWe can use Lagrange multipliers to solve this constrained problem. The Lagrange function is defined as:\n[ (, ) = | - |_2^2 + (||_1 - t) ]\nwhere () is the Lagrange multiplier. The optimal solution (^*) satisfies:\n[ = -^( - ) + () = 0 ]\n[ ||_1 t ]\n[ (||_1 - t) = 0 ]\n\n\n\n\n\nLasso Regression uses L1 regularization to achieve feature selection, and its optimization problem can be transformed into a constrained optimization problem.\nLagrange Multipliers provide a method to handle constraints in optimization problems by converting them into unconstrained problems and introducing multipliers to adjust the regularization strength.\n\nIn Lasso regression, the L1 regularization constraint can be handled using Lagrange multipliers, converting the problem into one with multipliers that adjust the strength of regularization. ### Problem\nFind extreme values of\n\\[ f(x, y) = \\cos x + \\cos y + \\cos (x + y). \\]\n\n\nSince cosine is a periodic function, we can consider the region ( 0 x ), ( 0 y ) (bounded and closed region) to find the maximal and minimal values.\nFirstly, we consider the interior of the region to find stationary points:\n\\[ f_x = -\\sin x - \\sin (x + y) = 0 \\] \\[ f_y = -\\sin y - \\sin (x + y) = 0 \\]\nThis implies:\n\\[ \\sin x = \\sin y \\]\nThen, inside the region (not on boundary), we have three cases:\n\n( y = x )\n( y = - x ) for ( 0 &lt; x &lt; )\n( y = 3- x ) for ( &lt; x &lt; 2)\n\n\n\n\\[ \\sin x + \\sin (x + y) = \\sin x + 2 \\sin x \\cos x = 0 \\] \\[ \\sin x (2 \\cos x + 1) = 0 \\]\nThis implies:\n\\[ x = \\pi, y = \\pi \\] or \\[ x = \\frac{2\\pi}{3}, y = \\frac{2\\pi}{3} \\] or \\[ x = \\frac{4\\pi}{3}, y = \\frac{4\\pi}{3} \\]\nEvaluating the function at these points:\n\\[ f(\\pi, \\pi) = -1 \\] \\[ f\\left( \\frac{2\\pi}{3}, \\frac{2\\pi}{3} \\right) = -\\frac{3}{2} \\] \\[ f\\left( \\frac{4\\pi}{3}, \\frac{4\\pi}{3} \\right) = -\\frac{3}{2} \\]\n\n\n\n\\[ \\sin x + \\sin (x + y) = \\sin x + \\sin \\pi = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 0 \\] (on boundary)\n\n\n\n\\[ \\sin x + \\sin (3\\pi - x) = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 2\\pi \\] (also on boundary)\n\n\n\n\nDue to periodic property, we consider:\n\\[ x = 0, 0 \\leq y \\leq 2\\pi \\] and \\[ y = 0, 0 \\leq x \\leq 2\\pi \\]\nEvaluating the function at these boundaries:\n\\[ f(0, y) = 1 + 2 \\cos y, \\min = -1, \\max = 3 \\] \\[ f(x, 0) = 1 + 2 \\cos x, \\min = -1, \\max = 3 \\]\nSo,\n\\[ \\max f(x, y) = 3 \\text{ at } (2n\\pi, 2k\\pi) \\text{ for any } n, k \\in \\mathbb{Z} \\] \\[ \\min f(x, y) = -\\frac{3}{2} \\text{ at } \\left( (2n+1)\\pi \\pm \\frac{\\pi}{3}, (2k+1)\\pi \\pm \\frac{\\pi}{3} \\right) \\]"
  },
  {
    "objectID": "Lagrange.html#lagrange-multipliers",
    "href": "Lagrange.html#lagrange-multipliers",
    "title": "Extreme Values of Function using not only Lagrange but the same philosophy",
    "section": "",
    "text": "The method of Lagrange multipliers is used to solve optimization problems with constraints. It involves introducing Lagrange multipliers to incorporate constraints into the objective function, converting constrained problems into unconstrained problems. Given an objective function ( f() ) with constraints ( g_i() = 0 ), the Lagrange function is:\n[ (, ) = f() + _{i} _i g_i() ]\nwhere (_i) are the Lagrange multipliers. By setting the derivatives of () to zero, we can find the optimal solution."
  },
  {
    "objectID": "Lagrange.html#relationship-between-lasso-and-lagrange-multipliers",
    "href": "Lagrange.html#relationship-between-lasso-and-lagrange-multipliers",
    "title": "Extreme Values of Function using not only Lagrange but the same philosophy",
    "section": "",
    "text": "In Lasso regression, the L1 norm regularization term (||_1) can be viewed as a constraint. The Lasso problem can be reformulated as a constrained optimization problem:\n[ | - |_2^2 ||_1 t ]\nwhere (t) is a non-negative constant representing the limit on the regularization strength.\n\n\nWe can use Lagrange multipliers to solve this constrained problem. The Lagrange function is defined as:\n[ (, ) = | - |_2^2 + (||_1 - t) ]\nwhere () is the Lagrange multiplier. The optimal solution (^*) satisfies:\n[ = -^( - ) + () = 0 ]\n[ ||_1 t ]\n[ (||_1 - t) = 0 ]"
  },
  {
    "objectID": "Lagrange.html#summary",
    "href": "Lagrange.html#summary",
    "title": "Extreme Values of Function using not only Lagrange but the same philosophy",
    "section": "",
    "text": "Lasso Regression uses L1 regularization to achieve feature selection, and its optimization problem can be transformed into a constrained optimization problem.\nLagrange Multipliers provide a method to handle constraints in optimization problems by converting them into unconstrained problems and introducing multipliers to adjust the regularization strength.\n\nIn Lasso regression, the L1 regularization constraint can be handled using Lagrange multipliers, converting the problem into one with multipliers that adjust the strength of regularization. ### Problem\nFind extreme values of\n\\[ f(x, y) = \\cos x + \\cos y + \\cos (x + y). \\]\n\n\nSince cosine is a periodic function, we can consider the region ( 0 x ), ( 0 y ) (bounded and closed region) to find the maximal and minimal values.\nFirstly, we consider the interior of the region to find stationary points:\n\\[ f_x = -\\sin x - \\sin (x + y) = 0 \\] \\[ f_y = -\\sin y - \\sin (x + y) = 0 \\]\nThis implies:\n\\[ \\sin x = \\sin y \\]\nThen, inside the region (not on boundary), we have three cases:\n\n( y = x )\n( y = - x ) for ( 0 &lt; x &lt; )\n( y = 3- x ) for ( &lt; x &lt; 2)\n\n\n\n\\[ \\sin x + \\sin (x + y) = \\sin x + 2 \\sin x \\cos x = 0 \\] \\[ \\sin x (2 \\cos x + 1) = 0 \\]\nThis implies:\n\\[ x = \\pi, y = \\pi \\] or \\[ x = \\frac{2\\pi}{3}, y = \\frac{2\\pi}{3} \\] or \\[ x = \\frac{4\\pi}{3}, y = \\frac{4\\pi}{3} \\]\nEvaluating the function at these points:\n\\[ f(\\pi, \\pi) = -1 \\] \\[ f\\left( \\frac{2\\pi}{3}, \\frac{2\\pi}{3} \\right) = -\\frac{3}{2} \\] \\[ f\\left( \\frac{4\\pi}{3}, \\frac{4\\pi}{3} \\right) = -\\frac{3}{2} \\]\n\n\n\n\\[ \\sin x + \\sin (x + y) = \\sin x + \\sin \\pi = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 0 \\] (on boundary)\n\n\n\n\\[ \\sin x + \\sin (3\\pi - x) = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 2\\pi \\] (also on boundary)\n\n\n\n\nDue to periodic property, we consider:\n\\[ x = 0, 0 \\leq y \\leq 2\\pi \\] and \\[ y = 0, 0 \\leq x \\leq 2\\pi \\]\nEvaluating the function at these boundaries:\n\\[ f(0, y) = 1 + 2 \\cos y, \\min = -1, \\max = 3 \\] \\[ f(x, 0) = 1 + 2 \\cos x, \\min = -1, \\max = 3 \\]\nSo,\n\\[ \\max f(x, y) = 3 \\text{ at } (2n\\pi, 2k\\pi) \\text{ for any } n, k \\in \\mathbb{Z} \\] \\[ \\min f(x, y) = -\\frac{3}{2} \\text{ at } \\left( (2n+1)\\pi \\pm \\frac{\\pi}{3}, (2k+1)\\pi \\pm \\frac{\\pi}{3} \\right) \\]"
  },
  {
    "objectID": "understand directional derivative deeply.html",
    "href": "understand directional derivative deeply.html",
    "title": "one application problem about directional derivative and gradient",
    "section": "",
    "text": "Thanks to Boyun Pang to let me know this interesting question and thanks to Dr. Wang to teach me such clever method. The temperature ( T ) in degrees Celsius at ((x, y, z)) is given by\n\\(T = \\frac{10}{{x^2 + y^2 + z^2}}\\)\nwhere distances are in meters. A bee is flying away from the hot spot at the origin on a spiral path so that its position vector at time ( t ) seconds is \\(\\mathbf{r}(t) = t \\cos(\\pi t) \\mathbf{i} + t \\sin(\\pi t) \\mathbf{j} + t \\mathbf{k}\\)\nDetermine the rate of change of ( T ) in each case:\n\nWith respect to distance traveled at time ( t=1 ).\nWith respect to time at ( t = 1 ).\n\n(Think of two ways to do this.)\n\\[\n\\frac{dT}{dS} = \\nabla f \\cdot \\mathbf{R}'(S)\n\\]\n\\[\n\\frac{dT}{dS} = f_x \\cdot x'(S) + f_y \\cdot y'(S) + f_z \\cdot z'(S)\n\\]\n\\[\n||\\vec{r(t)}||=\\int_{0}^{t} \\sqrt{x'^2 + y'^2 + z'^2} \\, dt = S(t)\n\\]\n\\[\n\\mathbf{r}(t(S)) = \\mathbf{R}(S)\n\\]\n\\[\n\\vec{R(S)}' =\\mathbf{r}(t(S)) \\mathbf{t}(S)=\\mathbf{r}(t(S))\\frac{1}{\\mathbf{S}(t)}(inverse function)\n=\\frac{\\mathbf{r}’(t)}{||\\mathbf{r}’(t)||}\n\\]\n\\[\n\\mathbf{R}'(S) = \\frac{\\mathbf{r}'(t(S)) \\cdot t'(S)}{\\sqrt{x'^2 + y'^2 + z'^2}}\n\\]\n\\[\n\\mathbf{R}(t) = \\mathbf{r}(t)\n\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html",
    "href": "Mathematical Analysis 1.html",
    "title": "Analysis1",
    "section": "",
    "text": "everything must be a reason during the process of proof. Patience is necessary!\nlimit\nreal number"
  },
  {
    "objectID": "Mathematical Analysis 1.html#statement",
    "href": "Mathematical Analysis 1.html#statement",
    "title": "Analysis1",
    "section": "statement",
    "text": "statement\np ### —An assertion that is either true of false but not both\ne.g. \n\nNegation of a statement p is a statement which means the opposite of P\n~p—-read “not p”\n\n\nQuantifiers\nall, every, each,no(none)—universal quantifiers\nsome,there exists, there is at least on, etc.—existential quantifiers\nP: some a’s are b’s\n~P: all a’s are not b’s/ no a’s are b’s\nP: some a are not b\n~p: all a are b\n\n\nTruth table: give the truth values of related statements in all possible cases\n(relevant to boolean in computer science)\n# Example of boolean logic in a program\nis_raining = True\nhas_umbrella = False\n\nif is_raining and not has_umbrella:\n    print(\"You need an umbrella!\")\nelse:\n    print(\"You're good to go!\")\ncompound statement: combining several statements via logical operations\nConjunction: p^q. p and q\nDisjunction: p v q–p or q\none of them is true, p v q is true, so we only need to decide if oe of them is true, if it is, the p v q is true\np V (~p) is always true\na statement that is always true is called a tautology"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math is charismatic",
    "section": "",
    "text": "Many many and many…math is always expressing the secret in our magic nature…\n\n\nThe math teachers I have learned from are teaching math in an interesting and sophisticated way with different charismatic philosophy of themselves.\n\n\nI want to log the profound and inspiring things that I learned from my math teachers, including notes, philosophy and the like.\n\nAdvanced Linear Algebra and Linear Algebra\nAnalysis\nPDE intro\nmulti-variable calculus(geometric guys, sequence, …)\nmodeling knowledge\ninteresting problems"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Homepage",
    "section": "",
    "text": "this is first line"
  },
  {
    "objectID": "mth107.html",
    "href": "mth107.html",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "matrix-vector multiplication\ndifferentiation/integration\nODE(mth106)\nRecurrence Relations and Statistical Models\n\n\n\n\n\nA matrix A in this space is a real matrix, which maps vectors in \\(\\mathbb{R^n}\\) to another vector in \\(\\mathbb{R^n}\\) through multiplication: \\[\nA: \\mathbb{R^n} \\to \\mathbb{R^n}\n\\] \\[\nA \\cdot v = w \\in \\mathbb{R^n}\n\\]\nreal matrix\nconcrete\n\n\n\nlinear transformation: \\[\nT: V \\to W\n\\] abstract and more general vector space over \\(\\mathbb{R^n}\\) or \\(\\mathbb{C^n}\\)\n\n\n\n\n\n\\[\na \\in A\n\\] eg. \\[\n3 \\in \\mathbb{Z}\n\\]\n\\(\\emptyset\\)\nTwo sets A and B are equal if and only if they contain the same elements: \\[\nA = B\n\\] if and only if \\[\n\\forall x \\, (x \\in A \\iff x \\in B)\n\\] $$ A B\n\\[ intersection: \\] A B\n\\[ union: \\] A B $$\n\n\n\n\nmaps(functions)\n\\[\nf: A \\to B, \\quad a \\mapsto f(a)\n\\] composition: Given two functions \\(f: B \\to C\\) and \\(g: A \\to B\\), the composition of f and g is denoted as: \\[\n(f \\circ g)(x) = f(g(x)), \\quad \\text{for all} \\, x \\in A\n\\]\n\\[\na \\mapsto c(a \\mapsto b \\mapsto c)\n\\]\n\n\nif \\(a_1 \\neq a_2\\) then \\(f(a_1) \\neq f(a_2)\\)\nif \\(f(a_1) = f(a_2)\\), then \\(a_1=a_2\\)\n\n\n\nfor \\(f: A \\to B\\)\n\\[\n\\forall b \\in B, \\, \\exists a \\in A \\, \\text{such that} \\, f(a) = b\n\\]\n\n\n\nfor \\(f: A \\to B\\) \\[\n\\forall b \\in B, \\, \\exists! a \\in A \\, \\text{such that} \\, f(a) = b\n\\] （use \\(\\exists!\\)expresses only exist one）\n\n\n\n\n\n\n\\(A \\subseteq B\\)\n\n\n\n\\[\n\\emptyset \\to B\n\\]\n\n\n\nif A \\(\\subseteq\\) B, we have a map: \\[\n\\iota: A \\to B\n\\]\n\\[\n\\iota(a) = a\n\\], called the inclusion of A (into B)\n\n\n\nThat is:\nif A \\(\\subseteq\\) B and \\(f:B \\to C\\), we have a map for all a \\(\\in\\) A: \\[\nf|_A: A \\to C\n\\]:\n\\[\nf|_A(a) = f(a)\n\\] called the restriction of f to A\n\\[\nf|_A = f \\circ \\iota_A\n\\] because \\[\nf \\circ \\iota_A(a)=f(\\iota_A(a))=f(a)=f|_A(a)\n\\]\nthe reason to define it:\n\ndomains differ\n\nFor example, consider a map \\(g|_B: B \\to C\\), where the function takes each element \\(x\\) and maps it to \\(x + 2\\). Let the sets be as follows:\n\n\\(A = \\mathbb{N} \\subseteq B = \\mathbb{R}\\)\n\\(C = \\mathbb{R}\\) is the codomain of the function.\n\nThe map \\(g\\) is defined as: \\[ g: B \\to C, \\quad g(x) = x + 2 \\quad \\text{for all } x \\in B \\]\nNow, consider the restriction of \\(g\\) to \\(A\\), denoted \\(g|_A: A \\to C\\), where: \\[ g|_A: A \\to C, \\quad g|_A(x) = x + 2 \\quad \\text{for all } x \\in A \\]\nAlthough both \\(g: \\mathbb{R} \\to \\mathbb{R}\\) and \\(g|_A: \\mathbb{N} \\to \\mathbb{R}\\) follow the same rule \\(x \\mapsto x + 2\\), they cannot be considered the same map because their domains differ.\n(here, A \\(\\subseteq\\) B)\n\nfocus on the specific area within the whole area\n\n\n\n\nif a and b are sets we define \\(B^A={f:A---&gt;B (map)}\\) the set of all maps from A to B\n\\[\nB^A = \\{ f: A \\to B \\}\n\\] \\(B^\\emptyset = \\{ \\emptyset \\to B \\}\\) has only 1 element even if B=\\(\\emptyset\\)\n\nThe set \\(B^{\\emptyset}\\) contains only the zero function. proof: suppose f,g \\(\\in\\) \\(B^\\emptyset = \\{f: \\emptyset \\to B \\}\\), imagine f != g, we can derive \\(\\exists x \\in \\emptyset\\) s.t. f(x) != g(x), which is absurd, so f=g\n\nsuppose the only one element is sth.:\nsth. should follow the quality of the set, which is:\nsth. + sth. = sth.\n\\(\\lambda\\)sth. =sth.\nso sth. = 0\n\n\n|| #\n|A|: number of elements in A(set)\nif A and B are finite sets #A=n #B=m, then \\(|B^A| = m^n\\)\n\n\n\n\n\n\n\n\nMLR\n\n\nmatrix\n\n\nIn linear regression, we often work with matrix and vector representations. Given a matrix \\(X\\) (which represents the design matrix with each row corresponding to one observation and each column to a predictor), and a vector \\(Y\\) (representing the observed responses), the regression model can be written as:\n\\[\nY = X \\beta + \\epsilon\n\\]\nWhere: - \\(Y\\): The \\(n \\times 1\\) response vector, - \\(X\\): The \\(n \\times p\\) design matrix, - \\(\\beta\\): The \\(p \\times 1\\) vector of unknown coefficients, - \\(\\epsilon\\): The \\(n \\times 1\\) vector of errors or residuals.\nThe ordinary least squares (OLS) estimator for \\(\\beta\\), denoted \\(\\hat{\\beta}\\), is obtained by minimizing the residual sum of squares:\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\n\n\nConsider two vectors \\(X_1\\) and \\(X_2\\). If \\(X_1^T X_2 = 0\\), we say the vectors are orthogonal, meaning they are uncorrelated in terms of inner product space. In regression, orthogonality implies no collinearity between predictors.\nOrthogonality plays an important role in simplifying matrix operations in regression. For example, if \\(X_1\\) is orthogonal to \\(X_2\\), the matrix \\(X^T X\\) will have off-diagonal elements equal to zero, simplifying the computation of the inverse.\n\n\n\nIf the matrix \\(X^T X\\) is invertible, the OLS estimator exists and is unique. One important property of the matrix \\(X^T X\\) is that it is symmetric and positive semi-definite. If \\(X\\) has full column rank (meaning that the columns of \\(X\\) are linearly independent), then \\(X^T X\\) is positive definite, and its inverse exists.\n\n\nIf \\(X^T X\\) is invertible, then:\n\\[\n(X^T X)^{-1} X^T X = I_p\n\\]\nWhere \\(I_p\\) is the identity matrix of size \\(p \\times p\\).\n\n\n\n\nThe projection matrix (or hat matrix) \\(P\\) is defined as:\n\\[\nP = X (X^T X)^{-1} X^T\n\\]\nThis matrix \\(P\\) projects the observed data vector \\(Y\\) onto the column space of \\(X\\), giving the fitted values \\(\\hat{Y}\\):\n\\[\n\\hat{Y} = P Y\n\\]\nThe residuals \\(\\hat{\\epsilon}\\), which are the differences between the observed and fitted values, are given by:\n\\[\n\\hat{\\epsilon} = Y - \\hat{Y} = (I_n - P) Y\n\\]\nWhere \\(I_n\\) is the \\(n \\times n\\) identity matrix. The projection matrix \\(P\\) has several key properties: - \\(P^2 = P\\) (idempotent), - \\(P^T = P\\) (symmetric).\n\n\n\nThe residual sum of squares (RSS) is given by:\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2 = \\hat{\\epsilon}^T \\hat{\\epsilon} = (Y - X \\hat{\\beta})^T (Y - X \\hat{\\beta})\n\\]\nSubstituting \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\), we get:\n\\[\n\\text{RSS} = Y^T (I_n - P) Y\n\\]\n\n\n\nThe variance-covariance matrix of the OLS estimator \\(\\hat{\\beta}\\) is:\n\\[\n\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n\\]\nWhere \\(\\sigma^2\\) is the variance of the error terms. This matrix provides information about the precision of the estimated coefficients.\n\n\n\n\nConsider a vector \\(y\\) in the vector space spanned by the columns of \\(X\\). The projection of \\(y\\) onto this space can be written as:\n\\[\n\\hat{y} = P y\n\\]\nIf \\(P\\) is the projection matrix, then the following properties hold: - \\(P^T = P\\) (symmetric), - \\(P^2 = P\\) (idempotent), - \\((I - P)\\) is also a projection matrix (projecting onto the orthogonal complement).\nThus, we can decompose any vector \\(Y\\) as:\n\\[\nY = \\hat{Y} + (Y - \\hat{Y}) = P Y + (I - P) Y\n\\]\nWhere \\(\\hat{Y}\\) is the projection onto the space spanned by \\(X\\), and \\((Y - \\hat{Y})\\) is the projection onto the orthogonal complement.\n\n\n\n\nA vector \\(\\perp\\) to a matrix \\(X\\) (i.e., orthogonal to the column space of the matrix \\(X\\)) implies that:\n\n\\[\nX^T v = 0\n\\]\nThis condition means that the vector \\(v\\) is orthogonal to every column of the matrix \\(X\\). In regression, this condition often arises when discussing residuals, which are orthogonal to the fitted values (i.e., the column space of \\(X\\)).\n\n\n\n\nIf \\(X^T v = 0\\), then \\(v \\perp X\\) (i.e., \\(v\\) is orthogonal to the column space of \\(X\\)).\nIf \\(X\\) has linearly independent columns, then \\(X^T X\\) is invertible.\nIf \\(X\\) does not have full column rank, then \\(X^T X\\) is not invertible. In this case, we cannot directly compute \\((X^T X)^{-1}\\).\n\n\n\n\nFor \\(X\\) to have full column rank, the number of columns \\(p\\) must be less than or equal to the number of observations \\(n\\):\n列空间的正交补空间只有零向量 he orthogonal complement of a column space has only zero vectors\nIf \\(X\\) has full column rank, then \\(X^T X\\) is invertible. This property is crucial for solving the normal equations in ordinary least squares (OLS) regression.\n\n\n\nGiven that \\(X^T X\\) is symmetric, the following property holds:\n\\[\n((X^T X)^{-1})^T = (X^T X)^{-1}\n\\]\nThis is a key result in linear regression, allowing us to solve for the OLS estimator of \\(\\beta\\):\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\nThis property ensures that the normal equations have a unique solution when \\(X^T X\\) is invertible.\n\n\n\n\n\n正交性：矢量与矩阵的正交性在回归中经常用于描述残差与拟合值之间的关系。\n矩阵的性质：矩阵 \\(X^T X\\) 的可逆性取决于 \\(X\\) 的列满秩（线性无关性）。当 \\(X\\) 的列满秩时，\\(X^T X\\) 是可逆的，从而确保了线性回归模型的唯一解。\n矩阵运算：通过矩阵运算 \\((X^T X)^{-1}\\)，可以得到 OLS 回归系数的估计值 \\(\\hat{\\beta}\\)。"
  },
  {
    "objectID": "mth107.html#linearty-is-everywherelinearty-is-the-easiest-one",
    "href": "mth107.html#linearty-is-everywherelinearty-is-the-easiest-one",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "matrix-vector multiplication\ndifferentiation/integration\nODE(mth106)\nRecurrence Relations and Statistical Models"
  },
  {
    "objectID": "mth107.html#comparison",
    "href": "mth107.html#comparison",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "A matrix A in this space is a real matrix, which maps vectors in \\(\\mathbb{R^n}\\) to another vector in \\(\\mathbb{R^n}\\) through multiplication: \\[\nA: \\mathbb{R^n} \\to \\mathbb{R^n}\n\\] \\[\nA \\cdot v = w \\in \\mathbb{R^n}\n\\]\nreal matrix\nconcrete\n\n\n\nlinear transformation: \\[\nT: V \\to W\n\\] abstract and more general vector space over \\(\\mathbb{R^n}\\) or \\(\\mathbb{C^n}\\)"
  },
  {
    "objectID": "mth107.html#notations",
    "href": "mth107.html#notations",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "\\[\na \\in A\n\\] eg. \\[\n3 \\in \\mathbb{Z}\n\\]\n\\(\\emptyset\\)\nTwo sets A and B are equal if and only if they contain the same elements: \\[\nA = B\n\\] if and only if \\[\n\\forall x \\, (x \\in A \\iff x \\in B)\n\\] $$ A B\n\\[ intersection: \\] A B\n\\[ union: \\] A B $$"
  },
  {
    "objectID": "mth107.html#maps",
    "href": "mth107.html#maps",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "maps(functions)\n\\[\nf: A \\to B, \\quad a \\mapsto f(a)\n\\] composition: Given two functions \\(f: B \\to C\\) and \\(g: A \\to B\\), the composition of f and g is denoted as: \\[\n(f \\circ g)(x) = f(g(x)), \\quad \\text{for all} \\, x \\in A\n\\]\n\\[\na \\mapsto c(a \\mapsto b \\mapsto c)\n\\]\n\n\nif \\(a_1 \\neq a_2\\) then \\(f(a_1) \\neq f(a_2)\\)\nif \\(f(a_1) = f(a_2)\\), then \\(a_1=a_2\\)\n\n\n\nfor \\(f: A \\to B\\)\n\\[\n\\forall b \\in B, \\, \\exists a \\in A \\, \\text{such that} \\, f(a) = b\n\\]\n\n\n\nfor \\(f: A \\to B\\) \\[\n\\forall b \\in B, \\, \\exists! a \\in A \\, \\text{such that} \\, f(a) = b\n\\] （use \\(\\exists!\\)expresses only exist one）"
  },
  {
    "objectID": "mth107.html#subsetsinclusions-restrictions",
    "href": "mth107.html#subsetsinclusions-restrictions",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "\\(A \\subseteq B\\)\n\n\n\n\\[\n\\emptyset \\to B\n\\]\n\n\n\nif A \\(\\subseteq\\) B, we have a map: \\[\n\\iota: A \\to B\n\\]\n\\[\n\\iota(a) = a\n\\], called the inclusion of A (into B)\n\n\n\nThat is:\nif A \\(\\subseteq\\) B and \\(f:B \\to C\\), we have a map for all a \\(\\in\\) A: \\[\nf|_A: A \\to C\n\\]:\n\\[\nf|_A(a) = f(a)\n\\] called the restriction of f to A\n\\[\nf|_A = f \\circ \\iota_A\n\\] because \\[\nf \\circ \\iota_A(a)=f(\\iota_A(a))=f(a)=f|_A(a)\n\\]\nthe reason to define it:\n\ndomains differ\n\nFor example, consider a map \\(g|_B: B \\to C\\), where the function takes each element \\(x\\) and maps it to \\(x + 2\\). Let the sets be as follows:\n\n\\(A = \\mathbb{N} \\subseteq B = \\mathbb{R}\\)\n\\(C = \\mathbb{R}\\) is the codomain of the function.\n\nThe map \\(g\\) is defined as: \\[ g: B \\to C, \\quad g(x) = x + 2 \\quad \\text{for all } x \\in B \\]\nNow, consider the restriction of \\(g\\) to \\(A\\), denoted \\(g|_A: A \\to C\\), where: \\[ g|_A: A \\to C, \\quad g|_A(x) = x + 2 \\quad \\text{for all } x \\in A \\]\nAlthough both \\(g: \\mathbb{R} \\to \\mathbb{R}\\) and \\(g|_A: \\mathbb{N} \\to \\mathbb{R}\\) follow the same rule \\(x \\mapsto x + 2\\), they cannot be considered the same map because their domains differ.\n(here, A \\(\\subseteq\\) B)\n\nfocus on the specific area within the whole area\n\n\n\n\nif a and b are sets we define \\(B^A={f:A---&gt;B (map)}\\) the set of all maps from A to B\n\\[\nB^A = \\{ f: A \\to B \\}\n\\] \\(B^\\emptyset = \\{ \\emptyset \\to B \\}\\) has only 1 element even if B=\\(\\emptyset\\)\n\nThe set \\(B^{\\emptyset}\\) contains only the zero function. proof: suppose f,g \\(\\in\\) \\(B^\\emptyset = \\{f: \\emptyset \\to B \\}\\), imagine f != g, we can derive \\(\\exists x \\in \\emptyset\\) s.t. f(x) != g(x), which is absurd, so f=g\n\nsuppose the only one element is sth.:\nsth. should follow the quality of the set, which is:\nsth. + sth. = sth.\n\\(\\lambda\\)sth. =sth.\nso sth. = 0\n\n\n|| #\n|A|: number of elements in A(set)\nif A and B are finite sets #A=n #B=m, then \\(|B^A| = m^n\\)"
  },
  {
    "objectID": "mth107.html#finite-space",
    "href": "mth107.html#finite-space",
    "title": "MTH107—NOTEs",
    "section": "finite space",
    "text": "finite space\nFor finite spaces ,only R^n and C^n.\n(For infinite spaces, there are many, e.g. a continuous set: [0,1] to …)"
  },
  {
    "objectID": "mth107.html#rn-notification",
    "href": "mth107.html#rn-notification",
    "title": "MTH107—NOTEs",
    "section": "R^n notification",
    "text": "R^n notification\n\\[\n\\mathbb{R}^n = \\{ (x_1, x_2, \\dots, x_n) \\mid x_i \\in \\mathbb{R} \\text{ for } i = 1, 2, \\dots, n \\}\n\\] real numbers\n\ndefine 2 oprations on the set R^n\naddition: \\[\n(x_1, x_2, \\dots, x_n) + (y_1, y_2, \\dots, y_n) = (x_1 + y_1, x_2 + y_2, \\dots, x_n + y_n)\n\\]\n\nLeft-hand side: \\((x_1, x_2, \\dots, x_n), (y_1, y_2, \\dots, y_n) \\in \\mathbb{R}^n\\)\nRight-hand side: The result of \\(x_i + y_i\\) is in \\(\\mathbb{R}\\).\n\nscalar multiplication: for lambda \\(\\in\\) R,\n\\[\n\\lambda \\cdot (x_1, x_2, \\dots, x_n) = (\\lambda x_1, \\lambda x_2, \\dots, \\lambda x_n)\n\\]\n\nLeft-hand side: \\(\\lambda \\in \\mathbb{R}, (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n\\).\nRight-hand side: The result \\(\\lambda x_i \\in \\mathbb{R}\\), for each i.\n\nthese 2 oprations generalize the standard operations on R2 and R3\n\n\ngeometric realization\n矢量三角形–addition\n直线上–scalar multiplication\nmention: for n greater than or equal to 4 we cannot visualize vectors in the real world but e can still use them to solove real world problems\nif a and b are finite sets\n\n\nto simplify notations we sometimes use a single letter for vectors:\nx= \\(\\vec x\\) =(x_1,x_2,….)\nR^n as a vector space\nso we have(rn,+, mutiplication notation). the operations satisfies some useful properties that turn rn into a real vector space"
  },
  {
    "objectID": "mth107.html#relation-between-it-and-cdot",
    "href": "mth107.html#relation-between-it-and-cdot",
    "title": "MTH107—NOTEs",
    "section": "relation between it and + \\(\\cdot\\)",
    "text": "relation between it and + \\(\\cdot\\)\nit means that + and \\(\\cdot\\) satisfy the following axioms:\n\n\\(\\forall x,y \\in rn\\): x+y=y+x. commutativity\n\\(\\forall x,y,z \\in rn\\): (x+y)+z=x+(y+z). associativity\nx + 0 = 0 + x neutral element for addition\n\\(x + (-x) = (-x) + x = 0\\) inverse for addition -x=y\n\\(1 \\cdot x = x\\) Identity Element for Scalar Multiplication\n\\((\\lambda \\mu) \\cdot x = \\lambda \\cdot (\\mu \\cdot x)\\) compatibilily of multiplication\n\\(\\lambda \\cdot (x + y) = \\lambda \\cdot x + \\lambda \\cdot y\\) Distributivity of Scalar Multiplication Over Vector Addition\n\\((\\lambda + \\mu) \\cdot x = \\lambda \\cdot x + \\mu \\cdot x\\) Distributivity of Scalar Addition\n\n\n?\nwant to ensure if a set satisfies the 2 operations it is not satisfy all 1-8 axioms instead of F^n"
  },
  {
    "objectID": "mth107.html#complex-vector-space",
    "href": "mth107.html#complex-vector-space",
    "title": "MTH107—NOTEs",
    "section": "complex Vector Space",
    "text": "complex Vector Space\n\\(i^2 = -1\\)\nDefine \\(\\mathbb {C}^n\\) as the set of all ordered n-tuples of complex numbers: \\[\n\\mathbb{C}^n = \\{ (z_1, z_2, \\dots, z_n) \\mid z_i \\in \\mathbb{C}, \\, i = 1, 2, \\dots, n \\}\n\\]\n\nOperations on C^n\nWe define addition and scalar multiplication on \\(\\mathbb{C}^n\\) in the same way as we did on \\(\\mathbb{R}^n\\), but using complex numbers:\n\nAddition:\nFor two vectors \\((z_1, z_2, \\dots, z_n)\\) and \\((w_1, w_2, \\dots, w_n) \\in \\mathbb{C}^n\\):\n\\[\n(z_1, z_2, \\dots, z_n) + (w_1, w_2, \\dots, w_n) = (z_1 + w_1, z_2 + w_2, \\dots, z_n + w_n)\n\\]\n\n\nScalar Multiplication:\nFor a scalar \\(\\lambda \\in \\mathbb{C}\\) and a vector \\((z_1, z_2, \\dots, z_n) \\in \\mathbb{C}^n\\):\n\\[\n\\lambda \\cdot (z_1, z_2, \\dots, z_n) = (\\lambda z_1, \\lambda z_2, \\dots, \\lambda z_n)\n\\]\n\n\n\nComplex Vector Space\nThus, \\((\\mathbb{C}^n, +, \\cdot)\\) is a complex vector space because it satisfies the vector space axioms 1-8, where we replaced \\(\\mathbb{R}^n\\) with \\(\\mathbb{C}^n\\). Many of the results from MTH107 will hold regardless of whether we are using \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\), so we will often use \\(\\mathbb{F}\\) to represent either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\nFor example, \\(\\mathbb{F}^n\\) is an \\(\\mathbb{F}\\)-vector space, where \\(\\mathbb{F}\\) could be either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\n\n\nGeneralization (Not on the Exam):\n\nFinite Field Example:\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements.\nMany of our results hold in a more general setting, where \\(\\mathbb{F}\\) is a field—a set in which we can perform addition, multiplication, subtraction, and division (except division by zero).\nExamples of fields include:\n\n\\(\\mathbb{R}\\): the real numbers\n\\(\\mathbb{C}\\): the complex numbers\n\\(\\mathbb{Q}\\): the rational numbers\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements\n\n\n\n\nAbstract Vector Space:\nWe can generalize this idea by replacing \\(\\mathbb{F}^n\\) with some abstract space \\(V\\), define addition \\(+\\) and scalar multiplication \\(\\cdot\\), and check if they satisfy the eight vector space axioms.\n\n\nDefinitions:\n\n?\nso the scalar multiplication could be F but the addition must happens on V\nif V and W are sets, V\\(\\times\\) W = {(v,w)|v\\(\\in\\) V, w\\(\\in\\) W}\nFor a set \\(V\\), addition on \\(V\\) is a map:\n\\[\nV \\times V \\to V\n\\]\nIt maps an element set to their addition. For example, if \\(V = \\mathbb{R}^2\\):\n\\[\n(v, w) \\mapsto v + w\n\\]\nExample: \\((1,2) + (3,4) = (4,6)\\), which is also in \\(V\\).\nA scalar multiplication is a map:\n\\[\nF \\times V \\to V\n\\]\nFor example, \\((\\lambda, v) \\mapsto \\lambda v\\).\nAn \\(F\\)-vector space is a set \\(V\\) with an addition \\(+\\) and scalar multiplication \\(\\cdot\\) by elements of \\(F\\), such that \\((V, +, \\cdot)\\) satisfies the vector space axioms 1-8, where \\(\\mathbb{R}\\) is replaced by \\(F\\), and \\(\\mathbb{R}^n\\) is replaced by \\(V\\).\n\n\n\nRemarks and Examples:\n\nVectors: Vectors are elements of \\(V\\), denoted as \\(v \\in V\\).\nField \\(F\\): The choice of \\(F\\) matters! For example, we will see later that \\(\\mathbb{C}^n\\) is a complex vector space of dimension \\(n\\), but is also a real vector space of dimension \\(2n\\)."
  },
  {
    "objectID": "mth107.html#prove",
    "href": "mth107.html#prove",
    "title": "MTH107—NOTEs",
    "section": "？prove？",
    "text": "？prove？\nabove field F\n107’s learning need of C\n2n proof???\n\nExamples:\nFor any field \\(F\\),\n\nTrivial Vector Space: the set \\(V = \\{0\\}\\) is a trivial \\(F\\)-vector space with addition \\(0 + 0 = 0\\) and scalar multiplication \\(\\lambda \\cdot 0 = 0\\).\nFinite-Dimensional Vector Space: \\(F^n\\) is an \\(F\\)-vector space, and \\(F^0 = \\{0\\}\\).\nInfinite-Dimensional Vector Space: Let \\(F^\\infty\\) be the space of infinite sequences, where:\n\n\\[\nF^\\infty = \\{ (x_1, x_2, \\dots) \\mid x_i \\in F, \\, i = 1, 2, \\dots \\}\n\\]\nAddition and scalar multiplication are defined component-wise:\n\\[\n(x_1, x_2, \\dots) + (y_1, y_2, \\dots) = (x_1 + y_1, x_2 + y_2, \\dots)\n\\]\n\nFunction Space: Let \\(S\\) be a set, then:\n\n\\[\nF^S = \\{ f: S \\to F \\, \\text{(maps from S to F)} \\}\n\\]\nAddition and scalar multiplication are defined pointwise. For functions \\(f, g \\in F^S\\) and \\(\\lambda \\in F\\):\n\\[\n(f + g)(x) = f(x) + g(x) \\quad \\forall x \\in S\n\\]\n\\[\n(\\lambda \\cdot f)(x) = \\lambda \\cdot f(x) \\quad \\forall x \\in S\n\\]\n\\(F^S\\) is an F-vector space"
  },
  {
    "objectID": "mth107.html#section-3",
    "href": "mth107.html#section-3",
    "title": "MTH107—NOTEs",
    "section": "？",
    "text": "？\n二维映射到三维的linear transformation’s geometric meaning"
  },
  {
    "objectID": "mth107.html#a",
    "href": "mth107.html#a",
    "title": "MTH107—NOTEs",
    "section": "a",
    "text": "a\n\nclaim\n\\(F^S\\) is a vector space\nProof: we need to check the 8 axioms from the defination\n\nf + g = g+f?\n\n2 fun are equal if and only if they have values agrees on every \\(s \\in S\\)\n\\[\n(f+g)(x)=f(x)+g(x)\n\\] which is \\(\\in F\\)\n…..\n\n\ngeneral propeties of vector space\nLet V be a vector space over F\nWe will prove some properties of V using only the defination (axioms 1-8)\n\nProposition\n\nThe zero(additive identity) is unique. That is: \\(\\exists ! 0\\in V s.t. 0+V=V, \\forall v\\in V\\)\n\nproof: Suppose we have 2 zero elements: 0 and 0’\n0=0’+0=0+0’=0’\n\n\\(\\forall v\\in V\\) there exists a unique additive inverse\n\nSupppose w and w’ are 2 inverses for v, w=0+w=(v+w’)+w=v+(w’+w)=v+(w+w’)=(v+w)+w’=0+w’=w’\n\n\\(\\forall v\\in V\\), \\(O_F\\cdot V=O_V\\)\n\nproof:\n\n\\(\\forall x\\in F: x\\cdot O_V=O_F\\)\n\nproof: \\(x\\cdot\\)\n\n\nreminder of computing inverse of a matrix\nmethod1:\ndet(A)\n每一个位置的det构成的矩阵：B\ncofactor matrix:\\((-1)^{n+m}\\)\nC=B\\(\\times\\) cofactor matrix\n\\(A^{-1}=1/det(A)\\) times \\(C^T\\)\nmethod2: work for the tansformation of a matrix"
  },
  {
    "objectID": "sequence.html",
    "href": "sequence.html",
    "title": "(sequence)sinx/x’s 0-infinite’s integral",
    "section": "",
    "text": "Dr.Fajin Wei teach me how to do it. He is very good at Calculus and Modeling. He is kind to help our students. Thanks to him very much!\n\nintegral problems with solution of alternating integral test\nquestion: does \\[\\int_{0}^{\\infty}sinx/x\\,dx\\] converge?\nIt is easy to think of these 2 famous but difficultly proved formula: \\[\\int_{-\\infty}^{\\infty}sinx/x\\,dx=\\pi\n\\] \\[\\int_{-\\infty}^{\\infty}e^{-x^2}\\,dx=\\pi\n\\] However, they are not useful, which is how charasmatic the math is! I love math!\n\\[=\\Sigma_{n=1}^{\\infty}\\int_{2(n-1)\\pi}^{2n\\pi}sinx/x\\,dx\n\\] Then we let \\(x-[2(n-1)\\pi]=y\\)(dx=dy)\nso \\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{2\\pi}siny/(y+[2(n-1)\\pi])\\,dy\\] =\\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}siny/(y+[2(n-1)\\pi])\\,dy+\\Sigma_{n=1}^{\\infty}\\int_{\\pi}^{2\\pi}siny/(y+[2(n-1)\\pi])\\,dy\\]\nThen we let y-\\(\\pi\\)=z(dy=dz)\n\\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}siny/(y+[2(n-1)\\pi])\\,dy+\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}-sinz/(z+[(2n-1)\\pi])\\,dz\\] \\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}sinm/(m+[2(n-1)\\pi])\\,dm+\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}-sinm/(m+[(2n-1)\\pi])\\,dm\\]\nso if \\[a_n=\\int_{0}^{\\pi}sinm/(m+(n-1)\\pi)\\] \\[\\int_{0}^{\\infty}sinx/x\\,dx=a_1-a_2+a_3-a_4+...+a_n\\], which is an alternating series!\nCoincidently, it is decresing and \\(a_n&lt;\\int_{0}^{\\pi}1/(n-1)\\pi\\,dm=1/(n-1)\\), and \\(\\lim_{{n \\to \\infty}} \\left( \\frac{1}{n-1}  \\right) = 0\\) so \\(\\lim_{{n \\to \\infty}}a_n=0\\) So it converges(Alternating series test).\n\n\nsimilar question 2\nThanks for Dr.Chi-Kun Lin to teach me this kind of problems!\n\\[\n\\int_0^\\infty \\frac{1}{1 + x^p \\sin^2 x} \\, dx\n\\]\n\n\n\n\\[\n\\sum_{n=0}^\\infty \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n + \\frac{1}{2} \\right)^2 p \\sin^2 t} \\, dt = \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n + \\frac{1}{2} \\right)^2 p \\sin^2 t} \\, dt\n\\]\n\\[\\Sigma\\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n\\pi/2 + t \\right)^ p \\sin^2 t} \\, dt + \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left(( n + 1 \\right)\\pi/2-t)^ p sin^2t} \\, dt\\]\nfor:\n\\[\nx_n = \\Sigma\\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n\\pi/2 + t \\right)^ p \\sin^2 t} \\, dt + \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left(( n + 1 \\right)\\pi/2-t)^ p sin^2t} \\, dt,\n\\]\nthere has\n\\[2 \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right)^ p t^2} \\, dt \\leq x_n \\leq 2 \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right) ^p \\frac{4}{\\pi^2} t^2} \\, dt\\]\n\nHowever, the integrals on both sides of the inequality can be calculated separately. For example:\n\n\\[\n\\int_0^1 \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right)^ p t^2} \\, dt = \\frac{1}{\\sqrt{((n + 1) \\frac{\\pi}{2} )^p}} \\arctan \\left( \\sqrt{(n + 1) \\frac{\\pi}{2}}^{2p}\\pi/2 \\right) \\geq \\frac{1}{\\sqrt{(n + 1) \\frac{\\pi}{2} }^{2p}  } \\frac{\\pi}{4}\n\\]"
  },
  {
    "objectID": "mth1113.html",
    "href": "mth1113.html",
    "title": "Intro to probability and statistics",
    "section": "",
    "text": "dotplot没有纵轴in r(vs scallar plot)\nprice of a textbook is discrete\nzip code is categorical\ndotplot and scallar plot\ndo not manipulate data—experimental\ntable 2.1\n11\ncluster/stratified\nstratified: according to certain carecteristic\ncluster: randomly groups\nhuge data for 顺序的 shuffle is ok\nbut ..\nthe likelihood is totally different"
  },
  {
    "objectID": "mth1113.html#populations-and-samples",
    "href": "mth1113.html#populations-and-samples",
    "title": "Intro to probability and statistics",
    "section": "populations and samples",
    "text": "populations and samples\npopulation: The entire collection of individuals or objects about which information is desired\nsample: A sample is a subset of the population, selected for study.\nthen select the sample\nthen we could summarize it using 2 branches of stat.— Decriptive stat.(methods for organizing and summarizing data.) or inferential stat.(generalizing from a sample(incomplete information) to the population from which the sample was selected and assessing the reliability of such generalizations.So we run the risk(An important aspect of statistics and making statistics inferences involves quantifying the chance of making an incorrect conclusions.))\n\ndescriptive stat\n\n\ninferential stat\nsample"
  },
  {
    "objectID": "mth1113.html#bias",
    "href": "mth1113.html#bias",
    "title": "Intro to probability and statistics",
    "section": "bias",
    "text": "bias\nselection bias\nmeasurement or response bias"
  },
  {
    "objectID": "mth1113.html#random-sampling",
    "href": "mth1113.html#random-sampling",
    "title": "Intro to probability and statistics",
    "section": "random sampling",
    "text": "random sampling\nthe same chance to be selected"
  },
  {
    "objectID": "mth1113.html#strata-and-cluster",
    "href": "mth1113.html#strata-and-cluster",
    "title": "Intro to probability and statistics",
    "section": "strata and cluster",
    "text": "strata and cluster\ncluster reflect general characteristic about"
  },
  {
    "objectID": "Steinmetz.html",
    "href": "Steinmetz.html",
    "title": "surface and volume of Steinmetz solid",
    "section": "",
    "text": "Professor. Wang Duo, a very charismatic and responsible Calculus teacher who has worked for our country in the area of math for at least 50 years (which was his target before and he has realized his dream!), always says “You should try to solve one problem many times with new methods to handle and understand knowledge better”\nFor math learning, focusing too much on exams is not so meaningful. We should experience its beauty from the bottom of our hearts. I did not reach my target in the Multivariable Calculus and the Linear Algebra models, but Dr. Bohuan Lin said:“Exams (especially written exams) require one to figure out solutions within a very short time period, and moreover, under an intense atmosphere. From my point of view, it is really not a problem if one fails to solve those difficult and nonstandard problems under such a condition. Of course, for math study (so as for the study of other things), we should not be satisfied with merely being able to solve”standard problems”, but just try to challenge and improve yourself with deep/difficult questions under a daily condition with a natural mood, since this is the common situation in which you will be working on various tasks in your future career. ”\nThere are many ways to solve problems, and all of them are interesting when multiple integrals occur."
  },
  {
    "objectID": "Steinmetz.html#cylinders",
    "href": "Steinmetz.html#cylinders",
    "title": "surface and volume of Steinmetz solid",
    "section": "2 cylinders",
    "text": "2 cylinders\nIt is easy\n\nsurface\n\n\nvolume\nuse method of sections directly\n\n\n3 cylinders\n\n\nsurface\n\nsee the photo attached, which is the easiest way for computation:\n\n\n\nvolume\n\nmethod 1—do not draw it(using conditional inferences)\nThanks to Dr.Bohuan Lin to teach me such a clever way which we do not need to draw the picture(only use conditional equations to solve it is interesting and a little difficult to handle it correctly).\nThe picture behind the method is attached to “Steinmetz(mou he fang gai)’s photo behind.png”\nWe have the set ( D ) defined as: \\[\nD \\triangleq \\left\\{(x, y, z) \\mid \\begin{cases}\nx^2 + y^2 \\leq 4 \\\\\ny^2 + z^2 \\leq 4 \\\\\nz^2 + x^2 \\leq 4\n\\end{cases} \\right\\}\n\\]\nFrom \\[x^2+ y^2 \\leq 4 , x^2 + z^2 \\leq 4 \\] we derive: if \\[\n|x| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |z| \\leq \\sqrt{2}\n\\] (and &lt; is the same shape)\nSimilarly:\nif \\[|z| \\geq \\sqrt{2} ,|x| \\leq \\sqrt{2} \\quad \\text{then} \\quad |y| \\leq \\sqrt{2} \\]\nAnd: if \\[\n|z| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |x| \\leq \\sqrt{2}\n\\] Therefore: \\[\nD = D_{\\leq \\sqrt{2}} \\cup \\bar{D}\n\\]\nWhere: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\] \\[\n\\begin{aligned}\n&= \\left\\{ (x, y, z) \\in \\mathbb{R}^3 \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\} \\\\\n&= [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}]\n\\end{aligned}\n\\]\nAnd: \\[\n\\bar{D} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus: \\[\n\\bar{D} = \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}}\n\\]\n(This also implies: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\])\nWe derive: \\[\n\\bar{D} = \\left\\{ (x, y, z) \\mid |x| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |y| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus, combining all components, we get: \\[\nD = \\left( D_{\\leq \\sqrt{2}} \\cup \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}} \\right)\n\\] So we could decompose it into a cube in the center and 6 common volume. the 6 volume: 6 \\(\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\nIn summary, the whole volume is \\((2 \\sqrt 2)^3 +6\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\n\n\n\nmethod 2—cross-section method\nThanks to Dr. Haoran Chen for teaching us such method.\nWe suppose z&gt;x, z&gt;y,（then *3） then we could continue decompose it into (1)z~(\\(\\sqrt 2\\),2) and (2)z~(\\(0,\\)$$)(based on whether the square is out of the circle)\n\n\n\nmethod 3\nsee the photo attached"
  },
  {
    "objectID": "eng pde.html",
    "href": "eng pde.html",
    "title": "A Kun’s PDE Lecture",
    "section": "",
    "text": "the most important thing in geometry is about measure.\n\n\n\nsimplize authority独断权威\n\n\n\ndemocracy\nNewton\nanatomy\nf(x)约等于 \\(\\Sigma a_nx^n\\)—-Talor series\nbe patient to learn analysis"
  },
  {
    "objectID": "eng pde.html#reformation",
    "href": "eng pde.html#reformation",
    "title": "A Kun’s PDE Lecture",
    "section": "Reformation",
    "text": "Reformation"
  },
  {
    "objectID": "eng pde.html#renaissance",
    "href": "eng pde.html#renaissance",
    "title": "A Kun’s PDE Lecture",
    "section": "Renaissance",
    "text": "Renaissance"
  },
  {
    "objectID": "eng pde.html#enlightenment",
    "href": "eng pde.html#enlightenment",
    "title": "A Kun’s PDE Lecture",
    "section": "Enlightenment",
    "text": "Enlightenment"
  },
  {
    "objectID": "eng pde.html#industrial-revolution",
    "href": "eng pde.html#industrial-revolution",
    "title": "A Kun’s PDE Lecture",
    "section": "Industrial Revolution",
    "text": "Industrial Revolution\nNo dynasty lasts over 300 years, except Song dynasty which lasted over 300 years, with scholars serving as prime ministers. Su Shi snored."
  },
  {
    "objectID": "eng pde.html#wave-equation",
    "href": "eng pde.html#wave-equation",
    "title": "A Kun’s PDE Lecture",
    "section": "Wave Equation",
    "text": "Wave Equation\nVibration of a string.\nTaylor, d’Alembert, Daniel Bernoulli, Jacob Bernoulli (Euler), Jacob Bernoulli (distribution).\n\\[\n\\frac{d^2x}{dt^2}\n\\]\nThe truths of this world have all been discovered by Newton.\n\\[\nu(x,t) \\text{: amplitude, } [u] = L\n\\]\n$$ [p] = M/L\n$$\n\\[\np(x,t) \\text{: density (1-dimensional)}\n\\]\n[T] = [ma] = [m][a] = ML/t^2\n\\(T=T_1=T_2\\) tension\n(0-L)\nF = Tsin() - Tsin\nm = \\(\\rho\\) x\n2nd law: \\(\\rho\\) x"
  },
  {
    "objectID": "eng pde.html#linear-transformation",
    "href": "eng pde.html#linear-transformation",
    "title": "A Kun’s PDE Lecture",
    "section": "linear transformation",
    "text": "linear transformation\nKeep the parallelogram diagonal 保持平行四边形的对角线 x+y\nkeep a straight line 保持直线—a\n向量空间 vector space\n如果是从二维映射到三维怎么办。2dim—3dim？\n代数结构\n保留加法 keep the addition\nhomomorphism\nmorphism–形象"
  },
  {
    "objectID": "eng pde.html#da..-bernulli-separation-of-variables",
    "href": "eng pde.html#da..-bernulli-separation-of-variables",
    "title": "A Kun’s PDE Lecture",
    "section": "Da.. Bernulli Separation of Variables",
    "text": "Da.. Bernulli Separation of Variables\n\\(u(x,\\theta)=\\phi(x)T(t)\\). (Gassian integral formula derivation also use this method)\n(A continuous function can be approximated by a polynomial, which is a super polynomial)\n(the defination of “density”)\n(Seperation variable: God to God, Caesar to Caesar—-x to x, t to t)\n分离变数\n\\[\n\\frac{T^{''}(t)}{C^2 T(t)} = -\\frac{\\phi^{''}(x)}{\\phi(x)}\n\\]\n\\[\n\\begin{aligned}\n&\\text{PDE:} \\quad \\text{分离变量法,separete variables} \\rightarrow \\text{ODE} \\\\\n&\\phi'' + \\lambda \\phi = 0 \\\\\n&T'' + \\lambda c^2 T = 0 \\\\\n&\\text{(B.C.) 边界条件：} \\\\\n&u(0,t) = \\phi(0) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(0) = 0 \\quad \\text{(trivial 非平凡解,boring)} \\\\\n&u(L,t) = \\phi(L) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(L) = 0\n\\end{aligned}\n\\]\n\\[\n\\lambda &lt; 0 \\quad , \\quad \\phi = e^{mx} \\quad \\text{(不可能，从0到0。0--0， impossible)}\n\\]\n\\[\n\\lambda = 0 \\quad , \\quad \\phi = Ax + B \\quad \\text{直线运动， straight move}\n\\]\n\\[\n\\lambda &gt; 0 \\quad , \\quad \\phi = c_1 \\cos(\\sqrt{\\lambda} x) + c_2 \\sin(\\sqrt{\\lambda} x)\n\\] ### 解的推导\n考虑方程： \\[\n\\phi'' + \\lambda \\phi = 0\n\\] 其中 () 是一个常数，解的形式取决于 () 的值。\n\n1. 当 (&lt; 0)\n当 (&lt; 0) 时，我们令 (= -^2) ，于是方程变为： \\[\n\\phi'' - \\mu^2 \\phi = 0\n\\] 其通解为： \\[\n\\phi(x) = A e^{\\mu x} + B e^{-\\mu x}\n\\] 这种解形式表示指数发散或衰减，通常是不稳定解。\n\n\n2. 当 (= 0)\n当 (= 0) 时，方程变为： \\[\n\\phi'' = 0\n\\] 这是一个线性方程，其通解为： \\[\n\\phi(x) = A x + B\n\\] 这表示直线运动。\n\n\n3. 当 (&gt; 0)\n当 (&gt; 0) 时，我们令 (= ^2) ，方程变为： \\[\n\\phi'' + \\mu^2 \\phi = 0\n\\] 其解为： \\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x) ---wave\n\\] 这种解表示周期性波动。\n\n推导\n对于形如 ( \\(\\phi'' + \\mu^2 \\phi = 0\\) ) 的常微分方程，我们可以猜测它的解是指数形式，即：\n\\[\n\\phi(x) = e^{rx}\n\\]\n这里 ( r ) 是需要确定的参数。\n\n\n\n代入微分方程：\n将 ( (x) = e^{rx} ) 代入原方程，得到：\n\\[\nr^2 e^{rx} + \\mu^2 e^{rx} = 0\n\\]\n由于 ( e^{rx} )，可以消掉这个项，剩下的是特征方程：\n\\[\nr^2 + \\mu^2 = 0\n\\] 这个特征方程的解为：\n\\[\nr = \\pm i \\mu\n\\]\n这是一个虚数解。\n当特征根为纯虚数时，方程的通解可以写成正弦和余弦的组合形式，根据欧拉公式 ( e^{ix} = (x) + i (x) )，我们得到通解为：\n\\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x)\n\\]\n其中，( A ) 和 ( B ) 是待定常数，它们可以通过边界条件或初始条件来确定。\ncharacteristic\n\\[\n\\phi(x) = C_1 \\cos(\\sqrt{\\lambda}x) + C_2 \\sin(\\sqrt{\\lambda}x)\n\\]\n边界条件： \\[\n\\phi(0) = 0 \\Rightarrow C_1 = 0\n\\]\n\\[\n\\phi(L) = C_2 \\sin(\\sqrt{\\lambda}L) = 0\n\\]\n因此， \\[\n\\sqrt{\\lambda_n}L = n\\pi \\quad (n = 1, 2, 3, \\dots)\n\\]\n得到本征值： \\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n对应的本征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] Sturm-Liouville Problem:\n\\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n因此，特征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] \\[\nT_n(t) = C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right)\n\\]\n\\[\nu_n(x,t) = T_n(t) \\phi_n(x)\n\\]\n代入展开： \\[\nu(x,t) = \\sum_{n=1}^{\\infty} C_n \\sin \\left( \\frac{n\\pi x}{L} \\right) \\left[ C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right) \\right]\n\\]"
  },
  {
    "objectID": "eng pde.html#law-of-large-number",
    "href": "eng pde.html#law-of-large-number",
    "title": "A Kun’s PDE Lecture",
    "section": "law of large number",
    "text": "law of large number\neg. The reason the casino makes money is because even though the people who go there have a little more than a half chance of winning, the casino has more money than you, and if you stay long enough, you will win, so don’t go to the casino, because you can’t go deeper than the casino"
  },
  {
    "objectID": "Mathematical-Analysis/Mathematical Analysis 1.html",
    "href": "Mathematical-Analysis/Mathematical Analysis 1.html",
    "title": "Analysis1",
    "section": "",
    "text": "a problem about divisible using greatest common factor to solve(even Bezou’s theorem)\nQ:“Proof that if a positive integer \\(p\\) is not a perfect square, then \\(\\sqrt{p}\\) is irrational.”\nSol: Proof by Contradiction:\nAssume \\(\\sqrt{p}\\) is rational.\nSince \\(p\\) is not a perfect square, there exist two coprime positive integers \\(m\\) and \\(n\\) with \\(n &gt; 1\\) such that \\[\n\\sqrt{p} = \\frac{m}{n}\n\\]\nThen \\[\np = \\frac{m^2}{n^2}\n\\]\nwhich implies \\[\nm^2 = n^2 p\n\\] i.e. \\(n^2 \\mid m^2\\). Then We want to show that \\(n \\mid m\\):\nSuppose \\(m^2 = k n^2\\), where \\(k \\in \\mathbb{Z}\\).\nSince \\(m\\) and \\(n\\) are both integers, based on prime factorization, we have:\n\\[\nm = p_1^{a_1} p_2^{a_2} \\cdots p_k^{a_k}\n\\]\n\\[\nn = p_1^{b_1} p_2^{b_2} \\cdots p_k^{b_k}\n\\]\nwhere \\(p_i\\) are prime numbers, and \\(a_i, b_i\\) are non-negative integers.\nSo,\n\\[\nm^2 = p_1^{2a_1} p_2^{2a_2} \\cdots p_k^{2a_k}\n\\]\nand\n\\[\nn^2 = p_1^{2b_1} p_2^{2b_2} \\cdots p_k^{2b_k}\n\\]\nSince \\(m^2 = k n^2\\), we have:\n\\[\np_1^{2a_1} p_2^{2a_2} \\cdots p_k^{2a_k} = k \\cdot p_1^{2b_1} p_2^{2b_2} \\cdots p_k^{2b_k}\n\\]\nThis implies:\n\\[\n2a_i \\geq 2b_i \\quad \\text{for all } i\n\\]\nSo,\n\\[\na_i \\geq b_i \\quad \\text{for all } i\n\\]\nThus, \\(m\\) is divisible by \\(n\\). Therefore, \\(n \\mid m\\), which is opposite to m and n are coprime. So, \\(\\sqrt{p}\\) is irrational.\nmethod 2: Proof by Contradiction:\nAssume \\(\\sqrt{p}\\) is rational.\nSince \\(p\\) is not a perfect square, there exist two coprime positive integers \\(m\\) and \\(n\\) with \\(n &gt; 1\\) such that \\[\n\\sqrt{p} = \\frac{m}{n}\n\\]\nThen \\[\np = \\frac{m^2}{n^2}\n\\]\nwhich implies \\[\nm^2 = n^2 p\n\\] i.e. \\(n^2 \\mid m^2\\). So \\(n \\mid m^2\\)\nSince \\(n &gt; 1\\), it follows that there exists a prime number \\(r\\) such that \\(r \\mid n\\). (proof using Fundamental Theorem of Arithmetic:\nEvery integer greater than 1 is either a prime or can be uniquely factored into prime numbers.\nIf \\(n\\) is a prime number, then \\(r = n\\) and clearly \\(r \\mid n\\);\nIf \\(n\\) is not a prime number, it must be decomposable into a product of prime factors. Therefore, we can write: \\[\nn = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k}\n\\]\nwhere \\(p_i\\) are prime numbers and \\(e_i\\) are positive integers.\nSince \\(n\\) is a product of prime factors, at least one of these prime factors \\(p_i\\) must divide \\(n\\).\nLet \\(r = p_i\\), which is one of the prime factors. Then \\(r \\mid n\\).\nThus, in both cases, whether \\(n\\) is a prime or not, there exists at least one prime number \\(r\\) such that \\(r \\mid n\\).)\nThus \\[\nr \\mid m^2 \\quad \\text{and} \\quad r \\mid m\n\\] (proof of \\[\nr \\mid m^2 \\quad \\text{and} \\quad r \\mid m\n\\]: proof1 using prime factorization as method 1’s:\n\\(r \\mid m^2\\) and r is a prime so \\(r=p_i\\) corresponding to the exponential of \\(2a_i\\)\nsince \\(a_i \\geq 0\\), so \\(a_i\\) is at least 1 . proof2 using if r is a prime and r|ab, then r|a or r|b:\nSuppose \\(r \\nmid a\\) and \\(r \\nmid b\\) (contradiction assumption).\nLet \\(\\gcd(a \\cdot b, r) = d\\) (where \\(d\\) is the greatest common divisor of \\(a \\cdot b\\) and \\(r\\)).\nSince \\(d \\mid r\\), \\(d\\) is a divisor of \\(r\\) (since \\(r\\) is a prime number).\nTherefore, \\(d\\) is either \\(r\\) or \\(1\\) (since \\(r\\) is prime).\nif \\(d = r\\), we have \\(\\gcd(a \\cdot b, r) = r\\). This implies \\(r \\mid a \\cdot b\\).\nSince we assumed \\(r \\nmid a\\) and \\(r \\nmid b\\), it contradicts our initial statement based on Properties of greatest common divisor with its proof using Bezou’s theorem;\nIf \\(d = 1\\), we have \\(\\gcd(a \\cdot b, r) = 1\\). This implies \\(r \\nmid a \\cdot b\\), which contradicts our assumption that \\(r \\mid a \\cdot b\\).\nThus, the contradiction shows that our assumption \\(r \\nmid a\\) and \\(r \\nmid b\\) must be false. Therefore, \\(r\\) must divide at least one of \\(a\\) or \\(b\\).\n)\nSince \\(m\\) and \\(n\\) are coprime, this leads to a contradiction.\nTherefore, \\(\\sqrt{p}\\) must be an irrational number."
  },
  {
    "objectID": "mth107.html#section-4",
    "href": "mth107.html#section-4",
    "title": "MTH107—NOTEs",
    "section": "？",
    "text": "？\n二维映射到三维的linear transformation"
  },
  {
    "objectID": "mth107.html#elements-of-f1-2-3-dots-n",
    "href": "mth107.html#elements-of-f1-2-3-dots-n",
    "title": "MTH107—NOTEs",
    "section": "1. Elements of \\(F^{\\{1, 2, 3, \\dots, n\\}}\\)",
    "text": "1. Elements of \\(F^{\\{1, 2, 3, \\dots, n\\}}\\)\nBy definition, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is the set of all functions from \\(\\{1, 2, 3, \\dots, n\\}\\) to \\(F\\). Each function \\(f\\) can be written as:\n\\[\nf = (f(1), f(2), \\dots, f(n)).\n\\]"
  },
  {
    "objectID": "mth107.html#elements-of-fn",
    "href": "mth107.html#elements-of-fn",
    "title": "MTH107—NOTEs",
    "section": "2. Elements of \\(F^n\\)",
    "text": "2. Elements of \\(F^n\\)\n\\(F^n\\) is the set of ordered \\(n\\)-tuples \\((a_1, a_2, \\dots, a_n)\\), where each \\(a_i \\in F\\)."
  },
  {
    "objectID": "mth107.html#constructing-a-bijection",
    "href": "mth107.html#constructing-a-bijection",
    "title": "MTH107—NOTEs",
    "section": "3. Constructing a Bijection",
    "text": "3. Constructing a Bijection\n\nMapping from \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) to \\(F^n\\)\nFor any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), we define the corresponding tuple in \\(F^n\\) as:\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\n\n\nMapping from \\(F^n\\) to \\(F^{\\{1, 2, 3, \\dots, n\\}}\\)\nFor any tuple \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) as:\n\\[\nT^{-1}(a_1, a_2, \\dots, a_n)(i) = a_i, \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]"
  },
  {
    "objectID": "mth107.html#prove-that-the-mapping-is-a-bijection",
    "href": "mth107.html#prove-that-the-mapping-is-a-bijection",
    "title": "MTH107—NOTEs",
    "section": "4. Prove that the Mapping is a Bijection",
    "text": "4. Prove that the Mapping is a Bijection\n\n(a) \\(T^{-1}(T(f)) = f\\)\nLet \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\). Applying \\(T\\) gives the tuple:\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nNow, apply \\(T^{-1}\\) to this tuple:\n\\[\nT^{-1}(f(1), f(2), \\dots, f(n))(i) = f(i), \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nThus,\n\\[\nT^{-1}(T(f)) = f.\n\\]\n\n\n(b) \\(T(T^{-1}(a_1, a_2, \\dots, a_n)) = (a_1, a_2, \\dots, a_n)\\)\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\). Applying \\(T^{-1}\\), we get the function \\(f\\) such that \\(f(i) = a_i\\) for each \\(i\\). Now, applying \\(T\\) to this function:\n\\[\nT(T^{-1}(a_1, a_2, \\dots, a_n)) = (a_1, a_2, \\dots, a_n).\n\\]\nThus,\n\\[\nT \\circ T^{-1} = \\text{id}_{F^n}.\n\\]"
  },
  {
    "objectID": "mth107.html#conclusion",
    "href": "mth107.html#conclusion",
    "title": "MTH107—NOTEs",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nSince \\(T\\) and \\(T^{-1}\\) are inverses of each other, we have established a bijection between \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\). Therefore, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\) are equivalent as sets and vector spaces."
  },
  {
    "objectID": "eng pde.html#cotinuous-proof",
    "href": "eng pde.html#cotinuous-proof",
    "title": "A Kun’s PDE Lecture",
    "section": "cotinuous proof",
    "text": "cotinuous proof\nf is continuous at \\(x_0\\)\nis equal to\nFor every \\(\\varepsilon &gt; 0\\), there exists \\(\\delta &gt; 0\\) such that if \\(|x - x_0| &lt; \\delta\\), then \\(|f(x) - f(x_0)| &lt; \\varepsilon\\).\n\\(\\forall \\epithlong g\\)"
  },
  {
    "objectID": "eng pde.html#geometry",
    "href": "eng pde.html#geometry",
    "title": "A Kun’s PDE Lecture",
    "section": "",
    "text": "the most important thing in geometry is about measure."
  },
  {
    "objectID": "eng pde.html#algebra",
    "href": "eng pde.html#algebra",
    "title": "A Kun’s PDE Lecture",
    "section": "",
    "text": "simplize authority独断权威"
  },
  {
    "objectID": "eng pde.html#analysis",
    "href": "eng pde.html#analysis",
    "title": "A Kun’s PDE Lecture",
    "section": "",
    "text": "democracy\nNewton\nanatomy\nf(x)约等于 \\(\\Sigma a_nx^n\\)—-Talor series\nbe patient to learn analysis"
  },
  {
    "objectID": "eng pde.html#does-math-have-elements",
    "href": "eng pde.html#does-math-have-elements",
    "title": "A Kun’s PDE Lecture",
    "section": "Does math have elements?",
    "text": "Does math have elements?\n             ---Paul Halmos\n             Geometric Series\n\n1-ab and 1-ba —- invertible\nI-AB invertible\n— I-BA invertible (hint: \\(I+B(I-AB)^{-1}A\\))\ndo not 杀鸡用牛刀—-\nprove the harmonic series is divergent:\nidea: geometric series:\n1+1/2+(1/3+1/4)+….\n1+(1/2+1/3)+(1/4+..+1/9)+(1/10+…+1/27)+…\\(\\geq\\) 1+(1/3)\nwhich is relevant to fuliyejishu\n学数学不要兵来将挡水来土掩，要有一个整套的思想，不要背书 idea: geometric series\n(1-ba){-1}=1/1-ba=1+ba+baba+..=1+b(1+ab+abab+..)a=1+b(1-ab){-1}a\n(1-ba)[1+b(1-ab)^{-1}a]\n1+b(1-ab){-1}a-ba-bab(1-ab){-1}a=(commutive law)=1-ba+b(1-ab){-1}a-bab(1-ab){-1}a=1-ba+b(1-ab)(1-ab)^{-1}a\n=1-ba+b1a\n=1-ba+ba\n=1 —-so do not recite boring things, remember based on understanding.\nDo not read Gassian, read Oral\nfirstly, for matrix, AB \\(\\noequal\\) BA—-f(g) is not equal to g(f)\n学学问要有感觉\nchat with different famous … 西方的没落–fu springer\nliangzilixue hysenber—max born\nthe true meaning of matrix is the linear transformation(fun f)"
  },
  {
    "objectID": "eng pde.html#p-series",
    "href": "eng pde.html#p-series",
    "title": "A Kun’s PDE Lecture",
    "section": "p-series",
    "text": "p-series\nIntegral test with dimensional analysis to know p&gt;1 for convergence\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}  \\approx  \\int_1^\\infty 1/x^p \\, dx\\approx 1/[x]^p*[x]=[x]^{1-p} ([x]--&gt;\\infty)---&gt;1-p\\leq 0\\)\nagain, geometric series,\n1/12+(1/22+1/32)+(1/42+..)\n\\(\\leq 1+(1/2^2+1/2^2)+\\)\n= 1+2/2^2 +4/42+8/82z=…=2\n\nEuler\nEuler idea: Viete fumula\nrelation of root and coefficient\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}=\\pi^2/6\\)\n(x-a)(x-b)=x^2-(a+b)x+ab\n(1-x/a)(1-x/b)=1-(1/a+1/b)x+1/ab*x^2\n(1-x)(1+x)\n(1-x)(1+x)(1-x/2)(1+x/2)\n(1-x)(1+x)(1-x/2)(1+x/2)…(1-x/n)(1+x/n)+…\n=1-(1/12+1/22+…+1/n2)x2+…\nQ: find a function whose roots are +-1, +-2, +-3,….\nso Euler changed this Q:\nrewrite:\n(1-x/)(1+x/)(1-x/2)(1+x/2)…\n= 1-(1/12+1/22+1/n2+…)1/2x^2+…\n先猜答案\nguess sinx = (1-x/)(1+x/)…(1-x/n)(1+x/n)\nbut 0 is a solution\nso change to sinx/x\nsinx/x =k…..\nk=limsinx/x=1\n1/x is 振幅\nsinx/x=1-(1/12+1/22+…+1/n2+…)x2/^2+…\n=1/x\n天才是创意\ncreate a thery of math\nand six/x = 1/x(x-x3/3!+…)=1-x3/6\nso we know"
  },
  {
    "objectID": "Mathematical Analysis 1.html#p101.4",
    "href": "Mathematical Analysis 1.html#p101.4",
    "title": "Analysis1",
    "section": "P10,1.4",
    "text": "P10,1.4\nEquivalence of statements: Two statements are logically equivalent, if they have the same truth values in all possible situations.\n\\(A\\equiv B\\)\n‘abstract non-sense’\nThem(De Morgan’s laws)\nA,B\n\\(\\sim (A \\land B) \\equiv (\\sim A) \\lor (\\sim B)\\)\n\\(\\sim (A \\lor B) \\equiv (\\sim A) \\land (\\sim B)\\)\ndraw truth table to look at values\n交的话（and），都T才T；并的话（or），1T则T\nConditional:\nIf p(hypothesis/assumption/condition), then q(conclusion/consequence/result).\nread: p implies q/assume p, then q.\nkey point: the implication is False when the rule is broken\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\nA & B & A \\rightarrow B \\\\\n\\hline\nT & T & T \\\\\nT & F & F \\\\\nF & T & T \\\\\nF & F & T \\\\\n\\hline\n\\end{array}\n\\] Def: p, q,p–&gt;q\n1)Converce: q—&gt;p,i.e. if q, then p\n2)Contrapositive: (q)–&gt;(p), i.e. if not q, then not p\nProp: (p–&gt;q)\\(\\equiv\\) ((q)–&gt;(p)) \\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\np & q & \\sim p & \\sim q & p \\rightarrow q & \\sim q \\rightarrow \\sim p & (p \\rightarrow q) \\equiv (\\sim q \\rightarrow \\sim p) \\\\\n\\hline\nT & T & F & F & T & T & T \\\\\nT & F & F & T & F & F & T \\\\\nF & T & T & F & T & T & T \\\\\nF & F & T & T & T & T & T \\\\\n\\hline\n\\end{array}\n\\]\nProof by contradiction: we want to prove p—&gt;q, we prove (q)—-&gt;(p) instead\nproof: Assume ~q, if , then if , then … –&gt; ~p, which is contradict to the original assumption p\n   Hence the assumption ~q is false, i.e. q is true.#\n   \ne.g.: n is a natural number. Prove that if n^2 is divisible by 2(p), then n is divisible by 2(q).\nProof: Assume that n is not divisible by 2(~q),—&gt; n is odd(defination), i.e. n =2k+1,k\\(\\in Z\\)\n—&gt; n^2 =(2k+1)^2(multiplicaiton)=2()+1, which is odd\n—&gt; n^2 is not divisible by 2(~p),\nThus the aassumption that n is not divisible by 2 is false so n is divisible by 2.#\nbiconditional: p,q,\n(p–&gt;q)\\(\\land\\)(q—&gt;p)\nstatement converse \\[\n\\begin{array}{|c|c|c|c|c|}\n\\hline\np & q & p \\rightarrow q & q \\rightarrow p & p \\leftrightarrow q \\\\\n\\hline\nT & T & T & T & T \\\\\nT & F & F & T & F \\\\\nF & T & T & F & F \\\\\nF & F & T & T & T \\\\\n\\hline\n\\end{array}\n\\]\nThem: the only case p&lt;–&gt;q is true is then both p and q ture or false"
  },
  {
    "objectID": "mth1113.html#experiments-and-obeservational-study",
    "href": "mth1113.html#experiments-and-obeservational-study",
    "title": "Intro to probability and statistics",
    "section": "experiments and obeservational study",
    "text": "experiments and obeservational study"
  },
  {
    "objectID": "mth1113.html#systematic-sampling",
    "href": "mth1113.html#systematic-sampling",
    "title": "Intro to probability and statistics",
    "section": "systematic sampling",
    "text": "systematic sampling"
  },
  {
    "objectID": "mth107.html#relation-between-rn-and-.",
    "href": "mth107.html#relation-between-rn-and-.",
    "title": "MTH107—NOTEs",
    "section": "relation between rn and + .",
    "text": "relation between rn and + .\n\\((R^n,+,\\cdot)\\)\nit means that + and \\(\\cdot\\) satisfy the following axioms:\n\n\\(\\forall x,y \\in rn\\): x+y=y+x. commutativity\n\\(\\forall x,y,z \\in rn\\): (x+y)+z=x+(y+z). associativity\nx + 0 = 0 + x neutral element for addition\n\\(x + (-x) = (-x) + x = 0\\)\n\n——inverse for addition -x=y\n\n\\(1 \\cdot x = x\\) Identity Element for Scalar Multiplication\n\\((\\lambda \\mu) \\cdot x = \\lambda \\cdot (\\mu \\cdot x)\\) compatibilily of multiplication\n\\(\\lambda \\cdot (x + y) = \\lambda \\cdot x + \\lambda \\cdot y\\) Distributivity of Scalar Multiplication Over Vector Addition\n\\((\\lambda + \\mu) \\cdot x = \\lambda \\cdot x + \\mu \\cdot x\\) Distributivity of Scalar Addition\n\n\n?\nask again, sorry: since negetive number set satisfies 2 oprations but not 8 axioms I want to ensure if a finite set satisfies the 2 operations it is not satisfy all 1-8 axioms instead of F^n\nis that because of the defination of F-vector space?(abstract vector space-defination) ## Cn complex Vector Space\n\\(i^2 = -1\\)\nDefine \\(\\mathbb {C}^n\\) as the set of all ordered n-tuples of complex numbers: \\[\n\\mathbb{C}^n = \\{ (z_1, z_2, \\dots, z_n) \\mid z_i \\in \\mathbb{C}, \\, i = 1, 2, \\dots, n \\}\n\\]\n\n\nOperations on C^n\nWe define addition and scalar multiplication on \\(\\mathbb{C}^n\\) in the same way as we did on \\(\\mathbb{R}^n\\), but using complex numbers:\n\nAddition:\nFor two vectors \\((z_1, z_2, \\dots, z_n)\\) and \\((w_1, w_2, \\dots, w_n) \\in \\mathbb{C}^n\\):\n\\[\n(z_1, z_2, \\dots, z_n) + (w_1, w_2, \\dots, w_n) = (z_1 + w_1, z_2 + w_2, \\dots, z_n + w_n)\n\\]\n\n\nScalar Multiplication:\nFor a scalar \\(\\lambda \\in \\mathbb{C}\\) and a vector \\((z_1, z_2, \\dots, z_n) \\in \\mathbb{C}^n\\):\n\\[\n\\lambda \\cdot (z_1, z_2, \\dots, z_n) = (\\lambda z_1, \\lambda z_2, \\dots, \\lambda z_n)\n\\]\nThus, \\((\\mathbb{C}^n, +, \\cdot)\\) is a complex vector space because it satisfies the vector space axioms 1-8, where we replaced \\(\\mathbb{R}^n\\) with \\(\\mathbb{C}^n\\). Many of the results from MTH107 will hold regardless of whether we are using \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\), so we will often use \\(\\mathbb{F}\\) to represent either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\nFor example, \\(\\mathbb{F}^n\\) is an \\(\\mathbb{F}\\)-vector space, where \\(\\mathbb{F}\\) could be either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\n\n\n\nGeneralization (Not on the Exam):\n\nFinite Field Example:\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements.\nMany of our results hold in a more general setting, where \\(\\mathbb{F}\\) is a field—a set in which we can perform addition, multiplication, subtraction, and division (except division by zero).\nExamples of fields include:\n\n\\(\\mathbb{R}\\): the real numbers\n\\(\\mathbb{C}\\): the complex numbers\n\\(\\mathbb{Q}\\): the rational numbers\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements"
  },
  {
    "objectID": "mth107.html#abstract-vector-space",
    "href": "mth107.html#abstract-vector-space",
    "title": "MTH107—NOTEs",
    "section": "Abstract Vector Space:",
    "text": "Abstract Vector Space:\nWe can generalize this idea by replacing \\(\\mathbb{F}^n\\) with some abstract space \\(V\\), define addition \\(+\\) and scalar multiplication \\(\\cdot\\), and check if they satisfy the eight vector space axioms.\n\nDefinitions:\nif V and W are sets, V\\(\\times\\) W = {(v,w)|v\\(\\in\\) V, w\\(\\in\\) W}\nFor a set \\(V\\), addition on \\(V\\) is a map:\n\n？\ndifference？ 4-d based on definition\n\\[\nV \\times V \\to V\n\\]\nIt maps an element set to their addition. For example, if \\(V = \\mathbb{R}^2\\):\n\\[\n(v, w) \\mapsto v + w\n\\]\nExample: \\((1,2) + (3,4) = (4,6)\\), which is also in \\(V\\).\nA scalar multiplication is a map:\n\\[\nF \\times V \\to V\n\\]\nFor example, \\((\\lambda, v) \\mapsto \\lambda v\\).\nAn \\(F\\)-vector space is a set \\(V\\) with an addition \\(+\\) and scalar multiplication \\(\\cdot\\) by elements of \\(F\\), such that \\((V, +, \\cdot)\\) satisfies the vector space axioms 1-8, where \\(\\mathbb{R}\\) is replaced by \\(F\\), and \\(\\mathbb{R}^n\\) is replaced by \\(V\\).\n\n\n\nRemarks and Examples:\n\nVectors: Vectors are elements of \\(V\\), denoted as \\(v \\in V\\).\nField \\(F\\): The choice of \\(F\\) matters! For example, we will see later that \\(\\mathbb{C}^n\\) is a complex vector space of dimension \\(n\\), but is also a real vector space of dimension \\(2n\\)."
  },
  {
    "objectID": "mth107.html#section-1",
    "href": "mth107.html#section-1",
    "title": "MTH107—NOTEs",
    "section": "?",
    "text": "?\nis there any quick way to think this kind of question instead of proving 8 axioms one by one?\n\nproof of FS\n\\(F^S\\) is a vector space\nProof: we need to check the 8 axioms from the defination\n(core: use the element here to x and then see the equality of the 2 function on the each side of the equality using the quality of \\(\\mathbb F^n\\))\n\nf + g = g+f?\n\nthese 2 functions ((f+g)(x) and (g+f)(x)) are equal if and only if they have values agrees on every \\(s \\in S\\)\n\\[\n(f+g)(x)=f(x)+g(x)\n\\] which is \\(\\in F\\) so it is equal to g(x)+f(x)(addition axiom)=(g+f)(x)\n2)(f+g)+h = f+(g+h)\n[(f+g)+h](x)=(f+g)(x)+h(x)=[f(x)+g(x)]+h(x), which are \\(\\in F\\), so\n=f(x)+[g(x)+h(x)](axiom 3)=f(x)+(g+h)(x)=[f+(g+h)](x)\n\n\\(\\exists 0: 0+f=f\\) for any \\(f \\in F^S\\)\n\nyes, define the zero function \\(0_F:\\) to be the constant function \\(0_F\\), i.e. \\(0_F(x)=0_F\\)\n\nfor \\(f\\in F^S\\), can we find an inverse?\n\n-f is defined by (-f)(x)=-f(x)\ncheck: (f+(-f))(x)=f(x)+(-f(x))=\\(0_F\\)\nso f+(-f)=\\(0_{F^S}\\)\n\n1f=f \\(（1\\cdot f)(x)=1\\cdot f(x)(\\in F)=f(x)\\)\n(ab)f=a(bf)\n\npass\n7)a(f+g)=af+ag\na(f+g)(x)=a(f(x)+g(x))=af(x)+ag(x)=(af+ag)(x)\n8)(a+b)f=af+bf\n(a+b)f(x)=af(x)+bf(x)\n(af+bf)(x)=(af)(x)+(bf)(x)=af(x)+bf(x)\n\n\nSpecial Cases:\n\nEmpty Set: If \\(S = \\emptyset\\), then:\n\n\\[\nF^{\\emptyset} = \\{ 0 \\}\n\\] the proof has been proved above on “set of all maps from A to B(f)”\n\nFinite Set: If \\(S = \\{1, 2, \\dots, n\\}\\), then:\n\n\\[\nF^{\\{1, 2, \\dots, n\\}} = F^n\n\\]\n\nproof of special 2:\nLet \\(F^n\\) represent the n-dimensional vector space over a field \\(F\\), and \\(F^{\\{1, \\dots, n\\}}\\) represent the set of functions from the set \\(\\{1, \\dots, n\\}\\) to \\(F\\), i.e., it assigns a scalar from \\(F\\) to each index in \\(\\{1, \\dots, n\\}\\).\nWe will prove that there exists a linear map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) such that:\n\n\\(T\\) is a bijection (i.e., both injective and surjective).\n\\(T\\) preserves addition: \\[\nT(f + g) = T(f) + T(g)\n\\] for all \\(f, g \\in F^n\\).\n\\(T\\) preserves scalar multiplication: \\[\nT(a f) = a T(f)\n\\] for all \\(f \\in F^n\\) and \\(a \\in F\\).\n\nA map that satisfies these two conditions is called a linear map, and we will learn about it later in class. A linear map that is a bijection is called an isomorphism.\nDefine the Map \\(T\\)\nDefine the map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) as follows:\nFor each vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function \\(T((a_1, a_2, \\dots, a_n)) = f \\in F^{\\{1, \\dots, n\\}}\\) by: \\[\nf(i) = a_i \\quad \\text{for each} \\, i = 1, 2, \\dots, n.\n\\] Thus, \\(T((a_1, a_2, \\dots, a_n)) = (f(1), f(2), \\dots, f(n)) = (a_1, a_2, \\dots, a_n)\\).\n\nInjectivity: Suppose \\(T((a_1, a_2, \\dots, a_n)) = T((b_1, b_2, \\dots, b_n))\\). This implies that for each \\(i\\), \\(a_i = b_i\\). Therefore, \\((a_1, a_2, \\dots, a_n) = (b_1, b_2, \\dots, b_n)\\), so \\(T\\) is injective.\nSurjectivity: Given any function \\(f \\in F^{\\{1, \\dots, n\\}}\\), we can find a vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\) such that \\(f(i) = a_i\\) for each \\(i = 1, 2, \\dots, n\\). Therefore, \\(T\\) is surjective.\n\nSince \\(T\\) is both injective and surjective, it is a bijection.\nProve that \\(T\\) Preserves Addition\nLet \\((a_1, a_2, \\dots, a_n), (b_1, b_2, \\dots, b_n) \\in F^n\\). Then:\n\\[\nT((a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n)) = T((a_1 + b_1, a_2 + b_2, \\dots, a_n + b_n)).\n\\]\n\\[\nf(i) = a_i + b_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\)) the other hand site: \\[\nT((a_1, a_2, \\dots, a_n)) + T((b_1, b_2, \\dots, b_n)) = (a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n),\n\\] which results f, also.\nProve that \\(T\\) Preserves Scalar Multiplication\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\) and \\(c \\in F\\). Then: \\[\nT(c \\cdot (a_1, a_2, \\dots, a_n)) = T((c a_1, c a_2, \\dots, c a_n)).\n\\]\n\\[\nf(i) = c a_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\))\nthe other hand site: \\[\nc \\cdot T((a_1, a_2, \\dots, a_n)) = c \\cdot (a_1, a_2, \\dots, a_n),\n\\] which also results in \\(f\\)\nSince T is an isomorphism (bijection + linear), then \\(T^{-1}\\) is automatically linear. That is we automatically have \\(T^{-1}(x+y)= T^{-1}(x)+ T^{-1}(y)\\), and the same for scalar multiplication. So they are the same in vector space, too.\nWe have shown that \\(T\\) is a bijection and preserves both addition and scalar multiplication. Therefore, \\(T\\) is a linear isomorphism, and \\(F^n\\) and \\(F^{\\{1, \\dots, n\\}}\\) are isomorphic as vector spaces.\n(Last remark: If \\(T\\) is an isomorphism (i.e., bijective and linear), then \\(T^{-1}\\) is automatically linear. That is, we automatically have \\(T^{-1}(x + y) = T^{-1}(x) + T^{-1}(y)\\), and the same holds for scalar multiplication. Therefore, there is no need to check these properties for \\(T^{-1}\\).)\n(a more detailed one proving bijetive: \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is Equivalent to \\(F^n\\)\nTo prove that \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is equivalent to \\(F^n\\), we show there is a bijection between the two sets, meaning each element in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) corresponds to a unique element in \\(F^n\\), and vice versa.\nBy definition, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is the set of all functions from \\(\\{1, 2, 3, \\dots, n\\}\\) to \\(F\\). Each function \\(f\\) can be written as:\n\\[\nf = (f(1), f(2), \\dots, f(n)).\n\\]\n\\(F^n\\) is the set of ordered \\(n\\)-tuples \\((a_1, a_2, \\dots, a_n)\\), where each \\(a_i \\in F\\).\nMapping from \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) to \\(F^n\\)—\nFor any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), we define the corresponding tuple in \\(F^n\\) as:\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nMapping from \\(F^n\\) to \\(F^{\\{1, 2, 3, \\dots, n\\}}\\)—-\nFor any tuple \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) as:\n\\[\nT^{-1}(a_1, a_2, \\dots, a_n)(i) = a_i, \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nLet \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\)–\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nthen, define\n\\[\nT^{-1}(f(1), f(2), \\dots, f(n))(i) = f(i), \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nThus,\n\\[\nT^{-1}(T(f)) = f.\n\\]\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\). Applying \\(T^{-1}\\), we get the function \\(f\\) such that \\(f(i) = a_i\\) for each \\(i\\). And then,\n\\[\nT(T^{-1}(a_1, a_2, \\dots, a_n)) = (a_1, a_2, \\dots, a_n).\n\\]\nThus,\n\\[\nT \\circ T^{-1} = \\text{id}_{F^n}.\n\\] This equation means that for any element \\((a_1, a_2, \\dots, a_n) \\in F^n\\), applying \\(T^{-1}\\) to obtain a function \\(f\\), and then applying \\(T\\) to \\(f\\), returns the original tuple:\n\\[\nT^{-1} \\circ T = \\text{id}_{F^{\\{1,2,3,...n\\}}}.\n\\]\nThis equation means that for any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), applying \\(T\\) to obtain a tuple \\((f(1), f(2), \\dots, f(n))\\), and then applying \\(T^{-1}\\) to that tuple, returns the original function:\nSince \\(T\\) and \\(T^{-1}\\) are inverses of each other, we have established a bijection between \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\). Therefore, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\) are equivalent as sets and vector spaces.)"
  },
  {
    "objectID": "mth107.html#general-properties-of-vector-space",
    "href": "mth107.html#general-properties-of-vector-space",
    "title": "MTH107—NOTEs",
    "section": "General properties of vector space",
    "text": "General properties of vector space\nLet V be a vector space over F\nWe will prove some properties of V using only the defination (axioms 1-8)\n\nProposition\n\nThe zero(additive identity) is unique. That is: \\(\\exists ! 0\\in V s.t. 0+V=V, \\forall v\\in V\\)\n\nproof: Suppose we have 2 zero elements: 0 and 0’\n0=0’+0=0+0’=0’\n\n\\(\\forall v\\in V\\) there exists a unique additive inverse\n\nSupppose w and w’ are 2 inverses for v, w=0+w=(v+w’)+w=v+(w’+w)=v+(w+w’)=(v+w)+w’=0+w’=w’\n\n\\(\\forall v\\in V\\), \\(O_F\\cdot V=O_V\\)\n\n\n\n?\nwhy \\(0_F\\cdot V=0_V+w\\) F—&gt;V????\nwhy we could think of let w be the inverse of \\(0_F\\cdot V\\)? I always just could recite I mean remember the process instead of write it down smoothly\n(see 0_f as 0 is also okay)\nproof: \\(0_F\\)\\(\\cdot\\)V= \\((0_F+0_F)\\)\\(\\cdot\\)V=\\(0_Fv+0_Fv\\)\nlet w be the inverse of \\(0_F\\cdot v\\)(use proposition 2)\nthen \\(0_V\\)=\\(0_F\\cdot v+w\\)=\\((0_Fv+0_Fv)+w=0_Fv+(0_Fv+w)=0_Fv\\)\n\n\\(\\forall x\\in F: x\\cdot O_V=O_F\\)\n\nproof: \\(x\\cdot 0_V=x\\cdot (0_V+0_V)=x\\cdot 0_V+x\\cdot 0_V\\)\nso \\(0_V=x\\cdot 0_V\\)\n\n?\ndifference between o_Fv=0_v and xo_v=0_f(3 and 4 proposition)\n\n\n\nProperty\n\\(\\forall v\\in V,(-1)\\cdot V=-V\\)\nproof: V+(-1)V=(1+(-1))\\(\\cdot\\)V=0\\(\\cdot\\)V=0\nso (-1)V is an addictive inverse of V\nso we finish proof(by the addictive inverse)\n\nreminder of computing inverse of a matrix\nmethod1:\ndet(A)\n每一个位置的det构成的矩阵：B\ncofactor matrix:\\((-1)^{n+m}\\)\nC=B\\(\\times\\) cofactor matrix\n\\(A^{-1}=1/det(A)\\) times \\(C^T\\)\nmethod2: work for the tansformation of a matrix"
  },
  {
    "objectID": "mth107.html#section-2",
    "href": "mth107.html#section-2",
    "title": "MTH107—NOTEs",
    "section": "?",
    "text": "?\nis there any quick way to think this kind of question instead of proving 8 axioms one by one?\n\nproof of FS\n\\(F^S\\) is a vector space\nProof: we need to check the 8 axioms from the defination\n(core: use the element here to x and then see the equality of the 2 function on the each side of the equality using the quality of \\(\\mathbb F^n\\))\n\nf + g = g+f?\n\nthese 2 functions ((f+g)(x) and (g+f)(x)) are equal if and only if they have values agrees on every \\(s \\in S\\)\n\\[\n(f+g)(x)=f(x)+g(x)\n\\] which is \\(\\in F\\) so it is equal to g(x)+f(x)(addition axiom)=(g+f)(x)\n2)(f+g)+h = f+(g+h)\n[(f+g)+h](x)=(f+g)(x)+h(x)=[f(x)+g(x)]+h(x), which are \\(\\in F\\), so\n=f(x)+[g(x)+h(x)](axiom 3)=f(x)+(g+h)(x)=[f+(g+h)](x)\n\n\\(\\exists 0: 0+f=f\\) for any \\(f \\in F^S\\)\n\nyes, define the zero function \\(0_F:\\) to be the constant function \\(0_F\\), i.e. \\(0_F(x)=0_F\\)\n\nfor \\(f\\in F^S\\), can we find an inverse?\n\n-f is defined by (-f)(x)=-f(x)\ncheck: (f+(-f))(x)=f(x)+(-f(x))=\\(0_F\\)\nso f+(-f)=\\(0_{F^S}\\)\n\n1f=f \\(（1\\cdot f)(x)=1\\cdot f(x)(\\in F)=f(x)\\)\n(ab)f=a(bf)\n\npass\n7)a(f+g)=af+ag\na(f+g)(x)=a(f(x)+g(x))=af(x)+ag(x)=(af+ag)(x)\n8)(a+b)f=af+bf\n(a+b)f(x)=af(x)+bf(x)\n(af+bf)(x)=(af)(x)+(bf)(x)=af(x)+bf(x)\n\n\nSpecial Cases:\n\nEmpty Set: If \\(S = \\emptyset\\), then:\n\n\\[\nF^{\\emptyset} = \\{ 0 \\}\n\\] the proof has been proved above on “set of all maps from A to B(f)”\n\nFinite Set: If \\(S = \\{1, 2, \\dots, n\\}\\), then:\n\n\\[\nF^{\\{1, 2, \\dots, n\\}} = F^n\n\\]\n\nproof of special 2:\nLet \\(F^n\\) represent the n-dimensional vector space over a field \\(F\\), and \\(F^{\\{1, \\dots, n\\}}\\) represent the set of functions from the set \\(\\{1, \\dots, n\\}\\) to \\(F\\), i.e., it assigns a scalar from \\(F\\) to each index in \\(\\{1, \\dots, n\\}\\).\nWe will prove that there exists a linear map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) such that:\n\n\\(T\\) is a bijection (i.e., both injective and surjective).\n\\(T\\) preserves addition: \\[\nT(f + g) = T(f) + T(g)\n\\] for all \\(f, g \\in F^n\\).\n\\(T\\) preserves scalar multiplication: \\[\nT(a f) = a T(f)\n\\] for all \\(f \\in F^n\\) and \\(a \\in F\\).\n\nA map that satisfies these two conditions is called a linear map, and we will learn about it later in class. A linear map that is a bijection is called an isomorphism.\nDefine the Map \\(T\\)\nDefine the map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) as follows:\nFor each vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function \\(T((a_1, a_2, \\dots, a_n)) = f \\in F^{\\{1, \\dots, n\\}}\\) by: \\[\nf(i) = a_i \\quad \\text{for each} \\, i = 1, 2, \\dots, n.\n\\] Thus, \\(T((a_1, a_2, \\dots, a_n)) = (f(1), f(2), \\dots, f(n)) = (a_1, a_2, \\dots, a_n)\\).\n\nInjectivity: Suppose \\(T((a_1, a_2, \\dots, a_n)) = T((b_1, b_2, \\dots, b_n))\\). This implies that for each \\(i\\), \\(a_i = b_i\\). Therefore, \\((a_1, a_2, \\dots, a_n) = (b_1, b_2, \\dots, b_n)\\), so \\(T\\) is injective.\nSurjectivity: Given any function \\(f \\in F^{\\{1, \\dots, n\\}}\\), we can find a vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\) such that \\(f(i) = a_i\\) for each \\(i = 1, 2, \\dots, n\\). Therefore, \\(T\\) is surjective.\n\nSince \\(T\\) is both injective and surjective, it is a bijection.\nProve that \\(T\\) Preserves Addition\nLet \\((a_1, a_2, \\dots, a_n), (b_1, b_2, \\dots, b_n) \\in F^n\\). Then:\n\\[\nT((a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n)) = T((a_1 + b_1, a_2 + b_2, \\dots, a_n + b_n)).\n\\]\n\\[\nf(i) = a_i + b_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\)) the other hand site: \\[\nT((a_1, a_2, \\dots, a_n)) + T((b_1, b_2, \\dots, b_n)) = (a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n),\n\\] which results f, also.\nProve that \\(T\\) Preserves Scalar Multiplication\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\) and \\(c \\in F\\). Then: \\[\nT(c \\cdot (a_1, a_2, \\dots, a_n)) = T((c a_1, c a_2, \\dots, c a_n)).\n\\]\n\\[\nf(i) = c a_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\))\nthe other hand site: \\[\nc \\cdot T((a_1, a_2, \\dots, a_n)) = c \\cdot (a_1, a_2, \\dots, a_n),\n\\] which also results in \\(f\\)\nSince T is an isomorphism (bijection + linear), then \\(T^{-1}\\) is automatically linear. That is we automatically have \\(T^{-1}(x+y)= T^{-1}(x)+ T^{-1}(y)\\), and the same for scalar multiplication. So they are the same in vector space, too.\nWe have shown that \\(T\\) is a bijection and preserves both addition and scalar multiplication. Therefore, \\(T\\) is a linear isomorphism, and \\(F^n\\) and \\(F^{\\{1, \\dots, n\\}}\\) are isomorphic as vector spaces.\n(Last remark: If \\(T\\) is an isomorphism (i.e., bijective and linear), then \\(T^{-1}\\) is automatically linear. That is, we automatically have \\(T^{-1}(x + y) = T^{-1}(x) + T^{-1}(y)\\), and the same holds for scalar multiplication. Therefore, there is no need to check these properties for \\(T^{-1}\\).)\n(a more detailed one proving bijetive: \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is Equivalent to \\(F^n\\)\nTo prove that \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is equivalent to \\(F^n\\), we show there is a bijection between the two sets, meaning each element in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) corresponds to a unique element in \\(F^n\\), and vice versa.\nBy definition, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is the set of all functions from \\(\\{1, 2, 3, \\dots, n\\}\\) to \\(F\\). Each function \\(f\\) can be written as:\n\\[\nf = (f(1), f(2), \\dots, f(n)).\n\\]\n\\(F^n\\) is the set of ordered \\(n\\)-tuples \\((a_1, a_2, \\dots, a_n)\\), where each \\(a_i \\in F\\).\nMapping from \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) to \\(F^n\\)—\nFor any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), we define the corresponding tuple in \\(F^n\\) as:\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nMapping from \\(F^n\\) to \\(F^{\\{1, 2, 3, \\dots, n\\}}\\)—-\nFor any tuple \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) as:\n\\[\nT^{-1}(a_1, a_2, \\dots, a_n)(i) = a_i, \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nLet \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\)–\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nthen, define\n\\[\nT^{-1}(f(1), f(2), \\dots, f(n))(i) = f(i), \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nThus,\n\\[\nT^{-1}(T(f)) = f.\n\\]\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\). Applying \\(T^{-1}\\), we get the function \\(f\\) such that \\(f(i) = a_i\\) for each \\(i\\). And then,\n\\[\nT(T^{-1}(a_1, a_2, \\dots, a_n)) = (a_1, a_2, \\dots, a_n).\n\\]\nThus,\n\\[\nT \\circ T^{-1} = \\text{id}_{F^n}.\n\\] This equation means that for any element \\((a_1, a_2, \\dots, a_n) \\in F^n\\), applying \\(T^{-1}\\) to obtain a function \\(f\\), and then applying \\(T\\) to \\(f\\), returns the original tuple:\n\\[\nT^{-1} \\circ T = \\text{id}_{F^{\\{1,2,3,...n\\}}}.\n\\]\nThis equation means that for any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), applying \\(T\\) to obtain a tuple \\((f(1), f(2), \\dots, f(n))\\), and then applying \\(T^{-1}\\) to that tuple, returns the original function:\nSince \\(T\\) and \\(T^{-1}\\) are inverses of each other, we have established a bijection between \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\). Therefore, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\) are equivalent as sets and vector spaces.)"
  },
  {
    "objectID": "mth107.html#problems",
    "href": "mth107.html#problems",
    "title": "MTH107—NOTEs",
    "section": "problems",
    "text": "problems\n\nsummarize1: give opposite example and draw pictures are okay.\n\neg.\neg."
  },
  {
    "objectID": "mth107.html#understand-again",
    "href": "mth107.html#understand-again",
    "title": "MTH107—NOTEs",
    "section": "understand again",
    "text": "understand again\n\n\n\nunderstand vector space"
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Math is charismatic",
    "section": "",
    "text": "The math teachers I have learned from are teaching math in an interesting and sophisticated way with different charismatic philosophy of themselves.\n\n\nI want to log the profound and inspiring things that I learned from my math teachers, including notes, philosophy and the like.\n\nAdvanced Linear Algebra and Linear Algebra\nAnalysis\nPDE intro\nmulti-variable calculus(geometric guys, sequence, …)\nmodeling knowledge\ninteresting problems"
  },
  {
    "objectID": "mth1113.html#intro",
    "href": "mth1113.html#intro",
    "title": "Intro to probability and statistics",
    "section": "Intro",
    "text": "Intro\nstat is a large field in math involving the collection, organization, analysis,interpretation, and presentation of data(a collection of observations on one or more variables(A characteristic whose value may change from one observation to another))\nStatistics is the scientific discipline that provides methods to help us make sense of data.\nIt is important to be able to:\n1 Extract information from tables, charts, and graphs.\n2 Follow numerical arguments.\n3 Understand the basics of how data should be gathered, summarized, and analysed to draw statistical conclusions.\nThe Data Analysis Process\n1 Understanding the nature of the research problem or goals.\n2 Deciding what to measure and how.\n3 Collecting data.\n4 Data summarization and preliminary analysis.\n5 Formal Data Analysis (Statistical Methods).\n6 Interpretation of the results."
  },
  {
    "objectID": "mth1113.html#types-of-data",
    "href": "mth1113.html#types-of-data",
    "title": "Intro to probability and statistics",
    "section": "Types of data",
    "text": "Types of data\n\nuni data set and bivariate and multivariate\n\n\ncategorical and numerical(discrete and continuous) with plot using excel (data analysis) or rstudio plot (ggplot2)\nfor categorial data we could use a bar chart which is a graph of a frequency distribution for categorical data.\nfor a small numerical data we could use dotplot\n\ndiscrete\n\n\nlibrary(ggplot2)\n\n# creat data：Wechat number\ndiscrete_data &lt;- data.frame(value = c(30, 15, 20,30,60))\n\n# plot\nggplot(discrete_data, aes(x = value)) +\n  geom_dotplot(binwidth = 1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Discrete Data (Number of Wechats)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ncontinuous\n\n\n# 手动输入毕业率数据\nall_athletes &lt;- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball &lt;- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 设置画布的高度，以便将两个图绘制在同一页面上\nplot.new()\nplot.window(xlim = c(0, 100), ylim = c(0.5, 2.5))\n\n# 绘制 Basketball 数据的 dotplot\nstripchart(basketball, method = \"stack\", at = 2, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 绘制 All Athletes 数据的 dotplot\nstripchart(all_athletes, method = \"stack\", at = 1, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 添加 X 轴\naxis(1, at = seq(10, 100, by = 10), labels = seq(10, 100, by = 10))\n\n# 添加标签\ntext(-5, 2, \"Basketball\", xpd = TRUE, adj = 1)\ntext(-5, 1, \"All Athletes\", xpd = TRUE, adj = 1)\n\n# 添加横线\nabline(h = 1.5, col = \"black\", lwd = 2)\n\n# 添加 X 轴标签\ntitle(xlab = \"Graduation rates (%)\")\n\n\n\n\n\n\n\n\n\n# creat data--time spent in minutes\ncontinuous_data &lt;- data.frame(value = c(6, 5.25, 3.62,1,2,3.1,3.2,4,5,6,7,4,10))\n\n# dotplot\nggplot(continuous_data, aes(x = value)) +\n  geom_dotplot(binwidth = 0.1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Continuous Data (Time Spent in Minutes)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\n# 毕业率数据\nschool &lt;- 33:68\nall_athletes &lt;- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball &lt;- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 创建数据框\ndata &lt;- data.frame(school, all_athletes, basketball)\n\n# 画图\nggplot() +\n  geom_dotplot(data = data, aes(x = all_athletes, y = \"All Athletes\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5) +\n  geom_dotplot(data = data, aes(x = basketball, y = \"Basketball\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5, color = \"red\") +\n  xlab(\"Graduation rates (%)\") +\n  ylab(\"\") +\n  theme_minimal() +\n  ggtitle(\"Dotplot of Graduation Rates for All Athletes and Basketball Players\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nhistogram excel plot"
  },
  {
    "objectID": "mth1113.html#collect-data-sensibly",
    "href": "mth1113.html#collect-data-sensibly",
    "title": "Intro to probability and statistics",
    "section": "collect data sensibly",
    "text": "collect data sensibly\n\nexperiments and obeservational study\n\n\nbias\nselection bias\nmeasurement or response bias\n\n\nrandom sampling\nthe same chance to be selected\n\n\nstrata and cluster\ncluster reflect general characteristic about\n\n\nsystematic sampling"
  },
  {
    "objectID": "mth107.html#relevant-to-dear-regression-analysis",
    "href": "mth107.html#relevant-to-dear-regression-analysis",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "MLR\n\n\nmatrix\n\n\nIn linear regression, we often work with matrix and vector representations. Given a matrix \\(X\\) (which represents the design matrix with each row corresponding to one observation and each column to a predictor), and a vector \\(Y\\) (representing the observed responses), the regression model can be written as:\n\\[\nY = X \\beta + \\epsilon\n\\]\nWhere: - \\(Y\\): The \\(n \\times 1\\) response vector, - \\(X\\): The \\(n \\times p\\) design matrix, - \\(\\beta\\): The \\(p \\times 1\\) vector of unknown coefficients, - \\(\\epsilon\\): The \\(n \\times 1\\) vector of errors or residuals.\nThe ordinary least squares (OLS) estimator for \\(\\beta\\), denoted \\(\\hat{\\beta}\\), is obtained by minimizing the residual sum of squares:\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\n\n\nConsider two vectors \\(X_1\\) and \\(X_2\\). If \\(X_1^T X_2 = 0\\), we say the vectors are orthogonal, meaning they are uncorrelated in terms of inner product space. In regression, orthogonality implies no collinearity between predictors.\nOrthogonality plays an important role in simplifying matrix operations in regression. For example, if \\(X_1\\) is orthogonal to \\(X_2\\), the matrix \\(X^T X\\) will have off-diagonal elements equal to zero, simplifying the computation of the inverse.\n\n\n\nIf the matrix \\(X^T X\\) is invertible, the OLS estimator exists and is unique. One important property of the matrix \\(X^T X\\) is that it is symmetric and positive semi-definite. If \\(X\\) has full column rank (meaning that the columns of \\(X\\) are linearly independent), then \\(X^T X\\) is positive definite, and its inverse exists.\n\n\nIf \\(X^T X\\) is invertible, then:\n\\[\n(X^T X)^{-1} X^T X = I_p\n\\]\nWhere \\(I_p\\) is the identity matrix of size \\(p \\times p\\).\n\n\n\n\nThe projection matrix (or hat matrix) \\(P\\) is defined as:\n\\[\nP = X (X^T X)^{-1} X^T\n\\]\nThis matrix \\(P\\) projects the observed data vector \\(Y\\) onto the column space of \\(X\\), giving the fitted values \\(\\hat{Y}\\):\n\\[\n\\hat{Y} = P Y\n\\]\nThe residuals \\(\\hat{\\epsilon}\\), which are the differences between the observed and fitted values, are given by:\n\\[\n\\hat{\\epsilon} = Y - \\hat{Y} = (I_n - P) Y\n\\]\nWhere \\(I_n\\) is the \\(n \\times n\\) identity matrix. The projection matrix \\(P\\) has several key properties: - \\(P^2 = P\\) (idempotent), - \\(P^T = P\\) (symmetric).\n\n\n\nThe residual sum of squares (RSS) is given by:\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2 = \\hat{\\epsilon}^T \\hat{\\epsilon} = (Y - X \\hat{\\beta})^T (Y - X \\hat{\\beta})\n\\]\nSubstituting \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\), we get:\n\\[\n\\text{RSS} = Y^T (I_n - P) Y\n\\]\n\n\n\nThe variance-covariance matrix of the OLS estimator \\(\\hat{\\beta}\\) is:\n\\[\n\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n\\]\nWhere \\(\\sigma^2\\) is the variance of the error terms. This matrix provides information about the precision of the estimated coefficients.\n\n\n\n\nConsider a vector \\(y\\) in the vector space spanned by the columns of \\(X\\). The projection of \\(y\\) onto this space can be written as:\n\\[\n\\hat{y} = P y\n\\]\nIf \\(P\\) is the projection matrix, then the following properties hold: - \\(P^T = P\\) (symmetric), - \\(P^2 = P\\) (idempotent), - \\((I - P)\\) is also a projection matrix (projecting onto the orthogonal complement).\nThus, we can decompose any vector \\(Y\\) as:\n\\[\nY = \\hat{Y} + (Y - \\hat{Y}) = P Y + (I - P) Y\n\\]\nWhere \\(\\hat{Y}\\) is the projection onto the space spanned by \\(X\\), and \\((Y - \\hat{Y})\\) is the projection onto the orthogonal complement.\n\n\n\n\nA vector \\(\\perp\\) to a matrix \\(X\\) (i.e., orthogonal to the column space of the matrix \\(X\\)) implies that:\n\n\\[\nX^T v = 0\n\\]\nThis condition means that the vector \\(v\\) is orthogonal to every column of the matrix \\(X\\). In regression, this condition often arises when discussing residuals, which are orthogonal to the fitted values (i.e., the column space of \\(X\\)).\n\n\n\n\nIf \\(X^T v = 0\\), then \\(v \\perp X\\) (i.e., \\(v\\) is orthogonal to the column space of \\(X\\)).\nIf \\(X\\) has linearly independent columns, then \\(X^T X\\) is invertible.\nIf \\(X\\) does not have full column rank, then \\(X^T X\\) is not invertible. In this case, we cannot directly compute \\((X^T X)^{-1}\\).\n\n\n\n\nFor \\(X\\) to have full column rank, the number of columns \\(p\\) must be less than or equal to the number of observations \\(n\\):\n列空间的正交补空间只有零向量 he orthogonal complement of a column space has only zero vectors\nIf \\(X\\) has full column rank, then \\(X^T X\\) is invertible. This property is crucial for solving the normal equations in ordinary least squares (OLS) regression.\n\n\n\nGiven that \\(X^T X\\) is symmetric, the following property holds:\n\\[\n((X^T X)^{-1})^T = (X^T X)^{-1}\n\\]\nThis is a key result in linear regression, allowing us to solve for the OLS estimator of \\(\\beta\\):\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\nThis property ensures that the normal equations have a unique solution when \\(X^T X\\) is invertible.\n\n\n\n\n\n正交性：矢量与矩阵的正交性在回归中经常用于描述残差与拟合值之间的关系。\n矩阵的性质：矩阵 \\(X^T X\\) 的可逆性取决于 \\(X\\) 的列满秩（线性无关性）。当 \\(X\\) 的列满秩时，\\(X^T X\\) 是可逆的，从而确保了线性回归模型的唯一解。\n矩阵运算：通过矩阵运算 \\((X^T X)^{-1}\\)，可以得到 OLS 回归系数的估计值 \\(\\hat{\\beta}\\)。"
  }
]