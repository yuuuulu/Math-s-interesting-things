[
  {
    "objectID": "eng pde.html#what-is-a-number",
    "href": "eng pde.html#what-is-a-number",
    "title": "A Kun’s PDE Lecture",
    "section": "what is a number",
    "text": "what is a number\n\nQ, rational number q/p (ratio)\n\n有理数用直尺画，根号用圆规–算术平均大于几何平均\n什么是理性\n古巴比伦用宗教解释不理解的事情\n古希腊人（大自然是可理解的）带给现代人最珍贵的礼物就是理性\n爱因斯坦：大自然最不可理解的地方是大自然竟然是可理解的\n古希腊的传人\n理性的工具是数学，希腊人对数学的重视\n科技（建筑）和科学（要有主张和实验）\n\\(\\mathbb R\\)\nE.Steim\n分离变数法–傅立叶级数—分析\n微积分\n\n傅立叶 analysis\ncomplex analysis\nreal analysis\nfunctional analysis"
  },
  {
    "objectID": "eng pde.html#geometry",
    "href": "eng pde.html#geometry",
    "title": "A Kun’s PDE Lecture",
    "section": "geometry",
    "text": "geometry\nthe most important thing in geometry is about measure."
  },
  {
    "objectID": "eng pde.html#algebra",
    "href": "eng pde.html#algebra",
    "title": "A Kun’s PDE Lecture",
    "section": "algebra",
    "text": "algebra\nsimplize authority独断权威"
  },
  {
    "objectID": "eng pde.html#analysis",
    "href": "eng pde.html#analysis",
    "title": "A Kun’s PDE Lecture",
    "section": "analysis",
    "text": "analysis\ndemocracy\nNewton\nanatomy\nf(x)约等于 \\(\\Sigma a_nx^n\\)—-Talor series\nbe patient to learn analysis"
  },
  {
    "objectID": "eng pde.html#reformation",
    "href": "eng pde.html#reformation",
    "title": "A Kun’s PDE Lecture",
    "section": "Reformation",
    "text": "Reformation"
  },
  {
    "objectID": "eng pde.html#renaissance",
    "href": "eng pde.html#renaissance",
    "title": "A Kun’s PDE Lecture",
    "section": "Renaissance",
    "text": "Renaissance"
  },
  {
    "objectID": "eng pde.html#enlightenment",
    "href": "eng pde.html#enlightenment",
    "title": "A Kun’s PDE Lecture",
    "section": "Enlightenment",
    "text": "Enlightenment"
  },
  {
    "objectID": "eng pde.html#industrial-revolution",
    "href": "eng pde.html#industrial-revolution",
    "title": "A Kun’s PDE Lecture",
    "section": "Industrial Revolution",
    "text": "Industrial Revolution\nNo dynasty lasts over 300 years, except Song dynasty which lasted over 300 years, with scholars serving as prime ministers. Su Shi snored."
  },
  {
    "objectID": "eng pde.html#wave-equation",
    "href": "eng pde.html#wave-equation",
    "title": "A Kun’s PDE Lecture",
    "section": "Wave Equation",
    "text": "Wave Equation\nVibration of a string.\nTaylor, d’Alembert, Daniel Bernoulli, Jacob Bernoulli (Euler), Jacob Bernoulli (distribution).\n\\[\n\\frac{d^2x}{dt^2}\n\\]\nThe truths of this world have all been discovered by Newton.\n\\[\nu(x,t) \\text{: amplitude, } [u] = L\n\\]\n$$ [p] = M/L\n$$\n\\[\np(x,t) \\text{: density (1-dimensional)}\n\\]\n[T] = [ma] = [m][a] = ML/t^2\n\\(T=T_1=T_2\\) tension\n(0-L)\nF = Tsin() - Tsin\nm = \\(\\rho\\) x\n2nd law: \\(\\rho\\) x"
  },
  {
    "objectID": "eng pde.html#linear-transformation",
    "href": "eng pde.html#linear-transformation",
    "title": "A Kun’s PDE Lecture",
    "section": "linear transformation",
    "text": "linear transformation\nKeep the parallelogram diagonal 保持平行四边形的对角线 x+y\nkeep a straight line 保持直线—a\n向量空间 vector space\n如果是从二维映射到三维怎么办。2dim—3dim？\n代数结构\n保留加法 keep the addition\nhomomorphism\nmorphism–形象"
  },
  {
    "objectID": "eng pde.html#da..-bernulli-separation-of-variables",
    "href": "eng pde.html#da..-bernulli-separation-of-variables",
    "title": "A Kun’s PDE Lecture",
    "section": "Da.. Bernulli Separation of Variables",
    "text": "Da.. Bernulli Separation of Variables\n\\(u(x,\\theta)=\\phi(x)T(t)\\). (Gassian integral formula derivation also use this method)\n(A continuous function can be approximated by a polynomial, which is a super polynomial)\n(the defination of “density”)\n(Seperation variable: God to God, Caesar to Caesar—-x to x, t to t)\n分离变数\n\\[\n\\frac{T^{''}(t)}{C^2 T(t)} = -\\frac{\\phi^{''}(x)}{\\phi(x)}\n\\]\n\\[\n\\begin{aligned}\n&\\text{PDE:} \\quad \\text{分离变量法,separete variables} \\rightarrow \\text{ODE} \\\\\n&\\phi'' + \\lambda \\phi = 0 \\\\\n&T'' + \\lambda c^2 T = 0 \\\\\n&\\text{(B.C.) 边界条件：} \\\\\n&u(0,t) = \\phi(0) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(0) = 0 \\quad \\text{(trivial 非平凡解,boring)} \\\\\n&u(L,t) = \\phi(L) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(L) = 0\n\\end{aligned}\n\\]\n\\[\n\\lambda &lt; 0 \\quad , \\quad \\phi = e^{mx} \\quad \\text{(不可能，从0到0。0--0， impossible)}\n\\]\n\\[\n\\lambda = 0 \\quad , \\quad \\phi = Ax + B \\quad \\text{直线运动， straight move}\n\\]\n\\[\n\\lambda &gt; 0 \\quad , \\quad \\phi = c_1 \\cos(\\sqrt{\\lambda} x) + c_2 \\sin(\\sqrt{\\lambda} x)\n\\] ### 解的推导\n考虑方程： \\[\n\\phi'' + \\lambda \\phi = 0\n\\] 其中 () 是一个常数，解的形式取决于 () 的值。\n\n1. 当 (&lt; 0)\n当 (&lt; 0) 时，我们令 (= -^2) ，于是方程变为： \\[\n\\phi'' - \\mu^2 \\phi = 0\n\\] 其通解为： \\[\n\\phi(x) = A e^{\\mu x} + B e^{-\\mu x}\n\\] 这种解形式表示指数发散或衰减，通常是不稳定解。\n\n\n2. 当 (= 0)\n当 (= 0) 时，方程变为： \\[\n\\phi'' = 0\n\\] 这是一个线性方程，其通解为： \\[\n\\phi(x) = A x + B\n\\] 这表示直线运动。\n\n\n3. 当 (&gt; 0)\n当 (&gt; 0) 时，我们令 (= ^2) ，方程变为： \\[\n\\phi'' + \\mu^2 \\phi = 0\n\\] 其解为： \\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x) ---wave\n\\] 这种解表示周期性波动。\n\n推导\n对于形如 ( \\(\\phi'' + \\mu^2 \\phi = 0\\) ) 的常微分方程，我们可以猜测它的解是指数形式，即：\n\\[\n\\phi(x) = e^{rx}\n\\]\n这里 ( r ) 是需要确定的参数。\n\n\n\n代入微分方程：\n将 ( (x) = e^{rx} ) 代入原方程，得到：\n\\[\nr^2 e^{rx} + \\mu^2 e^{rx} = 0\n\\]\n由于 ( e^{rx} )，可以消掉这个项，剩下的是特征方程：\n\\[\nr^2 + \\mu^2 = 0\n\\] 这个特征方程的解为：\n\\[\nr = \\pm i \\mu\n\\]\n这是一个虚数解。\n当特征根为纯虚数时，方程的通解可以写成正弦和余弦的组合形式，根据欧拉公式 ( e^{ix} = (x) + i (x) )，我们得到通解为：\n\\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x)\n\\]\n其中，( A ) 和 ( B ) 是待定常数，它们可以通过边界条件或初始条件来确定。\ncharacteristic\n\\[\n\\phi(x) = C_1 \\cos(\\sqrt{\\lambda}x) + C_2 \\sin(\\sqrt{\\lambda}x)\n\\]\n边界条件： \\[\n\\phi(0) = 0 \\Rightarrow C_1 = 0\n\\]\n\\[\n\\phi(L) = C_2 \\sin(\\sqrt{\\lambda}L) = 0\n\\]\n因此， \\[\n\\sqrt{\\lambda_n}L = n\\pi \\quad (n = 1, 2, 3, \\dots)\n\\]\n得到本征值： \\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n对应的本征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] Sturm-Liouville Problem:\n\\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n因此，特征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] \\[\nT_n(t) = C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right)\n\\]\n\\[\nu_n(x,t) = T_n(t) \\phi_n(x)\n\\]\n代入展开： \\[\nu(x,t) = \\sum_{n=1}^{\\infty} C_n \\sin \\left( \\frac{n\\pi x}{L} \\right) \\left[ C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right) \\right]\n\\]"
  },
  {
    "objectID": "eng pde.html#law-of-large-number",
    "href": "eng pde.html#law-of-large-number",
    "title": "A Kun’s PDE Lecture",
    "section": "law of large number",
    "text": "law of large number\neg. The reason the casino makes money is because even though the people who go there have a little more than a half chance of winning, the casino has more money than you, and if you stay long enough, you will win, so don’t go to the casino, because you can’t go deeper than the casino"
  },
  {
    "objectID": "eng pde.html#does-math-have-elements",
    "href": "eng pde.html#does-math-have-elements",
    "title": "A Kun’s PDE Lecture",
    "section": "Does math have elements?",
    "text": "Does math have elements?\n             ---Paul Halmos\n             Geometric Series\n\n1-ab and 1-ba —- invertible\nI-AB invertible\n— I-BA invertible (hint: \\(I+B(I-AB)^{-1}A\\))\ndo not 杀鸡用牛刀—-\nprove the harmonic series is divergent:\nidea: geometric series:\n1+1/2+(1/3+1/4)+….\n1+(1/2+1/3)+(1/4+..+1/9)+(1/10+…+1/27)+…\\(\\geq\\) 1+(1/3)\nwhich is relevant to fuliyejishu\n学数学不要兵来将挡水来土掩，要有一个整套的思想，不要背书 idea: geometric series\n(1-ba){-1}=1/1-ba=1+ba+baba+..=1+b(1+ab+abab+..)a=1+b(1-ab){-1}a\n(1-ba)[1+b(1-ab)^{-1}a]\n1+b(1-ab){-1}a-ba-bab(1-ab){-1}a=(commutive law)=1-ba+b(1-ab){-1}a-bab(1-ab){-1}a=1-ba+b(1-ab)(1-ab)^{-1}a\n=1-ba+b1a\n=1-ba+ba\n=1 —-so do not recite boring things, remember based on understanding.\nDo not read Gassian, read Oral\nfirstly, for matrix, AB \\(\\noequal\\) BA—-f(g) is not equal to g(f)\n学学问要有感觉\nchat with different famous … 西方的没落–fu springer\nliangzilixue hysenber—max born\nthe true meaning of matrix is the linear transformation(fun f)"
  },
  {
    "objectID": "eng pde.html#p-series",
    "href": "eng pde.html#p-series",
    "title": "A Kun’s PDE Lecture",
    "section": "p-series",
    "text": "p-series\nIntegral test with dimensional analysis to know p&gt;1 for convergence\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}  \\approx  \\int_1^\\infty 1/x^p \\, dx\\approx 1/[x]^p*[x]=[x]^{1-p} ([x]--&gt;\\infty)---&gt;1-p\\leq 0\\)\nagain, geometric series,\n1/12+(1/22+1/32)+(1/42+..)\n\\(\\leq 1+(1/2^2+1/2^2)+\\)\n= 1+2/2^2 +4/42+8/82z=…=2\n\nEuler\nEuler idea: Viete fumula\nrelation of root and coefficient\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}=\\pi^2/6\\)\n(x-a)(x-b)=x^2-(a+b)x+ab\n(1-x/a)(1-x/b)=1-(1/a+1/b)x+1/ab*x^2\n(1-x)(1+x)\n(1-x)(1+x)(1-x/2)(1+x/2)\n(1-x)(1+x)(1-x/2)(1+x/2)…(1-x/n)(1+x/n)+…\n=1-(1/12+1/22+…+1/n2)x2+…\nQ: find a function whose roots are +-1, +-2, +-3,….\nso Euler changed this Q:\nrewrite:\n(1-x/)(1+x/)(1-x/2)(1+x/2)…\n= 1-(1/12+1/22+1/n2+…)1/2x^2+…\n先猜答案\nguess sinx = (1-x/)(1+x/)…(1-x/n)(1+x/n)\nbut 0 is a solution\nso change to sinx/x\nsinx/x =k…..\nk=limsinx/x=1\n1/x is 振幅\nsinx/x=1-(1/12+1/22+…+1/n2+…)x2/^2+…\n=1/x\n天才是创意\ncreate a thery of math\nand six/x = 1/x(x-x3/3!+…)=1-x3/6\nso we know"
  },
  {
    "objectID": "ProfZhu.html",
    "href": "ProfZhu.html",
    "title": "EM",
    "section": "",
    "text": "other unknown parameters\n\n\ncencer\n\n\n\npf\\(_1\\)(X)+(1-p)f\\(_2\\)(X)"
  },
  {
    "objectID": "Mathematical Analysis 1.html",
    "href": "Mathematical Analysis 1.html",
    "title": "Analysis1 and Analysis 2",
    "section": "",
    "text": "喜欢数学分析的一年结束了，遗憾也该结束了–还是不够专注不够认真被外界打扰被绩点主义困扰，什么时候可以静心更沉淀一下呢。为什么会喜欢数分呢，是大二总是太无聊了喜欢去拿题目找老师询问然后聊天吗，是喜欢去用不同的counter example看待定理但无奈接受于自己永远不会举counter example的事实吗，是喜欢多种方法思考同一道题目然后感叹学问的魅力吗，是在期末考试中Archimedean Property使得1/n在uniform continuity的delta内然后用squeeze Theorem证明出来的结果吗，是喜欢数列逼近的直观画图的直观在证明前就一眼看出来结果的小骄傲吗，还是感叹于用recurrence equation去简单得到n维球体积的直观吗，是体会到尽管是这么历史悠久严谨的数学还是不能刻画大自然从而欣然接受自己无限渺小的心态吗，是钦佩于也是经常处于迷茫之中但是还坚定不移的走应用数学纯数学路途的老师们的坚毅吗，还是沉浸于数学和生活道理结合起来的老师们口中的小故事呢，比如Minkowski得盲肠炎在过世前感叹竟然死在相对论发展的时代，比起这些，所有遗憾似乎都全部消失。"
  },
  {
    "objectID": "Mathematical Analysis 1.html#analysis-limit-approximation-inequality",
    "href": "Mathematical Analysis 1.html#analysis-limit-approximation-inequality",
    "title": "Analysis1 and Analysis 2",
    "section": "",
    "text": "喜欢数学分析的一年结束了，遗憾也该结束了–还是不够专注不够认真被外界打扰被绩点主义困扰，什么时候可以静心更沉淀一下呢。为什么会喜欢数分呢，是大二总是太无聊了喜欢去拿题目找老师询问然后聊天吗，是喜欢去用不同的counter example看待定理但无奈接受于自己永远不会举counter example的事实吗，是喜欢多种方法思考同一道题目然后感叹学问的魅力吗，是在期末考试中Archimedean Property使得1/n在uniform continuity的delta内然后用squeeze Theorem证明出来的结果吗，是喜欢数列逼近的直观画图的直观在证明前就一眼看出来结果的小骄傲吗，还是感叹于用recurrence equation去简单得到n维球体积的直观吗，是体会到尽管是这么历史悠久严谨的数学还是不能刻画大自然从而欣然接受自己无限渺小的心态吗，是钦佩于也是经常处于迷茫之中但是还坚定不移的走应用数学纯数学路途的老师们的坚毅吗，还是沉浸于数学和生活道理结合起来的老师们口中的小故事呢，比如Minkowski得盲肠炎在过世前感叹竟然死在相对论发展的时代，比起这些，所有遗憾似乎都全部消失。"
  },
  {
    "objectID": "Mathematical Analysis 1.html#weierstrass-continuous-nowhere-differentiable-function",
    "href": "Mathematical Analysis 1.html#weierstrass-continuous-nowhere-differentiable-function",
    "title": "Analysis1 and Analysis 2",
    "section": "Weierstrass continuous nowhere differentiable function",
    "text": "Weierstrass continuous nowhere differentiable function\nThis example’s proof tells that the example of x to be 0 is important\nThe series construction method of this proof is very intrinsic."
  },
  {
    "objectID": "Mathematical Analysis 1.html#final-exam-review-most-are-about-uniform-convergence-which-is-really-really-intereting-for-me-since-i-have-got-some-exercises-from-prof.-andrew-lin-and-also-dr.-chi-kwong-fok-has-helped-me-a-lot-in-his-office-hours",
    "href": "Mathematical Analysis 1.html#final-exam-review-most-are-about-uniform-convergence-which-is-really-really-intereting-for-me-since-i-have-got-some-exercises-from-prof.-andrew-lin-and-also-dr.-chi-kwong-fok-has-helped-me-a-lot-in-his-office-hours",
    "title": "Analysis1 and Analysis 2",
    "section": "Final exam review (most are about uniform convergence, which is really, really intereting for me since I have got some exercises from Prof. Andrew Lin and also Dr. Chi-Kwong Fok has helped me a lot in his office hours)",
    "text": "Final exam review (most are about uniform convergence, which is really, really intereting for me since I have got some exercises from Prof. Andrew Lin and also Dr. Chi-Kwong Fok has helped me a lot in his office hours)\nFor \\(x=0\\) or if \\(x\\) is irrational, and \\(x\\in[c,d]\\)\n\\[\n\\sup \\left| \\frac{x}{n} \\left(1 + \\frac{1}{n}\\right) \\right| = \\max \\left\\{ \\left| \\frac{c}{n} \\left(1 + \\frac{1}{n}\\right) \\right|, \\left| \\frac{d}{n} \\left(1 + \\frac{1}{n}\\right) \\right| \\right\\}\n\\]\n\\[\n\\Rightarrow \\limsup_{n \\to \\infty} \\left| \\frac{x}{n} \\left(1 + \\frac{x}{n}\\right) \\right| = 0 \\Rightarrow \\text{this could not say anything}\n\\]\nFor \\(x = \\frac{a}{b}, b &gt; 0\\) and a, b are integers.\n\\[\nh_n(x) = a + \\frac{a}{n} \\left(1 + \\frac{1}{b}+\\frac{1}{bn}\\right) = a + \\frac{a}{n} + \\frac{x}{n} + \\frac{x}{n^2} \\text{ ---Since } \\frac{a}{b} = x\n\\]\n\\[\n\\left |h_n(x) - a\\right|= \\left| \\frac{a}{n} + \\frac{x}{n}+\\frac{x}{n^2}\\right|\n\\]\nTake a to be the sequence \\(n\\)\n\\[\n\\lim_{n \\to \\infty} |h_n(x) - a_n| = \\lim_{n \\to \\infty} \\left| \\frac{n}{n} + \\frac{x}{n} + \\frac{x}{n^2} \\right| = 1 \\ne 0\n\\]\nBy the definition of uniform convergence, we have concluded that \\(h_n(x)\\) is not uniformly convergent.\n\nNext one\n\nWe know that\n\\[\n\\frac{(-1)^n}{\\sqrt{n}} \\sin \\left(1 + \\frac{x}{n}\\right)\n\\]\nis defined on \\(A\\), which is a compact set and \\(A \\subseteq [-M, M], M &gt; 0\\).\nThis series if from Fourier series but wecan also use this method to prove.\nIdea: when n is large enough, \\(\\sin \\left(1 + \\frac{x}{n}\\right)\\) is always positive and also \\(|\\sin x| \\leq |x|, \\forall x \\in \\mathbb{R}\\)\nThis means that\n\\[\n\\sin \\left(1 + \\frac{x}{n}\\right) &gt; 0\n\\]\nSince \\(\\frac{\\sin \\left(1 + \\frac{x}{n}\\right)}{\\sqrt{n}} &gt; 0\\) and it is decreasing, by Alternating test, it is pointwise convergent.\nFor the uniform convergence,\n\\[\n\\frac{(-1)^n}{\\sqrt{n}} \\sin \\left(1 + \\frac{x}{n}\\right) = \\frac{(-1)^n}{\\sqrt{n}} \\left( \\sin1\\cos \\frac{x}{n} + \\cos1\\sin \\frac{x}{n}\\right)\n\\]\nNow divide this into two part:\nFor \\(\\cos 1 \\cdot \\frac{(-1)^n}{\\sqrt{n}} \\sin \\frac{x}{n}\\)\nSince \\(|\\sin x| \\leq |x| \\leq \\frac{1}{\\sqrt{n}} \\forall x \\in \\mathbb{R}\\)\n\\[\n-\\frac{(-1)^n |x|}{\\sqrt{n}\\cdot n} \\leq \\frac{(-1)^n}{\\sqrt{n}} \\sin \\frac{x}{n} \\leq \\frac{(-1)^n |x|}{\\sqrt{n}\\cdot n}\n\\]\n\\[\n\\left| \\frac{(-1)^n}{\\sqrt{n}} \\sin \\frac{x}{n} \\right| \\leq \\left| \\frac{(-1)^n}{\\sqrt{n}} \\frac{|x|}{n} \\right| = \\frac{|x|}{n^{3/2}} \\leq \\frac{M}{n^{3/2}}\n\\]\nBy P-test, \\(\\sum_{n=1}^{\\infty} \\frac{M}{n^{3/2}}\\) is convergent.\nBy Weierstrass M-test for series,\n\\[\n\\sum_{n=1}^{\\infty} \\frac{(-1)^n}{\\sqrt{n}} \\sin \\frac{x}{n} \\text{ converges uniformly.}\n\\]\nFor \\(\\frac{(-1)^n}{\\sqrt{n}} \\cos \\frac{x}{n}\\).\n\\[\n\\frac{(-1)^n}{\\sqrt{n}} \\cos \\frac{x}{n} = \\frac{(-1)^n}{\\sqrt{n}} \\left( 1 - 2 \\sin^2 \\frac{x}{2n} \\right)\\\\= \\frac{(-1)^n}{\\sqrt{n}} - \\frac{2(-1)^n}{\\sqrt{n}} \\sin^2 \\frac{x}{2n}\n\\]\nFor \\(\\frac{(-1)^n}{\\sqrt{n}}\\), since it is not relevant to x, we only need to consider the second part.\nFor \\(\\frac{(-1)^n}{\\sqrt{n}} \\sin^2 \\frac{x}{2n}\\)\n\\[\n\\left| \\frac{(-1)^n}{\\sqrt{n}} \\sin^2 \\frac{x}{2n} \\right| \\leq \\frac{1}{\\sqrt{n}} \\left| \\frac{x}{2n} \\right|^2\n\\]\nBy p-test,\\(\\sum_{n=1}^{\\infty}\\frac{1}{\\sqrt{n}} \\left| \\frac{x}{2n} \\right|^2\\) is convergent.\nBy Weierstrass M-test for series,\n\\(\\frac{(-1)^n}{\\sqrt{n}} \\sin^2 \\frac{x}{2n}\\) converges uniformly."
  },
  {
    "objectID": "Mathematical Analysis 1.html#while-the-dream-of-becoming-a-pure-mathematician-has-faded-my-pursuit-of-mathematical-analysis-remains-strong-driven-by-a-desire-to-express-my-thoughts-with-precision-and-elegance.-my-ultimate-aspiration-of-analysis-is-to-present-ideas-that-captivate-others-compelling-them-to-listen.-this-is-not-just-about-sharing-information-its-about-conveying-logical-insights-that-have-been-distilled-through-my-own-deep-engagement-with-the-material-reflecting-a-genuine-and-personal-comprehension-of-the-subject-matter.",
    "href": "Mathematical Analysis 1.html#while-the-dream-of-becoming-a-pure-mathematician-has-faded-my-pursuit-of-mathematical-analysis-remains-strong-driven-by-a-desire-to-express-my-thoughts-with-precision-and-elegance.-my-ultimate-aspiration-of-analysis-is-to-present-ideas-that-captivate-others-compelling-them-to-listen.-this-is-not-just-about-sharing-information-its-about-conveying-logical-insights-that-have-been-distilled-through-my-own-deep-engagement-with-the-material-reflecting-a-genuine-and-personal-comprehension-of-the-subject-matter.",
    "title": "Analysis1 and Analysis 2",
    "section": "While the dream of becoming a pure mathematician has faded, my pursuit of Mathematical Analysis remains strong, driven by a desire to express my thoughts with precision and elegance. My ultimate aspiration of Analysis is to present ideas that captivate others, compelling them to listen. This is not just about sharing information; it’s about conveying logical insights that have been distilled through my own deep engagement with the material, reflecting a genuine and personal comprehension of the subject matter.",
    "text": "While the dream of becoming a pure mathematician has faded, my pursuit of Mathematical Analysis remains strong, driven by a desire to express my thoughts with precision and elegance. My ultimate aspiration of Analysis is to present ideas that captivate others, compelling them to listen. This is not just about sharing information; it’s about conveying logical insights that have been distilled through my own deep engagement with the material, reflecting a genuine and personal comprehension of the subject matter."
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-question-is-not-trivial-we-should-know-where-is-the-difficulty-of-it",
    "href": "Mathematical Analysis 1.html#if-a-question-is-not-trivial-we-should-know-where-is-the-difficulty-of-it",
    "title": "Analysis1 and Analysis 2",
    "section": "If a question is not trivial, we should know where is the difficulty of it",
    "text": "If a question is not trivial, we should know where is the difficulty of it\nHere the difficulty is uncountable\neg. f is a continuous function on [0,1] \\(\\int_0^1 f(x) x^n dx =0\\), prove that f(x)=0 for all x in [0,1].\nwe have \\(\\int_0^1 f(x) [a_nx^n+...+ax+a_0]dx=0\\)\nWe have \\(\\int_0^1 |f(x)|^2 dx=0\\) (Weierstrass Approximation Theorem and Since $ |f(x)|^2$ is not negative, Suppose f(x) is not equal to 0, then we could find a point x0 where f(x0) is not equal to 0, then we could find a small interval around x0 where f(x) is not equal to 0, then this interval’s integral could not be =0, which is a contradiction (the meaning of “continuity” is that if a point is greater than 0, then there is a neighborhood of this point where the function is greater than 0.))\nWe have \\(|f|^2=0\\) a.e. \\(x\\in [0,1]\\)\nSo \\(f(x)=0\\) a.e. \\(x\\in [0,1]\\)\nuncountable is the difficulty of this question (density argument : The closure of Q is R means that \\(\\forall x \\in R\\) \\(\\exists x_n \\in Q\\) such that \\(x_n\\) converges to x)\nSo we use the proof by contradiction:"
  },
  {
    "objectID": "Mathematical Analysis 1.html#epislon-delta-language-is-not-important-the-most-important-thing-is-to-understand-the-essence-of-the-proof-and-the-logic-behind-it.-it-is-better-to-see-everything-intuitively.",
    "href": "Mathematical Analysis 1.html#epislon-delta-language-is-not-important-the-most-important-thing-is-to-understand-the-essence-of-the-proof-and-the-logic-behind-it.-it-is-better-to-see-everything-intuitively.",
    "title": "Analysis1 and Analysis 2",
    "section": "epislon-delta language is not important, the most important thing is to understand the essence of the proof and the logic behind it. It is better to see everything intuitively.",
    "text": "epislon-delta language is not important, the most important thing is to understand the essence of the proof and the logic behind it. It is better to see everything intuitively.\n\neg. The sum of continuous functions\n\n2 functions\nn functions\ninfinite functions —not continuous! –We could not choose the minimum \\(\\delta\\) to make the sum continuous."
  },
  {
    "objectID": "Mathematical Analysis 1.html#easy-to-think-wrongly",
    "href": "Mathematical Analysis 1.html#easy-to-think-wrongly",
    "title": "Analysis1 and Analysis 2",
    "section": "easy to think wrongly",
    "text": "easy to think wrongly\nIf \\(f'(x)\\) exists, \\((f^2)'\\)= \\(2f(x)f'(x)\\) but the counter direction might not be true\n\neg. If \\(f^2\\) is differentialble and \\(f^2 &gt; 0\\), then |f| is differentiable since we could write |f| as some function of f(x) and \\(f^2\\) is differentiable. So we could use the chain rule to show that |f| is differentiable. eg. \\(|f|=e^{1/2lnf^2}\\) or just \\(|f|=\\sqrt {f^2}\\)\neg. If \\(f^2\\) is differentiable, and \\(f^2 &gt; 0\\) \\(f\\) may not differentiable since f itself may NOT continuous at any \\(x\\in \\mathbb{R}\\). f(x) is 1 when x is rational and -1 when x is irrational. f(x) is not continuous at any point in \\(\\mathbb{R}\\), so f(x) is not differentiable at any point in \\(\\mathbb{R}\\)."
  },
  {
    "objectID": "Mathematical Analysis 1.html#integrability",
    "href": "Mathematical Analysis 1.html#integrability",
    "title": "Analysis1 and Analysis 2",
    "section": "Integrability",
    "text": "Integrability\nWhen we consider a continuous functon’s integrability on [a,b], we could divide the function into 2 parts—one is good and the other is bad. For the good part, the uniform continuity guarantes the integrability. For the bad part, we could use the property of boundedness to show that the integral is finite using very thin intervals to cover the bad part. So we could use the property of boundedness to show that the integral is finite using very thin intervals to cover the bad part."
  },
  {
    "objectID": "Mathematical Analysis 1.html#all-ways-to-distinguish-whether-it-is-uniformly-continuous",
    "href": "Mathematical Analysis 1.html#all-ways-to-distinguish-whether-it-is-uniformly-continuous",
    "title": "Analysis1 and Analysis 2",
    "section": "All ways to distinguish whether it is uniformly continuous",
    "text": "All ways to distinguish whether it is uniformly continuous\nplease fill this before your final exam!\n\npractice before my mid-term exam (my way to think that of question is like this)\n\\(f(x) = x^4\\) is not uniformly continuous on \\(\\mathbb{R}\\).\nChoose \\(x_n = \\sqrt[4]{n+1}\\), \\(y_n = \\sqrt[4]{n}\\).\nSince \\(x^{1/4}\\) is differentiable on \\([n, n+1]\\), by Lagrange’s MVT,\n\\[|x_n - y_n| = \\frac{1}{4} \\xi^{-\\frac{3}{4}} (n+1-n), \\text{ where } \\xi \\in (n, n+1)\\]\nSo \\(|x_n - y_n| = \\frac{1}{4} \\xi^{-\\frac{3}{4}} &lt; \\frac{1}{4} n^{-\\frac{3}{4}} = \\frac{1}{4 \\sqrt[4]{n^3}}\\)\nSo \\(0 &lt; |x_n - y_n| &lt; \\frac{1}{4 \\sqrt[4]{n^3}}\\)\nSo \\(\\lim_{n \\to \\infty} |x_n - y_n| = 0\\) by squeeze theorem\nSo for \\(\\varepsilon_0 = \\frac{1}{2}\\), by the definition of limit，\\(\\forall \\delta &gt; 0\\), \\(\\exists m &gt; 0\\) s.t. for \\(n_0\\in \\mathbb{N} &gt; m\\), \\(|x_{n_0} - y_{n_0}| &lt; \\delta\\) but \\(|f(x_{n_0}) - f(y_{n_0})| = 1 &gt; \\varepsilon_0 = \\frac{1}{2}\\),\nThis satisfies the negation of the definition of “uniformly continuous”."
  },
  {
    "objectID": "Mathematical Analysis 1.html#eg-of-an-interesting-function",
    "href": "Mathematical Analysis 1.html#eg-of-an-interesting-function",
    "title": "Analysis1 and Analysis 2",
    "section": "eg of an interesting function",
    "text": "eg of an interesting function\n\\[f_n(x)=nx^n(1-x^2)^n\\]\nThis function has a pointwise limit of 0 by squeeze theorem and L’Hospital’s theorem.\nIt is easy to check the change of its limit and integral is not the same, i.e. \\(lim_n \\int_0^1 f_n(x)\\) is not equal to \\(\\int_0^1 lim_n f_n(x)\\), so it is not uniformly convergent.\nIf we look at the limit of its supremum, we could find that the supremum of this function is \\(\\infty\\) when n is large enough. So we could use the supremum to show that the limit of this function is not uniformly convergent. This is not trivial to understand since its pointwise limit is 0 but the supremum is \\(\\infty\\). That is because when n is very large, there is a very thin but tall peak, which makes the supremum of this function to be \\(\\infty\\).\nOr we also firstly draw this picture and find intuitively that its supremum is \\(\\infty\\) when n is large enough. So we could use the supremum to show that the limit of this function is not uniformly convergent.\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Define the function\nf_n &lt;- function(x, n) {\n  return(n * x^n * (1 - x^2)^n)\n}\n\n# Set the value of n\nn &lt;- 100\n\n# Create a sequence of x values from -1 to 1\nx_values &lt;- seq(0, 1, length.out = 1000)\n\n# Compute the corresponding y values\ny_values &lt;- sapply(x_values, f_n, n = n)\n\n# Create a data frame for ggplot\ndata &lt;- data.frame(x = x_values, y = y_values)\n\n# Plot the function using ggplot2\nggplot(data, aes(x = x, y = y)) +\n  geom_line(color = \"blue\") +\n  labs(title = paste(\"Plot of f_n(x) = nx^n(1 - x^2)^n for n =\", n),\n       x = \"x\",\n       y = \"f_n(x)\") +\n  theme_minimal()"
  },
  {
    "objectID": "Mathematical Analysis 1.html#existence-means-beyond-any-human-efforts-and-objectively-existence.-we-take-the-value-that-we-need-means-a-human-effort-and-because-it-is-arbitrary-we-have-the-freedom-to-choose-one.-dr.-zhang",
    "href": "Mathematical Analysis 1.html#existence-means-beyond-any-human-efforts-and-objectively-existence.-we-take-the-value-that-we-need-means-a-human-effort-and-because-it-is-arbitrary-we-have-the-freedom-to-choose-one.-dr.-zhang",
    "title": "Analysis1 and Analysis 2",
    "section": "‘Existence’ means beyond any human efforts, and objectively existence. We take the value that we need means a human effort, and because it is arbitrary, we have the freedom to choose one. — Dr. Zhang",
    "text": "‘Existence’ means beyond any human efforts, and objectively existence. We take the value that we need means a human effort, and because it is arbitrary, we have the freedom to choose one. — Dr. Zhang"
  },
  {
    "objectID": "Mathematical Analysis 1.html#theorem",
    "href": "Mathematical Analysis 1.html#theorem",
    "title": "Analysis1 and Analysis 2",
    "section": "Theorem",
    "text": "Theorem\nLet \\(O\\subset R\\) be an open set then \\(O=\\cup_{n=1}^\\infty I_n\\) where \\(I_n\\) is an open interval in R"
  },
  {
    "objectID": "Mathematical Analysis 1.html#def",
    "href": "Mathematical Analysis 1.html#def",
    "title": "Analysis1 and Analysis 2",
    "section": "Def",
    "text": "Def\nA collection of open sets covers a set A if \\(A\\subset \\cup O_\\alpha\\). The collection {\\(O_\\alpha\\)} is called an open cover of A."
  },
  {
    "objectID": "Mathematical Analysis 1.html#theorem-1",
    "href": "Mathematical Analysis 1.html#theorem-1",
    "title": "Analysis1 and Analysis 2",
    "section": "Theorem",
    "text": "Theorem\nLet C={\\(O_\\alpha\\)} be a collection of open sets of real numbers then there is a countable subcollection {\\(O_i\\)} of C such that \\(\\cup_{O\\in C}O=\\cup_{n=1}^\\infty O_i\\)\nAny open cover of a set of real numbers contains a countable subcover."
  },
  {
    "objectID": "Mathematical Analysis 1.html#i-have-proved-that",
    "href": "Mathematical Analysis 1.html#i-have-proved-that",
    "title": "Analysis1 and Analysis 2",
    "section": "i have proved that:",
    "text": "i have proved that:\nthe closure of \\(Q\\) = R &lt;–&gt; \\(R\\approx Q\\) &lt;–&gt; \\(\\forall x\\in R, \\exists x_n\\in Q\\) such that\\(x_n--&gt;x\\). Q is a countable, dense subset of R, R is separable."
  },
  {
    "objectID": "Mathematical Analysis 1.html#albel-guass-is-a-fox-走过的地方狐狸尾巴扫掉了",
    "href": "Mathematical Analysis 1.html#albel-guass-is-a-fox-走过的地方狐狸尾巴扫掉了",
    "title": "Analysis1 and Analysis 2",
    "section": "albel: guass is a fox, 走过的地方狐狸尾巴扫掉了",
    "text": "albel: guass is a fox, 走过的地方狐狸尾巴扫掉了"
  },
  {
    "objectID": "Mathematical Analysis 1.html#algebra-of-continuity",
    "href": "Mathematical Analysis 1.html#algebra-of-continuity",
    "title": "Analysis1 and Analysis 2",
    "section": "algebra of continuity",
    "text": "algebra of continuity\n\\(\\exists \\delta = min\\){\\(\\delta_1,\\delta_2\\)}"
  },
  {
    "objectID": "Mathematical Analysis 1.html#history",
    "href": "Mathematical Analysis 1.html#history",
    "title": "Analysis1 and Analysis 2",
    "section": "history",
    "text": "history\ncauchy 1821(decrease indefinitely with those of \\(\\alpha\\))–weierstrass 1874 (\\(\\epsilon-\\delta\\))—–"
  },
  {
    "objectID": "Mathematical Analysis 1.html#understanding-in-a-long-process",
    "href": "Mathematical Analysis 1.html#understanding-in-a-long-process",
    "title": "Analysis1 and Analysis 2",
    "section": "understanding in a long process",
    "text": "understanding in a long process\n\n\\(\\delta\\) decides how good the continuity of the function is."
  },
  {
    "objectID": "Mathematical Analysis 1.html#cauchy-1822",
    "href": "Mathematical Analysis 1.html#cauchy-1822",
    "title": "Analysis1 and Analysis 2",
    "section": "Cauchy 1822",
    "text": "Cauchy 1822\nWhat is a limit?–number\nUntil 1870, the answer to “What is the number” had been known.\n(这样来看，学习数分的一年痛苦就不算什么)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#ordercardinality",
    "href": "Mathematical Analysis 1.html#ordercardinality",
    "title": "Analysis1 and Analysis 2",
    "section": "order–cardinality",
    "text": "order–cardinality\n1，2，3，n 2，4，6，2n\nf(n)=2n\n\nexplaination：无穷大加一个数还是等于无穷大\n\n\\(f:N-&gt;N\\cup\\){\\(a_1,a_2,a_3\\)}\nf(1)=\\(a_1\\) f(2)=\\(a_2\\),..f(4)=1, f(n+3)=n, this function is one-to-one – bijection\n–无穷大加上任意一个实属x还是无穷大\nN x N is countable\n\\(S \\subset N\\) S is countable\nf(m,n)–&gt;\\(2^m3^n\\)（只要取互质的两个数就ok）\nN x N–&gt; N\nf: 1-1\n\\(2^{m_1}3^{n_1}\\)=\\(2^{m_2}3^{n_2}\\)—\\(m_1=m_2,n_1=n_2\\)\nf(N x N)\\(\\subset\\) N\n只要映射到N的subset就可以了\n2维3维n维\n(0,1) is uncountable 反证法\nCantor’s diagonal process\n欧几里得是第一个反证法—有无穷多个质数\n{p1 p2 p3…}\np=p1+p2+pn+…+1\np1|p…pn|p—p is a prime—contradiction to countable\n(0,1)={x1,…x} countable\nx1=01a11a12a13. a1n\\(\\in\\){0,1,2,…9} x2=01a21a22a23. a2n\\(\\in\\){0,1,2,…9}\nconstruct a real number\ny=0.b1b2b3…\nb1 budengyu a11 y budengyu x1\nb2 budengyu a22 y budengyu x2\nb3 budengyu ann y budengyu xn\nbn in {a1,…a}–&gt; y\\(\\in\\)(0,1) but y\\(\\notin\\){\\(x_1,...x_n\\)}\nEx: [0,1) 约等于 (0,1)\nproof: Let{p1,p2,….}=(0,1)\\(\\cap Q\\) 分成有理数和无理数\nf(0)=p1 f(p1)=p2 f(pn+1)=pn f(x)=x \\(x\\in (0,1)-\\){p1,..pn}\n概率 特征函数\nCantor set\n进位的概念"
  },
  {
    "objectID": "Mathematical Analysis 1.html#section",
    "href": "Mathematical Analysis 1.html#section",
    "title": "Analysis1 and Analysis 2",
    "section": "",
    "text": "导数 积分 无穷级数 –都是极限 极限是什么–数数是什么 – Augustin Cauchy(1789-1846)\n\n人文素养\n\ncantor 1872年是不平凡的一年， 有（Weierstrass, Dedekind, Meray, Heine, Cantor）同时提出了实属系统建构的理解 出版著作解决困惑人们2500多年的问题\nWeierstrass 法律 数学 喝酒 干架 退学 在中学里边仍然没有与学术脱节 复变函数 椭圆函数（研究在复数平面上的解析函数， C=\\(R^2\\) a+ib–&gt;(a,b) is an isomorphism） \\(C \\cup {\\infty}\\) 相似于球面 \\(S^2\\) 约等于 \\(R^2\\cup \\infty\\) 变成closed and bounded one-point- compactfication 为什么实数不bounded因为无穷 无穷大这点不是实数（不满足实数性质—无穷大加无穷大还是无穷大；无穷大减无穷大是any number）因为\n任何单调有界的实属序列都有极限存在—dedekind的假设\ndedekind cut\nif rational number\n毕达哥拉斯的年代的万物皆数是有理数\n古希腊哲学 古希腊数学\nThales\n做测量工作创出非欧几何这门学问 真正的天才是能够创出一门学问的\n牛顿是科学第一人 当人们迷迷糊糊的时候 他打开理性的光\n这一切是黑暗的 上帝说让牛顿去吧 于是就有了光\n改编圣经的话–文采\n毕达哥拉斯定理的证明都是非常有创意的东西\n毕达哥拉斯定理十个证明\n读慢一点\n心浮气躁\n数学是需要耐心的\n一个城市的发展不应该是盖房子 应该是好的博物馆音乐厅公园 提高人民素养\nvolume 2 多变数 stolze定理 divergent定理\nlimit sup limit inf –这个东西就是我的实数\nCantor-==柯西序列。距离概念\n\\(|a_m-a_n|--&gt;0\\). d(\\(a_m,a_n\\)–&gt;0) metric\nequivalent equation equivalent class 实数 实数是一个集合\ncantor–集合论\n定义是最难的\n有共通之处–作为定义\n会用定理很重要 不是高斯\n用多了就知道是怎么一回事\n定理给了条件 哪里用到什么条件 怎么用的\n碰到好老师的重要性 姜立夫 量纲自然\ncomplete的完备不是完美 不是 门掉了一个螺丝 找一个螺丝完完全全放上去发挥作用\n认识自己 我是螺丝 完完全全发挥我的作用 把长处 恩赐发挥出来 就是complete\n有理数在实数中有很多孔隙 把空隙。把accumulation point收紧来\nclosure of A = B\n\\(\\forall b\\in B, \\exists\\){\\(a_n\\)} \\(\\subset A\\) such that \\(a_n--&gt;b\\)\n$AB $ not equals to $&gt; 0 $\n实数是唯一一个complete ordered field\n代数结构—分配律\neg.complex number没有order所以我们定义norm\n\n和闭区间套定理极限思想的联系和区别？？？"
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-is-an-infinite-subset-of-n-then-a-ni.e.-a-is-countably-infinite.",
    "href": "Mathematical Analysis 1.html#if-a-is-an-infinite-subset-of-n-then-a-ni.e.-a-is-countably-infinite.",
    "title": "Analysis1 and Analysis 2",
    "section": "If A is an infinite subset of N, then |A| = |N|,i.e. A is countably infinite.",
    "text": "If A is an infinite subset of N, then |A| = |N|,i.e. A is countably infinite.\n\n\nin fact， N could be any countable set"
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-not-empty-set-has-upper-bound-then-it-must-have-the-unique-sup.",
    "href": "Mathematical Analysis 1.html#if-a-not-empty-set-has-upper-bound-then-it-must-have-the-unique-sup.",
    "title": "Analysis1 and Analysis 2",
    "section": "if a not empty set has upper bound, then it must have the unique sup.",
    "text": "if a not empty set has upper bound, then it must have the unique sup.\nProve: if a not empty set has upper bound, then it must have the unique sup.\n\nMethod 1: (Completeness axiom)\n\\[\n\\begin{gathered}\nX \\neq \\phi, \\quad Y=\\{y \\in \\mathbb{R} \\mid \\forall x \\in X(x \\leq y)\\} \\neq \\phi \\\\\n\\\\\n\\forall x \\in X, \\forall y \\in Y, x \\leq y \\\\\n\\\\\n\\Rightarrow \\exists c \\in \\mathbb R, \\forall x \\in X, \\forall y \\in Y, \\\\\n\\\\\nx \\leqslant c \\leqslant y(\\text { Completeness axiom) } \\\\\n\\\\\n\\Rightarrow(c \\in Y) \\wedge(\\{\\forall y \\in Y \\mid y \\geqslant c\\}) \\\\\n\\\\\n\\Rightarrow c=\\min Y\n\\end{gathered}\n\\]\nSo \\(c\\) is the only sup. (uniqueness of minimum element of a set—2 inequalities lead to the equality leading to the only one result).\n\n\nMethod 2: Cantor Nested Interval Property(limit思想).asdfaskfkasdjklsad\nWe choose a random upper bound \\(\\gamma, x \\in E(\\) the set) \\[\n[x, r]=\\left[a_1 b_1\\right] \\supset\\left[a_2, b_2\\right] \\supset \\cdots \\left[a_n, b_n\\right]\n\\] (each time we use the method of bisection to choose one side Including the point in \\(E\\) ) $$\n\\[\\begin{aligned}\n& \\text { Since }\\left[a_1, b_1\\right] \\supset\\left[a_2, b_2\\right] \\cdots, b_n-a_n=\\frac{\\gamma-x}{2^{n-1}} \\rightarrow 0, \\\\\n& \\beta \\in\\left[a_n, b_n\\right],(n=1,2, \\cdots), \\lim _{n \\rightarrow \\infty} a_n=\\lim _{n \\rightarrow \\infty} b_n=\\beta \\text {(limit thinking of the Cantor Nested Interval) } \\\\\n& \\Rightarrow \\forall c \\in E, c \\leqslant b_n \\Rightarrow c \\leqslant \\beta \\text { (E }  \\text {is never on the right of} [a_n,b_n]) \\\\\n& \\Rightarrow \\forall \\varepsilon&gt;0, \\exists d \\in E, d&gt;\\beta-\\varepsilon\\text { (each} [a_n,b_n] \\text { has points in E.(or see the attached picture to see more picisely))}\n\\end{aligned}\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-not-empty-set-has-lower-bound-then-it-must-have-the-unique-inf.",
    "href": "Mathematical Analysis 1.html#if-a-not-empty-set-has-lower-bound-then-it-must-have-the-unique-inf.",
    "title": "Analysis1 and Analysis 2",
    "section": "If a not empty set has lower bound, then it must have the unique inf.",
    "text": "If a not empty set has lower bound, then it must have the unique inf.\n\nMethod 1: same as before.\n\n\nMethod 2: Based on before.\nSuppose We choose \\(m\\) as a lower bound of \\(E\\). \\[\n\\begin{aligned}\n& \\Rightarrow \\forall x \\in E, x \\geqslant m,-x \\leqslant-m \\text {. } \\\\\n& \\text { Let } F=\\{-x \\mid x \\in E\\} \\text {. } \\\\\n& \\Rightarrow \\beta=\\sup F \\\\\n& \\Rightarrow-x \\leqslant \\beta, x \\geqslant-\\beta \\text {. } \\\\\n& \\forall \\varepsilon&gt;0, \\exists-d \\in E,-d&gt;\\beta-\\varepsilon, \\\\\n& \\Rightarrow d&lt;-\\beta+\\varepsilon \\\\\n& \\Rightarrow-\\beta=\\inf E\n\\end{aligned}\n\\] \\[\n\\Rightarrow-\\sup (-E)=\\operatorname{inf} E \\text {. }\n\\]\n\n\nMethod 3 more generalized than Method 2’s conclusion (if c is negative then inf(cA)=csupA)\nIn fact, If \\(c&lt;0\\), then \\(\\sup (cA)=\\operatorname{cinf} A, \\inf (c A)=\\operatorname{csup} A\\). (and in particular. sup \\((-B)=-\\) inf \\(B\\) )\nSince if M=supA, \\(\\forall x\\in A,x\\leq M,cx\\geq cM\\), which indicates that cM is the lower bound of cX.\nSo, \\(cA\\) has lower bound ( \\(s\\) )if and only if \\(A\\) has upper bound(s).(inverse, also true) \\(cA\\) is not empty if and only if A is not empty.\nSo, cA has \\(\\inf (cA)\\) if and only if \\(A\\) has supA \\[\n\\begin{aligned}\n& \\forall c x \\in C A, c x \\geqslant c M \\\\\n& \\text { if } \\exists c M', \\forall c x \\in c A, c x \\geqslant c M^{\\prime}, c M^{\\prime}&gt;c M, \\\\\n& X \\leq M^{\\prime}, M^{\\prime}&lt;M(\\forall x \\in A)\n\\end{aligned}\n\\]\nContradicting to the condition that \\(M=\\sup A\\)\nSo \\(\\nexists C M^{\\prime}\\) So cM is the largest lower bound \\[\n\\begin{aligned}\n& \\text { so } c M=\\inf (c A) \\\\\n& \\text { so } c\\sup A=\\inf (c A)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html#exercise-12s-sup",
    "href": "Mathematical Analysis 1.html#exercise-12s-sup",
    "title": "Analysis1 and Analysis 2",
    "section": "exercise: (1,2]’s sup–",
    "text": "exercise: (1,2]’s sup–\n\nMethod 1\n\n2 is an upper bound of [1,2) (obviously)\nif \\(\\exists \\varepsilon&gt;0\\), s.t \\(2-\\varepsilon\\) is also an upper bound of \\([1,2)\\) \\[\n\\begin{aligned}\n& \\because 1 \\in[1,2) \\\\\n& \\therefore2-\\varepsilon \\geqslant 1\n\\end{aligned}\n\\] choose \\(2-\\frac{\\varepsilon}{2} \\in(2-\\varepsilon, 2)\\) Then \\(2-\\frac{\\varepsilon}{2} \\in[1,2)\\) So \\(\\exists\\left(2-\\frac{\\varepsilon}{2}\\right) \\in[1,2)\\) while \\(\\left(2-\\frac{\\varepsilon}{2}\\right)&gt;(2-\\varepsilon)\\) So \\(2-\\varepsilon\\) is not an upper bound, contradicting to the suppose. So we have proved that 2 is the smallest upper bound, i.e. \\(\\sup [1,2)=2\\).\n\n\n\nMethod 2\n\n2 is an upper bound of [1,2) (obviously)\n\\(\\forall \\varepsilon&gt;0, \\exists b \\in[1,2)\\) with \\(b&gt;2-\\varepsilon\\). We can take \\(b=\\max \\left\\{2-\\frac{\\varepsilon}{2}, 1\\right\\}\\)."
  },
  {
    "objectID": "Mathematical Analysis 1.html#exercise-supcsupabsupasupb-from-professor-a-kun",
    "href": "Mathematical Analysis 1.html#exercise-supcsupabsupasupb-from-professor-a-kun",
    "title": "Analysis1 and Analysis 2",
    "section": "exercise: supC=sup(A+B)=supA+supB from Professor A Kun",
    "text": "exercise: supC=sup(A+B)=supA+supB from Professor A Kun\n\\[\n\\begin{array}{rl}\n\\text { if } A\\subset R,  B\\subset R, \\text { define: } \\\\\nC  :=A+B=\\{z \\in R: z=x+y, x \\in A, y \\in B\\} \\\\\nD  :=A-B=\\{z \\in R: z=x-y, x \\in A, y \\in B\\}\n\\end{array}\n\\] show that \\[\n\\begin{aligned}\n& \\text { (1) } \\sup C=\\sup (A+B)=\\sup A+\\sup B \\\\\n& \\text { (2) } \\sup D=\\sup (A-B)=\\sup A-\\inf B\n\\end{aligned}\n\\] (1) Proof:: Obviously,\\(C\\) has upper bounds if and only if \\(A\\) and \\(B\\) have upper bounds, and \\(C\\) is not empty. So \\(C\\) has sup C if and only if and only if \\(A\\) has sup \\(A\\) and \\(B\\) has sup \\(B\\). (Completeness axiom).\n\nprove: \\(\\sup C \\leqslant \\sin A+\\sup B\\). \\[\n\\because x+y \\leqslant \\sup A+\\sup  B\n\\] \\(\\therefore(\\operatorname{sip} A+\\sup B)\\) is an upper bound of \\(C\\) \\[\n\\therefore \\sin C \\leqslant \\sin A+\\sup B\n\\]\nProve : \\(\\sup C \\geqslant \\operatorname{supA}+\\) sup B \\[\n\\begin{aligned}\n& \\because \\forall \\varepsilon&gt;0, \\exists x \\in A, y \\in B \\text {, s.t. } \\\\\n& \\operatorname{sup} A-\\varepsilon&lt;x, \\operatorname{sup} B-\\varepsilon&lt;y . \\\\\n& \\Rightarrow \\sup A+\\sup B-2 \\varepsilon&lt;x+y \\\\\n& \\text { i.e. }(\\operatorname{sup} A+\\sup B-2 \\varepsilon)_{\\text {max }}&lt;(x+y)_{\\text {max }}\n\\end{aligned}\n\\]\n\nSince \\(x+y \\leq\\) sup c We have \\(\\sup A+\\sup B-2 \\varepsilon&lt;\\sup c, \\forall \\varepsilon&gt;0\\) \\[\n\\begin{aligned}\n& \\text { i.e. } \\operatorname{(sup} A+\\sup B-2 \\varepsilon)_{\\text {max }}&lt;\\sup C \\\\\n\\end{aligned}\n\\]\nWe. As \\(\\varepsilon \\rightarrow 0\\), We have \\(\\operatorname{sup} A+\\sup B \\leqslant \\operatorname{} \\operatorname{sup} C\\) So, we have \\(\\operatorname{Sup} C=\\operatorname{Sup} A+\\operatorname{Sup} B\\). Then, we have \\(\\sup D=\\sup (A-B)\\) \\[\n\\begin{aligned}\n& =\\sup (A+(-B)) \\\\\n& =\\sup A+\\sup (-B) \\\\\n& =\\sup A-\\inf B\n\\end{aligned}\n\\] (We have proved \\(\\sup (-B)=-\\inf B\\) before)\n\nanother example\n Let M ∈ R and A, B be two bounded, negative subsets of R,0 &lt; x, y &lt; M, ∀x ∈ A, y ∈ B\nwhen we are proving sup C = sup(AB) = sup A · sup B\nOn the other hand, from the definition of $a^*$ and $b^*, \\forall \\epsilon&gt;0$ there exist $a \\in A$ and $b \\in B$ such that\n\\[\na^*-\\epsilon&lt;a&lt;a^* \\quad \\text { and } \\quad b^*-\\epsilon&lt;b&lt;b^*\n\\]\nThen \\[\n\\left(a^*-\\epsilon\\right)\\left(b^*-\\epsilon\\right)&lt;a b \\leqslant a^* b^*\n\\] or ignoring \\(\\epsilon^2\\) term (If ε &gt; 0, then ε, 3ε, ε², ε⁵, they all represent “any number greater than zero”, and ε’ also represents any number greater than zero, so they are equivalent, that is, we can say that they are equal to ε’.) \\[\na^* b^*-(a+b) \\epsilon=a^* b^*-\\epsilon^{\\prime}&lt;a b \\leq c^*\n\\]\nThis is true for all \\(\\epsilon^{\\prime}&gt;0\\), so \\[\n\\sup A \\sup B=a^* b^* \\leq c^*=\\sup C\n\\]\nCombining the above two inequalities, we can conclude that \\(\\sup A \\sup B=\\) \\(\\sup C\\). ## an exercise about think good Archimedean number from Professor Andrew Lin(A Kun)\n\nConsider the set \\[\nA=\\left\\{\\left.(-1)^n\\left(1-\\frac{1}{n}\\right) \\right\\rvert\\, n \\in \\mathbb{Z}^{+}\\right\\} .\n\\]\n\n\nShow that 1 is an upper bound for \\(A\\).\nShow that if \\(d\\) is an upper bound for \\(A\\), then \\(d \\geq 1\\).\nUse (a) and (b) to show that \\(\\sup A=1\\). [Solution]:\nWe will show that for any \\(x \\in A, x \\leq 1\\). Since \\(x \\in A\\), then \\(x=(-1)^n(1-\\) \\(1 / n\\) ) for some \\(n \\in \\mathbb{Z}^{+}\\). Since \\(\\frac{1}{n}&gt;0\\), then \\(1-\\frac{1}{n}&lt;1\\). We argue our desired inequality in two cases. If \\(n\\) is even, then \\(x=(-1)^n(1-1 / n)=1-1 / n&lt;1\\). If \\(n\\) is odd, then \\(x=(-1)^n(1-1 / n)=-1+1 / n&lt;0&lt;1\\). In either case, \\(x \\leq 1\\) (in fact, \\(x&lt;1\\) ) and 1 is an upper bound for \\(A\\).\nLet \\(d\\) be an upper bound for \\(A\\). Thus, \\((-1)^n(1-1 / n) \\leq d\\) for all \\(n \\in \\mathbb{Z}^{+}\\). Assume, to the contrary that \\(d&lt;1\\). Thus, \\(1-d&gt;0\\). By the Archimedean Property, there exists an \\(n \\in \\mathbb{Z}^{+}\\)such that \\(1&lt;(1-d) n\\). Since \\(n&gt;0\\), we can rewrite this as \\(\\frac{1}{n}&lt;1-d\\), which is equivalent to \\(d&lt;1-\\frac{1}{n}\\). If \\(n\\) is even, then \\((-1)^n=1\\) and we have that \\[\nd&lt;(-1)^n\\left(1-\\frac{1}{n}\\right) \\in A\n\\] contradicting the fact that \\(d\\) is an upper bound. If \\(n\\) is odd, then consider instead \\(n+1\\), which is even. Then, \\((-1)^{n+1}=1\\) and \\[\nd&lt;1-\\frac{1}{n}&lt;(-1)^{n+1}\\left(1-\\frac{1}{n+1}\\right) \\in A\n\\]\n\nThis again contradicts that \\(d\\) is an upper bound for \\(A\\). Either way, we reach a contradiction and therefore conclude that \\(d \\geq 1\\).\n\nobviously\n\nlower bound + inequaility —smallest upper bound—sup"
  },
  {
    "objectID": "Mathematical Analysis 1.html#another-exercise-about-another-episilon-from-professor-a-kun",
    "href": "Mathematical Analysis 1.html#another-exercise-about-another-episilon-from-professor-a-kun",
    "title": "Analysis1 and Analysis 2",
    "section": "another exercise about another episilon from Professor A Kun",
    "text": "another exercise about another episilon from Professor A Kun\n\nFind the least upper bound for the following set and \\[\nA=\\left\\{\\frac{1}{2}, \\frac{2}{3}, \\frac{3}{4}, \\cdots, \\frac{n}{n+1}, \\cdots\\right\\}\n\\] [Solution]: We note that every element of \\(A\\) is less than 1 since \\[\n\\frac{n}{n+1}&lt;1, \\quad n=1,2,3, \\cdots\n\\]\n\nWe claim that the least upper bound is \\(1, \\sup A=1\\). Assume that 1 is not the least upper bound. Then there is an \\(\\epsilon&gt;0\\) such that \\(1-\\epsilon\\) is also an upper bound. However, we claim that there is a natural number \\(n\\) such that \\[\n1-\\epsilon&lt;\\frac{n}{n+1}\n\\]\nThis inequality is equivalent to the following sequence of inequalities \\[\n1-\\frac{n}{n+1}&lt;\\epsilon \\quad \\Longleftrightarrow \\quad \\frac{1}{\\epsilon}-1&lt;n\n\\]\nReversing the above sequence of inequalities shows that if \\(n&gt;\\frac{1}{\\epsilon}-1\\), then \\(1-\\epsilon&lt;\\frac{n}{n+1}\\) showing that \\(1-\\epsilon\\) is not an upper bound for \\(A\\).\n(if n &gt; 1/ \\(\\epsilon\\)-1, \\(\\nexists \\epsilon\\) s.t. 1-\\(\\epsilon\\) is an upper bound, contradicting to our suppose)\nThis verifies our answer."
  },
  {
    "objectID": "Mathematical Analysis 1.html#prove-cantor-nested-interval-property",
    "href": "Mathematical Analysis 1.html#prove-cantor-nested-interval-property",
    "title": "Analysis1 and Analysis 2",
    "section": "Prove Cantor Nested Interval Property",
    "text": "Prove Cantor Nested Interval Property\nProve:\n\n\\(\\exists c \\in\\) all closed internals\n\ni.e. if the non-empty closed intervals \\(I_1 \\supset I_2 \\supset I_3 \\cdots, \\exists c \\in R\\), s.t. \\(c\\in I_i, \\forall i \\in \\mathbb{N}\\), \\(c \\in\\) \\(\\bigcap_{i=1}^{\\infty} I_i\\)\n\nIf the limit of the lengths of these intervals are 0 then the point is unique\n\ni.e. if\\(\\left|I_n\\right| \\rightarrow 0\\), c is unique\n\n\n\\[\n\\forall \\varepsilon&gt;0, \\exists I_n(|I_n| &lt; \\varepsilon) \\Rightarrow c \\text { ! }\n\\]\nProof:\n\nproof of 1\n\n\\(I_1=[a_1, b_1]\\)\nclaim \\(\\forall I_n=[a_n, b_n], I_m=[a_m, b_m]\\) i.e. \\(a_n \\leq b_m\\)\n(if \\(a_n&gt;b_m\\), then \\(a_m \\leq b_m&lt;a_n&lt;b_n\\), which means they are separate internals without any intersection.) \\[\n\\begin{aligned}\n&\\text { Let } X=\\left\\{a_n\\right\\} (\\text { left endpoint set) } \\\\\n&\\text { Let } Y=\\left\\{b_m\\right\\}(\\text { right endpoint set) } \\\\\n& \\forall a_n \\in X, \\forall b_m \\in Y, a_n \\leq c\\leq b_m\n\\end{aligned}\n\\] \\(\\Rightarrow \\exists c \\in \\mathbb{R}\\), s.t., \\(\\forall a_n \\in X\\) \\(\\forall b_m \\in Y,a_n\\leqslant b_m\\).(completeness axiom) We then let \\(m=n \\Rightarrow c \\in I_n\\)\n\nproof of 2\n\nBased on “Any implication is equivalent to its contrapositive”\n\\[\n\\begin{aligned}\n& \\text { if } \\exists c_1&lt;c_2, \\text { s.t. } c_1, c_2 \\in I_n, \\\\\n& \\forall n \\in \\mathbb{R}, a_n \\leqslant c_1&lt;c_2 \\leqslant b_n, \\\\\n& \\Rightarrow\\left|I_n\\right|=b_n-a_n \\geqslant c_2-c_1\n\\end{aligned}\n\\]\nWe then choose \\(\\varepsilon=\\frac{c_2-c_1}{2}&gt;0\\)\nthen there exists no \\(I_n\\) s.t. \\(\\left|I_n\\right|&lt;\\varepsilon\\), since\n\\(|I_n|\\geqslant c_2-c_1 \\geqslant \\frac{c_2-c_1}{2}\\)\nSo if \\(\\exists I_n s. t .\\left|I_n\\right|&lt;\\varepsilon\\), \\(c!\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#prove-finite-covering-lemma",
    "href": "Mathematical Analysis 1.html#prove-finite-covering-lemma",
    "title": "Analysis1 and Analysis 2",
    "section": "prove finite covering lemma",
    "text": "prove finite covering lemma\nProve：Finite Covering Lemma \\[\n\\begin{gathered}\n\\text { Def: } S:=\\{X\\}, \\text { ( } x \\text { is a set) } \\\\\nY \\subset \\bigcup_{X \\in S} X\n\\end{gathered}\n\\]\nWe say. \\(S\\) is a cover of \\(Y\\). \\[\n\\text { i.e. } \\forall y \\in Y, \\exists X \\in S,(y \\in X)\n\\]\nFinite Covering Lemma:\nIf \\(I:[a, b]\\), \\[\nI \\subset \\bigcup_{n \\in I} U_n, U_n=\\left(\\alpha_n, \\beta_n\\right)\n\\] \\(\\exists U_1, \\cdots U_k\\), s.t., \\(I\\subset \\bigcup_{i=1}^k U_i\\)\n（Summray of Finite Covering Lemma: A closed internal can be covered by finite number of open internals）\nproof:\nsuppose \\(I=[a, b]\\) could not be coverd by finite number open intervals:\nThen we use the method of bisection to separate I,s.t.\n\\[\n\\begin{aligned}\n& I=I_1 \\supset I_2 \\supset I_3 \\cdots I_n \\supset \\cdots \\text { ( all In can not be covered } \\\\\n& \\left|I_n\\right|=\\frac{b-a}{2^n} \\rightarrow 0 \\\\\n& \\text { Gover) } \\\\\n& \\Rightarrow \\exists: C \\in \\bigcap_{i=1}^{\\infty} I_i \\text { (Cantor Nested Interral Property) } \\\\\n& c_i\\in[a_i,b_i]\\\\\n& \\Rightarrow C\\in I\\subset \\bigcup U_i\\text {(based on the given condition of the proof problem)}\\\\\n& \\Rightarrow \\exists U=[\\alpha,\\beta],s.t.c\\in U，let ：\\varepsilon =min[c-\\alpha,\\beta-c]\\\\\n& \\Rightarrow I_n\\subset U\n\\end{aligned}\n\\]\n\nwhich indicates that at least this \\(I_n\\) is covered by U, contradicting to the suppose.\nso the finite covering lemma is true."
  },
  {
    "objectID": "Mathematical Analysis 1.html#statement",
    "href": "Mathematical Analysis 1.html#statement",
    "title": "Analysis1 and Analysis 2",
    "section": "statement",
    "text": "statement\np\n\n—An assertion that is either true or false but not both\ne.g.\n\nNegation of a statement p is a statement which means the opposite of P\n~p—-read “not p”\n\n\nQuantifiers\nall, every, each,no(none)—universal quantifiers\nsome,there exists, there is at least on, etc.—existential quantifiers\nP: some a’s are b’s\n~P: all a’s are not b’s/ no a’s are b’s\nP: some a are not b\n~p: all a are b\n\n\nTruth table: give the truth values of related statements in all possible cases\n(relevant to boolean in computer science)\n# Example of boolean logic in a program\nis_raining = True\nhas_umbrella = False\n\nif is_raining and not has_umbrella: # which is the only situation that implication fails\n    print(\"You need an umbrella!\")\nelse:\n    print(\"You're good to go!\")\ncompound statement: combining several statements via logical operations\nConjunction: p^q. p and q\nDisjunction: p v q–p or q\none of them is true, p v q is true, so we only need to decide if oe of them is true, if it is, the p v q is true\np V (~p) is always true\na statement that is always true is called a tautology"
  },
  {
    "objectID": "Mathematical Analysis 1.html#p101.4",
    "href": "Mathematical Analysis 1.html#p101.4",
    "title": "Analysis1 and Analysis 2",
    "section": "P10,1.4",
    "text": "P10,1.4\nEquivalence of statements: Two statements are logically equivalent, if they have the same truth values in all possible situations.\n\\(A\\equiv B\\)\n‘abstract non-sense’\nThem(De Morgan’s laws)\nA,B\n\\(\\sim (A \\land B) \\equiv (\\sim A) \\lor (\\sim B)\\)\n\\(\\sim (A \\lor B) \\equiv (\\sim A) \\land (\\sim B)\\)\ndraw truth table to look at values\n交的话（and），都T才T；并的话（or），1T则T\nConditional:\nIf p(hypothesis/assumption/condition), then q(conclusion/consequence/result).\nread: p implies q/assume p, then q.\nkey point: the implication is False when the rule is broken\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\nA & B & A \\rightarrow B \\\\\n\\hline\nT & T & T \\\\\nT & F & F \\\\\nF & T & T \\\\\nF & F & T \\\\\n\\hline\n\\end{array}\n\\] Def: p, q,p–&gt;q\n1)Converse: q—&gt;p,i.e. if q, then p\n2)Contrapositive: (q)–&gt;(p), i.e. if not q, then not p\nProp: (p–&gt;q)\\(\\equiv\\) ((q)–&gt;(p))(a statement and its contrapositive are logically equivalent) \\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\np & q & \\sim p & \\sim q & p \\rightarrow q & \\sim q \\rightarrow \\sim p & (p \\rightarrow q) \\equiv (\\sim q \\rightarrow \\sim p) \\\\\n\\hline\nT & T & F & F & T & T & T \\\\\nT & F & F & T & F & F & T \\\\\nF & T & T & F & T & T & T \\\\\nF & F & T & T & T & T & T \\\\\n\\hline\n\\end{array}\n\\]\n(always remember: implication fails only when the rule brokens instead of……(others))\nProof by contradiction: we want to prove p—&gt;q, we prove (q)—-&gt;(p) instead\nproof: Assume ~q, if , then if , then … –&gt; ~p, which is contradict to the original assumption p\n   Hence the assumption ~q is false, i.e. q is true.#\n   \ne.g.: n is a natural number. Prove that if n^2 is divisible by 2(p), then n is divisible by 2(q).\nProof: Assume that n is not divisible by 2(~q),—&gt; n is odd(defination), i.e. n =2k+1,k\\(\\in Z\\)\n—&gt; n^2 =(2k+1)^2(multiplicaiton)=2()+1, which is odd\n—&gt; n^2 is not divisible by 2(~p),\nThus the aassumption that n is not divisible by 2 is false so n is divisible by 2.#\nbiconditional: p,q,\n(p–&gt;q)\\(\\land\\)(q—&gt;p)—p &lt;–&gt; q—-reads‘p if and only if q’.\nstatement converse\n\\[\n\\begin{array}{|c|c|c|c|c|}\n\\hline\np & q & p \\rightarrow q & q \\rightarrow p & p \\leftrightarrow q \\\\\n\\hline\nT & T & T & T & T \\\\\nT & F & F & T & F \\\\\nF & T & T & F & F \\\\\nF & F & T & T & T \\\\\n\\hline\n\\end{array}\n\\]\nThem.: the only case p&lt;–&gt;q is true is then both p and q ture or false"
  },
  {
    "objectID": "Mathematical Analysis 1.html#proposition",
    "href": "Mathematical Analysis 1.html#proposition",
    "title": "Analysis1 and Analysis 2",
    "section": "proposition",
    "text": "proposition\n$x,yQ $, x is less than y, \\(\\exists z\\in Q\\) s.t. x&lt;z&lt;y.\nproof: \\(x,y\\in Q, x&lt;y\\),x=m/n,x=p/q,z=x+y/2\\(\\in\\)(x+x/2,y+y/2),i.e.x&lt;z&lt;y,z=pn+mq/2qn\\(\\in \\mathbb Q\\) by defination."
  },
  {
    "objectID": "Mathematical Analysis 1.html#russel-paradox",
    "href": "Mathematical Analysis 1.html#russel-paradox",
    "title": "Analysis1 and Analysis 2",
    "section": "Russel paradox",
    "text": "Russel paradox\nlet R be the set of all sets that are not a member of themself. i.e.R={S|S\\(\\notin\\)S}\nso if \\(R\\in R\\),R\\(\\notin\\) R\nif \\(R\\notin R\\), R\\(\\in R\\)\nif \\(x\\in A\\),then \\(x\\in B\\) is the subset defination of A \\(\\subseteq\\) B.\nif \\(A\\subset B\\), A is a proper subset of B. i.e. \\(A\\subset B\\) if \\(A\\subseteq B\\) and \\(\\exists x\\in B\\), s.t. \\(x\\notin\\) A\n\\(N\\subset Z \\subset Q \\subset R\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#proposition-1",
    "href": "Mathematical Analysis 1.html#proposition-1",
    "title": "Analysis1 and Analysis 2",
    "section": "Proposition",
    "text": "Proposition\nthe empty set is a subset of any set\nproof:\nmethod1: (truth table)\nmethod2:"
  },
  {
    "objectID": "Mathematical Analysis 1.html#def-1",
    "href": "Mathematical Analysis 1.html#def-1",
    "title": "Analysis1 and Analysis 2",
    "section": "Def",
    "text": "Def\nthe complement of A is denoted by \\(A^c\\)\nthe intersection \\(A \\cap B\\)\nthe union \\(A \\cup B\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#countability-and-bijections",
    "href": "Mathematical Analysis 1.html#countability-and-bijections",
    "title": "Analysis1 and Analysis 2",
    "section": "Countability and Bijections",
    "text": "Countability and Bijections\nCountable Set: A set is said to be countable if it is either finite or has the same size (cardinality) as the set of natural numbers \\(\\mathbb{N}\\). A set is countably infinite if there exists a bijection (a one-to-one correspondence) between that set and \\(\\mathbb{N}\\).\nUncountable Set: A set is uncountable if no such bijection exists, meaning its cardinality is strictly greater than that of \\(\\mathbb{N}\\).\nBijection: A bijection between two sets \\(A\\) and \\(B\\) is a function \\(f: A \\to B\\) that is both injective (one-to-one) and surjective (onto).\n\nThe Problem: Showing that \\(\\mathcal{P}(\\mathbb{N})\\) is Uncountable\n\nmethod 1: Consider the set \\(A = \\{ n \\in \\mathbb{N} \\mid n \\notin f(n) \\}\\), f : N → P(N).\nwe should prove it is not a bijection. so we could prove it is not surjective (P(N) is much bigger than N).\nnow we ask if f is surjective:\nIf yes, there exists \\(n_0\\in N\\) s.t. f\\((n_0)=\\)A, A\\(\\subset\\)N while A \\(\\in\\) P(N)\nnow we ask: Does \\(n_o\\in f(n_0)\\)\n\nif yes, \\(n_0\\notin A=f(n_0)\\)\nif no, \\(n_0\\in A=f(n_0)\\)\n\n(or we discuss the set, we suppose x is in A and based on the condition of being in set A we get A is empty, which is contradicts to “x is in A”)\nso f is not surjective, which means f could not be bijective. so p(N) is uncountable.\n\neg of a bijection between 2 sets(namely 2 sets with the same size)\n\nN-&gt;Z\nn|-&gt;n/2 if n even\nn|-&gt; n+1/2 if n odd\n\n\n\n\nan eg question-Logical Puzzle on Truth of Statements relavent to truth table\nConsider the following 99 statements:\n\n\\(S_1\\): “Among these 99 statements, there is at most one true statement.”\n\\(S_2\\): “Among these 99 statements, there are at most two true statements.”\n…\n\\(S_{99}\\): “Among these 99 statements, there are at most 99 true statements.”\n\nThe goal is to determine which statements among these 99 are true.\n(hint:Consider the chain of implications from statement \\(S_n\\) to \\(S_{n+1}\\). Which statement implies which? )\nsol: we fail the hypothesis of \\(S_{n+1}\\Rightarrow S_n\\)，but because\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\nS_n & S_{n+1} & S_n \\Rightarrow S_{n+1} \\\\\n\\hline\nT & T & T \\\\\nT & F & F \\\\\nF & T & T \\\\\nF & F & T \\\\\n\\hline\n\\end{array}\n\\]\nThus, we conclude that the truth of \\(S_n\\) implies the truth of all subsequent statements \\(S_{n+1}\\).\n)\n\\[\nS_n \\Rightarrow S_{n+1}\n\\] so there exists an \\(S_n\\), after which are true while before which are false.\nso:There are \\(100 - n\\) true statements.\n\\[\n100 - n \\leq n \\implies n \\geq 50\n\\]\nThere are \\(n - 1\\) false statements.\n\\[\nn - 1+1\\leq 100 - n \\implies n \\leq 50\n\\] so The 50th statement \\(S_{50}\\) is true. Statements \\(S_1\\) to \\(S_{49}\\) are false, and statements \\(S_{50}\\) to \\(S_{99}\\) are true."
  },
  {
    "objectID": "Mathematical Analysis 1.html#cardinality",
    "href": "Mathematical Analysis 1.html#cardinality",
    "title": "Analysis1 and Analysis 2",
    "section": "Cardinality",
    "text": "Cardinality\n\nnumebr of elements in a finite set: Def: let A be a set, if A contains finitely many elements, then the number of elements of A is called the cardinality of A, denoted by |A|(|A|\\(\\in Z_{\\geq0}\\)\nA, B have infinitely many elements. If \\(\\exists f:A-&gt;B\\) is a bijective map, then we regard the A, B have the same “cardinality”, i.e.|A|=|B|"
  },
  {
    "objectID": "Mathematical Analysis 1.html#a-x-bcartesian-product",
    "href": "Mathematical Analysis 1.html#a-x-bcartesian-product",
    "title": "Analysis1 and Analysis 2",
    "section": "A x B–Cartesian product",
    "text": "A x B–Cartesian product\n\neg\n|A|=3, |B|=2, |A x B|=6"
  },
  {
    "objectID": "Mathematical Analysis 1.html#map",
    "href": "Mathematical Analysis 1.html#map",
    "title": "Analysis1 and Analysis 2",
    "section": "map",
    "text": "map\nQuestion from the Cardinality:\nWhat happens if A has infinitely many elements?\nDef: LEt A and B be 2 sets, A map(function)f: A–&gt;B assigns each element in A\nthere has 3 kinds of maps(see 107)\n\neg\nthe graph of f(x) in A x B is f: A–&gt; B(x \\(\\in A\\),y\\(\\in B\\))"
  },
  {
    "objectID": "Mathematical Analysis 1.html#inverse-defination",
    "href": "Mathematical Analysis 1.html#inverse-defination",
    "title": "Analysis1 and Analysis 2",
    "section": "inverse defination",
    "text": "inverse defination\nif f is a bijective map, so as \\(f^{-1}\\)."
  },
  {
    "objectID": "Mathematical Analysis 1.html#countable",
    "href": "Mathematical Analysis 1.html#countable",
    "title": "Analysis1 and Analysis 2",
    "section": "countable",
    "text": "countable\n\nA set A is called countable if A iseither a finite set of there exists a bijective map f: A to N.(Otherwise is uncountable)\n\n\neg\n\n\\(B=[x|x=2n,n\\in N\\) is countable\n\nproof: for x|–&gt;x/2 of B–&gt;N i) injective , $x_1 $ not = \\(x_2\\), f(x_1)=x/2, = f(x_2)\n   ii) surjective, $\\forall y\\in N $, $\\exists x\\in B$ s.t. ,f(2y)=x, $2y\\in B$\n   \n   so bijective\n   \n\nZ is countable (负无穷到正无穷范围内的整数)\n\nf:N–&gt;Z(n|–&gt;n/2 if even, 1-n/2 if odd)\nn|–&gt;n/2 if even makes the positive integer possible\n1-n/2 if odd makes negative integer possible\nand both of them are bijections\nso Z is countable"
  },
  {
    "objectID": "Mathematical Analysis 1.html#triangle-inequality-is-a-strong-tool",
    "href": "Mathematical Analysis 1.html#triangle-inequality-is-a-strong-tool",
    "title": "Analysis1 and Analysis 2",
    "section": "triangle inequality is a strong tool",
    "text": "triangle inequality is a strong tool\n\\[\n\\begin{aligned}\n& \\lim _{x \\rightarrow \\infty}\\left|\\frac{\\frac{11}{x}-\\frac{9}{x^2}}{25+\\frac{35}{x}+\\frac{6}{x^2}}\\right| \\\\\n& |25+\\frac{35}{x}+\\frac{10}{x^2}\\left|=\\left|25-\\left(\\frac{35}{-x}-\\frac{10}{x^2}\\right)\\right|\\right. \\\\\n& \\left.\\geqslant|25|-| \\frac{35}{x}+\\frac{10}{x^2} \\right\\rvert\\\\ \\\\\n& \\geqslant 25-\\left|\\frac{35}{x}\\right|-\\left|\\frac{10}{x^2}\\right|\\\\\n& \\forall x&gt;10 \\\\\n\\end{aligned}\n\\] \\[\n25-| \\frac{35}{x}\\left|-\\left|\\frac{10}{x^2}\\right|&gt;25-\\left(\\frac{35}{10}+\\frac{10}{10^2}\\right)=M_1\\right.\n\\] Take \\(M_2=\\frac{22}{M_1 \\varepsilon}, \\forall|x|&gt;\\frac{22}{M_1 \\varepsilon},\\left|\\frac{11}{x}\\right|&lt;\\frac{M_1 \\varepsilon}{2}\\) Take \\(M_3=\\sqrt{\\frac{18}{m_1 \\varepsilon}}, \\forall|x|&gt;\\sqrt{\\frac{18}{m_1 \\varepsilon}},\\left|\\frac{9}{x^2}\\right|&lt;\\frac{m_1 \\varepsilon}{2}\\) \\[\n\\left|\\frac{11}{x}-\\frac{9}{x^2}\\right|&lt;\\left|\\frac{11}{x}\\right|+\\left|\\frac{9}{x^2}\\right|\n\\]\nSo Take \\(M=m \\cdot a x\\left\\{10, \\frac{22}{M_1 a}, \\sqrt{\\frac{18}{m_1 \\varepsilon}}\\right\\}\\), we hame \\(\\forall \\varepsilon&gt;0, \\forall|x|&gt;m\\), such that…&lt;\\(\\epsilon\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#understanding-deeplyexplain-by-myself",
    "href": "Mathematical Analysis 1.html#understanding-deeplyexplain-by-myself",
    "title": "Analysis1 and Analysis 2",
    "section": "understanding deeply(explain by myself…)",
    "text": "understanding deeply(explain by myself…)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#proof-of-some-theroem",
    "href": "Mathematical Analysis 1.html#proof-of-some-theroem",
    "title": "Analysis1 and Analysis 2",
    "section": "proof of some theroem",
    "text": "proof of some theroem\n\nboring things(just practivce epsilon-delta language…)\n\n\\(\\lim _{x \\rightarrow a} f(x)=\\ell\\) if and only if \\(\\lim _{x \\rightarrow a^{+}} f(x)=\\lim _{x \\rightarrow a^{-}} f(x)=\\ell\\).\n\nNecessary Condition Let \\(f(x) \\rightarrow l\\) as \\(x \\rightarrow c\\).\nThen from the definition of the limit of a function: \\[\n\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: 0&lt;|x-c|&lt;\\delta \\Longrightarrow|f(x)-l|&lt;\\epsilon\n\\]\nSo for any given \\(\\epsilon\\), there exists a \\(\\delta\\) such that: \\[\n0&lt;|x-c|&lt;\\delta\n\\] implies that: \\[\nl-\\epsilon&lt;f(x)&lt;l+\\epsilon\n\\]\nNow: \\[\n\\begin{array}{ll}\n& 0&lt;|x-c|&lt;\\delta \\\\\n\\leadsto \\quad & -\\delta&lt;-(x-c)&lt;0 \\\\\n& \\vee \\quad 0&lt;(x-c)&lt;\\delta \\\\\n\\leadsto \\quad & c-\\delta&lt;x&lt;c \\\\\n& \\vee \\quad c&lt;x&lt;c+\\delta\n\\end{array}\n\\]\nThat is: \\(\\forall \\epsilon&gt;0: \\exists \\delta&gt;0\\) :\n(1): \\(\\quad c-\\delta&lt;x&lt;c \\Longrightarrow\\|f(x)-l\\|&lt;\\epsilon\\)\n\n: \\(\\quad c&lt;x&lt;c+\\delta \\Longrightarrow\\|f(x)-l\\|&lt;\\epsilon\\)\n\nSo given that particular value of \\(\\epsilon\\), we can find a value of \\(\\delta\\) such that the conditions for both:\n(1): \\(\\quad f\\) tending to the limit \\(l\\) as \\(x\\) tends to \\(c\\) from the left\nand :\n\n: \\(\\quad f\\) tending to the limit \\(l\\) as \\(x\\) tends to \\(c\\) from the right.\n\nThus: \\[\n\\lim _{x \\rightarrow c} f(x)=l\n\\] implies that: \\[\n\\lim _{x \\rightarrow c^{-}} f(x)=l\n\\] and: \\[\n\\lim _{x \\rightarrow c^{+}} f(x)=l\n\\]\nSufficient Condition\nLet \\(f(x) \\rightarrow l\\) as \\(x \\rightarrow c^{-}\\)and \\(f(x) \\rightarrow l\\) as \\(x \\rightarrow c^{+}\\).\nThis means that:\n(1): \\(\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: c-\\delta&lt;x&lt;c \\Longrightarrow|f(x)-l|&lt;\\epsilon\\)\nand :\n(2): \\(\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: c&lt;x&lt;c+\\delta \\Longrightarrow|f(x)-l|&lt;\\epsilon\\)\nIn the same manner as above, the conditions on \\(\\delta\\) give us that: \\[\n\\begin{array}{ll}\n& c-\\delta&lt;x&lt;c \\\\\n\\wedge \\quad & c&lt;x&lt;c+\\delta \\\\\n& 0&lt;|x-c|&lt;\\delta\n\\end{array}\n\\]\nSo: \\[\n\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: 0&lt;|x-c|&lt;\\delta \\Longrightarrow|f(x)-l|&lt;\\epsilon\n\\]\nThus: \\[\n\\lim _{x \\rightarrow c^{-}} f(x)=l\n\\] and: \\[\n\\lim _{x \\rightarrow c^{+}} f(x)=l\n\\] together imply that: \\[\n\\lim _{x \\rightarrow c} f(x)=l\n\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html#decimal-scale",
    "href": "Mathematical Analysis 1.html#decimal-scale",
    "title": "Analysis1 and Analysis 2",
    "section": "decimal scale",
    "text": "decimal scale\n\\(x=0.a_1a_2a_3...\\)\n[0,1]={\\(\\sum_{n=1}^\\infty a_n/10^n,n\\in{0,1,...9}\\)}\n# [0,1]=\\(10^{\\infty}\\)—(uncountable)\n|{\\(x_1\\)}|\\(\\subseteq (x_1-\\epsilon,x_1+\\epsilon), \\forall \\epsilon &gt;0\\) i.e.|{\\(x_1\\)}|=0"
  },
  {
    "objectID": "Mathematical Analysis 1.html#defination",
    "href": "Mathematical Analysis 1.html#defination",
    "title": "Analysis1 and Analysis 2",
    "section": "Defination",
    "text": "Defination"
  },
  {
    "objectID": "Mathematical Analysis 1.html#连续性用sup构造数列",
    "href": "Mathematical Analysis 1.html#连续性用sup构造数列",
    "title": "Analysis1 and Analysis 2",
    "section": "连续性+用sup构造数列",
    "text": "连续性+用sup构造数列"
  },
  {
    "objectID": "Mathematical Analysis 1.html#用sequential-criterion证明连续性取合适的epsilon来说明数列存在",
    "href": "Mathematical Analysis 1.html#用sequential-criterion证明连续性取合适的epsilon来说明数列存在",
    "title": "Analysis1 and Analysis 2",
    "section": "用sequential criterion证明连续性取合适的epsilon来说明数列存在",
    "text": "用sequential criterion证明连续性取合适的epsilon来说明数列存在\neg.\nA={1/2n} where n \\(\\in N\\)\nf(x)=0 when x \\(\\in[0,1]\\) and x is not in A\nf(x)=2x when x \\(\\in A\\)\n\nProve f is continuous at x when x \\(\\in[0,1]\\) and x is not in A\n\n\nnotice!!! Do not choose sequence convergent to a point that we want blindly, since there may exists no such sequence like that! For example, here, by the density of rational/irrational number, there exists no sequence composed by elements in A convergent to a point at [0,1]so we should think of another way to prove this—If b\\(\\in [0,1]\\A\\), then there is an n_0 such that$ 1/2(n_0+1)&lt;b&lt;1/2n_0,$ i.e. b\\(\\in (1/2(n_0+1),1/2n_0).\\) For any sequence $a_n [0,1] $with $ lim_n a_n=b,$ there is an N&gt;0 such that \\(a_n\\in (1/2(n_0+1),1/2n_0).\\) ( It works as the following: We take an \\(\\epsilon=1/2 min{|1/2(n_0+1)-b|, |1/2n_0-b|}, then there exists an N.0 such that |a_n-b|&lt;\\epsilon.\\) ===&gt; when n&gt;N, a_n\\(\\in(b- \\epsilon, b+ \\epsilon)\\subset (1/2(n_0+1),1/2n_0) )\\) Notice that (1/2(n_0+1),1/2n_0) does not intersect with A. Therefore f(a_n)=0. lim_nf(a_n)=0=f(b). Hence f is continuous at b.\nmethod 1 sequential\nmethod 2\n\n\nProve f is not continuous at each point on A\n\n\nmethod 1 limit or \\(\\epsilon-\\delta\\)\nmethod 2 sequential creterion"
  },
  {
    "objectID": "Mathematical Analysis 1.html#eg-question-from-dr.-chi-kwong-fok-about-weirestrass-extream-value-thm-continuous-function-in-a-compact-set-attains-its-max-and-min",
    "href": "Mathematical Analysis 1.html#eg-question-from-dr.-chi-kwong-fok-about-weirestrass-extream-value-thm-continuous-function-in-a-compact-set-attains-its-max-and-min",
    "title": "Analysis1 and Analysis 2",
    "section": "eg question from Dr. Chi-Kwong Fok about Weirestrass-Extream Value THM (continuous function in a compact set attains its max and min)",
    "text": "eg question from Dr. Chi-Kwong Fok about Weirestrass-Extream Value THM (continuous function in a compact set attains its max and min)\nIf f is a continuous and non-surjective function and the supremum of it in [0,\\(+\\infty\\)] is not in its range. show that $ &gt;0, x &gt; 0, z R z &gt; x, $\nFirstly, negate it: \\(\\exists \\epsilon_0 &gt;0,\\) s.t. \\(\\exists x_0 &gt;0, \\forall z \\in R\\) with z&gt;\\(x_0\\), f(z) $ M-_0, $ where M is sup R(f) —(1)\nSecondly, write down the def of M is sup:\n\\(\\forall \\epsilon &gt;0, \\exists x\\in [0,+ \\infty],\\) s.t. f(x)&gt;\\(M-\\epsilon\\). Now take the global $$ to be \\(\\epsilon_0\\) —(2)\nBy (1) and (2) we know that \\(\\forall \\epsilon &gt;0, \\exists x\\in [0,x_0],\\) s.t. f(x)&gt;\\(M-\\epsilon\\) Now take the global $$ to be \\(\\epsilon_0\\)\nBy Weirestrass-Extream Value THM (continuous function in a compact set attains its max and min) we know it attains its maximum in [0,x_0]. Suppose this maximum is not the sup, then M=supR(f) &lt; its maximum, then by the IVP it must attain sup R(f) contradiction. =–also. —contradiction"
  },
  {
    "objectID": "Mathematical Analysis 1.html#compact--closed-and-bounded-is-just-appropriate-for-the-real-number-set-but-for-the-general-metric-space-is-not-truethe-real-meaning-is-every-open-cover-has-a-finite-subcover",
    "href": "Mathematical Analysis 1.html#compact--closed-and-bounded-is-just-appropriate-for-the-real-number-set-but-for-the-general-metric-space-is-not-truethe-real-meaning-is-every-open-cover-has-a-finite-subcover",
    "title": "Analysis1 and Analysis 2",
    "section": "compact -“closed and bounded” is just appropriate for the real number set, but for the general metric space is not true—the real meaning is “every open cover has a finite subcover”",
    "text": "compact -“closed and bounded” is just appropriate for the real number set, but for the general metric space is not true—the real meaning is “every open cover has a finite subcover”"
  },
  {
    "objectID": "Mathematical Analysis 1.html#gauss-is-a-fox-and-the-foxs-tail-sweeps-away-the-places-he-has-passed-through.-abel",
    "href": "Mathematical Analysis 1.html#gauss-is-a-fox-and-the-foxs-tail-sweeps-away-the-places-he-has-passed-through.-abel",
    "title": "Analysis1 and Analysis 2",
    "section": "Gauss is a fox, and the fox’s tail sweeps away the places he has passed through. —Abel",
    "text": "Gauss is a fox, and the fox’s tail sweeps away the places he has passed through. —Abel\nGauss had the habit of not writing down his proofs, which made it difficult for others to understand his work. He had good habits of writing his daily work in his diary, which was a good way to keep track of his progress."
  },
  {
    "objectID": "Mathematical Analysis 1.html#read-euler-read-euler-he-is-the-master-of-us-all.-laplace",
    "href": "Mathematical Analysis 1.html#read-euler-read-euler-he-is-the-master-of-us-all.-laplace",
    "title": "Analysis1 and Analysis 2",
    "section": "Read Euler, read Euler, he is the master of us all. —Laplace",
    "text": "Read Euler, read Euler, he is the master of us all. —Laplace\nEuler telled us both the true things and the false things in mathematics. He demonstrated the whole thinking process of mathematics.\nNot only did Euler did good mathematics research but also he was a good teacher. He had cultivated lots of students. He wrote a lot of books and papers, which were very helpful for the development of mathematics."
  },
  {
    "objectID": "stat_learn.html",
    "href": "stat_learn.html",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "",
    "text": "Efron, B., & Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). An introduction to statistical learning.\nCasella, G., & Berger, R. (2024). Statistical inference. Chapman and Hall/CRC.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning."
  },
  {
    "objectID": "stat_learn.html#ols-mle-solution-to-lr-with-issues",
    "href": "stat_learn.html#ols-mle-solution-to-lr-with-issues",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "OLS / MLE Solution to LR with issues",
    "text": "OLS / MLE Solution to LR with issues\nWe have learned the equivalence of Ordinary Least Squares (OLS) and Maximum Likelihood Estimation (MLE) in the context of linear regression assuming the error term follows a Gaussian distribution but with limitations as below:\n\nIssues:\n\nEven if the OLS or MLE solution is optimal in terms of minimizing the error, it may lead to overfitting, especially when the number of features is large relative to the number of observations. This can result in high variance and poor generalization to new data.\nHow to handle the issues?\n\\[\\begin{align*}\n\\text{LASSO:} & \\quad Argmin_{\\beta} \\sum_{i=1}^{N} \\left( y_i - \\sum_{j}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j}^{p} |\\beta_j|, \\quad \\text{where } \\lambda &gt; 0 \\\\\n\\text{Ridge:} & \\quad Argmin_{\\beta} \\sum_{i=1}^{N} \\left( y_i - \\sum_{j}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j}^{p} \\beta_j^2, \\quad \\text{where } \\lambda &gt; 0\n\\end{align*}\\]\n\nOLS perspective thinking: This shrinkage method not only achieves the objective of making the error to be smallest like OLS, but also prevents overfitting by penalizing large coefficients.\nMLE perspective thinking: Compared with MLE, this method could be regarded as a Bayesian estimation that can avoid overfitting by introducing the prior distribution \\(P(\\beta)\\), especially when the data volume is small, the prior information can play a regularization role."
  },
  {
    "objectID": "stat_learn.html#why-does-the-terms-of-the-regularization-look-like-this",
    "href": "stat_learn.html#why-does-the-terms-of-the-regularization-look-like-this",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Why does the terms of the regularization look like this?",
    "text": "Why does the terms of the regularization look like this?\nJust like what we have learned about the relationship between the OLS and MLE and get the idea of why the error term is the square shape, now we are going to use the perspective of Bayesian to understand the penalized terms\n\nChoose Prior Distribution \\(P(\\beta)\\)\nIdea:\nWe insist that \\(\\beta\\) should be more likely to be small also with small variances (very near to 0). This means we should pick a distribution that has a peak around zero and decays quickly as we move away from zero.\n\nPrior Distribution: In the context of penalized regression, we choose a prior distribution for the coefficients \\(\\beta\\) that reflects our beliefs about their values. Common choices include:\n\nLaplace Prior: Assumes coefficients follow a Laplace distribution, leading to \\(L^1\\) regularization (Lasso Regression).\nGaussian Prior: Assumes coefficients are normally distributed around zero, which leads to \\(L^2\\) regularization (Ridge Regression).\n\n\nThis also means it would take extreame evidence with the data that we see in order to accept very large and very high variances beta because of the prior.\n\n\nChoose Prior Distribution \\(P(\\beta)\\)\n\n\n\n\n\n\nMaximize the posterior probability to get the point estimation of \\(\\beta\\) – Ridge\n\\[\nP(\\beta|y) \\propto P(y|\\beta) \\cdot P(\\beta)\n\\]\n\\[\nP(y|\\beta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{-\\frac{(y - \\beta^T x)^2}{2\\sigma^2}\\right\\}\n\\]\nAnd for the prior part, we assume \\(beta\\) follows a Gaussian distribution, $(0, ’^2) $,\n\\[\nP(\\beta) = \\frac{1}{\\sqrt{2\\pi}\\sigma'} \\exp\\left\\{-\\frac{\\beta^2}{2\\sigma'^2}\\right\\}\n\\]\nThus,\n\\[\n\\hat{\\beta} = \\arg\\max_\\beta \\log P(\\beta|Y)\n\\]\n\\[\n= \\arg\\max_{\\beta} \\log P(Y|\\beta)P(\\beta)\n\\]\n\\[\n= \\arg\\max_{\\beta} \\log \\prod_{i=1}^N P(y_i|\\beta)P(\\beta)\n\\]\n\\[\n= \\arg\\max_{\\beta}  \\sum_{i=1}^N \\log[P(y_i|\\beta)P(\\beta)]\n\\]\n\\[\n= \\arg\\max_{\\beta}  \\sum_{i=1}^N \\left[ \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma } \\exp\\left\\{-\\frac{(y - \\beta^T x)^2}{2\\sigma^2}\\right\\}\\right) + \\log(\\frac{1}{\\sqrt{2\\pi}\\sigma'})- \\frac{\\beta^2}{2\\sigma'^2} \\right]\n\\]\n\\[\n= \\arg\\max_{\\beta} \\sum_{i=1}^N \\left[ \\log\\left(\\frac{1}{2\\pi \\sigma' \\sigma}\\right) - \\frac{(y_i - \\beta^T x_i)^2}{2\\sigma^2} - \\frac{\\beta^2}{2\\sigma'^2} \\right]\n\\]\n\\[\n= \\arg\\min_\\beta \\sum_{i=1}^N \\left[ \\frac{(y_i - \\beta^T x_i)^2}{2\\sigma^2} + \\frac{\\beta^2}{2\\sigma'^2} \\right]\n\\]\n\\[\n= \\arg\\min_\\beta (Y-X\\beta)^T (\\frac{1}{2\\sigma^2}I_N) (Y-X\\beta) + \\frac{1}{\\sigma'^2} \\beta^2\n\\]\n((Weighted least squares with the weight \\(\\frac{1}{2\\sigma^2}I_N\\) and the penalty term \\(\\frac{1}{\\sigma'^2} \\beta^2\\)\nIf we let \\(\\lambda = \\frac{1}{\\sigma'^2}\\), then the loss function is \\(L(\\beta) = \\sum_{i=1}^N (\\beta^T x_i - y_i)^2 + \\lambda \\beta^T \\beta\\), which is equivalent to the minimization of the regularized least squares in \\(L^2\\).\n\nRemark of the Ridge Regression\n\nRidge regression solution is also the posterior mean, this is because the likelihood of the data given the parameters is Gaussian, and the prior is also Gaussian, which results in a posterior that is also Gaussian by the property of the conjugate prior of the Gaussian distribution, i.e.\nNEED to ADD!\n\n\nMaximize the posterior probability to get the point estimation of \\(\\beta\\) – Lasso\nNow the prior part is assumed to be a Laplace distribution, \\(\\beta \\sim \\text{Laplace}(0, b), i.e., P(\\beta) = \\frac{1}{2b} \\exp\\left(-\\frac{|\\beta|}{b}\\right)\\).\n\\[\n\\arg\\max_{\\beta}  \\sum_{i=1}^N \\left[ \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_{\\beta} } \\exp\\left\\{-\\frac{(y - \\beta^T x)^2}{2\\sigma^2}\\right\\}\\right) - \\log 2b - \\frac{|\\beta|}{b} \\right]\n\\]\n\\[\n= \\arg\\max_{\\beta} \\sum_{i=1}^N \\left[ \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_{\\beta} 2b}\\right) - \\frac{(y_i - \\beta^T x_i)^2}{2\\sigma^2} - \\frac{|\\beta|}{b} \\right]\n\\]\n\\[\n= \\arg\\min_\\beta \\sum_{i=1}^N \\left[ \\frac{(y_i - \\beta^T x_i)^2}{2\\sigma^2} + \\frac{|\\beta|}{b} \\right]\n\\]\n\\[\n= \\arg\\min_\\beta (Y-X\\beta)^T (\\frac{1}{2\\sigma^2}I_N) (Y-X\\beta) + \\frac{1}{b} |\\beta|\n\\]\nIf we let \\(\\lambda = \\frac{1}{b}\\), then the loss function is \\(L(\\beta) = \\sum_{i=1}^N (\\beta^T x_i - y_i)^2 + \\lambda |\\beta|\\), which is equivalent to the minimization of the regularized least squares in \\(L^1\\)."
  },
  {
    "objectID": "stat_learn.html#further-check-lambda-and-corresponding-term-in-the-posterior-likelihoodhow-the-penalized-terms-are-derived-from-the-bayesian-perspective",
    "href": "stat_learn.html#further-check-lambda-and-corresponding-term-in-the-posterior-likelihoodhow-the-penalized-terms-are-derived-from-the-bayesian-perspective",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Further check \\(\\lambda\\) and corresponding term in the posterior likelihood–How the penalized terms are derived from the Bayesian perspective",
    "text": "Further check \\(\\lambda\\) and corresponding term in the posterior likelihood–How the penalized terms are derived from the Bayesian perspective\n\n\n\n\n\n\nGaussian:\n\\(\\lambda = \\frac{1}{\\sigma'^2}\\) i.e. Larger \\(\\lambda\\) with less \\(\\sigma'^2\\) means more regularization corresponding to smaller variance of the prior distribution (normal)\n\n\nLaplace:\n\\(\\lambda = \\frac{1}{b}\\) i.e. Larger \\(\\lambda\\) with less \\(b\\) means more regularization corresponding to smaller variance of the prior distribution (laplace)"
  },
  {
    "objectID": "stat_learn.html#how-to-choose-the-lambda",
    "href": "stat_learn.html#how-to-choose-the-lambda",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "How to choose the \\(\\lambda\\)?",
    "text": "How to choose the \\(\\lambda\\)?\n\nCross-Validation: A common method to choose the regularization parameter \\(\\lambda\\) is through cross-validation. This involves splitting the data into training and validation sets, fitting the model with different values of \\(\\lambda\\), and selecting the one that minimizes the prediction error on the validation set."
  },
  {
    "objectID": "stat_learn.html#lagrange-multipliers-perspective-of-penalized-models",
    "href": "stat_learn.html#lagrange-multipliers-perspective-of-penalized-models",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Lagrange Multipliers Perspective of Penalized Models",
    "text": "Lagrange Multipliers Perspective of Penalized Models\nLagrange multipliers provide a way to incorporate constraints into optimization problems. In the context of penalized regression, we can view the regularization term as a constraint on the size of the coefficients.\nChoose Lasoo as an example, the Lagrange multipliers perspective could be expressed as:\n\\[\n\\left( \\hat{\\alpha}, \\hat{\\beta} \\right) = \\arg \\min \\left\\{ \\sum_{i=1}^{N} \\left( y_{i} - \\alpha - \\sum_{j} \\beta_{j} x_{ij} \\right)^{2} \\right\\} \\quad \\text{subject to} \\quad \\sum_{j} \\left| \\beta_{j} \\right| \\leq t.\n\\]\ni.e.\n\\[\ng(\\beta)= \\sum_{j} \\left| \\beta_{j} \\right| - t \\leq 0\n\\]\n\\[\nL(\\alpha,\\beta, \\lambda) = \\sum_{i=1}^{N} \\left( y_{i} - \\alpha - \\sum_{j} \\beta_{j} x_{ij} \\right)^{2} + \\lambda \\left( \\sum_{j} \\left| \\beta_{j} \\right| - t \\right)\n\\]\n\\[\n\\partial L / \\partial \\alpha = 0\n\\]\n\\[\n\\hat{\\alpha} = \\bar{y} - \\sum_{j} \\hat{\\beta}_{j} \\bar{x}_{j}\n\\]\n\\[\n\\partial L / \\partial \\beta_{j} = 0\n\\]\n\\[\n-2 \\sum_{i=1}^{N} x_{ij} \\left( y_{i} - \\hat{\\alpha} - \\sum_{k} \\hat{\\beta}_{k} x_{ik} \\right) + \\lambda \\cdot sign(\\hat{\\beta}_{j}) = 0\n\\]\nThis may use the iterative method to solve the equation.\nGenerally, the formula could be expressed as:\n\\[\n\\min_{\\beta} \\left\\{ \\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_p^p \\right\\}\n\\]\nwhere \\(\\lVert \\beta \\rVert_p^p\\) is the \\(L^p\\) norm of the coefficients, which serves as a penalty term.\nThe gradient of the objective function of Ridge with respect to \\(\\beta\\) is given by:\n\\[\n\\nabla J(\\beta) = -2X^T(y - X\\beta) + \\lambda \\nabla \\lVert \\beta \\rVert_p^p\n\\]\n(see matrix form (Go to Matrix Form) below for more details)\nThis should be set to zero to find the optimal \\(\\beta\\), which means the gradient of the loss function (the first term) is equal to the gradient of the penalty term (the second term) scaled by \\(\\lambda\\)."
  },
  {
    "objectID": "stat_learn.html#geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective",
    "href": "stat_learn.html#geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Geometric Interpretation of Penalized Models in Lagrange Multipliers Perspective",
    "text": "Geometric Interpretation of Penalized Models in Lagrange Multipliers Perspective"
  },
  {
    "objectID": "stat_learn.html#geometric-interpretation-of-lasso-in-lagrange-multipliers-perspective-and-bayesian-perspective",
    "href": "stat_learn.html#geometric-interpretation-of-lasso-in-lagrange-multipliers-perspective-and-bayesian-perspective",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Geometric Interpretation of LASSO in Lagrange Multipliers Perspective and Bayesian Perspective",
    "text": "Geometric Interpretation of LASSO in Lagrange Multipliers Perspective and Bayesian Perspective"
  },
  {
    "objectID": "stat_learn.html#story",
    "href": "stat_learn.html#story",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Story",
    "text": "Story\n\nHumorous story\n\nRyan Tibshirani interviews Rob Tibshirani about his work on the Lasso. Ryan said that when he was young he was into draw a diamond and circles and he happened to draw a circle that just touched the corner of a diamond and that led to a big idea Rob did not credited him for. “I did not cite you,” Rob answered humorously.\n\nIdea\n\nYes, this is actually an interesting geometric meaning of LASSO.\n\nTo me(a biostat student) —a big meaning!!\n\nRob Tibshirani previously went back to his hometown of Toronto when he was a professor in preventative medicine and biostatistics and he was working on LASSO here."
  },
  {
    "objectID": "stat_learn.html#storyrob-tibshiranis-motivation",
    "href": "stat_learn.html#storyrob-tibshiranis-motivation",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Story—Rob Tibshirani’s motivation",
    "text": "Story—Rob Tibshirani’s motivation\n\nPaper about non-negative garrote by Leo Breiman\nLeo started with least squares estimates and then multiply them by non-negative constants that were bounderd by some bounded T, which had the effect of producing a sparse solution because being non-negative with a bound of 0 forces sparsity.\nwhy choose “Lasso” name\nMore gentle than garrote and more softer term than garrote\nMany similar things that time\nYou do not need to do research on unique things, but promising things.\nFrom LM to GLM to Cox, this generalization is cool"
  },
  {
    "objectID": "stat_learn.html#understanding-of-lambda",
    "href": "stat_learn.html#understanding-of-lambda",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Understanding of \\(\\lambda\\)",
    "text": "Understanding of \\(\\lambda\\)\nLagrange Multipliers help explain how \\(\\lambda\\) balances fitting the data well:\nBigger \\(\\lambda\\) makes the ellipse (representing the constraint region in the parameter space in WLS, if it satisfies the homoscedasticity, it would be a circle) smaller. In this case, the gradient of \\(\\beta\\) becomes steeper with a larger \\(\\lambda\\), which means that the optimization algorithm will grow the penalty of \\(\\beta\\) to stay within the shrinking feasible region. This helps reduce variance and prevent overfitting, but overly large \\(\\lambda\\) can cause underfitting."
  },
  {
    "objectID": "stat_learn.html#matrix-form-of-penalized-regression-models",
    "href": "stat_learn.html#matrix-form-of-penalized-regression-models",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Matrix Form of Penalized Regression Models",
    "text": "Matrix Form of Penalized Regression Models\n\nRidge Regression\nObjective Function\n\\[\n\\min_{\\beta} \\left\\{ \\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_2^2 \\right\\}\n\\]\nMatrix Form Derivation:\n\nExpand the loss term: \\[\n  \\lVert y - X\\beta \\rVert_2^2 = (y - X\\beta)^T (y - X\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X\\beta.\n  \\]\nAdd the \\(L^2\\) penalty: \\[\n  J(\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X\\beta + \\lambda \\beta^T \\beta.\n  \\]\nCompute the gradient: \\[\n  \\frac{\\partial J}{\\partial \\beta} = \\nabla J(\\beta) = -2X^T(y - X\\beta) + \\lambda \\nabla \\lVert \\beta \\rVert_p^p=-2X^T y + 2X^T X\\beta + 2\\lambda \\beta.\n  \\]\nSet gradient to zero: \\[\n  -X^T y + X^T X\\beta + \\lambda \\beta = 0 \\implies (X^T X + \\lambda I)\\beta = X^T y.\n  \\]\nSolve for \\(\\beta\\): \\[\n  \\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T y\n  \\]\n\nWhy Regularization Helps:\nThe term \\(\\lambda I\\) ensures \\(X^T X + \\lambda I\\) is always invertible (since \\(\\lambda &gt; 0\\) adds positive values to the diagonal, guaranteeing full rank). This avoids singularity issues in \\(X^T X\\) when features are collinear."
  },
  {
    "objectID": "stat_learn.html#matrix-form-of-penalized-regression-models-1",
    "href": "stat_learn.html#matrix-form-of-penalized-regression-models-1",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Matrix Form of Penalized Regression Models",
    "text": "Matrix Form of Penalized Regression Models\n\nLasso Regression\nFormula:\n\\[\n\\hat{\\beta}_{\\text{lasso}} = \\arg\\min_{\\beta} \\left\\{ \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right\\}\n\\]\nWant\n\\[\n\\min_{\\beta} \\left\\{ \\|y - X\\beta\\|_2^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n\\]\nMatrix Form Derivation:\nLasso lacks a closed-form solution due to the non-differentiable L1 norm. Instead, we use the subgradient optimality condition:\n\nSubgradient equation: \\[\n  -2X^T(y - X\\beta) + \\lambda \\cdot \\text{sign}(\\beta) = 0,\n  \\] where \\(\\text{sign}(\\beta)\\) is defined component-wise:\n\nKey Insight\n\nFor \\(\\beta_j \\neq 0\\), the solution balances data fit and shrinkage.\nFor \\(\\beta_j = 0\\), the condition requires: \\[\n  \\left| 2 \\mathbf{x}_j^T (y - X\\beta) \\right| \\leq \\lambda.\n  \\] This induces sparsity (exact zeros in \\(\\beta\\)), which Ridge cannot achieve."
  },
  {
    "objectID": "stat_learn.html#differences-between-lasso-and-ridge",
    "href": "stat_learn.html#differences-between-lasso-and-ridge",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Differences between Lasso and Ridge",
    "text": "Differences between Lasso and Ridge\n\nLasso could realize variable selection by shrinking some coefficients to exactly zero, while Ridge regression shrinks all coefficients but does not set any to zero."
  },
  {
    "objectID": "stat_learn.html#r-code-example-of-penalized-linear-regression-models",
    "href": "stat_learn.html#r-code-example-of-penalized-linear-regression-models",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "R code Example of Penalized Linear Regression Models",
    "text": "R code Example of Penalized Linear Regression Models\n\n# Install and load necessary packages\n# install.packages(\"glmnet\")\n# install.packages(\"ISLR2\")\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nlibrary(ISLR2)\n\n# Load the Hitters dataset\ndata(Hitters)\n\n# Remove missing values\nHitters &lt;- na.omit(Hitters)\n\n# Define the predictor matrix and response variable\n# exclude the intercept term\nx &lt;- model.matrix(Salary ~ ., Hitters)[, -1] \ny &lt;- Hitters$Salary\n\n\n# Split the data into training and test sets\nset.seed(1)\ntrain &lt;- sample(1:nrow(x), nrow(x) / 2)\ntest &lt;- (-train)\ny.test &lt;- y[test]\n\n# Fit the Lasso model on the training data\ngrid &lt;- 10^seq(10, -2, length = 100)\n# LASSO-LR\nlasso.mod &lt;- glmnet(x[train, ], y[train], alpha = 1,\n                    lambda = grid, family = 'gaussian')\n# Plot the Lasso model to visualize coefficient paths\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\n\n\n\n\n\n# Perform cross-validation to find the optimal lambda\nset.seed(1)\ncv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\n\n\n\n\n\n\n\n\n\nbestlam &lt;- cv.out$lambda.min\n# Make predictions on the test set using the optimal lambda\nlasso.pred &lt;- predict(lasso.mod, s = bestlam, newx = x[test, ])\ntest.mse &lt;- mean((lasso.pred - y.test)^2)\nprint(paste(\"Test MSE with Lasso and optimal lambda:\", test.mse))\n\n[1] \"Test MSE with Lasso and optimal lambda: 143673.618543046\"\n\n# Fit the Lasso model on the full dataset using the optimal lambda\nlasso.full &lt;- glmnet(x, y, alpha = 1,lambda = bestlam)\nlasso.coef &lt;- coef(lasso.full)\nprint(\"Lasso coefficients on the full dataset:\")\n\n[1] \"Lasso coefficients on the full dataset:\"\n\nprint(lasso.coef)\n\n20 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept)   -3.42073206\nAtBat          .         \nHits           2.02965136\nHmRun          .         \nRuns           .         \nRBI            .         \nWalks          2.24850782\nYears          .         \nCAtBat         .         \nCHits          .         \nCHmRun         0.04994886\nCRuns          0.22212444\nCRBI           0.40183027\nCWalks         .         \nLeagueN       20.83775664\nDivisionW   -116.39019204\nPutOuts        0.23768309\nAssists        .         \nErrors        -0.93567863\nNewLeagueN     .         \n\n\n\nresult_matrix &lt;- matrix(lasso.coef[lasso.coef != 0], ncol = 3, byrow = TRUE)\n\nWarning in matrix(lasso.coef[lasso.coef != 0], ncol = 3, byrow = TRUE): data\nlength [10] is not a sub-multiple or multiple of the number of rows [4]\n\nprint(result_matrix)\n\n            [,1]         [,2]      [,3]\n[1,] -3.42073206    2.0296514 2.2485078\n[2,]  0.04994886    0.2221244 0.4018303\n[3,] 20.83775664 -116.3901920 0.2376831\n[4,] -0.93567863   -3.4207321 2.0296514\n\n\n\n# 提取非零系数（排除截距）\nnon_zero_coef &lt;- lasso.coef[lasso.coef[,1] != 0, ][-1]  # 去除截距项\ncoef_df &lt;- data.frame(\n  Variable = names(non_zero_coef),\n  Coefficient = as.numeric(non_zero_coef)\n)\n\n# 按系数绝对值排序\ncoef_df &lt;- coef_df[order(abs(coef_df$Coefficient), decreasing = TRUE), ]\n\n# 绘制条形图\nlibrary(ggplot2)\n\nggplot(coef_df, aes(x = reorder(Variable, Coefficient), \n                   y = Coefficient, \n                   fill = ifelse(Coefficient &gt; 0, \"Positive\", \"Negative\"))) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Positive\" = \"dodgerblue\", \"Negative\" = \"firebrick\")) +\n  labs(title = \"Lasso Regression Coefficients\",\n       subtitle = paste(\"Optimal lambda =\", round(bestlam, 4)),\n       x = \"Predictor Variables\",\n       y = \"Coefficient Value\",\n       fill = \"Effect Direction\") +\n  coord_flip() +  # 水平条形图更易阅读变量名\n  theme_minimal() +\n  theme(legend.position = \"top\",\n        plot.title = element_text(face = \"bold\", size = 14),\n        axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "stat_learn.html#remark",
    "href": "stat_learn.html#remark",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Remark",
    "text": "Remark\nActually the penalized model could be applied in many regression models not only for linear regression, but also for logistic regression, Poisson regression, Cox regression, etc.\nThe penalized terms could be added to the loss function of these models in a similar way.\n\nR Code of Penalized Cox Regression - Lasso using glmnet package\nWant to minimize \\[\n-\\log \\left( \\prod_{i:\\delta_i=1} \\frac{\\exp\\left(\\sum_{j=1}^p x_{ij}\\beta_j\\right)}{\\sum_{i':y_{i'}\\geq y_i}\\exp\\left(\\sum_{j=1}^p x_{i'j}\\beta_j\\right)} \\right) + \\lambda P(\\beta),\n\\] where \\(\\delta_i\\) is the indicator function for censoring and \\(P(\\beta) = \\sum_{j=1}^p \\beta_j^2\\) corresponds to a ridge penalty, or \\(P(\\beta) = \\sum_{j=1}^p |\\beta_j|\\) corresponds to a lasso penalty.\nR Code example for LASSO in Cox Regression applied on selecting important features of pesticide poisoning\nClick here to see LASSO code provided by SURF2024 instructed by Dr.He"
  },
  {
    "objectID": "stat_learn.html#the-greatest-truths-are-the-simplest.",
    "href": "stat_learn.html#the-greatest-truths-are-the-simplest.",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "The greatest truths are the simplest.",
    "text": "The greatest truths are the simplest.\nPenalized regression models balance the trade-off between fitting the data well and keeping the model simple."
  },
  {
    "objectID": "stat_learn.html#everything-is-connected-to-each-other.-good-theory-interpretes-this-connection-interestingly.",
    "href": "stat_learn.html#everything-is-connected-to-each-other.-good-theory-interpretes-this-connection-interestingly.",
    "title": "Statistical_Learning (Mechine_Learning)",
    "section": "Everything is connected to each other. Good theory interpretes this connection interestingly.",
    "text": "Everything is connected to each other. Good theory interpretes this connection interestingly.\nLinear Regression: OLS and MLE\nPenalized regression models: Lagrange multiplier (restriction in the geometric meaning and also algebra interpretation) and Bayesian perspective (prior: history knowledge restriction)"
  },
  {
    "objectID": "Simple_ODEs (1).html",
    "href": "Simple_ODEs (1).html",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "",
    "text": "To solve the linear ODE $ ’ = A $ and visualize how different initial conditions affect the trajectories, we can use a saddle point matrix $ A $ with eigenvalues of opposite signs. This setup results in varied trajectory shapes such as diverging, converging, and hyperbolic paths depending on the initial conditions."
  },
  {
    "objectID": "Simple_ODEs (1).html#what-is-a-number",
    "href": "Simple_ODEs (1).html#what-is-a-number",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "what is a number",
    "text": "what is a number\n\nQ, rational number q/p (ratio)\n\n有理数用直尺画，根号用圆规–算术平均大于几何平均\n什么是理性\n古巴比伦用宗教解释不理解的事情\n古希腊人（大自然是可理解的）带给现代人最珍贵的礼物就是理性\n爱因斯坦：大自然最不可理解的地方是大自然竟然是可理解的\n古希腊的传人\n理性的工具是数学，希腊人对数学的重视\n科技（建筑）和科学（要有主张和实验）\n\\(\\mathbb R\\)\nE.Steim\n分离变数法–傅立叶级数—分析\n微积分\n\n傅立叶 analysis\ncomplex analysis\nreal analysis\nfunctional analysis"
  },
  {
    "objectID": "Simple_ODEs (1).html#geometry",
    "href": "Simple_ODEs (1).html#geometry",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "geometry",
    "text": "geometry\nthe most important thing in geometry is about measure."
  },
  {
    "objectID": "Simple_ODEs (1).html#algebra",
    "href": "Simple_ODEs (1).html#algebra",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "algebra",
    "text": "algebra\nsimplize authority独断权威"
  },
  {
    "objectID": "Simple_ODEs (1).html#analysis",
    "href": "Simple_ODEs (1).html#analysis",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "analysis",
    "text": "analysis\ndemocracy\nNewton\nanatomy\nf(x)约等于 \\(\\Sigma a_nx^n\\)—-Talor series\nbe patient to learn analysis"
  },
  {
    "objectID": "Simple_ODEs (1).html#reformation",
    "href": "Simple_ODEs (1).html#reformation",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Reformation",
    "text": "Reformation"
  },
  {
    "objectID": "Simple_ODEs (1).html#renaissance",
    "href": "Simple_ODEs (1).html#renaissance",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Renaissance",
    "text": "Renaissance"
  },
  {
    "objectID": "Simple_ODEs (1).html#enlightenment",
    "href": "Simple_ODEs (1).html#enlightenment",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Enlightenment",
    "text": "Enlightenment"
  },
  {
    "objectID": "Simple_ODEs (1).html#industrial-revolution",
    "href": "Simple_ODEs (1).html#industrial-revolution",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Industrial Revolution",
    "text": "Industrial Revolution\nNo dynasty lasts over 300 years, except Song dynasty which lasted over 300 years, with scholars serving as prime ministers. Su Shi snored."
  },
  {
    "objectID": "Simple_ODEs (1).html#wave-equation",
    "href": "Simple_ODEs (1).html#wave-equation",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Wave Equation",
    "text": "Wave Equation\nVibration of a string.\nTaylor, d’Alembert, Daniel Bernoulli, Jacob Bernoulli (Euler), Jacob Bernoulli (distribution).\n\\[\n\\frac{d^2x}{dt^2}\n\\]\nThe truths of this world have all been discovered by Newton.\n\\[\nu(x,t) \\text{: amplitude, } [u] = L\n\\]\n$$ [p] = M/L\n$$\n\\[\np(x,t) \\text{: density (1-dimensional)}\n\\]\n[T] = [ma] = [m][a] = ML/t^2\n\\(T=T_1=T_2\\) tension\n(0-L)\nF = Tsin() - Tsin\nm = \\(\\rho\\) x\n2nd law: \\(\\rho\\) x"
  },
  {
    "objectID": "Simple_ODEs (1).html#linear-transformation",
    "href": "Simple_ODEs (1).html#linear-transformation",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "linear transformation",
    "text": "linear transformation\nKeep the parallelogram diagonal 保持平行四边形的对角线 x+y\nkeep a straight line 保持直线—a\n向量空间 vector space\n如果是从二维映射到三维怎么办。2dim—3dim？\n代数结构\n保留加法 keep the addition\nhomomorphism\nmorphism–形象"
  },
  {
    "objectID": "Simple_ODEs (1).html#da..-bernulli-separation-of-variables",
    "href": "Simple_ODEs (1).html#da..-bernulli-separation-of-variables",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Da.. Bernulli Separation of Variables",
    "text": "Da.. Bernulli Separation of Variables\n\\(u(x,\\theta)=\\phi(x)T(t)\\). (Gassian integral formula derivation also use this method)\n(A continuous function can be approximated by a polynomial, which is a super polynomial)\n(the defination of “density”)\n(Seperation variable: God to God, Caesar to Caesar—-x to x, t to t)\n分离变数\n\\[\n\\frac{T^{''}(t)}{C^2 T(t)} = -\\frac{\\phi^{''}(x)}{\\phi(x)}\n\\]\n\\[\n\\begin{aligned}\n&\\text{PDE:} \\quad \\text{分离变量法,separete variables} \\rightarrow \\text{ODE} \\\\\n&\\phi'' + \\lambda \\phi = 0 \\\\\n&T'' + \\lambda c^2 T = 0 \\\\\n&\\text{(B.C.) 边界条件：} \\\\\n&u(0,t) = \\phi(0) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(0) = 0 \\quad \\text{(trivial 非平凡解,boring)} \\\\\n&u(L,t) = \\phi(L) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(L) = 0\n\\end{aligned}\n\\]\n\\[\n\\lambda &lt; 0 \\quad , \\quad \\phi = e^{mx} \\quad \\text{(不可能，从0到0。0--0， impossible)}\n\\]\n\\[\n\\lambda = 0 \\quad , \\quad \\phi = Ax + B \\quad \\text{直线运动， straight move}\n\\]\n\\[\n\\lambda &gt; 0 \\quad , \\quad \\phi = c_1 \\cos(\\sqrt{\\lambda} x) + c_2 \\sin(\\sqrt{\\lambda} x)\n\\] ### 解的推导\n考虑方程： \\[\n\\phi'' + \\lambda \\phi = 0\n\\] 其中 () 是一个常数，解的形式取决于 () 的值。\n\n1. 当 (&lt; 0)\n当 (&lt; 0) 时，我们令 (= -^2) ，于是方程变为： \\[\n\\phi'' - \\mu^2 \\phi = 0\n\\] 其通解为： \\[\n\\phi(x) = A e^{\\mu x} + B e^{-\\mu x}\n\\] 这种解形式表示指数发散或衰减，通常是不稳定解。\n\n\n2. 当 (= 0)\n当 (= 0) 时，方程变为： \\[\n\\phi'' = 0\n\\] 这是一个线性方程，其通解为： \\[\n\\phi(x) = A x + B\n\\] 这表示直线运动。\n\n\n3. 当 (&gt; 0)\n当 (&gt; 0) 时，我们令 (= ^2) ，方程变为： \\[\n\\phi'' + \\mu^2 \\phi = 0\n\\] 其解为： \\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x) ---wave\n\\] 这种解表示周期性波动。\n\n推导\n对于形如 ( \\(\\phi'' + \\mu^2 \\phi = 0\\) ) 的常微分方程，我们可以猜测它的解是指数形式，即：\n\\[\n\\phi(x) = e^{rx}\n\\]\n这里 ( r ) 是需要确定的参数。\n\n\n\n代入微分方程：\n将 ( (x) = e^{rx} ) 代入原方程，得到：\n\\[\nr^2 e^{rx} + \\mu^2 e^{rx} = 0\n\\]\n由于 ( e^{rx} )，可以消掉这个项，剩下的是特征方程：\n\\[\nr^2 + \\mu^2 = 0\n\\] 这个特征方程的解为：\n\\[\nr = \\pm i \\mu\n\\]\n这是一个虚数解。\n当特征根为纯虚数时，方程的通解可以写成正弦和余弦的组合形式，根据欧拉公式 ( e^{ix} = (x) + i (x) )，我们得到通解为：\n\\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x)\n\\]\n其中，( A ) 和 ( B ) 是待定常数，它们可以通过边界条件或初始条件来确定。\ncharacteristic\n\\[\n\\phi(x) = C_1 \\cos(\\sqrt{\\lambda}x) + C_2 \\sin(\\sqrt{\\lambda}x)\n\\]\n边界条件： \\[\n\\phi(0) = 0 \\Rightarrow C_1 = 0\n\\]\n\\[\n\\phi(L) = C_2 \\sin(\\sqrt{\\lambda}L) = 0\n\\]\n因此， \\[\n\\sqrt{\\lambda_n}L = n\\pi \\quad (n = 1, 2, 3, \\dots)\n\\]\n得到本征值： \\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n对应的本征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] Sturm-Liouville Problem:\n\\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n因此，特征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] \\[\nT_n(t) = C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right)\n\\]\n\\[\nu_n(x,t) = T_n(t) \\phi_n(x)\n\\]\n代入展开： \\[\nu(x,t) = \\sum_{n=1}^{\\infty} C_n \\sin \\left( \\frac{n\\pi x}{L} \\right) \\left[ C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right) \\right]\n\\]"
  },
  {
    "objectID": "Simple_ODEs (1).html#law-of-large-number",
    "href": "Simple_ODEs (1).html#law-of-large-number",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "law of large number",
    "text": "law of large number\neg. The reason the casino makes money is because even though the people who go there have a little more than a half chance of winning, the casino has more money than you, and if you stay long enough, you will win, so don’t go to the casino, because you can’t go deeper than the casino"
  },
  {
    "objectID": "Simple_ODEs (1).html#does-math-have-elements",
    "href": "Simple_ODEs (1).html#does-math-have-elements",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Does math have elements?",
    "text": "Does math have elements?\n             ---Paul Halmos\n             Geometric Series\n\n1-ab and 1-ba —- invertible\nI-AB invertible\n— I-BA invertible (hint: \\(I+B(I-AB)^{-1}A\\))\ndo not 杀鸡用牛刀—-\nprove the harmonic series is divergent:\nidea: geometric series:\n1+1/2+(1/3+1/4)+….\n1+(1/2+1/3)+(1/4+..+1/9)+(1/10+…+1/27)+…\\(\\geq\\) 1+(1/3)\nwhich is relevant to fuliyejishu\n学数学不要兵来将挡水来土掩，要有一个整套的思想，不要背书 idea: geometric series\n(1-ba){-1}=1/1-ba=1+ba+baba+..=1+b(1+ab+abab+..)a=1+b(1-ab){-1}a\n(1-ba)[1+b(1-ab)^{-1}a]\n1+b(1-ab){-1}a-ba-bab(1-ab){-1}a=(commutive law)=1-ba+b(1-ab){-1}a-bab(1-ab){-1}a=1-ba+b(1-ab)(1-ab)^{-1}a\n=1-ba+b1a\n=1-ba+ba\n=1 —-so do not recite boring things, remember based on understanding.\nDo not read Gassian, read Oral\nfirstly, for matrix, AB \\(\\noequal\\) BA—-f(g) is not equal to g(f)\n学学问要有感觉\nchat with different famous … 西方的没落–fu springer\nliangzilixue hysenber—max born\nthe true meaning of matrix is the linear transformation(fun f)"
  },
  {
    "objectID": "Simple_ODEs (1).html#p-series",
    "href": "Simple_ODEs (1).html#p-series",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "p-series",
    "text": "p-series\nIntegral test with dimensional analysis to know p&gt;1 for convergence\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}  \\approx  \\int_1^\\infty 1/x^p \\, dx\\approx 1/[x]^p*[x]=[x]^{1-p} ([x]--&gt;\\infty)---&gt;1-p\\leq 0\\)\nagain, geometric series,\n1/12+(1/22+1/32)+(1/42+..)\n\\(\\leq 1+(1/2^2+1/2^2)+\\)\n= 1+2/2^2 +4/42+8/82z=…=2\n\nEuler\nEuler idea: Viete fumula\nrelation of root and coefficient\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}=\\pi^2/6\\)\n(x-a)(x-b)=x^2-(a+b)x+ab\n(1-x/a)(1-x/b)=1-(1/a+1/b)x+1/ab*x^2\n(1-x)(1+x)\n(1-x)(1+x)(1-x/2)(1+x/2)\n(1-x)(1+x)(1-x/2)(1+x/2)…(1-x/n)(1+x/n)+…\n=1-(1/12+1/22+…+1/n2)x2+…\nQ: find a function whose roots are +-1, +-2, +-3,….\nso Euler changed this Q:\nrewrite:\n(1-x/)(1+x/)(1-x/2)(1+x/2)…\n= 1-(1/12+1/22+1/n2+…)1/2x^2+…\n先猜答案\nguess sinx = (1-x/)(1+x/)…(1-x/n)(1+x/n)\nbut 0 is a solution\nso change to sinx/x\nsinx/x =k…..\nk=limsinx/x=1\n1/x is 振幅\nsinx/x=1-(1/12+1/22+…+1/n2+…)x2/^2+…\n=1/x\n天才是创意\ncreate a thery of math\nand six/x = 1/x(x-x3/3!+…)=1-x3/6\nso we know"
  },
  {
    "objectID": "mth106.html",
    "href": "mth106.html",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "",
    "text": "Find the solution of the equation\n\\[\n\\frac{dy}{dx} - P(x)y = e^{2x}, \\quad y(0) = 1,\n\\]\nwhere \\[P(x) = \\begin{cases}\n1 & x \\in [0,1), \\\\\n0 & x \\notin [0,1).\n\\end{cases}\\]\n[Solution:] Consider the homogeneous equation\n\\[\n\\frac{dy}{dx} - P(x)y = 0,\n\\]\nby separating the variables we have \\(\\frac{dy}{y} = P(x)dx\\), namely \\(\\ln|y| = \\int_0^x P(t)dt + C_1\\), and we get\n\\[\ny = Ce^{\\int_0^x P(t)dt} = \\begin{cases}\nC & x &lt; 0 \\\\\nCe^x & x \\in [0,1) \\\\\nCe & x \\geq 1.\n\\end{cases}\n\\]\nNow by variation of parameters we suppose \\(u = u(x)\\), and \\(y = u(x)e^{\\int_0^x P(t)dt}\\) is the solution, hence\n\\[\nu'(x) = e^{2x - \\int_0^x P(t)dt} = \\begin{cases}\ne^{2x} & x &lt; 0, \\\\\ne^x & x \\in [0,1), \\\\\ne^{2x-1} & x \\geq 1.\n\\end{cases}\n\\]\n(this is because :\nThe differential equation is given by: \\[\ny' + p(x)y = f(x).\n\\]\nIntegrating factor is: \\[\ne^{\\int p(x)dx}.\n\\]\nMultiplying through by the integrating factor: \\[\n\\frac{d}{dx} \\left[ e^{\\int p(x)dx} y \\right] = f(x) e^{\\int p(x)dx}.\n\\]\nSolving for \\(y\\):\n\\[\ny e^{\\int p(x)dx} = \\int f(x) e^{\\int p(x)dx} dx + C.\n\\]\nThus, the general solution is: \\[\ny = Ce^{-\\int p(x)dx} + e^{-\\int p(x)dx} \\int f(x) e^{\\int p(x)dx} dx,\n\\] where \\(y_c\\) represents the complementary solution \\(Ce^{-\\int p(x)dx}\\) and \\(y_p\\) represents the particular solution \\(e^{-\\int p(x)dx} \\int f(x) e^{\\int p(x)dx}\\).\n)\nand \\(C(0) = 1\\) since \\(y(0) = 1\\).\nWe see that\n\\[\nu(x) = u(0) + \\int_0^x u'(t)dt = \\begin{cases}\n\\frac{e^{2x} + 1}{2} & x &lt; 0 \\\\\ne^x & x \\in [0,1) \\\\\n\\frac{e^{2x-1} + e}{2} & x \\geq 1.\n\\end{cases}\n\\]\nFinally we get\n\\[\ny = \\begin{cases}\n\\frac{e^{2x} + 1}{2} & x &lt; 0 \\\\\ne^{2x} & x \\in [0,1) \\\\\n\\frac{e^{2x} + e^2}{2} & x \\geq 1.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "mth106.html#proof",
    "href": "mth106.html#proof",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Proof",
    "text": "Proof"
  },
  {
    "objectID": "mth106.html#thm",
    "href": "mth106.html#thm",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "THM",
    "text": "THM\n\npiecewise continuous function\n\nA function f(x) is piecewise continuous on an interval [a, b] if it is continuous on each subinterval (open interval) of [a, b] and has a finite number of (or jump) discontinuities in the interval. The function can be defined in pieces, and at each piece, it must be continuous."
  },
  {
    "objectID": "mth106.html#thmcondition-for-convergence",
    "href": "mth106.html#thmcondition-for-convergence",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "thm–condition for convergence",
    "text": "thm–condition for convergence\nf, and f’ are piecewise continuous on [-p,p], then for every x $$ [-p,p], the Fourier series of f(x) converges to f(x) at every point x in [-p,p] where f is continuous. If f has a jump discontinuity at x, the Fourier series converges to the average of the left-hand and right-hand limits of f at x."
  },
  {
    "objectID": "mth106.html#use-this-idea-we-introduce-the-method-of-variation-of-parameters-of-finding-the-particular-solution-of-the-non-homogeneous-linear-equation-system",
    "href": "mth106.html#use-this-idea-we-introduce-the-method-of-variation-of-parameters-of-finding-the-particular-solution-of-the-non-homogeneous-linear-equation-system",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "use this idea we introduce the method of variation of parameters of finding the particular solution of the non-homogeneous linear equation system",
    "text": "use this idea we introduce the method of variation of parameters of finding the particular solution of the non-homogeneous linear equation system\n…."
  },
  {
    "objectID": "mth106.html#eg-of-variation-of-parameters-introduced-by-professor-chi-kun-lin",
    "href": "mth106.html#eg-of-variation-of-parameters-introduced-by-professor-chi-kun-lin",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Eg of Variation of Parameters introduced by Professor Chi-Kun Lin:",
    "text": "Eg of Variation of Parameters introduced by Professor Chi-Kun Lin:\nThe method of variation of parameters is a clever and elegant approach in solving non-homogeneous differential equations, especially in the context of physics. When we consider a differential equation, the non-homogeneous term F(x) can be seen as an external force acting on the system. In the absence of this force, the solution space of the homogeneous equation is spanned by a set of linearly independent solutions, such as \\(y_1\\) and \\(y_2\\). These solutions form the basis of the homogeneous solution through linear combinations.\nHowever, when the external force \\(F(x)\\) is introduced, it perturbs the system and necessitates the inclusion of a particular solution that specifically responds to this force. This particular solution still involves the original basis functions \\(y_1\\) and \\(y_2\\), but it introduces new coefficients \\(u_1(x)\\) and \\(u_2(x)\\) that are functions of the independent variable \\(x\\). These coefficients are not constants; instead, they vary in a way that reflects the influence of the external force. The idea is that these coefficients \\(u_1(x)\\) and \\(u_2(x)\\) can be adjusted to account for the systematic changes induced by the physical quantity represented by \\(F(x)\\).\nThe brilliance of the variation of parameters method lies in its ability to construct a particular solution by allowing these coefficients to vary. This method hypothesizes that by introducing these variable coefficients, the particular solution can effectively capture the response of the system to the external force. And indeed, this construction turns out to be effective. It provides a systematic way to find a particular solution that complements the homogeneous solution, thereby giving us the complete solution to the non-homogeneous differential equation. The name “variation of parameters” aptly describes the essence of this method, emphasizing the dynamic adjustment of the parameters (coefficients) in response to the external force."
  },
  {
    "objectID": "mth106.html#eg-of-the-behind-story-of-euler-lagrange-equation",
    "href": "mth106.html#eg-of-the-behind-story-of-euler-lagrange-equation",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Eg of the behind story of “Euler-Lagrange Equation”",
    "text": "Eg of the behind story of “Euler-Lagrange Equation”\n\\(\\frac{d}{dt}\\frac{\\partial L}{\\partial q^{`}\\alpha}-\\frac{\\partial L}{\\partial q \\alpha}\\)\nWhile it’s often said that Lagrange was Euler’s student and that Euler had already discovered some of Lagrange’s work but chose not to publish to allow Lagrange to gain recognition, the reality of their relationship was a bit more nuanced.\nEuler was indeed supportive of Lagrange’s work and recognized his talent. At the age of 19, Lagrange had made significant contributions to mathematics that caught Euler’s attention. Lagrange sent Euler his work on the calculus of variations, and Euler was very supportive. He recognized Lagrange’s talents and helped introduce him to the wider mathematical community.\nEuler held back his own work on the calculus of variations to let Lagrange publish first. When Euler left Berlin for St. Petersburg in 1766, he recommended that Lagrange succeed him as the director of the Berlin Academy. This act was seen as a testament to Euler’s generous spirit and his recognition of Lagrange’s abilities."
  },
  {
    "objectID": "mth106.html#to-study-odes-and-pdes-the-thinking-way-of-linear-algebra-should-be-used",
    "href": "mth106.html#to-study-odes-and-pdes-the-thinking-way-of-linear-algebra-should-be-used",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "To study ODEs and PDEs, the thinking way of Linear Algebra should be used!",
    "text": "To study ODEs and PDEs, the thinking way of Linear Algebra should be used!\n\\(x^{(2)}\\)= x’’ = \\(\\frac {d^2 x}{dy^2}\\)–this notation of the second order’s is actually share the same idea of “division”"
  },
  {
    "objectID": "mth106.html#order",
    "href": "mth106.html#order",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Order",
    "text": "Order"
  },
  {
    "objectID": "mth106.html#liniarity",
    "href": "mth106.html#liniarity",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Liniarity",
    "text": "Liniarity\n$a_n(x) + … +a_1(x) +a_0(x)y =g(x) $\n\neg\n\n(y-x)dx+4xdy=0\n\n\\(\\frac {d^2 y}{dx^2}+\\sin y=0\\) is not linear since it has nonlinear function of y"
  },
  {
    "objectID": "mth106.html#solution-of-an-ode",
    "href": "mth106.html#solution-of-an-ode",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "solution of an ODE",
    "text": "solution of an ODE\n\n? why continuous? —\nAn funtion defined on an interval I and possessing at least n derivatives that are continuous on I, which when substituted into an nth-order ordinary differential equation reduces the equation to an identity, is said to be a solution of the equation on the interval.\nWe cannot think solution of an ordinary differential equation without simultaneously thinking interval.\n\n\ntrivial solution –y=0, I\n\n\nSolution Curve SEE CODEs\n\n\nExplicit and implicit solution shape of ODEs\n\n\nSingular solution\n\n\nA solution of a system\nSystem:\n\\[\\cases{^{{\\frac{dx}{dt}}=f(t,x,y)} _{\\frac{dy}{dt}=g(t,x,y)}}\\] Solution: x =\\(\\Phi_1(t)\\), y=\\(\\Phi_2(t)\\) defined on a common interval I that satisfy each equation of the system on this interval."
  },
  {
    "objectID": "mth106.html#existence-and-uniqueness",
    "href": "mth106.html#existence-and-uniqueness",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Existence and Uniqueness",
    "text": "Existence and Uniqueness\nDoes a solution of the problem exist? If a solution exists, is it unique?"
  },
  {
    "objectID": "mth106.html#interval-of-existence-and-uniqueness",
    "href": "mth106.html#interval-of-existence-and-uniqueness",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Interval of Existence and Uniqueness",
    "text": "Interval of Existence and Uniqueness\nSuppose y(x) represents a solution of the first order initial- value problem. The following three sets on the real x-axis may not be the same:\nthe domain of the function y(x), D\nthe interval I over which the solution y(x) is defined or exists,\nand the interval \\(I_0\\) of existence and uniqueness.\n\\(I_0 \\subseteq I \\subseteq D\\)"
  },
  {
    "objectID": "mth106.html#newtons-law",
    "href": "mth106.html#newtons-law",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Newton’s law",
    "text": "Newton’s law"
  },
  {
    "objectID": "mth106.html#properties-of-the-solutions-of-autonomous-de",
    "href": "mth106.html#properties-of-the-solutions-of-autonomous-de",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Properties of the solutions of autonomous DE",
    "text": "Properties of the solutions of autonomous DE\n\nA solution curve"
  },
  {
    "objectID": "mth106.html#translation-propertyonly-autonomous-des-has-this-property",
    "href": "mth106.html#translation-propertyonly-autonomous-des-has-this-property",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "? Translation Property–only autonomous DEs has this property?",
    "text": "? Translation Property–only autonomous DEs has this property?\n\nmeaning: translation?\n\n\nIf y(x) is a solution of an autonomous differential equation dy ∕dx = f(y), then y1(x) = y(x − k) (y of (x-k)), k a constant, is also a solution."
  },
  {
    "objectID": "mth106.html#singular-solution-1",
    "href": "mth106.html#singular-solution-1",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "singular solution",
    "text": "singular solution\n\\(\\frac {dy}{h(y)}=g(x)dx\\). If r is a zero of the function h(y), substituting y=r into dy/dx=g(x)h(y) makes both sides zero (y = r is a constant solution of the differential equation). As a consequence, y = r might not show up in the family of solutions that are obtained after integration and simplification. Such a solution is called a singular solution."
  },
  {
    "objectID": "mth106.html#initial-value-problem",
    "href": "mth106.html#initial-value-problem",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "initial value problem",
    "text": "initial value problem\n\nintergral-defined function\n\n\neg"
  },
  {
    "objectID": "mth106.html#integrating-factor-method",
    "href": "mth106.html#integrating-factor-method",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Integrating factor method",
    "text": "Integrating factor method\n\nsolution\nidea: want to written into \\(\\frac{d}{dx}()=f(x)\\) then we assume after multiplying a factor \\(\\mu(x)\\) we can write \\(\\frac{d}{dx}(\\mu(x)y(x))=\\mu(x)f(x)\\)\nwe get \\(\\mu(x) = ce^{\\int{p(x)dx}{}}\\).\nlet c=1 we get the integrating factor\n\n\nIVP"
  },
  {
    "objectID": "mth106.html#error-function",
    "href": "mth106.html#error-function",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "?Error function",
    "text": "?Error function"
  },
  {
    "objectID": "mth106.html#exact-equations",
    "href": "mth106.html#exact-equations",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Exact equations",
    "text": "Exact equations\nz=f(x,y)=c"
  },
  {
    "objectID": "mth106.html#non-exact",
    "href": "mth106.html#non-exact",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Non-exact",
    "text": "Non-exact"
  },
  {
    "objectID": "mth106.html#homogeneous-linear-system",
    "href": "mth106.html#homogeneous-linear-system",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Homogeneous Linear System",
    "text": "Homogeneous Linear System\nJust follow the idea in \\(y''+my'+ny=0\\), when facing the linear system \\(\\cases{{\\vec {x'} = A\\vec x}\\\\{ay'+by=0}}\\), we also assume that \\(\\vec {x'} = \\vec ke^{\\lambda t}\\) and substitute it into \\(\\vec {x'} = Ax\\)"
  },
  {
    "objectID": "mth106.html#phase-portrait-and-stable-unstable-semi-stable-points",
    "href": "mth106.html#phase-portrait-and-stable-unstable-semi-stable-points",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "phase portrait and stable/ unstable / semi-stable points",
    "text": "phase portrait and stable/ unstable / semi-stable points"
  },
  {
    "objectID": "mth106.html#jordan-form-and-ode-system",
    "href": "mth106.html#jordan-form-and-ode-system",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "? JORDAN FORM AND ODE system?",
    "text": "? JORDAN FORM AND ODE system?\nWhen we solve \\(ay''+by'+cy=0\\) if m is a double root, then \\(y_1=e^{mx}\\), \\(y_2=xe^{mx}\\)\nNaturally, here we assume \\(\\vec x_2 =\\vec k t e^{\\lambda t}\\), substitute \\(\\vec x_2\\) into the system \\(\\vec {x'} = Ax\\);\nLHS = \\(\\vec {x_2'} = \\vec k(e^{\\lambda t }+\\lambda te^{\\lambda t})=A\\vec x_2\\)=\\(A\\vec k t e^{\\lambda t}\\) =RHS\nThen we get \\(\\vec k = \\vec 0\\).\nSo our assumption is not true.\nThere is an extra term"
  },
  {
    "objectID": "mth106.html#对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量",
    "href": "mth106.html#对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量",
    "text": "对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量"
  },
  {
    "objectID": "mth106.html#把一个矩阵化为对角矩阵-gauss--jordan-elimination",
    "href": "mth106.html#把一个矩阵化为对角矩阵-gauss--jordan-elimination",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "把一个矩阵化为对角矩阵 （gauss- Jordan elimination）",
    "text": "把一个矩阵化为对角矩阵 （gauss- Jordan elimination）"
  },
  {
    "objectID": "Calculus.html",
    "href": "Calculus.html",
    "title": "Calculus and Multivariable Calculus",
    "section": "",
    "text": "( See the attached vedio for further explanation: http://youtube.com/watch?v=Vf0nqYaLWIs )\nProfessor. Wang Duo, a very charismatic and responsible Calculus teacher who has worked for our country in the area of math for at least 50 years (which was his target before and he has realized his dream!), always says “You should try to solve one problem many times with new methods to handle and understand knowledge better”\nFor math learning, focusing too much on exams is not so meaningful. We should experience its beauty from the bottom of our hearts. I did not reach my target in the Multivariable Calculus and the Linear Algebra models, but Dr. Bohuan Lin said:“Exams (especially written exams) require one to figure out solutions within a very short time period, and moreover, under an intense atmosphere. From my point of view, it is really not a problem if one fails to solve those difficult and nonstandard problems under such a condition. Of course, for math study (so as for the study of other things), we should not be satisfied with merely being able to solve”standard problems”, but just try to challenge and improve yourself with deep/difficult questions under a daily condition with a natural mood, since this is the common situation in which you will be working on various tasks in your future career. ”\nThere are many ways to solve problems, and all of them are interesting when multiple integrals occur.\n\n\nIt is easy\n\n\n\n\n\nuse method of sections directly\n\n\n\n\n\n\n\nsee the photo attached, which is the easiest way for computation:\n\n\n\n\n\n\nThanks to Dr.Bohuan Lin to teach me such a clever way which we do not need to draw the picture(only use conditional equations to solve it is interesting and a little difficult to handle it correctly).\nThe picture behind the method is attached to “Steinmetz(mou he fang gai)’s photo behind.png”\nWe have the set ( D ) defined as: \\[\nD \\triangleq \\left\\{(x, y, z) \\mid \\begin{cases}\nx^2 + y^2 \\leq 4 \\\\\ny^2 + z^2 \\leq 4 \\\\\nz^2 + x^2 \\leq 4\n\\end{cases} \\right\\}\n\\]\nFrom \\[x^2+ y^2 \\leq 4 , x^2 + z^2 \\leq 4 \\] we derive: if \\[\n|x| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |z| \\leq \\sqrt{2}\n\\] (and &lt; is the same shape)\nSimilarly:\nif \\[|z| \\geq \\sqrt{2} ,|x| \\leq \\sqrt{2} \\quad \\text{then} \\quad |y| \\leq \\sqrt{2} \\]\nAnd: if \\[\n|z| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |x| \\leq \\sqrt{2}\n\\] Therefore: \\[\nD = D_{\\leq \\sqrt{2}} \\cup \\bar{D}\n\\]\nWhere: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\] \\[\n\\begin{aligned}\n&= \\left\\{ (x, y, z) \\in \\mathbb{R}^3 \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\} \\\\\n&= [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}]\n\\end{aligned}\n\\]\nAnd: \\[\n\\bar{D} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus: \\[\n\\bar{D} = \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}}\n\\]\n(This also implies: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\])\nWe derive: \\[\n\\bar{D} = \\left\\{ (x, y, z) \\mid |x| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |y| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus, combining all components, we get: \\[\nD = \\left( D_{\\leq \\sqrt{2}} \\cup \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}} \\right)\n\\] So we could decompose it into a cube in the center and 6 common volume. the 6 volume: 6 \\(\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\nIn summary, the whole volume is \\((2 \\sqrt 2)^3 +6\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\n\n\n\n\nThanks to Dr. Haoran Chen for teaching us such method.\nWe suppose z&gt;x, z&gt;y,（then *3） then we could continue decompose it into (1)z~(\\(\\sqrt 2\\),2) and (2)z~(\\(0,\\sqrt 2\\))(based on whether the square is out of the circle)\n\n\n\n\nsee the photo attached"
  },
  {
    "objectID": "Calculus.html#cylinders",
    "href": "Calculus.html#cylinders",
    "title": "Calculus and Multivariable Calculus",
    "section": "",
    "text": "It is easy\n\n\n\n\n\nuse method of sections directly\n\n\n\n\n\n\n\nsee the photo attached, which is the easiest way for computation:\n\n\n\n\n\n\nThanks to Dr.Bohuan Lin to teach me such a clever way which we do not need to draw the picture(only use conditional equations to solve it is interesting and a little difficult to handle it correctly).\nThe picture behind the method is attached to “Steinmetz(mou he fang gai)’s photo behind.png”\nWe have the set ( D ) defined as: \\[\nD \\triangleq \\left\\{(x, y, z) \\mid \\begin{cases}\nx^2 + y^2 \\leq 4 \\\\\ny^2 + z^2 \\leq 4 \\\\\nz^2 + x^2 \\leq 4\n\\end{cases} \\right\\}\n\\]\nFrom \\[x^2+ y^2 \\leq 4 , x^2 + z^2 \\leq 4 \\] we derive: if \\[\n|x| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |z| \\leq \\sqrt{2}\n\\] (and &lt; is the same shape)\nSimilarly:\nif \\[|z| \\geq \\sqrt{2} ,|x| \\leq \\sqrt{2} \\quad \\text{then} \\quad |y| \\leq \\sqrt{2} \\]\nAnd: if \\[\n|z| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |x| \\leq \\sqrt{2}\n\\] Therefore: \\[\nD = D_{\\leq \\sqrt{2}} \\cup \\bar{D}\n\\]\nWhere: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\] \\[\n\\begin{aligned}\n&= \\left\\{ (x, y, z) \\in \\mathbb{R}^3 \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\} \\\\\n&= [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}]\n\\end{aligned}\n\\]\nAnd: \\[\n\\bar{D} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus: \\[\n\\bar{D} = \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}}\n\\]\n(This also implies: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\])\nWe derive: \\[\n\\bar{D} = \\left\\{ (x, y, z) \\mid |x| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |y| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus, combining all components, we get: \\[\nD = \\left( D_{\\leq \\sqrt{2}} \\cup \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}} \\right)\n\\] So we could decompose it into a cube in the center and 6 common volume. the 6 volume: 6 \\(\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\nIn summary, the whole volume is \\((2 \\sqrt 2)^3 +6\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\n\n\n\n\nThanks to Dr. Haoran Chen for teaching us such method.\nWe suppose z&gt;x, z&gt;y,（then *3） then we could continue decompose it into (1)z~(\\(\\sqrt 2\\),2) and (2)z~(\\(0,\\sqrt 2\\))(based on whether the square is out of the circle)\n\n\n\n\nsee the photo attached"
  },
  {
    "objectID": "Calculus.html#integral-problems-with-solution-of-alternating-integral-test",
    "href": "Calculus.html#integral-problems-with-solution-of-alternating-integral-test",
    "title": "Calculus and Multivariable Calculus",
    "section": "integral problems with solution of alternating integral test",
    "text": "integral problems with solution of alternating integral test\nquestion: does \\[\\int_{0}^{\\infty}sinx/x\\,dx\\] converge?\nIt is easy to think of these 2 famous but difficultly proved formula: \\[\\int_{-\\infty}^{\\infty}sinx/x\\,dx=\\pi\n\\] \\[\\int_{-\\infty}^{\\infty}e^{-x^2}\\,dx=\\pi\n\\] However, they are not useful, which is how charasmatic the math is! I love math!\n\\[=\\Sigma_{n=1}^{\\infty}\\int_{2(n-1)\\pi}^{2n\\pi}sinx/x\\,dx\n\\] Then we let \\(x-[2(n-1)\\pi]=y\\)(dx=dy)\nso \\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{2\\pi}siny/(y+[2(n-1)\\pi])\\,dy\\] =\\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}siny/(y+[2(n-1)\\pi])\\,dy+\\Sigma_{n=1}^{\\infty}\\int_{\\pi}^{2\\pi}siny/(y+[2(n-1)\\pi])\\,dy\\]\nThen we let y-\\(\\pi\\)=z(dy=dz)\n\\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}siny/(y+[2(n-1)\\pi])\\,dy+\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}-sinz/(z+[(2n-1)\\pi])\\,dz\\] \\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}sinm/(m+[2(n-1)\\pi])\\,dm+\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}-sinm/(m+[(2n-1)\\pi])\\,dm\\]\nso if \\[a_n=\\int_{0}^{\\pi}sinm/(m+(n-1)\\pi)\\] \\[\\int_{0}^{\\infty}sinx/x\\,dx=a_1-a_2+a_3-a_4+...+a_n\\], which is an alternating series!\nCoincidently, it is decresing and \\(a_n&lt;\\int_{0}^{\\pi}1/(n-1)\\pi\\,dm=1/(n-1)\\), and \\(\\lim_{{n \\to \\infty}} \\left( \\frac{1}{n-1}  \\right) = 0\\) so \\(\\lim_{{n \\to \\infty}}a_n=0\\) So it converges(Alternating series test)."
  },
  {
    "objectID": "Calculus.html#similar-question-2",
    "href": "Calculus.html#similar-question-2",
    "title": "Calculus and Multivariable Calculus",
    "section": "similar question 2",
    "text": "similar question 2\nThanks for Dr.Chi-Kun Lin to teach me this kind of problems!\n\\[\n\\int_0^\\infty \\frac{1}{1 + x^p \\sin^2 x} \\, dx\n\\]\n\n\n\n\\[\n\\sum_{n=0}^\\infty \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n + \\frac{1}{2} \\right)^2 p \\sin^2 t} \\, dt = \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n + \\frac{1}{2} \\right)^2 p \\sin^2 t} \\, dt\n\\]\n\\[\\Sigma\\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n\\pi/2 + t \\right)^ p \\sin^2 t} \\, dt + \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left(( n + 1 \\right)\\pi/2-t)^ p sin^2t} \\, dt\\]\nfor:\n\\[\nx_n = \\Sigma\\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n\\pi/2 + t \\right)^ p \\sin^2 t} \\, dt + \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left(( n + 1 \\right)\\pi/2-t)^ p sin^2t} \\, dt,\n\\]\nthere has\n\\[2 \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right)^ p t^2} \\, dt \\leq x_n \\leq 2 \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right) ^p \\frac{4}{\\pi^2} t^2} \\, dt\\]\n\nHowever, the integrals on both sides of the inequality can be calculated separately. For example:\n\n\\[\n\\int_0^1 \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right)^ p t^2} \\, dt = \\frac{1}{\\sqrt{((n + 1) \\frac{\\pi}{2} )^p}} \\arctan \\left( \\sqrt{(n + 1) \\frac{\\pi}{2}}^{2p}\\pi/2 \\right) \\geq \\frac{1}{\\sqrt{(n + 1) \\frac{\\pi}{2} }^{2p}  } \\frac{\\pi}{4}\n\\]"
  },
  {
    "objectID": "Calculus.html#introduction-with-the-relationship-between-it-and-lasso",
    "href": "Calculus.html#introduction-with-the-relationship-between-it-and-lasso",
    "title": "Calculus and Multivariable Calculus",
    "section": "introduction with the relationship between it and lasso",
    "text": "introduction with the relationship between it and lasso\nThe Lagrange multiplier method can transform constrained problems into unconstrained problems, which is a great wisdom. It can be used to solve lasso, and lasso is an interesting and wonderful method for dimensionality reduction in statistics, which makes me feel that the journey of learning multivariate calculus is a joy："
  },
  {
    "objectID": "Calculus.html#lagrange-multipliers-perspective-of-penalized-models",
    "href": "Calculus.html#lagrange-multipliers-perspective-of-penalized-models",
    "title": "Calculus and Multivariable Calculus",
    "section": "Lagrange Multipliers Perspective of Penalized Models",
    "text": "Lagrange Multipliers Perspective of Penalized Models\nLagrange multipliers provide a way to incorporate constraints into optimization problems. In the context of penalized regression, we can view the regularization term as a constraint on the size of the coefficients.\nChoose Lasoo as an example, the Lagrange multipliers perspective could be expressed as:\n\\[\n\\left( \\hat{\\alpha}, \\hat{\\beta} \\right) = \\arg \\min \\left\\{ \\sum_{i=1}^{N} \\left( y_{i} - \\alpha - \\sum_{j} \\beta_{j} x_{ij} \\right)^{2} \\right\\} \\quad \\text{subject to} \\quad \\sum_{j} \\left| \\beta_{j} \\right| \\leq t.\n\\]\ni.e.\n\\[\ng(\\beta)= \\sum_{j} \\left| \\beta_{j} \\right| - t \\leq 0\n\\]\n\\[\nL(\\alpha,\\beta, \\lambda) = \\sum_{i=1}^{N} \\left( y_{i} - \\alpha - \\sum_{j} \\beta_{j} x_{ij} \\right)^{2} + \\lambda \\left( \\sum_{j} \\left| \\beta_{j} \\right| - t \\right)\n\\]\n\\[\n\\partial L / \\partial \\alpha = 0\n\\]\n\\[\n\\hat{\\alpha} = \\bar{y} - \\sum_{j} \\hat{\\beta}_{j} \\bar{x}_{j}\n\\]\n\\[\n\\partial L / \\partial \\beta_{j} = 0\n\\]\n\\[\n-2 \\sum_{i=1}^{N} x_{ij} \\left( y_{i} - \\hat{\\alpha} - \\sum_{k} \\hat{\\beta}_{k} x_{ik} \\right) + \\lambda \\cdot sign(\\hat{\\beta}_{j}) = 0\n\\]\nThis may use the iterative method to solve the equation.\nGenerally, the formula could be expressed as:\n\\[\n\\min_{\\beta} \\left\\{ \\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_p^p \\right\\}\n\\]\nwhere \\(\\lVert \\beta \\rVert_p^p\\) is the \\(L^p\\) norm of the coefficients, which serves as a penalty term.\nThe gradient of the objective function of Ridge with respect to \\(\\beta\\) is given by:\n\\[\n\\nabla J(\\beta) = -2X^T(y - X\\beta) + \\lambda \\nabla \\lVert \\beta \\rVert_p^p\n\\]\n(see matrix form (Go to Matrix Form) below for more details)\nThis should be set to zero to find the optimal \\(\\beta\\), which means the gradient of the loss function (the first term) is equal to the gradient of the penalty term (the second term) scaled by \\(\\lambda\\)."
  },
  {
    "objectID": "Calculus.html#geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective",
    "href": "Calculus.html#geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective",
    "title": "Calculus and Multivariable Calculus",
    "section": "Geometric Interpretation of Penalized Models in Lagrange Multipliers Perspective",
    "text": "Geometric Interpretation of Penalized Models in Lagrange Multipliers Perspective"
  },
  {
    "objectID": "Calculus.html#summary",
    "href": "Calculus.html#summary",
    "title": "Calculus and Multivariable Calculus",
    "section": "Summary",
    "text": "Summary\n\nLasso Regression uses L1 regularization to achieve feature selection, and its optimization problem can be transformed into a constrained optimization problem.\nLagrange Multipliers provide a method to handle constraints in optimization problems by converting them into unconstrained problems and introducing multipliers to adjust the regularization strength.\n\nIn Lasso regression, the L1 regularization constraint can be handled using Lagrange multipliers, converting the problem into one with multipliers that adjust the strength of regularization. ### Problem\nFind extreme values of\n\\[ f(x, y) = \\cos x + \\cos y + \\cos (x + y). \\]\n\nSolution\nSince cosine is a periodic function, we can consider the region ( 0 x ), ( 0 y ) (bounded and closed region) to find the maximal and minimal values.\nFirstly, we consider the interior of the region to find stationary points:\n\\[ f_x = -\\sin x - \\sin (x + y) = 0 \\] \\[ f_y = -\\sin y - \\sin (x + y) = 0 \\]\nThis implies:\n\\[ \\sin x = \\sin y \\]\nThen, inside the region (not on boundary), we have three cases:\n\n( y = x )\n( y = - x ) for ( 0 &lt; x &lt; )\n( y = 3- x ) for ( &lt; x &lt; 2)\n\n\nCase 1\n\\[ \\sin x + \\sin (x + y) = \\sin x + 2 \\sin x \\cos x = 0 \\] \\[ \\sin x (2 \\cos x + 1) = 0 \\]\nThis implies:\n\\[ x = \\pi, y = \\pi \\] or \\[ x = \\frac{2\\pi}{3}, y = \\frac{2\\pi}{3} \\] or \\[ x = \\frac{4\\pi}{3}, y = \\frac{4\\pi}{3} \\]\nEvaluating the function at these points:\n\\[ f(\\pi, \\pi) = -1 \\] \\[ f\\left( \\frac{2\\pi}{3}, \\frac{2\\pi}{3} \\right) = -\\frac{3}{2} \\] \\[ f\\left( \\frac{4\\pi}{3}, \\frac{4\\pi}{3} \\right) = -\\frac{3}{2} \\]\n\n\nCase 2\n\\[ \\sin x + \\sin (x + y) = \\sin x + \\sin \\pi = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 0 \\] (on boundary)\n\n\nCase 3\n\\[ \\sin x + \\sin (3\\pi - x) = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 2\\pi \\] (also on boundary)\n\n\n\nOn the Boundary\nDue to periodic property, we consider:\n\\[ x = 0, 0 \\leq y \\leq 2\\pi \\] and \\[ y = 0, 0 \\leq x \\leq 2\\pi \\]\nEvaluating the function at these boundaries:\n\\[ f(0, y) = 1 + 2 \\cos y, \\min = -1, \\max = 3 \\] \\[ f(x, 0) = 1 + 2 \\cos x, \\min = -1, \\max = 3 \\]\nSo,\n\\[ \\max f(x, y) = 3 \\text{ at } (2n\\pi, 2k\\pi) \\text{ for any } n, k \\in \\mathbb{Z} \\] \\[ \\min f(x, y) = -\\frac{3}{2} \\text{ at } \\left( (2n+1)\\pi \\pm \\frac{\\pi}{3}, (2k+1)\\pi \\pm \\frac{\\pi}{3} \\right) \\]"
  },
  {
    "objectID": "mth107.html",
    "href": "mth107.html",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "The first section here is the knowledge that attracts most of my interests on this module, followed by the lecture notes I tapped when I am on the journey of this module.\n\n\nI feel so lucky to learn from Professor Paul-Henry Leemann. He teaches Linear Algebra very well. He not only teahces us how to solve tutorials, but also the real application in the real world such as the application of eigenvectors on Google search. He is also a kind, responsible and humorous teacher. His lecture notes are clean and clear with multiple good examples helping us to understand more effectively.\nProfessor Andrew Lin teaches Linear Algebra very very well and interesting. He teaches me many tips to help me never remember sth., understand totally instead."
  },
  {
    "objectID": "mth107.html#thanks",
    "href": "mth107.html#thanks",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "I feel so lucky to learn from Professor Paul-Henry Leemann. He teaches Linear Algebra very well. He not only teahces us how to solve tutorials, but also the real application in the real world such as the application of eigenvectors on Google search. He is also a kind, responsible and humorous teacher. His lecture notes are clean and clear with multiple good examples helping us to understand more effectively.\nProfessor Andrew Lin teaches Linear Algebra very very well and interesting. He teaches me many tips to help me never remember sth., understand totally instead."
  },
  {
    "objectID": "mth107.html#eigenvalue-and-eigenspace",
    "href": "mth107.html#eigenvalue-and-eigenspace",
    "title": "MTH107—NOTEs",
    "section": "Eigenvalue and Eigenspace",
    "text": "Eigenvalue and Eigenspace\nAn operator on V has a diagnalizable matrix representation if it has the same number as dimV of eigenvectors .\nHowever, some operators have not enough eigenvectors to span a whole vector space so that the matrix could not be as simple as a diagnalizable matrix."
  },
  {
    "objectID": "mth107.html#generalized-eigenspace",
    "href": "mth107.html#generalized-eigenspace",
    "title": "MTH107—NOTEs",
    "section": "Generalized eigenspace",
    "text": "Generalized eigenspace\n\\(A=PJ_{\\lambda}P^{-1}\\)\nWhen P has linearly independent vectors(eigenvectors), A is diagonalizable\nSo we introduce the Jordan form which is as simple as possible, though not much simple as a diagnal matrix, which need generalized eigenvector to get P—\\((A-\\lambda I)^2x_2=(A-\\lambda I)x_1=0\\)\nSince \\(A=PJ_{\\lambda}P^{-1}\\), P=[\\(x_1,x_2\\)], AP=A[\\(x_1,x_2\\)]=[\\(\\lambda x_1, \\lambda x_2+x_1\\)]=PJ=[\\(x_1,x_2\\)]=\\(\\left[\\begin{array}{ll}\\lambda & 1 \\\\ 0 & \\lambda\\end{array}\\right]\\)\nThe introduce of generalized eigenspaces makes a matrix as simple as possible, though not much simple as a diagnal matrix for computation."
  },
  {
    "objectID": "mth107.html#cayley-hamlton-thm",
    "href": "mth107.html#cayley-hamlton-thm",
    "title": "MTH107—NOTEs",
    "section": "Cayley Hamlton Thm",
    "text": "Cayley Hamlton Thm\nV a finite dimensional vector space then \\(\\chi_T(T)=0_{l(v)}\\)\neg. A=$"
  },
  {
    "objectID": "mth107.html#jordan-form-makes-me-understand-linear-algebra-more-deeply-which-also-introduces-differential-equations-to-me",
    "href": "mth107.html#jordan-form-makes-me-understand-linear-algebra-more-deeply-which-also-introduces-differential-equations-to-me",
    "title": "MTH107—NOTEs",
    "section": "Jordan form makes me understand Linear Algebra more deeply, which also introduces differential equations to me",
    "text": "Jordan form makes me understand Linear Algebra more deeply, which also introduces differential equations to me\nJ = \\(\\lambda I+ N\\), where N is a nilpotent matrix —D-N decomposition A = D +N DN = ND\neg.\\(e^A=e^{D+N} ?= e^De^N\\)\n$e^{tJ_}= e^{tI} $\\(e^{tN}\\)=\\(e^{t\\lambda} [I+N+t^2/2! N^2+.....]\\)\n\nThe characteristic polynomial tells us the eigenvalues and the dimension of each generalized eigenspace, which is the number of times the eigenvalue appears along the diagonal of the Jordan form (also known as the “multiplicity” of λ)\nEach Jordan block with respect to a basis of an eigenvector and others are generalized eigenvectors which could be computed based on the eigenvector and \\((T-\\lambda Id)^m,\\)where m\\(\\leq dimV\\)since the dimension of each eigenspace tells us how many Jordan blocks corresponding to that eigenvalue there are in the Jordan form. This also means that the Jordan form is unique with permutation but the Jordan basis is not unique.\nThe size of the largest Jordan block corresponding to an eigenvalue λ of T is exactly the degree of the (t − λ) term in the minimal polynomial of T .i.e. The exponents of the different terms in the minimal polynomial tell us the sizes of the largest Jordan blocks corresponding to each eigenvalue.\n\nTo kill off a Jordan block of size k, we need the polynomial \\((t-\\lambda)^k\\). Any smaller Jordan block with the same eigenvalue will also be killed off under this polynomial\n\nWe could explain it more clearly\n\n\nFor one simple situation, which is the Jordan form with only 1 block\nSince the minimal polynomial applied by the operator T on V (\\(M_T(T)\\)) must be 0, the Jordan form with only 1 block has the minimal polynomial (t-\\(\\lambda\\))\\(^{dimV}\\)\nMore generally, if we have a Jordan form with 2 blocks B and C, with one same eigenvalue, Since the minimal polynomial applied by the operator T on V (\\(M_T(T)\\)) must be 0, \\((T-\\lambda I)^m\\) in \\(M_T(T)\\) should have m satisfying \\((B-\\lambda I)^m=0\\) and \\((C-\\lambda I)^m=0\\). In this case, since any smaller Jordan block with the same eigenvalue will also be killed off under the \\((T-\\lambda I)^m\\) in \\(M_T(T)\\), when the largest Jordan block is 0, smaller blocks are also 0. What is more, if \\(\\exists\\) a block with dimension greater than the power of the corresponding term in the polynomial, there then is a contradiction to the defination of the minimal polynomial(must be 0). If the largest Jordan block has the dimension strictly less than the power of the corresponding term in the polynomial, there then also is a contradiction to the defination of the minimal polynomial(smallest power)\nEach block has a basis of eigenvector and other basis are generalized eigenvector which are linearly independent.\nMore precisely, each term with power 1, means that the restriction of T to the corresponding generalised eigenspace is diagonalisable.So the minimal polynomial’s each term with the power of 1 means the matrix representation of the operator is diagonalizable, and each eigenvalue has the same number of corresponding power of characteristic polynomial’s term of linearly independent eigenvectors."
  },
  {
    "objectID": "mth107.html#matrix-basic-knowledge",
    "href": "mth107.html#matrix-basic-knowledge",
    "title": "MTH107—NOTEs",
    "section": "Matrix basic knowledge",
    "text": "Matrix basic knowledge\n\nAB \\(\\neq\\) BA – it is not a number, but a linear transformation (a function). AB is the composition of two functions, while BA is another composition of two functions. They are not the same in general."
  },
  {
    "objectID": "mth107.html#the-relationship-between-the-direct-sumdecomposition-and-diagolizable",
    "href": "mth107.html#the-relationship-between-the-direct-sumdecomposition-and-diagolizable",
    "title": "MTH107—NOTEs",
    "section": "The relationship between the direct sum(decomposition) and diagolizable",
    "text": "The relationship between the direct sum(decomposition) and diagolizable\n\nIf A:V–&gt; V, A is diagonalizable then V=RangeA direct sum NullA (proof idea: clarify the basis of A by the non-zero element and zero element on the diagonal of the diagonal shape matrix of A. Based on them we could find the basis of RangeA and NullA which span V while with the intersection of {0})"
  },
  {
    "objectID": "mth107.html#the-relationship-between-jordan-form-and-diffrential-equation",
    "href": "mth107.html#the-relationship-between-jordan-form-and-diffrential-equation",
    "title": "MTH107—NOTEs",
    "section": "The relationship between Jordan form and diffrential equation",
    "text": "The relationship between Jordan form and diffrential equation"
  },
  {
    "objectID": "mth107.html#diagnalize-2-matrix-at-the-same-time",
    "href": "mth107.html#diagnalize-2-matrix-at-the-same-time",
    "title": "MTH107—NOTEs",
    "section": "Diagnalize 2 matrix at the same time",
    "text": "Diagnalize 2 matrix at the same time"
  },
  {
    "objectID": "mth107.html#linearty-is-everywherelinearty-is-the-easiest-one",
    "href": "mth107.html#linearty-is-everywherelinearty-is-the-easiest-one",
    "title": "MTH107—NOTEs",
    "section": "Linearty is everywhere–Linearty is the easiest one",
    "text": "Linearty is everywhere–Linearty is the easiest one\nmatrix-vector multiplication\ndifferentiation/integration\nODE(mth106)\nRecurrence Relations and Statistical Models"
  },
  {
    "objectID": "mth107.html#comparison",
    "href": "mth107.html#comparison",
    "title": "MTH107—NOTEs",
    "section": "comparison",
    "text": "comparison\n\nReal R space\nA matrix A in this space is a real matrix, which maps vectors in \\(\\mathbb{R^n}\\) to another vector in \\(\\mathbb{R^n}\\) through multiplication: \\[\nA: \\mathbb{R^n} \\to \\mathbb{R^n}\n\\] \\[\nA \\cdot v = w \\in \\mathbb{R^n}\n\\]\nreal matrix\nconcrete\n\n\nAbstract vector space over R or C\nlinear transformation: \\[\nT: V \\to W\n\\] abstract and more general vector space over \\(\\mathbb{R^n}\\) or \\(\\mathbb{C^n}\\)"
  },
  {
    "objectID": "mth107.html#notations",
    "href": "mth107.html#notations",
    "title": "MTH107—NOTEs",
    "section": "Notations",
    "text": "Notations\n\n\\[\na \\in A\n\\] eg. \\[\n3 \\in \\mathbb{Z}\n\\]\n\\(\\emptyset\\)\nTwo sets A and B are equal if and only if they contain the same elements: \\[\nA = B\n\\] if and only if \\[\n\\forall x \\, (x \\in A \\iff x \\in B)\n\\] \\[ A \\subseteq B\\]\nintersection: \\[ A \\cap B\\]\nunion: \\[ A\\cup B \\]"
  },
  {
    "objectID": "mth107.html#maps",
    "href": "mth107.html#maps",
    "title": "MTH107—NOTEs",
    "section": "Maps",
    "text": "Maps\nmaps(functions)\n\\[\nf: A \\to B, \\quad a \\mapsto f(a)\n\\] composition: Given two functions \\(f: B \\to C\\) and \\(g: A \\to B\\), the composition of f and g is denoted as: \\[\n(f \\circ g)(x) = f(g(x)), \\quad \\text{for all} \\, x \\in A\n\\]\n\\[\na \\mapsto c(a \\mapsto b \\mapsto c)\n\\]\n\ninjective–one to one\nif \\(a_1 \\neq a_2\\) then \\(f(a_1) \\neq f(a_2)\\)\nif \\(f(a_1) = f(a_2)\\), then \\(a_1=a_2\\)\n\n\nsurjective–onto\nfor \\(f: A \\to B\\)\n\\[\n\\forall b \\in B, \\, \\exists a \\in A \\, \\text{such that} \\, f(a) = b\n\\]\n\n\nbijetive–both inj and suj\nfor \\(f: A \\to B\\) \\[\n\\forall b \\in B, \\, \\exists! a \\in A \\, \\text{such that} \\, f(a) = b\n\\] （use \\(\\exists!\\)expresses only exist one）\nf: x–&gt;y is a bijection if and only if \\(\\exists\\)g:y—&gt;x, s.t. f(g(y))=\\(id_y\\) and g(f(x))=\\(id_x\\)"
  },
  {
    "objectID": "mth107.html#subsetsinclusions-restrictions",
    "href": "mth107.html#subsetsinclusions-restrictions",
    "title": "MTH107—NOTEs",
    "section": "subsets,inclusions, restrictions",
    "text": "subsets,inclusions, restrictions\n\nsubsets\n\\(A \\subseteq B\\)\n\n\nempty set is a subset of every set B.\n\\[\n\\emptyset \\to B\n\\]\n\n\ninclusion\nif A \\(\\subseteq\\) B, we have a map: \\[\n\\iota: A \\to B\n\\]\n\\[\n\\iota(a) = a\n\\], called the inclusion of A (into B)\n\n\nrestriction\nThat is:\nif A \\(\\subseteq\\) B and \\(f:B \\to C\\), we have a map for all a \\(\\in\\) A: \\[\nf|_A: A \\to C\n\\]:\n\\[\nf|_A(a) = f(a)\n\\] called the restriction of f to A\n\\[\nf|_A = f \\circ \\iota_A\n\\] because \\[\nf \\circ \\iota_A(a)=f(\\iota_A(a))=f(a)=f|_A(a)\n\\]\nthe reason to define it:\n\ndomains differ\n\nFor example, consider a map \\(g|_B: B \\to C\\), where the function takes each element \\(x\\) and maps it to \\(x + 2\\). Let the sets be as follows:\n\n\\(A = \\mathbb{N} \\subseteq B = \\mathbb{R}\\)\n\\(C = \\mathbb{R}\\) is the codomain of the function.\n\nThe map \\(g\\) is defined as: \\[ g: B \\to C, \\quad g(x) = x + 2 \\quad \\text{for all } x \\in B \\]\nNow, consider the restriction of \\(g\\) to \\(A\\), denoted \\(g|_A: A \\to C\\), where: \\[ g|_A: A \\to C, \\quad g|_A(x) = x + 2 \\quad \\text{for all } x \\in A \\]\nAlthough both \\(g: \\mathbb{R} \\to \\mathbb{R}\\) and \\(g|_A: \\mathbb{N} \\to \\mathbb{R}\\) follow the same rule \\(x \\mapsto x + 2\\), they cannot be considered the same map because their domains differ.\n(here, A \\(\\subseteq\\) B)\n\nfocus on the specific area within the whole area\n\n\n\nset of all maps from A to B(f)\nif a and b are sets we define \\(B^A={f:A---&gt;B (map)}\\) the set of all maps from A to B\n\\[\nB^A = \\{ f: A \\to B \\}\n\\] \\(B^\\emptyset = \\{ \\emptyset \\to B \\}\\) has only 1 element even if B=\\(\\emptyset\\)\n\nThe set \\(B^{\\emptyset}\\) contains only the zero function. proof: suppose f,g \\(\\in\\) \\(B^\\emptyset = \\{f: \\emptyset \\to B \\}\\), imagine f != g, we can derive \\(\\exists x \\in \\emptyset\\) s.t. f(x) != g(x), which is absurd, so f=g\n\nsuppose the only one element is sth.:\nsth. should follow the quality of the set, which is:\nsth. + sth. = sth.\n\\(\\lambda\\)sth. =sth.\nso sth. = 0\n\nrelevant to cardinality:\n(Def: let A be a.set, if A containsfinitely many elements, then the number of elements of A is called the cardinality of A, denoted by |A|(|A|\\(\\in Z_{\\geq0}\\))\n|| #\n|A|: number of elements in A(set))\nif A and B are finite sets #A=n #B=m, then \\(|B^A| = m^n\\) (ok)"
  },
  {
    "objectID": "mth107.html#finite-space",
    "href": "mth107.html#finite-space",
    "title": "MTH107—NOTEs",
    "section": "finite space",
    "text": "finite space\nFor finite spaces ,only R^n and C^n.\n(For infinite spaces, there are many, e.g. a continuous set: [0,1] to …)"
  },
  {
    "objectID": "mth107.html#rn-notification",
    "href": "mth107.html#rn-notification",
    "title": "MTH107—NOTEs",
    "section": "R^n notification",
    "text": "R^n notification\n\\[\n\\mathbb{R}^n = \\{ (x_1, x_2, \\dots, x_n) \\mid x_i \\in \\mathbb{R} \\text{ for } i = 1, 2, \\dots, n \\}\n\\] real numbers\n\ndefine 2 oprations on the set R^n\naddition: \\[\n(x_1, x_2, \\dots, x_n) + (y_1, y_2, \\dots, y_n) = (x_1 + y_1, x_2 + y_2, \\dots, x_n + y_n)\n\\]\n\nLeft-hand side: \\((x_1, x_2, \\dots, x_n), (y_1, y_2, \\dots, y_n) \\in \\mathbb{R}^n\\)\nRight-hand side: The result of \\(x_i + y_i\\) is in \\(\\mathbb{R}\\).\n\nscalar multiplication: for lambda \\(\\in\\) R,\n\\[\n\\lambda \\cdot (x_1, x_2, \\dots, x_n) = (\\lambda x_1, \\lambda x_2, \\dots, \\lambda x_n)\n\\]\n\nLeft-hand side: \\(\\lambda \\in \\mathbb{R}, (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n\\).\nRight-hand side: The result \\(\\lambda x_i \\in \\mathbb{R}\\), for each i.\n\nthese 2 oprations generalize the standard operations on R2 and R3\n\n\ngeometric realization\n矢量三角形–addition\n直线上–scalar multiplication\nmention: for n greater than or equal to 4 we cannot visualize vectors in the real world but e can still use them to solove real world problems\nif a and b are finite sets\n\n\nto simplify notations we sometimes use a single letter for vectors:\nx= \\(\\vec x\\) =(x_1,x_2,….)\nR^n as a vector space\nso we have(rn,+, mutiplication notation). the operations satisfies some useful properties that turn rn into a real vector space"
  },
  {
    "objectID": "mth107.html#relation-between-rn-and-.",
    "href": "mth107.html#relation-between-rn-and-.",
    "title": "MTH107—NOTEs",
    "section": "relation between rn and + .",
    "text": "relation between rn and + .\n\\((R^n,+,\\cdot)\\)\nit means that + and \\(\\cdot\\) satisfy the following axioms:\n\n\\(\\forall x,y \\in rn\\): x+y=y+x. commutativity\n\\(\\forall x,y,z \\in rn\\): (x+y)+z=x+(y+z). associativity\nx + 0 = 0 + x neutral element for addition\n\\(x + (-x) = (-x) + x = 0\\)\n\n——inverse for addition -x=y\n\n\\(1 \\cdot x = x\\) Identity Element for Scalar Multiplication\n\\((\\lambda \\mu) \\cdot x = \\lambda \\cdot (\\mu \\cdot x)\\) compatibilily of multiplication\n\\(\\lambda \\cdot (x + y) = \\lambda \\cdot x + \\lambda \\cdot y\\) Distributivity of Scalar Multiplication Over Vector Addition\n\\((\\lambda + \\mu) \\cdot x = \\lambda \\cdot x + \\mu \\cdot x\\) Distributivity of Scalar Addition\n\n\n?\nalso fn—f{1,..,n}\n5st ahead and the other 3（prove？eg tutorial1）. should we prove again or directly use them? tutorial 1 vs. 2 ’ s proof. ask again, sorry: since negative number set satisfies 2 oprations but not 8 axioms I want to ensure if a finite set satisfies the 2 operations it is not satisfy all 1-8 axioms instead of F^n so if the problem is a vector space V, we could directly suppose \\(\\exists u\\in F\\) and then do the scalar multiplication in it?\nis that because of the defination of F-vector space?(abstract vector space-defination)"
  },
  {
    "objectID": "mth107.html#cn-complex-vector-space",
    "href": "mth107.html#cn-complex-vector-space",
    "title": "MTH107—NOTEs",
    "section": "Cn complex Vector Space",
    "text": "Cn complex Vector Space\n\\(i^2 = -1\\)\nDefine \\(\\mathbb {C}^n\\) as the set of all ordered n-tuples of complex numbers: \\[\n\\mathbb{C}^n = \\{ (z_1, z_2, \\dots, z_n) \\mid z_i \\in \\mathbb{C}, \\, i = 1, 2, \\dots, n \\}\n\\]\n\nOperations on C^n\nWe define addition and scalar multiplication on \\(\\mathbb{C}^n\\) in the same way as we did on \\(\\mathbb{R}^n\\), but using complex numbers:\n\nAddition:\nFor two vectors \\((z_1, z_2, \\dots, z_n)\\) and \\((w_1, w_2, \\dots, w_n) \\in \\mathbb{C}^n\\):\n\\[\n(z_1, z_2, \\dots, z_n) + (w_1, w_2, \\dots, w_n) = (z_1 + w_1, z_2 + w_2, \\dots, z_n + w_n)\n\\]\n\n\nScalar Multiplication:\nFor a scalar \\(\\lambda \\in \\mathbb{C}\\) and a vector \\((z_1, z_2, \\dots, z_n) \\in \\mathbb{C}^n\\):\n\\[\n\\lambda \\cdot (z_1, z_2, \\dots, z_n) = (\\lambda z_1, \\lambda z_2, \\dots, \\lambda z_n)\n\\]\nThus, \\((\\mathbb{C}^n, +, \\cdot)\\) is a complex vector space because it satisfies the vector space axioms 1-8, where we replaced \\(\\mathbb{R}^n\\) with \\(\\mathbb{C}^n\\). Many of the results from MTH107 will hold regardless of whether we are using \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\), so we will often use \\(\\mathbb{F}\\) to represent either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\nFor example, \\(\\mathbb{F}^n\\) is an \\(\\mathbb{F}\\)-vector space, where \\(\\mathbb{F}\\) could be either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\n\n\n\nGeneralization (Not on the Exam):\n\nFinite Field Example:\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements.\nMany of our results hold in a more general setting, where \\(\\mathbb{F}\\) is a field—a set in which we can perform addition, multiplication, subtraction, and division (except division by zero).\nExamples of fields include:\n\n\\(\\mathbb{R}\\): the real numbers\n\\(\\mathbb{C}\\): the complex numbers\n\\(\\mathbb{Q}\\): the rational numbers\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements"
  },
  {
    "objectID": "mth107.html#abstract-vector-space",
    "href": "mth107.html#abstract-vector-space",
    "title": "MTH107—NOTEs",
    "section": "Abstract Vector Space:",
    "text": "Abstract Vector Space:\nWe can generalize this idea by replacing \\(\\mathbb{F}^n\\) with some abstract space \\(V\\), define addition \\(+\\) and scalar multiplication \\(\\cdot\\), and check if they satisfy the eight vector space axioms.\n\nDefinitions:\nFor a set \\(V\\), addition on \\(V\\) is a map:\n\\[\nV \\times V \\to V\n\\]\nIt maps an element set to their addition. For example, if \\(V = \\mathbb{R}^2\\):\n\\[\n(v, w) \\mapsto v + w\n\\]\nExample: \\((1,2) + (3,4) = (4,6)\\), which is also in \\(V\\).\nA scalar multiplication is a map:\n\\[\nF \\times V \\to V\n\\]\nFor example, \\((\\lambda, v) \\mapsto \\lambda v\\).\n(remark:if V and W are sets, V\\(\\times\\) W = {(v,w)|v\\(\\in\\) V, w\\(\\in\\) W})\nAn \\(F\\)-vector space is a set \\(V\\) with an addition \\(+\\) and scalar multiplication \\(\\cdot\\) by elements of \\(F\\), such that \\((V, +, \\cdot)\\) satisfies the vector space axioms 1-8, where \\(\\mathbb{R}\\) is replaced by \\(F\\), and \\(\\mathbb{R}^n\\) is replaced by \\(V\\).\n\n\nRemarks and Examples:\n\nVectors: Vectors are elements of \\(V\\), denoted as \\(v \\in V\\).\nField \\(F\\): The choice of \\(F\\) matters! For example, we will see later that \\(\\mathbb{C}^n\\) is a complex vector space of dimension \\(n\\), but is also a real vector space of dimension \\(2n\\)."
  },
  {
    "objectID": "mth107.html#prove",
    "href": "mth107.html#prove",
    "title": "MTH107—NOTEs",
    "section": "？prove？",
    "text": "？prove？\nabove field F\n107’s learning need of C\n2n proof???\n\nExamples:\nFor any field \\(F\\),\n\nTrivial Vector Space: the set \\(V = \\{0\\}\\) is a trivial \\(F\\)-vector space with addition \\(0 + 0 = 0\\) and scalar multiplication \\(\\lambda \\cdot 0 = 0\\).\nFinite-Dimensional Vector Space: \\(F^n\\) is an \\(F\\)-vector space, and \\(F^0 = \\{0\\}\\).\nInfinite-Dimensional Vector Space: Let \\(F^\\infty\\) be the space of infinite sequences, where:\n\n\\[\nF^\\infty = \\{ (x_1, x_2, \\dots) \\mid x_i \\in F, \\, i = 1, 2, \\dots \\}\n\\]\nAddition and scalar multiplication are defined component-wise:\n\\[\n(x_1, x_2, \\dots) + (y_1, y_2, \\dots) = (x_1 + y_1, x_2 + y_2, \\dots)\n\\]\n\nFunction Space: Let \\(S\\) be a set, then:\n\n\\[\nF^S = \\{ f: S \\to F \\, \\text{(maps from S to F)} \\}\n\\]\nAddition and scalar multiplication are defined pointwise. For functions \\(f, g \\in F^S\\) and \\(\\lambda \\in F\\):\n\\[\n(f + g)(x) = f(x) + g(x) \\quad \\forall x \\in S\n\\]\n\\[\n(\\lambda \\cdot f)(x) = \\lambda \\cdot f(x) \\quad \\forall x \\in S\n\\]\n\\(F^S\\) is an F-vector space"
  },
  {
    "objectID": "mth107.html#section-1",
    "href": "mth107.html#section-1",
    "title": "MTH107—NOTEs",
    "section": "?",
    "text": "?\ndoes this thing have more explaination except for f：s to F\nis there any quick way to think this kind of question instead of proving 8 axioms one by one? such as geometry meaning like keep straight line and parallelogram in linear transformation\n\nproof of FS\n\\(F^S\\) is a vector space\nProof: we need to check the 8 axioms from the defination\n(core: use the element here to x and then see the equality of the 2 function on the each side of the equality using the quality of \\(\\mathbb F^n\\))\n\nf + g = g+f?\n\nthese 2 functions ((f+g)(x) and (g+f)(x)) are equal if and only if they have values agrees on every \\(s \\in S\\)\n\\[\n(f+g)(x)=f(x)+g(x)\n\\] which is \\(\\in F\\) so it is equal to g(x)+f(x)(addition axiom)=(g+f)(x)\n2)(f+g)+h = f+(g+h)\n[(f+g)+h](x)=(f+g)(x)+h(x)=[f(x)+g(x)]+h(x), which are \\(\\in F\\), so\n=f(x)+[g(x)+h(x)](axiom 3)=f(x)+(g+h)(x)=[f+(g+h)](x)\n\n\\(\\exists 0: 0+f=f\\) for any \\(f \\in F^S\\)\n\nyes, define the zero function \\(0_F:\\) to be the constant function \\(0_F\\), i.e. \\(0_F(x)=0_F\\)\n\nfor \\(f\\in F^S\\), can we find an inverse?\n\n-f is defined by (-f)(x)=-f(x)\ncheck: (f+(-f))(x)=f(x)+(-f(x))=\\(0_F\\)\nso f+(-f)=\\(0_{F^S}\\)\n\n1f=f \\(（1\\cdot f)(x)=1\\cdot f(x)(\\in F)=f(x)\\)\n(ab)f=a(bf)\n\npass\n7)a(f+g)=af+ag\na(f+g)(x)=a(f(x)+g(x))=af(x)+ag(x)=(af+ag)(x)\n8)(a+b)f=af+bf\n(a+b)f(x)=af(x)+bf(x)\n(af+bf)(x)=(af)(x)+(bf)(x)=af(x)+bf(x)\n\n\nSpecial Cases:\n\nEmpty Set: If \\(S = \\emptyset\\), then:\n\n\\[\nF^{\\emptyset} = \\{ 0 \\}\n\\] the proof has been proved above on “set of all maps from A to B(f)”\n\nFinite Set: If \\(S = \\{1, 2, \\dots, n\\}\\), then:\n\n\\[\nF^{\\{1, 2, \\dots, n\\}} = F^n\n\\]\n\nproof of special 2:\nLet \\(F^n\\) represent the n-dimensional vector space over a field \\(F\\), and \\(F^{\\{1, \\dots, n\\}}\\) represent the set of functions from the set \\(\\{1, \\dots, n\\}\\) to \\(F\\), i.e., it assigns a scalar from \\(F\\) to each index in \\(\\{1, \\dots, n\\}\\).\nWe will prove that there exists a linear map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) such that:\n\n\\(T\\) is a bijection (i.e., both injective and surjective).\n\\(T\\) preserves addition: \\[\nT(f + g) = T(f) + T(g)\n\\] for all \\(f, g \\in F^n\\).\n\\(T\\) preserves scalar multiplication: \\[\nT(a f) = a T(f)\n\\] for all \\(f \\in F^n\\) and \\(a \\in F\\).\n\nA map that satisfies these two conditions is called a linear map, and we will learn about it later in class. A linear map that is a bijection is called an isomorphism.\nDefine the Map \\(T\\)\nDefine the map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) as follows:\nFor each vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function \\(T((a_1, a_2, \\dots, a_n)) = f \\in F^{\\{1, \\dots, n\\}}\\) by: \\[\nf(i) = a_i \\quad \\text{for each} \\, i = 1, 2, \\dots, n.\n\\] Thus, \\(T((a_1, a_2, \\dots, a_n)) = (f(1), f(2), \\dots, f(n)) = (a_1, a_2, \\dots, a_n)\\).\n\nInjectivity: \\(T((a_1, a_2, \\dots, a_n)) = T((b_1, b_2, \\dots, b_n))\\). This implies that for each \\(i\\), \\(a_i = b_i\\). Therefore, \\((a_1, a_2, \\dots, a_n) = (b_1, b_2, \\dots, b_n)\\), so \\(T\\) is injective. i.e. if \\(T(a_1,...a_n)=T(b_1,...,b_n)\\), then \\(\\forall i,\\) T(a)(i)=ai=T(b)(i)=bi,so\\(\\forall i,a_i=b_i\\),we need a=b\nSurjectivity: Given any function \\(f \\in F^{\\{1, \\dots, n\\}}\\), we can find a vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\) such that \\(f(i) = a_i\\) for each \\(i = 1, 2, \\dots, n\\). Therefore, \\(T\\) is surjective.\n\ni.e., given \\(f\\in \\mathbb F^{\\{1,..n\\}}\\), then T(f(1),…f(n))=f with (f(1),…f(n))\\(\\in F^n\\)(T is {1,..n}–&gt;F) with (f(1),…f(n))\\(\\in F^n\\)\nSince \\(T\\) is both injective and surjective, it is a bijection.\nProve that \\(T\\) Preserves Addition\nLet \\((a_1, a_2, \\dots, a_n), (b_1, b_2, \\dots, b_n) \\in F^n\\). Then:\n\\[\nT((a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n)) = T((a_1 + b_1, a_2 + b_2, \\dots, a_n + b_n)).\n\\]\n\\[\nf(i) = a_i + b_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\)) the other hand site: \\[\nT((a_1, a_2, \\dots, a_n)) + T((b_1, b_2, \\dots, b_n)) = (a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n),\n\\] which results f, also.\nProve that \\(T\\) Preserves Scalar Multiplication\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\) and \\(c \\in F\\). Then: \\[\nT(c \\cdot (a_1, a_2, \\dots, a_n)) = T((c a_1, c a_2, \\dots, c a_n)).\n\\]\n\\[\nf(i) = c a_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\))\nthe other hand site: \\[\nc \\cdot T((a_1, a_2, \\dots, a_n)) = c \\cdot (a_1, a_2, \\dots, a_n),\n\\] which also results in \\(f\\)\nSince T is an isomorphism (bijection + linear), then \\(T^{-1}\\) is automatically linear. That is we automatically have \\(T^{-1}(x+y)= T^{-1}(x)+ T^{-1}(y)\\), and the same for scalar multiplication. So they are the same in vector space, too.\nWe have shown that \\(T\\) is a bijection and preserves both addition and scalar multiplication. Therefore, \\(T\\) is a linear isomorphism, and \\(F^n\\) and \\(F^{\\{1, \\dots, n\\}}\\) are isomorphic as vector spaces.\n(Last remark: If \\(T\\) is an isomorphism (i.e., bijective and linear), then \\(T^{-1}\\) is automatically linear. That is, we automatically have \\(T^{-1}(x + y) = T^{-1}(x) + T^{-1}(y)\\), and the same holds for scalar multiplication. Therefore, there is no need to check these properties for \\(T^{-1}\\).)\n(a more detailed one proving bijetive: \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is Equivalent to \\(F^n\\)\nTo prove that \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is equivalent to \\(F^n\\), we show there is a bijection between the two sets, meaning each element in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) corresponds to a unique element in \\(F^n\\), and vice versa.\nBy definition, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is the set of all functions from \\(\\{1, 2, 3, \\dots, n\\}\\) to \\(F\\). Each function \\(f\\) can be written as:\n\\[\nf = (f(1), f(2), \\dots, f(n)).\n\\]\n\\(F^n\\) is the set of ordered \\(n\\)-tuples \\((a_1, a_2, \\dots, a_n)\\), where each \\(a_i \\in F\\).\nMapping from \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) to \\(F^n\\)—\nFor any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), we define the corresponding tuple in \\(F^n\\) as:\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nMapping from \\(F^n\\) to \\(F^{\\{1, 2, 3, \\dots, n\\}}\\)—-\nFor any tuple \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) as:\n\\[\nT^{-1}(a_1, a_2, \\dots, a_n)(i) = a_i, \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nLet \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\)–\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nthen, define\n\\[\nT^{-1}(f(1), f(2), \\dots, f(n))(i) = f(i), \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nThus,\n\\[\nT^{-1}(T(f)) = f.\n\\]\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\). Applying \\(T^{-1}\\), we get the function \\(f\\) such that \\(f(i) = a_i\\) for each \\(i\\). And then,\n\\[\nT(T^{-1}(a_1, a_2, \\dots, a_n)) = (a_1, a_2, \\dots, a_n).\n\\]\nThus,\n\\[\nT \\circ T^{-1} = \\text{id}_{F^n}.\n\\] This equation means that for any element \\((a_1, a_2, \\dots, a_n) \\in F^n\\), applying \\(T^{-1}\\) to obtain a function \\(f\\), and then applying \\(T\\) to \\(f\\), returns the original tuple:\n\\[\nT^{-1} \\circ T = \\text{id}_{F^{\\{1,2,3,...n\\}}}.\n\\]\nThis equation means that for any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), applying \\(T\\) to obtain a tuple \\((f(1), f(2), \\dots, f(n))\\), and then applying \\(T^{-1}\\) to that tuple, returns the original function:\nSince \\(T\\) and \\(T^{-1}\\) are inverses of each other, we have established a bijection between \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\). Therefore, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\) are equivalent as sets and vector spaces.)"
  },
  {
    "objectID": "mth107.html#linear-transformation-makes-us-know-everything",
    "href": "mth107.html#linear-transformation-makes-us-know-everything",
    "title": "MTH107—NOTEs",
    "section": "linear transformation makes us know everything",
    "text": "linear transformation makes us know everything\n二维映射到三维的linear transformation’s geometric meaning： if you know a transformation， then you know everything"
  },
  {
    "objectID": "mth107.html#general-properties-of-vector-space",
    "href": "mth107.html#general-properties-of-vector-space",
    "title": "MTH107—NOTEs",
    "section": "General properties of vector space",
    "text": "General properties of vector space\nLet V be a vector space over F\nWe will prove some properties of V using only the defination (axioms 1-8)\n\nProposition\n\nThe zero(additive identity) is unique. That is: \\(\\exists ! 0\\in V s.t. 0+V=V, \\forall v\\in V\\)\n\nproof: Suppose we have 2 zero elements: 0 and 0’\n0=0’+0=0+0’=0’\n\n\\(\\forall v\\in V\\) there exists a unique additive inverse\n\nSupppose w and w’ are 2 inverses for v, w=0+w=(v+w’)+w=v+(w’+w)=v+(w+w’)=(v+w)+w’=0+w’=w’\n\n\\(\\forall v\\in V\\), \\(O_F\\cdot V=O_V\\)\n\n\n\n?\n怎么想到的 why \\(0_F\\cdot V=0_V+w\\) F—&gt;V????\nwhy we could think of let w be the inverse of \\(0_F\\cdot V\\)? I always just could recite I mean remember the process instead of write it down smoothly\n(see 0_f as 0 is also okay)\nproof: \\(0_F\\)\\(\\cdot\\)V= \\((0_F+0_F)\\)\\(\\cdot\\)V=\\(0_Fv+0_Fv\\)\nlet w be the inverse of \\(0_F\\cdot v\\)(use proposition 2)\nthen \\(0_V\\)=\\(0_F\\cdot v+w\\)=\\((0_Fv+0_Fv)+w=0_Fv+(0_Fv+w)=0_Fv\\)\n\n\\(\\forall x\\in F: x\\cdot O_V=O_F\\)\n\nproof: \\(x\\cdot 0_V=x\\cdot (0_V+0_V)=x\\cdot 0_V+x\\cdot 0_V\\)\nso \\(0_V=x\\cdot 0_V\\)\n\n?\ndifference between o_Fv=0_v and xo_v=0_f(3 and 4 proposition)\n\n\n\nProperty\n\\(\\forall v\\in V,(-1)\\cdot V=-V\\)\nproof: V+(-1)V=(1+(-1))\\(\\cdot\\)V=0\\(\\cdot\\)V=0\nso (-1)V is an addictive inverse of V\nso we finish proof(by the addictive inverse)\n\nreminder of computing inverse of a matrix\nmethod1:\ndet(A)\n每一个位置的det构成的矩阵：B\ncofactor matrix:\\((-1)^{n+m}\\)\nC=B\\(\\times\\) cofactor matrix\n\\(A^{-1}=1/det(A)\\) times \\(C^T\\)\nmethod2: work for the tansformation of a matrix"
  },
  {
    "objectID": "mth107.html#motivation",
    "href": "mth107.html#motivation",
    "title": "MTH107—NOTEs",
    "section": "motivation",
    "text": "motivation\n\nif sets have subsets, vector spaces have subspaces.\n\n(Mathematicians study subobjects to understand the big objects better)"
  },
  {
    "objectID": "mth107.html#def",
    "href": "mth107.html#def",
    "title": "MTH107—NOTEs",
    "section": "Def",
    "text": "Def\nLet \\(V(V,+_V,\\cdot_V)\\)(what is the link with restrictions here?: we have \\(U\\subseteq V\\), i.e. U\\(\\times\\)U={(\\(x_1,x_2|x_1,x_2\\in U\\))}\\(\\subseteq V\\times V\\), now we have \\(+_V|_{U\\times U}\\):U\\(\\times U\\)–&gt;V, and scalar multiplication is similar to it. ) be a vector space, a subset U of V is a subspace if and only if \\(U(U,+_V,\\cdot_V)\\) is a vector space(for the link with restrictions: Here, for the condition of U being a vector space, we need\n\\(+_V|_{U\\times U}\\):U\\(\\times U\\)–&gt;U i.e. for \\(u_1,u_2\\in U, u_1+_vu_2 \\in U, \\lambda \\cdot_vu_1\\in U)\\)\n\\(\\cdot_v|_{F\\times U}\\): \\(F\\times U\\)–&gt;U\n\\(0_V\\in U\\)\n, which could replace 2 operations and 8 axioms and we will prove later)\nex: R=U\\(\\subseteq\\)V=C (a C-vector space) is not a subspace because scalar multiplication is not internal(i\\(\\cdot \\pi\\)is not in R—-not satisfying \\(\\cdot_v|_{F\\times U}\\): \\(F\\times U\\)–&gt;U)\n—-so we should explicit \\(\\mathbb F\\) when saying vector spaces"
  },
  {
    "objectID": "mth107.html#section-4",
    "href": "mth107.html#section-4",
    "title": "MTH107—NOTEs",
    "section": "?",
    "text": "?\nC r vector space though i including???\n(ex: \\(\\mathbb C\\)is a R-vector space and \\(\\mathbb R \\subseteq \\mathbb C\\) is a R subspace)"
  },
  {
    "objectID": "mth107.html#propsition",
    "href": "mth107.html#propsition",
    "title": "MTH107—NOTEs",
    "section": "Propsition",
    "text": "Propsition\n\\(U\\subseteq V\\)is a subspace if and only if :\na 0\nb +\nc \\(\\cdot\\)\n\nproof\nfor 1-8 axioms we only need to prove 4) because others are either included by the proposition or share the qualities of them because the elements in U are the elements in V.\n4): there exists only one inverse in U\ni.e. V’s inverse is in U\nlet \\(u\\in U\\), then \\(u\\in V\\) so \\(\\exists v=(-u)\\in V\\), we need to check is -u in U s.t. -u+u=\\(0_v\\):\n-u=(-1)u \\(\\in U\\)(scalar multiplication)"
  },
  {
    "objectID": "mth107.html#eg",
    "href": "mth107.html#eg",
    "title": "MTH107—NOTEs",
    "section": "eg",
    "text": "eg\n\n\\(\\lambda \\in \\mathbb F\\), U:={\\(x_1,x_2,x_3,x_4|x_3=5x_4+\\lambda\\)} is a subset if and only if \\(\\lambda =0\\)\n\nproof:\nside one: 0=(0,0,0,0) is in U… and others satisfy the proposition also.\nside two: if \\(\\lambda =0\\), let x and y \\(\\in U\\), so \\(x_3=5x_4\\),\\(y_3=5y_4\\), so \\(x_3+y_3=5(x_4+y_4)\\), so the addition is satisfied.\nfor c, also\n\n\\(R_\\{geq0}\\) is not a vector space–(-1,2)—&gt; (-1)2=-2\\(\\notin R_{\\geq0}\\)\n\\(C^0\\)([0,1]):={f:[0,1]–&gt;R, continuous} is a subspace \\(\\mathbb R^{[0,1]}\\)"
  },
  {
    "objectID": "mth107.html#proof-of-all-kinds-of-subspaces-of-r2",
    "href": "mth107.html#proof-of-all-kinds-of-subspaces-of-r2",
    "title": "MTH107—NOTEs",
    "section": "proof of all kinds of subspaces of R2",
    "text": "proof of all kinds of subspaces of R2"
  },
  {
    "objectID": "mth107.html#sum-of-subspaces",
    "href": "mth107.html#sum-of-subspaces",
    "title": "MTH107—NOTEs",
    "section": "sum of subspaces",
    "text": "sum of subspaces\n\nfor sets, if \\(A \\subseteq C\\) and \\(B\\subseteq C\\) are 2 subsets then A U B\\(\\subseteq\\)C is still a subset of C and it is the smallest subset of C containing both A and B.\n\nQ: what about subspaces of vector spaces?\nin general if U W are Z subspaces of V then U U W is not a subspace\nex:\n\nin fact U u W is a subspace if and only if \\(U \\subseteq W\\) or \\(W \\subseteq U\\)\n\nQ: How to produce the smallest subspace of V containing both U and W?\n\nDef: ley U_1,…U_n be subspaces of V then their SUM is \\(U_1+...+U_n=\\{ u_1+...+u_n|u_i\\in U_i ,\\forall i\\}\\). This is a subset of V containing all possible sums of elements of the U\n\nex\n\nThm: let U_1,…U_b be subspace of V then U_1+…+U_nis the smallest subspace of V containing each of the \\(U_i\\)\n\nProof: we need to prove the following 3 things:\n\nU:=\\(U_1+...+U_n\\) is a subspace\n\n0=0+..0(U_i are subspaces)\\(\\in U\\)\nif U=sum Ui and V=sum Vi \\(\\in U\\) then U+V=(sum ui)+(sum vi )=(u1+v1)+…\\(\\in U\\)\nif \\(\\lambda \\in F, U\\in U\\)(U is the sum of Ui(these subspaces))\n\nthen lambda(u sum)=(lambda u_1)+…\\(\\in U\\)\n\\(\\forall i: U_i\\subseteq U\\)\n\nlet v\\(\\in U_i\\) hten v=0+0+0(U_{i-1})+v(U_i)+0+0+0\\(\\in U\\), which means \\(U_i\\in U\\)\n\nif W is another subpace of V containing all the \\(U_i\\) then \\(U\\subseteq W\\)\n\nfor any \\(u\\in U\\), U(sum)\\(\\in W\\)(\\(U_i\\) \\(\\in W\\))(sum of elements of W), which means U\\(\\subseteq W\\)\n\n\nRemark:\n\n\\(U\\subseteq W\\) if and only if U+W=W\nproof:\n\nwe can do U+W only if U and W are subspaces of the same space\n\n\nComparison of subsets(A B)/subspaces(U W):\n\n\\(A\\cap B\\) is the biggest subset contained in both A and B\n\\(A \\cup B\\) is the smallest subspace containing both U and W\nA \\(\\sqcup\\) (disjoint union) B: whenever \\(A\\cap B=\\emptyset\\),then |A|+|B|=|\\(A\\cup B\\)|\nbut as the disjoint union for subspaces:\nmaybe ask that \\(U \\cap W\\) is as small as possible, that is L \\(U\\cap W=\\{0\\}\\)\n\nDef Let U1-Un be the subspaces of V we say that U1+Un is a DIRECT SUM if (shuangjiantou) \\(\\forall U\\in U sum\\) there exists a unique way to write U=Usum\nex:\n\nin this case we write U_1 \\(\\oplus\\)… \\(\\oplus\\) U_n\n(if \\(u\\in  U_1 +\\)… \\(+\\) \\(U_n\\), then there always exists \\(U_i\\in U_1 +\\)… \\(+\\) U_n\\(\\in U_n\\), such that U=U_1 \\(+\\)… \\(+\\) \\(U_n\\), the defination is about uniqueness of such elements.)\nex 1).U:={(x,x,y,y)|x,y\\(\\in F\\)}\\(\\subseteq F^4\\)\nW:={(x,x,x,y)|x,y\\(\\in F\\)}\\(\\subseteq F^4\\)\nthen U+W={(x,x,y,z)|x,y,z\\(\\in F\\)}=:A\nproof:\nside1:u+v\\(\\in U+W\\) easy —xxyz\nside2: let (x,x,y,z)\\(\\in A\\) we want to find v=(a,a,b,b), w=(c,c,c,d),s.t. u+w=(x,x,y,z)\nso we need to solve a+c=x, b+c=y,b+d=z,\nb is free, d=z-b,c=y-b,a=x-c=x-y+b\nfor ex for b=0:\nu=\nw=\nso u+w=\nso \\(A\\subseteq U+W\\)\nex 3) : \\(U_i=\\){(0,0,…x(i th coordinate),0,0,0(n th coordinate))|\\(x\\in F\\)}\\(\\subseteq F^n\\)\nthen \\(U_1\\oplus ...\\oplus U_n=F^n\\)\nex 4) : \\(U_1=\\){(x,y,z)|\\(x,y\\in F\\)}\n\\(U_2\\)={(0,0,z)|\\(z\\in F\\)}\n\\(U_3\\)={(0,y,y)|\\(y\\in F\\)}\nthen \\(U_1+U_2+U_3=F^3\\), but not direct sum\neg:\nbut \\(U_1\\odot U_2=F^3\\)\n\\(U_1\\odot U_3=F^3\\)\n\\(U_2\\odot U_3=\\{(0,y,z)|y,z\\in F^3\\}\\)\n```\n\nThm let U1,….Un be subspaces of V then [U1+…+Un is a direct sum] if and only if [if 0=u1+…+un, then u1=…=0]\n\nproof: one side is obvious: \\(0\\in U_1+...+U_n\\)\n   the other side: let v $\\in U_1+...+U_n$ with v= u+ =v+ be 2 decompsition \n\n\n   so o=u-v=(u1+)-(V1+)=(u1-v1)+(u2-v2)\n\n\n   SO 0 = U1-V1-U2-V2=... by assumption\n\n\n   so we can derive: u1=v1...\n\n\n   so the deconposition of v is unique\n\nintersection of subspaces:\n\nlemma: if U W are subspaces aof V then \\(U\\cap W\\) is the biggest subspace contained in both U and W\n\nproof :\nsubspace:\n\n0\n\n\nlambda\n\n\n\n-   already know $\\subseteq$\n\n-   biggest: let $Z\\subseteq V$ be a subspace, contained both in U and W, because $U\\cap W$ is the biggest subset contained both in U and W then $Z\\subseteq U\\cap W$(as subset)\n\nDef if V=U\\(\\oplus\\)W we say that W is COMPLEMENTARY SUBSPACE of U(inside V)\ngive a conunter example to:\nfor U1 U2, W subspaces of V, then\n\nif U1+w=U2+w, then U1=U2:\nif \\(U_1\\oplus W=U_2\\oplus W\\) hten U_1=U_2"
  },
  {
    "objectID": "mth107.html#linear-independence",
    "href": "mth107.html#linear-independence",
    "title": "MTH107—NOTEs",
    "section": "linear independence",
    "text": "linear independence\nfinite????\na list of vectors v,…vi\\(\\in V\\) is\n\nlinearly independent: if and only if the vector equation \\(\\lambda v_1+..=o\\) admits a unique solution: (0,0,)\n\nlinearly dependence: non-trivial solution.\ndet=0\nv,v,…,0,…\nremark\ninfinite\n\nv1,…vn is linearly independent if and only if span(v1)+span(v2)..+span(vn) is a direct sum.\n\nex: a list of 1 vector \\(v\\in V\\) is lin independent if and only if V!=0\nex: \\(u,v\\in V\\) is linearly independent if and only if [\\(u\\notin span(V)\\) and \\(v\\notin span(U)\\)]\n\nex: (0,0), (1,0) but v\\(\\notin span(u)=\\){0}\nif both u. and v !=0 then it is enough to check only one of a or b\n\nex: v1,… linearly dependent iff \\(\\exists i\\) s.t. $v_i$span(v1,….)\n\none side: \\(\\exists \\lambda\\)\nthe other side:\nsmalest span list:\n\none include\nthe other include:\n\nthis lemmma tells us the if a family is lin dependent then we can remove “redundan” vectors without changing the span\n\nTHM: suppose V=span(v1,..vm)=span(w1,..wn). If v1,..vm is linearly independent then \\(m\\leq n\\)\n\nproof:let us consider v1,w2,….,wn\n\nthis is a spannign family for v, \\(v=span(w1,..wn)\\subseteq span(v1,w1,...wn)\\subseteq v\\)\n\n-it is linearly dependent(because v1\\(\\in span(w1,...wn)....\\))\n\nv1 !=0 because v1,…vm is lin independent\n\n\n\nNow we apply the lemma to the family\n\nwhich implies that \\(\\exists j\\) s.t. wj can be replaced by v1\nindeed 0!= \\(\\notin span()\\)={0}\nsee photo in particular, \\(m \\leq n\\). ok\n\namong finite spanning lists, linearly independent are the smallest one\ncorollary: if v=span(v1,…vm)=span(w1,..wn) and (v1,..) and (w1,..) are both lin indep then m=n\nex: \\(\\mathbb F^n\\), any list of \\(m\\geq n+1\\) vectors is lin dependent because (1,0,..),(0,1,…),…(0,…,1) (n elements here) and any of \\(\\leq n-1\\) vecters cannot span \\(\\mathbb {F^n}\\)"
  },
  {
    "objectID": "mth107.html#begin",
    "href": "mth107.html#begin",
    "title": "MTH107—NOTEs",
    "section": "Begin",
    "text": "Begin\n\nDef Let V and W be 2 vector spaces over the same filed F. A maoo T V–&gt;W is a linear map(linear transformation) if and only if \\(\\forall v \\in V\\) T(u+v)=T(u)+T(v), \\(\\forall v \\in V, \\lambda\\in F\\) T(\\(\\lambda \\cdot v\\))= \\(\\lambda\\cdot T(v)\\)\nLemma Let T"
  },
  {
    "objectID": "mth107.html#thm-an-inner-product-space-is-a-normed-space-by-defining.",
    "href": "mth107.html#thm-an-inner-product-space-is-a-normed-space-by-defining.",
    "title": "MTH107—NOTEs",
    "section": "Thm An inner product space is a normed space by defining.",
    "text": "Thm An inner product space is a normed space by defining.\n||a||=\\(\\sqrt{&lt;a,a&gt;}\\)\n|| || V–&gt; \\(R^+\\) length\n\n||a||\\(\\geq0, \\forall a\\in V\\)\n\n||a||=0 iff a=0\n\n||a+b|| \\(\\leq\\) ||a||+||b||\n\npf: &lt;a,a&gt;+&lt;a,b&gt;+&lt;b,a&gt;+&lt;b,b&gt;\n= \\(||a||^2|+2||a|||b||+||b||^2\\)\n\\(\\leq||a||^2+2||a|| ||b||+||b||^2\\)\n= \\((a+b)^2\\)"
  },
  {
    "objectID": "mth107.html#section-5",
    "href": "mth107.html#section-5",
    "title": "MTH107—NOTEs",
    "section": "1",
    "text": "1\nIf V=\\(U\\oplus W\\), then \\(P_u\\in \\mathcal L(V)\\) is a projection\nProposition Let \\(V\\) be a vector space and let \\(P \\in \\mathcal{L}(V)\\) be a projection. Then \\(V=\\operatorname{range}(P) \\oplus \\operatorname{null}(P)\\) and \\(P=P_{\\operatorname{range}(P)}\\).\nProof. It is clear that range \\((P)+\\operatorname{null}(P) \\subseteq V\\). So we need to prove that the sum is direct and that it is equal to \\(V\\). Let \\(v \\in V\\). Then one have \\(v=P(v)+(v-P(v))\\) with \\(P(v) \\in \\operatorname{range}(P)\\) and \\(v-P(v)\\) is in null \\((P)\\). Indeed, \\(P(v-P(v))=P(v)-P^2(v)=\\) \\(P(v)-P(v)=0\\). Now, for the direct sum part. If \\(v \\in \\operatorname{range}(P) \\cap \\operatorname{null}(P)\\) we have \\(v=P(w)\\) and \\(0=P(v)=P^2(w)=P(w)=v\\), so range \\((P) \\cap \\operatorname{null}(P)=0\\) which finishes the proof of range \\((P)+\\operatorname{null}(P) \\subseteq V\\).\nFinally, let \\(v \\in V\\). Then \\(v=P(v)+(v-P(v))\\) is the unique decomposition \\(v=u+w\\) with \\(u \\in \\operatorname{range}(P)\\) and \\(w \\in \\operatorname{null}(P)\\). Therefore, \\(=P_{\\text {range }(P)}(v)=P(v)\\) and we just demonstrated \\(P=P_{\\text {range }(P)}\\).\n???Another way to interpret this proposition is that there is a one-to-one correspondance between projections \\(P \\in \\mathcal{L}(V)\\) and direct sums decomposition \\(V=U \\oplus W\\) where order matter. We conclude this subsection by the following result that is a generalization of this proposition.\n\nThis means that projection is a kind of decomposition—along and onto decompose one vector in a unique way, which is an intuitive understanding of a special case of direct sum."
  },
  {
    "objectID": "mth107.html#extension",
    "href": "mth107.html#extension",
    "title": "MTH107—NOTEs",
    "section": "1 extension",
    "text": "1 extension\nSuppose \\(P_1, \\ldots, P_k \\in \\mathcal{L}(V)\\) are projections such that \\(P_1+\\cdots+P_k=\\operatorname{Id}_V\\) and \\(P_i P_j=0_{\\mathcal{L}(V)}\\) if \\(i \\neq j\\). Then \\(V=\\operatorname{range}\\left(P_1\\right) \\oplus \\cdots \\oplus \\operatorname{range}\\left(P_k\\right)\\).\nProof. Let \\(v\\) be any vector in \\(V\\). Then \\[\nv=\\operatorname{Id}_V v=\\left(P_1+\\cdots+P_k\\right) v=P_1 v+\\cdots+P_k v \\in \\operatorname{range}\\left(P_1\\right)+\\cdots+\\operatorname{range}\\left(P_k\\right)\n\\] and thus \\(V=P_1 v+\\cdots+P_k v \\in \\operatorname{range}\\left(P_1\\right)+\\cdots+\\) range \\(\\left(P_k\\right)\\). We now prove that the sum is direct. Suppose \\(0=v_1+\\cdots+v_k\\) with \\(v_i \\in \\operatorname{range}\\left(P_i\\right)\\) for \\(1 \\leq i \\leq k\\). We want to prove that all the \\(v_i\\) are zero. By definition of the range, there exist \\(w_i\\) such that \\(P_i w_i=v_i\\). Using that \\(P_i\\) is a projection we have \\(P_i v_i=P_i^2 w_i=P_i w_i=v_i\\) while \\(P_i v_j=P_i P_j w_j=0 w_j=0\\) if \\(i \\neq j\\). Now, if we apply \\(P_i\\) to both sides of (3.5) we obtain \\(0=v_i\\). By doing this for all \\(1 \\leq i \\leq k\\) we have that all the \\(v_i\\) are zero as desired, and so the sum is direct.\nNow that we have proven the Theorem, let us discuss why it is a generalization of the defination of a projection onto rangeP alone nullP with just 2 decomposition(direct sum) Let \\(P \\in \\mathcal{L}(V)\\) be a projection, \\(P_1:=P\\) and \\(P_2:=\\mathrm{Id}_V-P\\). One easily check the following properties: - \\(P_2\\) is a projection: \\(P_2^2=\\mathrm{Id}_V^2-\\mathrm{Id}_V P-P \\mathrm{Id}_V+P^2=\\mathrm{Id}_V-P-P+P=P_2\\); - \\(P 1+P_2=\\mathrm{Id}_V\\); - and \\(P_1 P_2=P^2-P=0=P_2 P_1\\)."
  },
  {
    "objectID": "mth107.html#easier-way-leftright-inverse",
    "href": "mth107.html#easier-way-leftright-inverse",
    "title": "MTH107—NOTEs",
    "section": "easier way (left/right inverse)",
    "text": "easier way (left/right inverse)\nAn easy way to create projections is to use left and right inverses.\nLet \\(V\\) and \\(W\\) be two vector spaces and let \\(T \\in \\mathcal{L}(V, W)\\) and \\(S \\in \\mathcal{L}(W, V)\\) be such that \\(T S=\\operatorname{Id}_W\\). Then \\(V=\\operatorname{range}(S) \\oplus \\operatorname{null}(T)\\) and \\(S T \\in \\mathcal{L}(V)\\) is a projection to range \\((S)\\) with nullspace \\(\\operatorname{null}(T)\\). Proof. We first show that \\(S T\\) is an abstract projection: \\((S T)^2=S(T S) T=S \\operatorname{Id}_W T=\\) \\(S T\\).\nIt now remains to show that range \\((S T)=\\operatorname{range}(S)\\) and \\(\\operatorname{null}(S T)=\\operatorname{null}(T)\\). If \\(v\\) is in range \\((S T)\\), then there exists \\(u \\in V\\) such that \\(v=S T(u)=S(T(u)) \\in \\operatorname{range}(S)\\). So range \\((S T) \\subseteq \\operatorname{range}(S)\\). For the other inclusion, let \\(v \\in \\operatorname{range}(S)\\). Thus there exists \\(w \\in W\\) with \\(v=S(w)=S \\operatorname{Id}_W(w)=S T S(w)=S T(T(w))\\) which shows that range \\((S) \\subseteq\\) range \\((S T)\\).\nFor the nullspaces, if \\(v \\in \\operatorname{null}(T)\\), then \\(S T(v)=S(0)=0\\) and hence \\(\\operatorname{null}(T) \\subseteq \\operatorname{null}(S T)\\). For the other inclusion, let \\(v\\) be an element in null \\((S T)\\). We have \\(0_V=S T(v)\\) and by applying \\(T\\) to both sides \\(0_W=T S T(v)=\\operatorname{Id}_W T(v)=T(v)\\), which shows that \\(\\operatorname{null}(S T) \\subseteq \\operatorname{null}(T)\\).\nObserve that for a given \\(T \\in \\mathcal{L}(V, W)\\) there exists a right inverse \\(S\\) such that \\(T S=\\operatorname{Id}_W\\) if and only if \\(T\\) is surjective. If such an inverse exists it is not unique, unless \\(T\\) is an isomorphism. This is related to the fact that given a general subspace \\(U(=\\operatorname{null}(T))\\) of \\(V\\), there exists more than one direct sum complement \\(X(=\\operatorname{range}(S))\\) such that \\(V=U \\oplus X\\). Let us exemplify this.\n(ps:ST may not be invertible. However, if T and S are operator \\(\\in L(V)\\), it is trivial that TS is invertible since T is surjective and S is injetive then they are all invertible so ST is invertible since \\(ST(T^{-1}S^{-1})=Id\\) and \\((T^{-1}S^{-1})ST=Id\\))"
  },
  {
    "objectID": "mth107.html#projection-is-a-useful-tool-to-do-approximation",
    "href": "mth107.html#projection-is-a-useful-tool-to-do-approximation",
    "title": "MTH107—NOTEs",
    "section": "projection is a useful tool to do approximation",
    "text": "projection is a useful tool to do approximation\nThe projection is the approximation and the vector minus projection is the error.\neg. use the inner product on continuous functions to approximate sine function using \\(P_4(R)\\)"
  },
  {
    "objectID": "mth107.html#infinite-here-will-also-be-true",
    "href": "mth107.html#infinite-here-will-also-be-true",
    "title": "MTH107—NOTEs",
    "section": "Infinite here will also be true",
    "text": "Infinite here will also be true"
  },
  {
    "objectID": "mth107.html#least-square-in-regression-analysis-1",
    "href": "mth107.html#least-square-in-regression-analysis-1",
    "title": "MTH107—NOTEs",
    "section": "Least-square in regression analysis",
    "text": "Least-square in regression analysis\n\na simple example"
  },
  {
    "objectID": "mth107.html#problems",
    "href": "mth107.html#problems",
    "title": "MTH107—NOTEs",
    "section": "problems",
    "text": "problems\n\nsummarize1: give opposite example and draw pictures are okay.\n\neg.\neg. of Tutorial 4:\nf: \\(\\mathbb{R}_{\\geq0}\\)—&gt;\\(\\mathbb{R}\\)\nx–&gt;\\(\\sqrt {x}\\)\ng: \\(\\mathbb R\\)—&gt;\\(\\mathbb{R}_{\\geq0}\\)\nx–&gt;\\(x^2\\)\ng(f(x))=\\(x^2\\)=\\(id_{\\mathbb R\\geq0}\\) f(g(x))=|\\(x\\)|is not equal to \\(id_{\\mathbb R}\\) because f(g(x)) is not equal to x.\neg."
  },
  {
    "objectID": "mth107.html#understand-again",
    "href": "mth107.html#understand-again",
    "title": "MTH107—NOTEs",
    "section": "understand again",
    "text": "understand again"
  },
  {
    "objectID": "mth1113.html#simple-random-sampling",
    "href": "mth1113.html#simple-random-sampling",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nWe can get information about the population by taking a sample from it. The sample should be representative of the population.\nA simple random sample of size n is a sample selected from a population in such a way that every possible sample of size n has the same chance of being selected and without replacement."
  },
  {
    "objectID": "mth1113.html#unbiased-estimator",
    "href": "mth1113.html#unbiased-estimator",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Unbiased estimator",
    "text": "Unbiased estimator\nAn estimator is a statistic used to estimate a population parameter. An estimator is unbiased if the expected value of the estimator equals the population parameter.\n\\(E(\\hat \\theta) = \\theta\\)\nwhere \\(\\hat \\theta\\) is the estimator and \\(\\theta\\) is the population parameter.\neg.\n\\[\n\\mathbb{E}[\\hat{p}]=\\mathbb{E}\\left[\\frac{X_1+X_2+\\cdots+X_n}{n}\\right]=\\frac{1}{n}\\left(\\mathbb{E}\\left[X_1\\right]+\\cdots+\\mathbb{E}\\left[X_n\\right]\\right)=p\n\\]"
  },
  {
    "objectID": "mth1113.html#variance",
    "href": "mth1113.html#variance",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Variance",
    "text": "Variance\n\\[\n\\operatorname{Var}[X]=\\mathbb{E}\\left[X^2\\right]-(\\mathbb{E}[X])^2\n\\]\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\hat{p}^2\\right] & =\\mathbb{E}\\left[\\left(\\frac{X_1+X_2+\\cdots+X_n}{n}\\right)^2\\right] \\\\\n& =\\frac{1}{n^2} \\mathbb{E}\\left[X_1^2+\\cdots+X_n^2+2\\left(X_1 X_2+X_1 X_3+\\cdots+X_{n-1} X_n\\right)\\right] \\\\\n& =\\frac{1}{n^2}\\left(n \\mathbb{E}\\left[X_1^2\\right]+2\\binom{n}{2} \\mathbb{E}\\left[X_1 X_2\\right]\\right) \\\\\n& =\\frac{1}{n} \\mathbb{E}\\left[X_1^2\\right]+\\frac{n-1}{n} \\mathbb{E}\\left[X_1 X_2\\right]\n\\end{aligned}\n\\]\n\\[\n\\mathbb{E}\\left[\\hat{p}^2\\right]=\\frac{1}{n} \\mathbb{E}\\left[X_1^2\\right]+\\frac{n-1}{n} \\mathbb{E}\\left[X_1 X_2\\right]\n\\]\nSince \\(X_1\\) is 0 or \\(1, X_1=X_1^2\\). Then \\(\\mathbb{E}\\left[X_1^2\\right]=\\mathbb{E}\\left[X_1\\right]=p\\).\nNotice: \\(X_1\\) and \\(X_2\\) are not independent.\n\\[\n\\mathbb{E}\\left[X_1 X_2\\right]=\\mathbb{P}\\left[X_1=1, X_2=1\\right]=\\mathbb{P}\\left[X_1=1\\right] \\mathbb{P}\\left[X_2=1 \\mid X_1=1\\right]\n\\]\n\\[\n\\mathbb{P}\\left[X_1=1\\right]=p, \\quad \\mathbb{P}\\left[X_2=1 \\mid X_1=1\\right]=\\frac{N p-1}{N-1}\n\\]\n\\[\n\\begin{aligned}\n\\operatorname{Var}[\\hat{p}] & =\\mathbb{E}\\left[\\hat{p}^2\\right]-(\\mathbb{E}[\\hat{p}])^2 \\\\\n& =\\frac{1}{n} p+\\frac{n-1}{n} p\\left(\\frac{N p-1}{N-1}\\right)-p^2 \\\\\n& =\\left(\\frac{1}{n}-\\frac{n-1}{n} \\frac{1}{N-1}\\right) p+\\left(\\frac{n-1}{n} \\frac{N}{N-1}-1\\right) p^2 \\\\\n& =\\frac{N-n}{n(N-1)} p+\\frac{n-N}{n(N-1)} p^2 \\\\\n& =\\frac{p(1-p)}{n} \\frac{N-n}{N-1}=\\frac{p(1-p)}{n}\\left(1-\\frac{n-1}{N-1}\\right)\n\\end{aligned}\n\\] When N is much bigger than n, it is \\(\\frac{p(1-p)}{n}\\), which is like we sample n things with replacement (independently)."
  },
  {
    "objectID": "mth1113.html#sampling-distribution",
    "href": "mth1113.html#sampling-distribution",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nThe sampling distribution of a statistic is the probability distribution of that statistic based on a random sample.\n\nSample mean of i.i.d. normals\nSample mean follows a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "mth1113.html#simulation",
    "href": "mth1113.html#simulation",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Simulation",
    "text": "Simulation\nBelow codes explains the central limit theorem (CLT) and the sampling distribution of the sample proportion.\n\nlibrary(ggplot2)\n\nset.seed(111)\n\npopulation_size &lt;- 12141897\n\np &lt;- 0.54\n\nnum_simulations &lt;- 50\n\nsample_size &lt;- 500\n\nrep()\n\nNULL\n\np_hat_values &lt;- replicate(num_simulations, {\n  # Simulate sampling from the population\n  sample &lt;- sample(c(rep(1,population_size * p), rep(0,population_size*(1-p))),sample_size, replace =FALSE) # replicate(num_simulations, {...})：这个函数会重复执行大括号中的代码num_simulations次，并将每次的结果存储在一个向量中； rep创建一个包含population_size * p个1和population_size * (1-p)个0的向量，模拟总体。\nmean(sample) #calculate the p_hat for each sample\n})\n\n\n  \nhistogram &lt;- ggplot(data.frame(p = p_hat_values), aes(x=p))+\n  geom_histogram(binwidth = 0.01, fill =\"blue\", color = \"black\")+\n  labs(title =\" \",\n       x = \"p_hat\",\n       y= \"Frequency\")+\n  theme_minimal()\n\nprint(histogram)"
  },
  {
    "objectID": "mth1113.html#moment-generating-functions",
    "href": "mth1113.html#moment-generating-functions",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Moment generating functions",
    "text": "Moment generating functions\nMoment generating function (MGF) and characteristic function are powerful functions that describe the underlying features of a random variable.\n\nDefinition\n\\(M_x(t) = \\mathbb E (e^{tx})\\)\n\n\nTheorems about MGF\n\nIf \\(X\\) and \\(Y\\) are random variables with the same MGF, which is finite on \\(\\left[-t_0, t_0\\right]\\) for some \\(t_0&gt;0\\), then \\(X\\) and \\(Y\\) have the same distribution.\n\nMGFs can be used as a tool to determine if two random variables have the identical CDF.\n\nLet \\(X_1, \\cdots, X_n\\) be independent random variables, with MGFs \\(M_{X_1}, \\cdots, M_{X_n}\\). Then the MGF of their sum is given by \\[\nM_{X_1+\\cdots+X_n}(t)=M_{X_1}(t) \\cdots M_{X_n}(t)\n\\]\n\n\nExample of Gamma and Exponential MGF\nA gamma distribution with shape \\(r=1\\) is an exponential distribution. If \\(X \\sim \\operatorname{Gamma}\\left(r_X, \\lambda\\right)\\) and \\(Y \\sim \\operatorname{Gamma}\\left(r_Y, \\lambda\\right)\\) are independent, then we have \\(X+Y \\sim \\operatorname{Gamma}\\left(r_X+r_Y, \\lambda\\right)\\). As a special case, if \\(X_1, X_2, \\cdots, X_n\\) are i.i.d. with \\(\\operatorname{Exp}(\\lambda)\\) distribution, then \\(X_1+X_2+\\cdots+X_n\\) has \\(\\operatorname{Gamma}(n, \\lambda)\\) distribution.\nSuppose \\(X \\sim \\operatorname{Exp}(\\lambda)\\), for \\(\\lambda&gt;0\\). Then \\[\nM_X(t)=\\mathbb{E}\\left[e^{t X}\\right]=\\int_0^{\\infty} e^{t x} \\lambda e^{-\\lambda x} d x=\\lambda \\int_0^{\\infty} e^{(t-\\lambda) x} d x\n\\]\nSimilar to Gamma MGF, the integral of Exponential MGF converges only for \\(t&lt;\\lambda\\). For \\(t&lt;\\lambda\\), we can integrate: \\[\nM_X(t)=\\lambda\\left[\\frac{e^{(t-\\lambda) x}}{t-\\lambda}\\right]_0^{\\infty}=\\frac{\\lambda}{\\lambda-t}\n\\]\nSince the Gamma MGF is \\(M_X(t)=\\frac{\\lambda^r}{(\\lambda-t)^r}\\) for any \\(t&lt;\\lambda\\) For shape \\(r=1\\), Exponential MGF = Gamma MGF."
  },
  {
    "objectID": "mth1113.html#distribution-transformation",
    "href": "mth1113.html#distribution-transformation",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Distribution Transformation",
    "text": "Distribution Transformation\n\nUniversality of the Uniform—From uniform you can get everything\nLet u~unif(0,1), F be a CDF(assume F is strictly increased, continuous). Then there comes a theorem: \\[\nx = F^{-1}(u)\n\\] Then \\[\nX \\sim F\n\\] (Proof: \\[\nP(X\\leq x)=P(F^{-1}(u)\\leq x)=P(F(F^{-1}(u))\\leq F(x))=P(u\\leq F(x))=F(x)\n\\])\nYou can convert from the random uniforms to whatever you want to simulate. One example is the simulation of Logistic distribution: F(X)=\\(e^x\\)/(\\(1+e^x\\))\nu~unif(0,1), \\[\nX = F^{-1}(u)=log(u/1-u)\n\\] and X~F\nAnother example is that we could try to use uniform to simulate normal distribution:\nThe Box-Muller transform generates pairs of independent standard normally distributed (zero mean, unit variance) random numbers, given a source of uniformly distributed random numbers.\nGiven two independent random variables \\(U_1\\) and \\(U_2\\) that are uniformly distributed on the interval (0, 1), we can generate two independent standard normal random variables \\(Z_0\\) and \\(Z_1\\) using the following formulas:\n\\[\nZ_0 = \\sqrt{-2 \\ln U_1} \\cos(2 \\pi U_2)\n\\]\n\\[\nZ_1 = \\sqrt{-2 \\ln U_1} \\sin(2 \\pi U_2)\n\\]\nInversely, the example could be: let \\(Z_0\\) and \\(Z_1\\) be standard normal random variables with the following values: \\(Z_0\\) = 0.5 and \\(Z_1\\) = -1.0.\n\nCompute CDF values:\nFor standard normal distribution, the CDF \\[ \\Phi(z) \\] is given by:\n\\[\n\\Phi(z) = \\frac{1}{2} \\left[ 1 + \\text{erf} \\left( \\frac{z}{\\sqrt{2}} \\right) \\right]\n\\]\n(ps:\\[\n\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} \\, dt\n\\])\n\nFor \\[ Z_0 = 0.5 \\]:\n\\[\n\\Phi(0.5) = \\frac{1}{2} \\left[ 1 + \\text{erf} \\left( \\frac{0.5}{\\sqrt{2}} \\right) \\right]\n\\]\nFor \\[ Z_1 = -1.0 \\]:\n\\[\n\\Phi(-1.0) = \\frac{1}{2} \\left[ 1 + \\text{erf} \\left( \\frac{-1.0}{\\sqrt{2}} \\right) \\right]\n\\]\n\nCompute Uniform values:\nSince the CDF values \\[ \\Phi(Z_0) \\] and \\[ \\Phi(Z_1) \\] are in the range [0, 1], we can directly use them as uniform random variables \\(U_0\\) and \\(U_1\\).\n\n\\[U_0 = \\Phi(0.5)\\]\n\\[U_1 = \\Phi(-1.0)\\]\n\nResult:\n\nUsing the error function values:\n\n\\[ \\Phi(0.5) \\approx 0.6915 \\]\n\\[ \\Phi(-1.0) \\approx 0.1587 \\]\n\n\nThus, the corresponding uniform distribution values are:\n\n\\[ U_0 \\approx 0.6915 \\]\n\\[ U_1 \\approx 0.1587 \\]\n\n\nAnother example is that we can create a function that has good quality as F in the theorem, for example, \\[\nu=F(x)=1-e^{-x}          (x&gt;0)\n\\] then we can simulate X~F:X=-ln(1-u)~F\nAlso, if \\[\nX \\sim F\n\\]\nThen, \\[\nF(x) \\sim unif(0,1)\n\\] e.g. let X~F, if\\[\nF(x_0)=1/3\\] then \\[P(F(X)\\leq 1/3)=P(X\\leq x_0)=F(x_0)=1/3 \\] which follows the uniform distribution(0,1) because 1/3 generates 1/3. (Uniform distribution: Probability(CDF) is proportional to length)\n\n\nNormal Distribution\n\nFrom Standard Normal Distribution to Normal Distribution\n\\(f(z)=ce^{-z^2/2}\\) is a function with good qualities such as symmetric……..\nTo generate normalization constant c using CDF=1, instead of using impossible integral methods to compute it we should try to find the area under it. In that case, we transfer the integral shape to the double integral’s multiplication then we get c: \\[\n\\int_{-\\infty}^{\\infty}\\exp(-z^2/2) dz=\\int_{-\\infty}^{\\infty}\\exp(-z^2/2) dz=\\int_{-\\infty}^{\\infty}\\exp(-x^2/2)dx\\int_{-\\infty}^{\\infty}\\exp(-y^2/2)dy\n\\] \\[\n=\\int_{0}^{\\infty}\\exp(-r^2/2)r dr\\int_{0}^{2π} d\\theta\\ =\\sqrt2\\pi\\\n\\] So \\[\nc=1/\\sqrt2\\pi\\\n\\] …….\n\n\n\nFrom…..\n111\n\nExponential distribution\nthe only one parameter is the rate parameter. The probability density function (pdf) of an exponential distribution with rate parameter (&gt; 0) is given by:\n\\[\nf_X(x) =\n\\begin{cases}\n\\lambda e^{-\\lambda x} & \\text{for } x \\geq 0, \\\\\n0 & \\text{for } x &lt; 0.\n\\end{cases}\n\\] The cumulative distribution function (cdf) of an exponential distribution with rate parameter (&gt; 0) is given by:\n\\[\nF_X(x) =\n\\begin{cases}\n1 - e^{-\\lambda x} & \\text{for } x \\geq 0, \\\\\n0 & \\text{for } x &lt; 0.\n\\end{cases}\n\\] 2.Let Y~x, then Y ~Expo(1)\nproof: since \\[\nP(Y\\leq y)=P(X\\leq y/\\lambda)=1-e^{-y}\n\\] (just plot it into x) and we could check that E[X]=Var[X]=1, so X=Y/\\(\\lambda\\) has E[x]=1/\\(\\lambda\\), Var[x]=1/\\(\\lambda^2\\)\ne.g. Memoryless Property(This property implies that the remaining lifetime distribution does not depend on how much time has already elapsed.)(The exponential distribution is the only continuous distribution that has the memoryless property):\\[ P(X\\geq s+t|X\\geq s)=P(X\\geq t)\\] which is actually satisfied by the exponential and we could prove it(though it intuitively makes sence): Here \\[\nP(X\\geq s)=1-P(X\\leq s)=e^{-\\lambda s}\n\\] \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{P(X \\geq s+t \\text { and } X \\geq s)}{P(X \\geq s)}\n\\]\nSince \\(X \\geq s+t\\) implies \\(X \\geq s\\), we can simplify the numerator: \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{P(X \\geq s+t)}{P(X \\geq s)}\n\\]\nNow, substitute the survival function for the exponential distribution: \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{e^{-\\lambda(s+t)}}{e^{-\\lambda s}}\n\\]\nSimplify the expression: \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{e^{-\\lambda s} \\cdot e^{-\\lambda t}}{e^{-\\lambda s}}=e^{-\\lambda t}\n\\]\nNotice that \\(e^{-\\lambda t}=P(X \\geq t)\\) : \\[\nP(X \\geq s+t \\mid X \\geq s)=P(X \\geq t)\n\\] usefulness of it: \\[\nX\\sim Expo(\\lambda),E(X|X&gt;a)=a+E(X-a|X&gt;a)=a+1/\\lambda\n\\]\ne.g.2The hazard rate (or failure rate) for an exponential distribution is constant over time. For an exponential random variable \\(X\\) with rate parameter \\(\\lambda\\), the hazard rate is: \\[\nh(t)=\\frac{f_X(t)}{1-F_X(t)}=\\lambda .\n\\]\nA constant hazard rate implies that the event is equally likely to occur at any point in time, which is a reasonable assumption for many processes, such as the lifetime of certain electronic components or the occurrence of certain types of random failures.\ne.g.3 The exponential distribution is closely related to the Poisson process, which is a process that models the occurrence of events happening independently at a constant average rate. If the times between consecutive events in a Poisson process are independent and identically distributed, then these interarrival times follow an exponential distribution. This relationship makes the exponential distribution a natural choice in contexts where events occur randomly over time, such as phone calls arriving at a switchboard or buses arriving at a bus stop.\n\n\nGa\nIf r.v.X~N(0,1), then \\(X^2\\)~\\(Ga\\)(1/2,1/2)\nProof: If r.v. (X N(0,1)), then (X^2 (, ))\nLet (X) be a random variable such that (X N(0,1)).\nThe probability density function (pdf) of (X) is: \\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}, \\quad -\\infty &lt; x &lt; \\infty.\n\\]\nFirst, we find the pdf of (Y = X^2).\nThe cumulative distribution function (CDF) of (Y) is given by: \\[\nF_Y(y) = P(Y \\leq y) = P(X^2 \\leq y).\n\\]\nSince (X^2 ), we only consider (y ): \\[\nF_Y(y) = P(-\\sqrt{y} \\leq X \\leq \\sqrt{y}).\n\\]\nUsing the CDF of the normal distribution, we have: \\[\nF_Y(y) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\, dx.\n\\]\nThe pdf of (Y) is the derivative of the CDF: \\[\nf_Y(y) = \\frac{d}{dy} F_Y(y).\n\\]\n\\[\nF_Y(y) = 2 \\int_{0}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\, dx.\n\\]\n\\[\nF_Y(y) = 2 \\int_{0}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{-u} \\frac{du}{\\sqrt{2u}} = \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\sqrt{y}} e^{-u} \\frac{du}{\\sqrt{2u}}.\n\\]\nThus, \\[\nf_Y(y) = \\frac{d}{dy} \\left( \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\sqrt{y}} e^{-u} \\frac{du}{\\sqrt{2u}} \\right).\n\\]\nDifferentiating with respect to y gives: \\[\nf_Y(y) = \\frac{1}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{y}{2}} \\cdot y^{-\\frac{1}{2}}.\n\\]\nSimplifying, we get: \\[\nf_Y(y) = \\frac{1}{\\sqrt{2\\pi}} y^{-\\frac{1}{2}} e^{-\\frac{y}{2}}.\n\\]\n\\[\nY = X^2 \\sim \\text{Ga}\\left(\\frac{1}{2}, \\frac{1}{2}\\right).\n\\]\n\n\nChi-square\nChi-square is a\n1.Let ( \\(Z_1\\), \\(Z_2\\), , \\(Z_i\\) ) are independent standard normal random variables(i.e. Z~N(0,1)), then the random variable ( X ) defined by\n\\[\nX = Z_1^2 + Z_2^2 + \\cdots + Z_i^2\n\\] (\\(Z_j\\)~iid.N(0,1))\nfollows a chi-square distribution with ( i ) degrees of freedom. So chi-square of 1 is the same thing as Gamma of (1/2,1/2) so chi-square of n is Gamma(n/2,1/2)\n2.If \\[\nZ_i = \\frac{x_i - \\mu}{\\sigma}\n\\] The sum of squared standardized deviations is:\n\\[\n\\sum_{i=1}^n Z_i^2 = \\sum_{i=1}^n \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]\nLet \\(x_1, x_2, \\cdots, x_n\\) be samples of \\(N\\left(\\mu, \\sigma^2\\right)\\) , \\(\\mu\\) is a known constant, find the distribution of statistics: \\[\nT=\\sum_{i=1}^n\\left(x_i-\\mu\\right)^2\n\\]\nsol: \\(y_i=\\left(x_i-\\mu\\right) / \\sigma, i=1,2, \\cdots, n\\), 则 \\(y_1, y_2, \\cdots, y_n\\) are iid r.v. of \\(N(0,1)\\),so\\[\n\\frac{T}{\\sigma^2}=\\sum_{i=1}^n\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2=\\sum_{i=1}^n y_i^2 \\sim \\chi^2(n),\n\\] (i.e.\\[\n\\sum_{i=1}^n Z_i^2 {\\sigma^2}\\sim \\chi^2(n)\\])\nBesides, \\(T\\)’s PDF is \\[\np(t)=\\frac{1}{\\left(2 \\sigma^2\\right)^{n / 2} \\Gamma(n / 2)} \\mathrm{e}^{-\\frac{1}{2 \\sigma^2} t^{\\frac{n}{2}}-1}, \\quad t&gt;0,\n\\]\nwhich is Gamma distribution \\(G a\\left(\\frac{n}{2}, \\frac{1}{2 \\sigma^2}\\right) \\cdot\\)\n3.chi-square is uesful because of theorems below:let \\(x_1, x_2, \\cdots, x_n\\) are samples from \\(N\\left(\\mu, \\sigma^2\\right)\\) , whose sample mean and sample variance are\\[\n\\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i \\text { and } s^2=\\frac{1}{n-1} \\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2,\n\\]\nthen we can get: (1) \\(\\bar{x}\\) and \\(s^2\\) are independent; (2) \\(\\bar{x} \\sim N\\left(\\mu, \\sigma^2 / n\\right)\\); (3) \\(\\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi^2(n-1)\\).\nProof: \\[\np\\left(x_1, x_2, \\cdots, x_n\\right)=\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\mathrm{e}^{-\\sum_{i=1}^n \\frac{\\left(x_i-\\mu\\right)^2}{2 \\sigma^2}}=\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\exp \\left\\{-\\frac{\\sum_{i=1}^n x_i^2-2 n \\bar{x} \\mu+n \\mu^2}{2 \\sigma^2}\\right\\}\n\\]\ndenote\\(\\boldsymbol{X}=\\left(x_1, x_2, \\cdots, x_n\\right)^{\\mathrm{T}}\\), then we create an \\(n\\)-dimension orthogonal \\(\\boldsymbol{A}\\) and every element in the first row is \\(1 / \\sqrt{n}\\), such as \\[\nA=\\left(\\begin{array}{ccccc}\n\\frac{1}{\\sqrt{n}} & \\frac{1}{\\sqrt{n}} & \\frac{1}{\\sqrt{n}} & \\cdots & \\frac{1}{\\sqrt{n}} \\\\\n\\frac{1}{\\sqrt{2 \\cdot 1}} & -\\frac{1}{\\sqrt{2 \\cdot 1}} & 0 & \\cdots & 0 \\\\\n\\frac{1}{\\sqrt{3 \\cdot 2}} & \\frac{1}{\\sqrt{3 \\cdot 2}} & -\\frac{2}{\\sqrt{3 \\cdot 2}} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n\\frac{1}{\\sqrt{n(n-1)}} & \\frac{1}{\\sqrt{n(n-1)}} & \\frac{1}{\\sqrt{n(n-1)}} & \\cdots & -\\frac{n-1}{\\sqrt{n(n-1)}}\n\\end{array}\\right),\n\\] 令 \\(\\boldsymbol{Y}=\\left(y_1, y_2, \\cdots, y_n\\right)^{\\mathrm{T}}=\\boldsymbol{A} \\boldsymbol{X}\\), \\(|Jacobi|=1\\), and we can find thatt\\[\n\\begin{gathered}\n\\bar{x}=\\frac{1}{\\sqrt{n}} y_1, \\\\\n\\sum_{i=1}^n y_i^2=\\boldsymbol{Y}^{\\mathrm{T}} \\boldsymbol{Y}=\\boldsymbol{X}^{\\mathrm{T}} \\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A} \\boldsymbol{X}=\\sum_{i=1}^n x_i^2,\n\\end{gathered}\n\\]\nso\\(y_1, y_2, \\cdots, y_n\\) ’s joint density function is \\[\n\\begin{aligned}\np\\left(y_1, y_2, \\cdots, y_n\\right) & =\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\exp \\left\\{-\\frac{\\sum_{i=1}^n y_i^2-2 \\sqrt{n} y_1 \\mu+n \\mu^2}{2 \\sigma^2}\\right\\} \\\\\n& =\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\exp \\left\\{-\\frac{\\sum_{i=2}^n y_i^2+\\left(y_1-\\sqrt{n} \\mu\\right)^2}{2 \\sigma^2}\\right\\}\n\\end{aligned}\n\\]\nSo, \\(\\boldsymbol{Y}=\\left(y_1, y_2, \\cdots, y_n\\right)^{\\mathrm{T}}\\) independently distributed as normal distribution and their variances are all equal to\\(\\sigma^2\\), but their means are not all the same because \\(y_2, \\cdots, y_n\\) ’s means are \\(0, y_1\\)’s is \\(\\sqrt{n} \\mu\\), which ends our proof of (2). \\[\n(n-1) s^2=\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2=\\sum_{i=1}^n x_i^2-(\\sqrt{n} \\bar{x})^2=\\sum_{i=1}^n y_i^2-y_1^2=\\sum_{i=2}^n y_i^2,\n\\]\nThis proves conclusion (1). Since \\(y_2, \\cdots, y_n\\) are independently and identically distributed as \\(N\\left(0, \\sigma^2\\right)\\), we have: \\[\n\\frac{(n-1) s^2}{\\sigma^2}=\\sum_{i=2}^n\\left(\\frac{y_i}{\\sigma}\\right)^2 \\sim \\chi^2(n-1) .\n\\]\nTheorem is proved. (similar to the proof above this maybe easier to understand:\\(\\begin{aligned} & i z\\left(Y_1, Y_2, \\cdots, Y_2\\right)^{\\top}=A\\left(X_1, \\cdots, x_n\\right)^{\\top} \\\\ & \\text { then } \\sum_{i=1}^n Y_i^2=\\left(Y_1, \\cdots, Y_n\\right)\\left(Y_1, \\cdots, Y_n\\right)^{\\top} \\\\ & =\\left[A\\left(x_1, \\cdots, x_n\\right)^{\\top}\\right]^{\\top}\\left[A\\left(x_1, \\cdots, X_n\\right)^{\\top}\\right] \\\\ & =\\left(x_1, \\cdots, x_n\\right) A^{\\top} A\\left(x_1, \\cdots, x_n\\right)^{\\top} \\\\ & =\\left(x_1, \\cdots, x_n\\right) E\\left(x_1, \\cdots, x_n\\right)^{\\top}=\\sum_{i=1}^n x_i^2 \\\\ & \\end{aligned}\\)) \\(\\begin{aligned} & \\text { besides } Y_1=\\frac{1}{\\sqrt{n}} x_1+\\cdots+\\frac{1}{\\sqrt{n}} x_n=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i \\\\ & \\text { and } Y_1=\\sqrt{n} \\cdot \\frac{1}{n} \\sum_{i=1}^n X_i=\\sqrt{n} \\bar{X}, \\text { then } \\bar{x}=\\frac{1}{\\sqrt{n}} y_i \\\\ & B S^2=\\frac{1}{n-1} \\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2=\\frac{1}{n-1}\\left[\\sum_{i=1}^n x_i^2-n \\bar{x}^2\\right] \\\\ & =\\frac{1}{n-1}\\left[\\sum_{i=1}^n Y_i^2-Y_1^2\\right]=\\frac{1}{n-1} \\sum_{i=2}^n Y_i^2 \\\\ & 2 \\oplus L=(\\sqrt{2 \\pi} \\sigma)^{-n} \\exp \\left[-\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n\\left(x_i-\\mu\\right)^2\\right] \\text {. } \\\\ & =(\\sqrt{2 \\pi} \\sigma)^{-n} \\exp \\left[-\\frac{1}{2 \\sigma^2}\\left(\\sum_{i=1}^n x_i^2-2 \\mu n \\bar{x}+\\mu^2 n\\right]\\right. \\\\ & =(\\sqrt{2 \\pi} \\sigma)^{-n} \\exp \\left[-\\frac{1}{2 \\sigma^2}\\left(\\sum_{1=1}^n y_i{ }^2-2 \\mu n \\frac{1}{\\sqrt{n}} Y_1+n \\mu^2\\right]\\right. \\\\ & \\end{aligned}\\) \\[=(\\sqrt2\\pi\\sigma)^{-1}exp[-1/2\\sigma^2(Y_1-\\sqrt nu)^2]×(\\sqrt2\\pi\\sigma)^{-1}exp[-1/2\\sigma^2{Y_2}^2]*...*(\\sqrt2\\pi\\sigma)^{-1}exp[-1/2\\sigma^2{Y_n}^2]\n\\]\nSo L is \\(Y_1\\)…\\(Y_n\\)’s joint density function and so they are independent. Besides, we have proved that its mean is \\(1/\\sqrt n\\)\\(Y_1\\) and \\(S^2\\)=\\(1/n-1 \\Sigma{i=2}Y_i^2\\), so the normal distribution’s mean and variance are independent.\nWhen the random variable \\(\\chi^2 \\sim \\chi^2(n)\\), for a given \\(\\alpha\\) (where \\(0&lt;\\) \\(\\alpha&lt;1\\) ), the value \\(\\chi_{1-\\alpha}^2(n)\\) satisfying the probability equation \\(P\\left(\\chi^2 \\leqslant \\chi_{1-\\alpha}^2(n)\\right)=1-\\) \\(\\alpha\\) is called the \\(1-\\alpha\\) quantile of the \\(\\chi^2\\) distribution with \\(n\\) degrees of freedom.\nSuppose the random variables \\(X_1 \\sim \\chi^2(m)\\) and \\(X_2 \\sim \\chi^2(n)\\), and \\(X_1\\) and \\(X_2\\) are independent. Then the distribution of \\(F=\\frac{X_1 / m}{X_2 / n}\\) is called the \\(\\mathrm{F}\\) distribution with \\(m\\) and \\(n\\) degrees of freedom, denoted as \\(F \\sim F(m, n)\\). Here, \\(m\\) is called the numerator degrees of freedom and \\(n\\) the denominator degrees of freedom. We derive the density function of the \\(\\mathrm{F}\\) distribution in two steps. First, we derive the density function of \\(Z=\\frac{X_1}{X_2}\\). Let \\(p_1(x)\\) and \\(p_2(x)\\) be the density functions of \\(\\chi^2(m)\\) and \\(\\chi^2(n)\\) respectively. According to the formula for the distribution of the quotient of independent random variables, the density function of \\(Z\\) is: \\[\n\\begin{gathered}\np_Z(z)=\\int_0^{\\infty} x_2 p_1\\left(z x_2\\right) p_2\\left(x_2\\right) \\mathrm{d} x_2 \\\\\n=\\frac{z^{\\frac{m}{2}-1}}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right) 2^{\\frac{m+n}{2}}} \\int_0^{\\infty} x_2^{\\frac{n}{2}-1} e^{-\\frac{x_2}{2}(1+z)} \\mathrm{d} x_2 .\n\\end{gathered}\n\\]\nUsing the transformation \\(u=\\frac{x_2}{2}(1+z)\\), we get: \\[\np_Z(z)=\\frac{z^{\\frac{m}{2}-1}(1+z)^{-\\frac{m+n}{2}}}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)} \\int_0^{\\infty} u^{\\frac{n}{2}-1} e^{-u} \\mathrm{~d} u\n\\]\nThe final integral is the gamma function \\(\\Gamma\\left(\\frac{n}{2}\\right)\\), so: \\[\np_Z(z)=\\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)} z^{\\frac{m}{2}-1}(1+z)^{-\\frac{m+n}{2}}, \\quad z \\geq 0 .\n\\]\nSecond, we derive the density function of \\(F=\\frac{n}{m} Z\\). Let the value of \\(F\\) be \\(y\\). For \\(y \\geq 0\\), we have: \\[\n\\begin{aligned}\np_F(y) & =p_Z\\left(\\frac{m}{n} y\\right) \\cdot \\frac{m}{n}=\\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)}\\left(\\frac{m}{n} y\\right)^{\\frac{m}{2}-1}\\left(1+\\frac{m}{n} y\\right)^{-\\frac{m+n}{2}} \\cdot \\frac{m}{n} \\\\\n& =\\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)}\\left(\\frac{m}{n}\\right)\\left(\\frac{m}{n} y\\right)^{\\downarrow \\frac{2}{2}-1}\\left(1+\\frac{m}{n} y\\right)^{-\\frac{m+n}{2}}\n\\end{aligned}\n\\]\nWhen the random variable \\(F \\sim F(m, n)\\), for a given \\(\\alpha\\) (where \\(0&lt;\\alpha&lt;1\\) ), the value \\(F_{1-\\alpha}(m, n)\\) satisfying the probability equation \\(P\\left(F \\leqslant F_{1-\\alpha}(m, n)\\right)=1-\\alpha\\) is called the \\(1-\\alpha\\) quantile of the \\(\\mathrm{F}\\) distribution with \\(m\\) and \\(n\\) degrees of freedom. By the construction of the \\(\\mathrm{F}\\) distribution, if \\(F \\sim F(m, n)\\), then \\(1 / F \\sim F(n, m)\\). Therefore, for a given \\(\\alpha\\) (where \\(0&lt;\\alpha&lt;1\\) ), \\[\n\\alpha=P\\left(\\frac{1}{F} \\leqslant F_\\alpha(n, m)\\right)=P\\left(F \\geqslant \\frac{1}{F_\\alpha(n, m)}\\right) .\n\\]\nThus, \\[\nP\\left(F \\leqslant \\frac{1}{F_\\alpha(n, m)}\\right)=1-\\alpha\n\\]\nThis implies \\[\nF_\\alpha(n, m)=\\frac{1}{F_{1-\\alpha}(m, n)} .\n\\]\nCorollary Suppose \\(x_1, x_2, \\cdots, x_m\\) is a sample from \\(N\\left(\\mu_1, \\sigma_1^2\\right)\\) and \\(y_1, y_2, \\cdots, y_n\\) is a sample from \\(N\\left(\\mu_2, \\sigma_2^2\\right)\\), and these two samples are independent. Let: \\[\ns_x^2=\\frac{1}{m-1} \\sum_{i=1}^m\\left(x_i-\\bar{x}\\right)^2, \\quad s_y^2=\\frac{1}{n-1} \\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2,\n\\] where \\[\n\\bar{x}=\\frac{1}{m} \\sum_{i=1}^m x_i, \\quad \\bar{y}=\\frac{1}{n} \\sum_{i=1}^n y_i\n\\] then \\[\nF=\\frac{s_x^2 / \\sigma_1^2}{s_y^2 / \\sigma_2^2} \\sim F(m-1, n-1) .\n\\]\nIn particular, if \\(\\sigma_1^2=\\sigma_2^2\\), then \\(F=\\frac{s_x}{s_y^2} \\sim F(m-1, n-1)\\). Proof: Since the two samples are independent, \\(s_x^2\\) and \\(s_y^2\\) are independent. According to a Theorem , we have \\[\n\\frac{(m-1) s_x^2}{\\sigma_1^2} \\sim \\chi^2(m-1), \\quad \\frac{(n-1) s_y^2}{\\sigma_2^2} \\sim \\chi^2(n-1) .\n\\]\nBy the definition of the \\(\\mathrm{F}\\) distribution, \\(F \\sim F(m-1, n-1)\\). Corollary: Suppose \\(x_1, x_2, \\cdots, x_n\\) is a sample from a normal distribution \\(N\\left(\\mu, \\sigma^2\\right)\\), and let \\(\\bar{x}\\) and \\(s^2\\) denote the sample mean and sample variance of the sample, respectively. Then \\[\nt=\\frac{\\sqrt{n}(\\bar{x}-\\mu)}{s} \\sim t(n-1) .\n\\]\nProof: From a Theorem we obtain \\[\n\\frac{\\bar{x}-\\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)\n\\]\nThen, \\[\n\\frac{\\sqrt{n}(\\bar{x}-\\mu)}{s}=\\frac{\\frac{\\bar{x}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) s^2 / \\sigma^2}{n-1}}}\n\\]\nSince the numerator is a standard normal variable and the denominator’s square root contains a \\(\\chi^2\\) variable with \\(n-1\\) degrees of freedom divided by its degrees of freedom, and they are independent, by the definition of the \\(t\\) distribution, \\(t \\sim t(n-1)\\). The proof is complete.\nCorollary: In the notation of Corollary , assume \\(\\sigma_1^2=\\sigma_2^2=\\sigma^2\\), and let \\[\ns_w^2=\\frac{(m-1) s_x^2+(n-1) s_y^2}{m+n-2}=\\frac{\\sum_{i=1}^m\\left(x_i-\\bar{x}\\right)^2+\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2}{m+n-2}\n\\]\nThen \\[\n\\frac{(\\bar{x}-\\bar{y})-\\left(\\mu_1-\\mu_2\\right)}{s_w \\sqrt{\\frac{1}{m}+\\frac{1}{n}}} \\sim t(m+n-2)\n\\]\nProof: Since \\(\\bar{x} \\sim N\\left(\\mu_1, \\frac{\\sigma^2}{m}\\right), \\bar{y} \\sim N\\left(\\mu_2, \\frac{\\sigma^2}{n}\\right)\\), and \\(\\bar{x}\\) and \\(\\bar{y}\\) are independent, we have\n\\[\n\\bar{x}-\\bar{y} \\sim N\\left(\\mu_1-\\mu_2,\\left(\\frac{1}{m}+\\frac{1}{n}\\right) \\sigma^2\\right) .\n\\]\nThus, \\[\n\\frac{(\\bar{x}-\\bar{y})-\\left(\\mu_1-\\mu_2\\right)}{\\sigma \\sqrt{\\frac{1}{m}+\\frac{1}{n}}} \\sim N(0,1) .\n\\]\nBy a Theorem , we know that \\(\\frac{(m-1) s_x^2}{\\sigma^2} \\sim \\chi^2(m-1)\\) and \\(\\frac{(n-1) s_y^2}{\\sigma^2} \\sim \\chi^2(n-1)\\), and they are independent. By additivity, we have \\[\n\\frac{(m+n-2) s_w^2}{\\sigma^2}=\\frac{(m-1) s_x^2+(n-1) s_y^2}{\\sigma^2} \\sim \\chi^2(m+n-2) .\n\\]\nSince \\(\\bar{x}-\\bar{y}\\) and \\(s_w^2\\) are independent, by the definition of the \\(\\mathrm{t}\\) distribution, we get the desired result. \\(\\square\\)\nOne interesting example shows the relationship of above distributions used charismatically to solve problems: r.v.: \\(X_1 ， X_2 ， X_3 ， X_4\\) indpendently identically distribute(iid) as \\(N\\left(0 . \\sigma^2\\right)\\). \\(Z=\\left(x_1^2+x_2^2\\right) /\\left(x_1^2+x_2^2+x_3^2+x_4^2\\right)\\) prove: \\(Z \\sim U(0.1)\\). \\[\n\\begin{aligned}\n& \\text { Solution: } Let Y=\\frac{X_3^2+X_4^2}{X_1^2+X_2^2}=\\frac{\\left[\\left(\\frac{X_3}{\\sigma}\\right)^2+\\left(\\frac{X_4}{\\sigma}\\right)^2\\right] / 2}{\\left[\\left(\\frac{X_1}{\\sigma}\\right)^2+\\left(\\frac{X_2}{\\sigma}\\right)^2\\right] / 2} \\sim F(2,2) . \\\\\n& \\text { i.e.  } f_Y(y)=\\frac{1}{(1+y)^2},  y&gt;0 \\\\\n& \\text { then } P(Z \\leq z)=P\\left(\\frac{1}{1+Y} \\leq z\\right)=P\\left(Y \\geqslant \\frac{1}{z}-1\\right) \\\\\n& =\\int_{\\frac{1}{z}-1}^{+\\infty} \\frac{1}{(1+y)^2} d y=z \\quad \\text { H } 0&lt;z&lt;1 . \\\\\n& \\therefore Z \\sim U(0.1)\n\\end{aligned}\n\\] (ps:$\n\\[\\begin{aligned} & f(x)=\\frac{\\Gamma\\left(\\frac{n_1+n_2}{2}\\right)}{\\Gamma\\left(\\frac{n_2}{2}\\right) \\Gamma\\left(\\frac{n_1}{2}\\right)}\\left(\\frac{n_1}{n_2}\\right)\\left(\\frac{n_1}{n_2} x\\right)^{\\frac{n_1}{2}-1}\\left(1+\\frac{n_1}{n_2} x\\right)^{\\frac{-1}{2}\\left(n_1+n_2\\right)} . \\\\ & \\text { of these } x&gt;0 \\text {. and } E(x)=\\frac{n_2}{n_2-2} \\text {, when } n_2&gt;2 \\text {. } \\\\ & \\operatorname{Var}(X)=\\frac{2 n_2^2\\left(n_1+n_2-2\\right)}{n_1\\left(n_2-2\\right)^2\\left(n_2-4\\right)} \\text {when $n_2&gt;4$. } \\\\ & \\end{aligned}\\]"
  },
  {
    "objectID": "mth1113.html#pdf-functionss-transformationfrom-books",
    "href": "mth1113.html#pdf-functionss-transformationfrom-books",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "PDF functions’s transformation(from books)",
    "text": "PDF functions’s transformation(from books)\n\nStandard normal distribution\nPdf : \\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\)\n\n\n\nChi-square distribution\n\n\nFrom standard normal distribution \\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\), we want to get Chi-square distribution \\(f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}\\)\nAssuming that \\(Z \\sim N(0, 1)\\)\n\\(W=Z^2\\)\nSo we can deduce the pdf for the variable W \\[\n\\begin{align}\nf_W(w)&=Pr(Z^2=w)\\\\\n& =f_Z(\\sqrt{w}) \\left| \\frac{d(\\sqrt{w})}{dw} \\right| + f_Z(-\\sqrt{w}) \\left| \\frac{d(-\\sqrt{w})}{dw} \\right|\\\\\n&=2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-w / 2} \\cdot \\frac{1}{2\\sqrt{w}} = \\frac{1}{\\sqrt{2\\pi w}} e^{-w / 2}\\\\\n\\end{align}\n\\]\n\\(\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} \\, dt\\)\n\\(\\Gamma(1/2)\\)=\\(\\sqrt(\\pi)\\),we can know that \\(W\\sim \\chi^2(1)\\)\n\nNow, we can introduce the variable Y.\n\\(Y=\\sum_{i=1}^k Z_i^2\\) (\\(Z_i\\) is independent of each other)\nWe want to get the mgf of Y \\[\n\\begin{align}\nM_Y(t)&=E[e^{tY}]\\\\\n& =E[e^{tZ_1^2}e^{tZ_2^2}...e^{tZ_k^2}]\\\\\n& =E[e^{tZ_1^2}]E[e^{tZ_2^2}]...E[e^{tZ_k^2}] \\\\\n& = \\prod_{i=1}^k M_{Z_i^2}(t)\\\\\n&=(1-2t)^{-r1/2}(1-2t)^{-r2/2}...(1-2t)^{-rk/2}\\\\\n& =(1-2t)^{-\\sum_{i=1}^kri/2}\\\\\n\\end{align}\n\\] Because the mgf of Chi-square function is \\((1-2t)^{-r/2}\\)\nr is the degree of freedom of this chi-square function\nSo \\(Y\\sim \\chi^2(r1+r2+...+rk)\\)\nr1,r2…rk represents the degree of freedom of every single sample\nHere, df=1 for every sample\n\n\nStudent- t distribution\n\n\nFrom standard normal distribution \\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\), we want to get Student- t distribution \\(f(t) = \\frac{\\Gamma \\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi} \\, \\Gamma \\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\\), where \\(\\Gamma\\) is the gamma function and \\(\\nu\\) is the degrees of freedom.\nGiven two independent variables Z and V (\\(Z \\sim N(0, 1)\\) & \\(V\\sim \\chi^2(\\nu)\\)), then we construct a new variable \\(T=\\frac{Z}{\\sqrt(V/\\nu)}\\).\nThe joint pdf is \\[\ng(z,v)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}\\frac{1}{2^{\\nu/2} \\Gamma(\\nu/2)} v^{\\nu/2 - 1} e^{-v/2}\n\\] Cdf of T is given by \\[\n\\begin{align}\nF(t) & =Pr(\\frac{Z}{\\sqrt(V/\\nu)}\\leq t)\\\\\n& =Pr(Z\\leq {\\sqrt(V/\\nu)}t)\\\\\n& =\\int_{0}^{\\infty} \\int_{-\\infty}^{\\sqrt(V/\\nu)t} g(z,v) \\, dz \\, dv\n\\end{align}\n\\] Simplify F(t) \\[\nF(t)=\\frac{1}{\\sqrt{\\pi}\\Gamma(\\nu/2)}\\int_{0}^{\\infty}[\\int_{-\\infty}^{\\sqrt(V/\\nu)t} \\frac{e^{-z^2/2}}{2^\\frac{\\nu+1}{2}}dz]v^{\\frac{\\nu}{2}-1}e^{-\\frac{v}{2}}dv\n\\]\nTo get pdf, we will differentiate F(t) \\[\n\\begin{align}\nf(t) &=F'(t)=\\frac{1}{\\sqrt{\\pi}\\Gamma(\\nu/2)}\\int_{0}^{\\infty} \\frac{e^{-(v/2)(t^2/\\nu)}}{2^{\\frac{\\nu+1}{2}}}\\sqrt\\frac{v}{\\nu}v^{\\nu/2-1}e^{-\\frac{v}{2}}dv\\\\\n&=\\frac{1}{\\sqrt{\\pi\\nu}\\Gamma(\\nu/2)}\\int_{0}^{\\infty}\\frac{v^{(\\nu+1)/2-1}}{2^{(\\nu+1)/2}}e^{-(\\nu/2)(1+t^2/\\nu)}dv\n\\end{align}\n\\] We need to make the change of variables: \\(y=(1+t^2/\\nu)v\\)\nAnd we need to change dv: \\(\\frac{dv}{dy}=\\frac{1}{1+t^2/\\nu}\\)\n\\[\n\\begin{align}\nf(t)&=\\frac{\\Gamma[(\\nu+1)/2]}{\\sqrt{\\pi\\nu}\\Gamma(\\nu/2)}[\\frac{1}{(1+t^2/\\nu)^{(\\nu+1)/2}}]\\int_{0}^{\\infty}\\frac{y^{(\\nu+1)/2-1}}{\\Gamma[(\\nu+1)/2]2^{(\\nu+1)/2}}e^{-y/2}dy\n\\end{align}\n\\] This part \\(\\int_{0}^{\\infty}\\frac{y^{(\\nu+1)/2-1}}{\\Gamma[(\\nu+1)/2]2^{(\\nu+1)/2}}e^{-y/2}dy\\) is equal to 1, because this part is the whole area under the chi-square distribution with \\(\\nu+1\\) degrees of freedom. So, the pdf for T can be written as follows \\[\nf(t)=\\frac{\\Gamma[(\\nu+1)/2]}{\\sqrt{\\pi\\nu}\\Gamma(\\nu/2)}\\frac{1}{(1+t^2/\\nu)^{(\\nu+1)/2}}\n\\]\n\n\nF-distribution\n\n\nWe will do some trnsformation on chi-square distribution to get F-distribution\nAssuming that we have two independent random variables \\[\nX \\sim \\chi^2(n_1) \\quad and\\quad  Y \\sim \\chi^2(n_2)\\\n\\] Now,we will define a new variable F \\[\nF = \\frac{(X / n_1)}{(Y / n_2)}\n\\] This looks a little complex, so let’s do some simplification.\n\\[\nU = \\frac{X}{n_1} \\quad \\text{and} \\quad V = \\frac{Y}{n_2}\\\n\\]\nSo, \\(F = \\frac{U}{V}\\).\nBecause, X and Y are independent of each other. Obviously, U and V are also independent of each other.\n\n\\[\nf_{U,V}(u,v) = f_U(u) f_V(v)\n\\] \\[\nf_U(u) = \\frac{(n_1 u)^{n_1/2 - 1} e^{-n_1 u/2}}{2^{n_1/2} \\Gamma(n_1/2)}\n\\] \\[\nf_V(v) = \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)}\n\\] To find the joint density function of F & V, we use Jacobian transformation \\[\nJ =\\left| \\frac{\\partial(U,V)}{\\partial(F,V)} \\right| = \\left| \\begin{matrix}\n\\frac{\\partial U}{\\partial F} & \\frac{\\partial U}{\\partial V} \\\\\n\\frac{\\partial V}{\\partial F} & \\frac{\\partial V}{\\partial V}\n\\end{matrix} \\right| = \\left| \\begin{matrix}\nV & F \\\\\n0 & 1\n\\end{matrix} \\right| = V\n\\] \\[\nf_{F,V}(f,v) = f_{U,V}(u,v) \\left| \\frac{\\partial(u,v)}{\\partial(f,v)} \\right|= f_{U,V}(u,v)v\n\\] Then, we substitute\\(f_U(u)\\) and \\(f_V(v)\\) into \\(f_{F,V}(f,v)\\) \\[\nf_{F,V}(f,v) = \\frac{(n_1 u)^{n_1/2 - 1} e^{-n_1 u/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\n\\] Use the condition u=fv \\[\n\\begin{align}\nf_{F,V}(f,v) &= \\frac{(n_1 fv)^{n_1/2 - 1} e^{-n_1 fv/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\\\\\n& =\\frac{(n_1 f v)^{n_1/2 - 1} e^{-n_1 f v/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\\\\\n& = \\frac{n_1^{n_1/2-1} f^{n_1/2 - 1} v^{n_1/2 - 1} e^{-n_1 f v/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{n_2^{n_2/2-1} v^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\\\\\n& = \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} v^{(n_1 + n_2)/2 - 1} e^{-(n_1 f + n_2) v/2}}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)}\n\\end{align}\n\\] We will integrate this density function with respect to V \\[\n\\begin{align}\nf_F(f) &= \\int_0^\\infty f_{F,V}(f,v) dv\\\\\n&= \\int_0^\\infty \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} v^{(n_1 + n_2)/2 - 1} e^{-(n_1 f + n_2) v/2}}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)} dv\\\\\n\\end{align}\n\\] Let’s do some substitutions to make it look simpler \\[\n\\begin{align}\nc = \\frac{n_1 f + n_2}{2}\n\\end{align}\n\\] This is the definition of Gamma function \\[\n\\int_0^\\infty v^{(n_1 + n_2)/2 - 1} e^{-c v} dv = \\frac{\\Gamma((n_1 + n_2)/2)}{c^{(n_1 + n_2)/2}}\n\\]\nThen \\[\nf_F(f) = \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1}}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)} \\cdot \\frac{\\Gamma((n_1 + n_2)/2)}{\\left(\\frac{n_1 f + n_2}{2}\\right)^{(n_1 + n_2)/2}}\n\\] Let’s rewrite it in an approximate F-distribution pdf form \\[\nf_F(f) = \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} \\Gamma((n_1 + n_2)/2)}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)} \\cdot \\left(\\frac{2}{n_1 f + n_2}\\right)^{(n_1 + n_2)/2}\n\\]\n\n\nbeta and Gamma\nWhen considering the product of two Gamma functions \\[(\\Gamma(a) \\Gamma(b))\\], we can write it as two independent integrals:\n\\[\n\\Gamma(a) \\Gamma(b) = \\left( \\int_0^\\infty t^{a-1} e^{-t} \\, dt \\right) \\left( \\int_0^\\infty s^{b-1} e^{-s} \\, ds \\right)\n\\]\nTo convert this product into a double integral, we can use a change of variables. Consider using polar coordinates in the two independent integrals, which allows us to use certain integration techniques to simplify them. First, we transform the integration region from Cartesian coordinates to polar coordinates:\n\\[\nt = r \\cos \\theta, \\quad s = r \\sin \\theta\n\\]\nThe Jacobian determinant is \\[r\\], so the integral can be rewritten as:\n\\[\n\\Gamma(a) \\Gamma(b) = \\int_0^\\infty \\int_0^\\infty t^{a-1} e^{-t} s^{b-1} e^{-s} \\, dt \\, ds\n\\]\nUsing the polar coordinate transformation, the integral becomes:\n\\[\n= \\int_0^\\frac{\\pi}{2} \\int_0^\\infty (r \\cos \\theta)^{a-1} e^{-r \\cos \\theta} (r \\sin \\theta)^{b-1} e^{-r \\sin \\theta} r \\, dr \\, d\\theta\n\\]\nSeparating all \\[r\\] and \\[\\theta\\] related terms:\n\\[\n= \\int_0^\\frac{\\pi}{2} (\\cos \\theta)^{a-1} (\\sin \\theta)^{b-1} \\, d\\theta \\int_0^\\infty r^{a+b-1} e^{-r(\\cos \\theta + \\sin \\theta)} \\, dr\n\\]\nFirst, compute the \\[r\\] integral part:\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r(\\cos \\theta + \\sin \\theta)} \\, dr = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}}\n\\]\nNext, compute the \\[\\theta\\] integral part:\n\\[\n\\int_0^\\frac{\\pi}{2} (\\cos \\theta)^{a-1} (\\sin \\theta)^{b-1} \\, d\\theta = B(a, b)\n\\]\nwhere \\[B(a, b)\\] is the Beta function, defined as:\n\\[\nB(a, b) = \\int_0^1 t^{a-1} (1-t)^{b-1} \\, dt\n\\]\nTherefore, we get:\n\\[\n\\Gamma(a) \\Gamma(b) = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}} \\cdot B(a, b)\n\\]\nSince \\[\\cos \\theta + \\sin \\theta = 1\\] (in polar coordinates), we have:\n\\[\n\\Gamma(a) \\Gamma(b) = \\Gamma(a+b) B(a, b)\n\\]\nThus, the double integral form of the Gamma function product directly comes from the polar transformation and the application of the Beta function. This is an important mathematical technique used to simplify complex integrals and functional relationships.\n\n\nWhy is the integral result as shown?\nThe given integral,\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr,\n\\]\ncan be evaluated using the definition of the Gamma function. The Gamma function \\[\\Gamma(z)\\] is defined as:\n\\[\n\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} \\, dt.\n\\]\nTo see why the integral can be expressed in terms of the Gamma function, let’s rewrite the integral in a form that matches the Gamma function’s definition. The integral has the form:\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr.\n\\]\nLet’s set \\[t = r (\\cos \\theta + \\sin \\theta)\\], then \\[r = \\frac{t}{\\cos \\theta + \\sin \\theta}\\] and \\[dr = \\frac{dt}{\\cos \\theta + \\sin \\theta}\\]. Substituting these into the integral gives:\n\\[\n\\int_0^\\infty \\left( \\frac{t}{\\cos \\theta + \\sin \\theta} \\right)^{a+b-1} e^{-t} \\cdot \\frac{dt}{\\cos \\theta + \\sin \\theta}.\n\\]\nSimplifying inside the integral:\n\\[\n\\int_0^\\infty \\frac{t^{a+b-1}}{(\\cos \\theta + \\sin \\theta)^{a+b}} e^{-t} \\, dt.\n\\]\nSince \\[(\\cos \\theta + \\sin \\theta)^{a+b}\\] is a constant with respect to \\[t\\], it can be factored out of the integral:\n\\[\n\\frac{1}{(\\cos \\theta + \\sin \\theta)^{a+b}} \\int_0^\\infty t^{a+b-1} e^{-t} \\, dt.\n\\]\nThe integral\n\\[\n\\int_0^\\infty t^{a+b-1} e^{-t} \\, dt\n\\]\nis recognized as the Gamma function \\[\\Gamma(a+b)\\]. Thus, we have:\n\\[\n\\frac{1}{(\\cos \\theta + \\sin \\theta)^{a+b}} \\Gamma(a+b).\n\\]\nTherefore,\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}}.\n\\]\nThis demonstrates why the integral is evaluated as shown:\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}}.\n\\] (method 2 for integral calculation: x=uv,y=u(1-v) J=-u)"
  },
  {
    "objectID": "mth1113.html#three-main-sampling-distribution",
    "href": "mth1113.html#three-main-sampling-distribution",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Three Main Sampling distribution",
    "text": "Three Main Sampling distribution\n\nChi-square distribution\nSuppose \\(X_1, \\cdots, X_n \\stackrel{\\text { i.i.d. }}{\\sim} \\mathcal{N}(0,1)\\).the distribution of the statistic \\[\nX_1^2+\\cdots+X_n^2\n\\] is called a chi-square distribution with \\(n\\) degrees of freedom, denoted by \\(\\chi^2(n)\\).\nBesides, random variable \\(X_i^2 \\sim \\operatorname{Gamma}\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\) corresponds to the chi-squared distribution with 1 degree of freedom, denoted as \\(\\chi_1^2\\).\nThis is derived by the MGF:\nSince \\[\nM_{X_1^2+\\cdots+X_n^2}(t)=M_{X_1^2}(t) \\times \\cdots \\times M_{X_n^2}(t)= \\begin{cases}\\infty & t \\geq \\frac{1}{2} \\\\ (1-2 t)^{-\\frac{n}{2}} & t&lt;\\frac{1}{2}\\end{cases}\n\\]\nThis is the MGF of the \\(\\operatorname{Gamma}\\left(\\frac{n}{2}, \\frac{1}{2}\\right)\\) distribution, so \\(X_1^2+\\cdots+X_n^2 \\sim \\operatorname{Gamma}\\left(\\frac{n}{2}, \\frac{1}{2}\\right)\\). This is called the chi-squared distribution with \\(\\mathbf{n}\\) degree of freedom, denoted \\(\\chi_n^2\\).\n\nProperties\n\nIf \\(W_1, \\ldots, W_n\\) are independent \\(\\chi^2\\) random variables with, respectively, \\(v_1, \\cdots, v_n\\) degrees of freedom, then the random variable \\(W_1+\\cdots+W_n\\) follows a \\(\\chi^2\\)-distribution with \\(v_1+\\cdots+v_n\\) degree of freedom.\nThe random variable \\(\\frac{(\\bar{X}-\\mu)^2}{\\sigma^2 / n}\\) follows a \\(\\chi^2\\)-distribution with 1 degree of freedom when \\(X\\) follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\n\n\nCode to plot examples of Chi-square distribution\n\nlibrary(ggplot2)\n\n# Create a sequence of values\nx &lt;- seq(0, 20, length.out = 200)\n\n# Calculate the density for different degrees of freedom\ndf4 &lt;- dchisq(x, df = 4)\ndf8 &lt;- dchisq(x, df = 8)\ndf12 &lt;- dchisq(x, df = 12)\n\nchi_data &lt;- data.frame(x, df4, df8, df12)\nggplot(chi_data, aes(x)) +\n  geom_line(aes(y = df4, color = \"df=4\")) +\n  geom_line(aes(y = df8, color = \"df=8\")) +\n  geom_line(aes(y = df12, color = \"df=12\")) +\n  labs(title = \"Chi-Square Distribution\",\n       x = \"Value\",\n       y = \"Density\") +\n  scale_color_manual(name = \"Degrees of Freedom\", values = c(\"df=4\" = \"blue\",\"df=8\" = \"red\", \"df=12\" = \"green\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nApplication\nChi-square distribution is primarily used in testing:\n\nGoodness-of-fit\nIndependence in contingency tables\n\n\n\n\nStudent’s t-distribution\n\nConstruction\nThe statistic \\(T=\\frac{\\bar{X}-\\mu}{S / \\sqrt{n}}\\) follows a \\(t\\)-distribution with \\(v=n-1\\) degrees of freedom when \\(X_1, \\cdots, X_n\\) are i.i.d. normal RVs.\n\\[\n\\bar{X} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\quad \\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0,1) \\quad \\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi_{n-1}^2\n\\]\nIf we know the population variance \\(\\sigma^2\\), we can easily do inference using the statistic \\(\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}}\\). However, \\(\\sigma^2\\) is usually unknown in practice.\n\\[\n\\bar{X} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\quad \\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0,1) \\quad \\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi_{n-1}^2\n\\]\nWe can construct the \\(t\\)-statistic using the sample variance \\(S^2\\) : \\[\nT=\\frac{\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) s^2}{\\sigma^2} /(n-1)}}=\\frac{\\bar{X}-\\mu}{S / \\sqrt{n}}\n\\]\nNotice the sample mean \\(\\bar{X}\\) and the sample variance \\(S^2\\) are independent (the proof is beyond the scope of this course). So the \\(T\\) is now a ratio of a standard normal variable and the square root of a \\(\\chi^2 \\mathrm{RV}\\) divided by its degrees of freedom. This is the definition of a \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\n\n\nProperties\nThe \\(t\\)-distribution is primarily used in contexts where the underlying population is assumed to be normally distributed, especially when the sample size is small. Used extensively in problems that deal with inference about population mean \\(\\mu\\) when population variance \\(\\sigma^2\\) is unknown; problems where one is trying to determine if means from two samples are significantly different when population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown.\n\n\nCode to plot examples of t-distribution\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a sequence of values\nx &lt;- seq(-4, 4, length.out = 1000)\n\n# Calculate the density for different degrees of freedom\ndf_values &lt;- c(1, 2, 3, 5, 10, 30)\ndistributions &lt;- lapply(df_values, function(df) dt(x, df = df))\n\n# Create a data frame to store the data\ndf_data &lt;- data.frame(x = x)\nfor (i in seq_along(df_values)) {\n  df_data[paste0(\"df\", df_values[i])] &lt;- distributions[[i]]\n}\n\n# Plot the densities\nggplot(df_data, aes(x = x)) +\n  geom_line(aes(y = df1, color = \"df=1\")) +\n  geom_line(aes(y = df2, color = \"df=2\")) +\n  geom_line(aes(y = df3, color = \"df=3\")) +\n  geom_line(aes(y = df5, color = \"df=5\")) +\n  geom_line(aes(y = df10, color = \"df=10\")) +\n  geom_line(aes(y = df30, color = \"df=30\")) +\n  geom_line(aes(y = dnorm(x), color = \"Standard Normal\")) +\n  scale_color_manual(name = \"Distribution\", \n                     values = c(\"df=1\" = \"red\", \"df=2\" = \"red\", \"df=3\" = \"red\", \n                                \"df=5\" = \"green\", \"df=10\" = \"green\", \"df=30\" = \"green\", \n                                \"Standard Normal\" = \"blue\")) +\n  labs(title = \"Density of the t-distribution compared to the standard normal distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nF-distribution\nLet \\(U\\) and \\(V\\) be two independent RVs following \\(\\chi^2\\) distributions with \\(\\nu_1\\) and \\(\\nu_2\\) degrees of freedom, respectively. Then the distribution of the random variable \\(F=\\frac{U / \\nu_1}{V / \\nu_2}\\) is known as \\(F\\)-distribution.\n\nExample\nIf \\(S_1^2\\) and \\(S_2^2\\) are the variances of independent RVs of size \\(n_1\\) and \\(n_2\\) taken from normal populations with variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) respectively, then \\[\nF=\\frac{S_1^2 / \\sigma_1^2}{S_2^2 / \\sigma_2^2}=\\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2}\n\\] follows an \\(F\\)-distribution with \\(\\nu_1=n_1-1\\) and \\(\\nu_2=n_2-1\\) degrees of freedom.\n\n\nCode to plot examples of F-distribution\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a sequence of values\nx &lt;- seq(0, 5, length.out = 1000)\n\n# Calculate the density for different degrees of freedom\ndf1_values &lt;- c(1, 2, 5, 10, 100)\ndf2_values &lt;- c(1, 1, 2, 1, 100)\ndistributions &lt;- lapply(1:length(df1_values), function(i) df(x, df1 = df1_values[i], df2 = df2_values[i]))\n\n# Create a data frame to store the data\ndf_data &lt;- data.frame(x = x)\nfor (i in seq_along(df1_values)) {\n  df_data[paste0(\"df1\", df1_values[i], \"_df2\", df2_values[i])] &lt;- distributions[[i]]\n}\n\n# Plot the densities\nggplot(df_data, aes(x = x)) +\n  geom_line(aes(y = df11_df21, color = \"d1=1, d2=1\")) +\n  geom_line(aes(y = df12_df21, color = \"d1=2, d2=1\")) +\n  geom_line(aes(y = df15_df22, color = \"d1=5, d2=2\")) +\n  geom_line(aes(y = df110_df21, color = \"d1=10, d2=1\")) +\n  geom_line(aes(y = df1100_df2100, color = \"d1=100, d2=100\")) +\n  scale_color_manual(name = \"\", \n                     values = c(\"d1=1, d2=1\" = \"red\", \"d1=2, d2=1\" = \"black\", \n                                \"d1=5, d2=2\" = \"blue\", \"d1=10, d2=1\" = \"green\", \n                                \"d1=100, d2=100\" = \"grey\")) +\n  labs(title = \"F-distribution with different degrees of freedom\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nApplication\nAnalysis of variance (ANOVA)"
  },
  {
    "objectID": "mth1113.html#point-estimation",
    "href": "mth1113.html#point-estimation",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Point Estimation",
    "text": "Point Estimation\nA point estimate of a population characteristic is a single number that is based on sample data and represents a plausible value of the characteristic."
  },
  {
    "objectID": "mth1113.html#introduction",
    "href": "mth1113.html#introduction",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Introduction",
    "text": "Introduction\n\nIf we were to construct a 95% confidence interval for some population characteristics (population proportion p or population mean \\(\\mu\\)), we would be using a method that is successful 95% of the time.\nThis is also about the question relevant to “How to choose a sample size”"
  },
  {
    "objectID": "mth1113.html#definition-1",
    "href": "mth1113.html#definition-1",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Definition",
    "text": "Definition\nA \\(100(1-\\alpha) \\%\\) confidence interval is an interval of the form \\(\\hat{\\theta}_L&lt;\\theta&lt;\\hat{\\theta}_U\\), where \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_U\\) are respectively values of \\(\\widehat{\\Theta}_L\\) and \\(\\widehat{\\Theta}_U\\) obtained for a particular sample, based on \\[\nP\\left(\\widehat{\\Theta}_L&lt;\\theta&lt;\\widehat{\\Theta}_U\\right)=1-\\alpha \\quad ; \\quad 0&lt;\\alpha&lt;1\n\\] in the estimation of population parameter \\(\\theta\\)."
  },
  {
    "objectID": "mth1113.html#interpretation",
    "href": "mth1113.html#interpretation",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Interpretation",
    "text": "Interpretation\nFor confidence level of 95% CI for any normal distribution: About 95% of the values are within 1.96 standard deviations of the mean. (Recall the concept of Z-scores)\nThat is, if \\(\\text{Estimate}± (Z \\times \\sigma)\\) was used to generate an interval estimate over and over again with different samples, in the long run 95% of the resulting intervals would include the actual value of the characteristic being estimated.\n\nThe confidence level 95% refers to the method used to construct the interval rather than to any particular interval, such as the one we obtained.\n\n\neg\n\nProportion\n\ncheck to make sure that the three necessary conditions are met:\n\n\\(n\\hat p \\ge 10, n(1-\\hat p) \\ge 10\\)\n\\(\\frac {\\hat p -p }{\\sigma/\\sqrt n} = 1.96\\))\n\nsample size question\n\nUsing sample proportion with 95% confidence interval, and we want ME no more than 5%, we may be interested in solving n from the following equation:\n0.05=1.96 \\(\\sqrt {\\frac {\\hat p(1-\\hat p) }{ n}}\\)\n\n\nCI on Mean\nHere, \\(\\bar x\\) is the sample mean from a simple random sample.\n\\(\\mu\\) is the population mean which we are interested in estimating.\n\nCI on \\(\\mu\\) with \\(\\sigma\\) known n \\(\\ge 30\\) or the population is normal — Use z-statistics\nCI: \\(\\bar x \\pm z_{\\alpha/2} \\frac {\\sigma}{\\sqrt n}\\), for example, 95% CI is \\(\\bar x \\pm 1.96 \\frac {\\sigma}{\\sqrt n}\\)\n\n\nOne-side Confidence Bound on \\(\\mu\\) with \\(\\sigma\\) known n \\(\\ge 30\\) or the population is normal — Use z-statistics\nUpper one-side bound: \\(\\mu &lt;\\bar x + z_{\\alpha} \\frac {\\sigma}{\\sqrt n}\\)\nLower one-side bound: \\(\\mu &gt;\\bar x - z_{\\alpha} \\frac {\\sigma}{\\sqrt n}\\)\nFor example, 95% Confidence bound on \\(\\mu\\) is \\(\\bar x \\pm 1.645 \\frac {\\sigma}{\\sqrt n}\\)\n\n\nCI on \\(\\mu\\) with \\(\\sigma\\) unknown and the population is normal — Use t-statistics (use s as the estimate for σ (t-statistics with df = n-1))\nCI: \\(\\bar x \\pm t_{\\alpha/2} \\frac {s}{\\sqrt n}\\), for example, 95% CI is \\(\\bar x \\pm t_{0.025} \\frac {s}{\\sqrt n}\\) and df = n-1\nRemark: The distribution of t is more spread out than the standard normal distribution but when n \\(\\ge 30\\), t and z are very close to each other.\n\n\nCI for \\(\\mu_1 - \\mu_2\\), both \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are known\nCI of \\(\\mu_1-\\mu_2\\): \\((\\bar x_1 - \\bar x_2) \\pm z_{\\alpha/2} \\sqrt {\\frac {\\sigma_1^2}{n_1} + \\frac {\\sigma_2^2}{n_2}}\\)\n\n\nCI for \\(\\mu_1 - \\mu_2\\), both \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown but assumed equal\nCI of \\(\\mu_1-\\mu_2\\): \\((\\bar x_1 - \\bar x_2) \\pm t_{\\alpha/2} s_p \\sqrt {\\frac {1}{n_1} + \\frac {1}{n_2}}\\) with df = \\(n_1+n_2-2\\) where \\(s_p = \\sqrt {\\frac {(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\n\n\nCI for paired observations\nPrevious, we have two independent samples, now we have two dependent samples. We can use the difference between the two samples to construct a confidence interval.\nCI of \\(\\mu_d\\): \\(\\bar d \\pm t_{\\alpha/2} \\frac {s_d}{\\sqrt n}\\) with df = n-1 where \\(s_d\\) is the sample standard deviation of the differences \\(d_i = x_{1i} - x_{2i}\\) and \\(\\bar d\\) is the sample mean of the differences.\n\n\nEstimating \\(\\sigma\\)\na \\(100(1-\\alpha)\\%\\) CI for \\(\\sigma^2\\) is \\(\\left(\\frac{(n-1) S^2}{\\chi_{\\alpha / 2, n-1}^2}, \\frac{(n-1) S^2}{\\chi_{1-\\alpha / 2, n-1}^2}\\right)\\)\nwhere \\(S^2\\) is the sample variance and \\(\\chi_{\\alpha / 2, n-1}^2\\) and \\(\\chi_{1-\\alpha / 2, n-1}^2\\) are the critical values of the chi-square distribution with \\(n-1\\) degrees of freedom.\n\n\nEstimating \\(\\sigma_1^2/ \\sigma_2^2\\)\nA \\(100(1-\\alpha)\\%\\) CI for \\(\\frac{\\sigma_1^2}{\\sigma_2^2}\\) using F-statistics with \\(f_{1-\\alpha/2}(n_1-1,n_2-1)=1/f_{\\alpha/2}(n_1-1,n_2-1)\\) is \\[\n\\frac{s_1^2}{s_2^2} \\frac{1}{f_{\\alpha / 2}\\left(n_1-1, n_2-1\\right)}&lt;\\frac{\\sigma_1^2}{\\sigma_2^2}&lt;\\frac{s_1^2}{s_2^2} f_{\\alpha / 2}\\left(n_2-1, n_1-1\\right)\n\\] where \\(f_{\\alpha / 2}\\left(v_1, v_2\\right)\\) is an \\(F\\)-value with \\(v_1\\) and \\(v_2\\) degrees of freedom, leaving an area of \\(\\alpha / 2\\) to the right, and \\(f_{\\alpha / 2}\\left(v_2, v_1\\right)\\) is a similar \\(F\\)-value with \\(v_2\\) and \\(v_1\\) degrees of freedom."
  },
  {
    "objectID": "mth1113.html#example-of-one-sample-t-test",
    "href": "mth1113.html#example-of-one-sample-t-test",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Example of one-sample t-test",
    "text": "Example of one-sample t-test\nA marine biologist is studying a species of fish known to have an average length of 20 cm in ocean populations. A new population in a freshwater lake is being analyzed to determine if the environmental differences have altered the fish’s average length. The biologist measures the lengths of 10 randomly selected fish, yielding the following data:\n22, 23, 21, 24, 22, 20, 25, 19, 23, 22\nAssuming the data satisfy the assumption of normality, please address the following using a significance level of 0.1:\n\na\n\nnull hypothesis: The mean length of fish is 20 cm (\\(H_0: \\mu = 20\\)).\nalternative hypothesis: The mean length of fish is not 20 cm (\\(H_1: \\mu \\ne 20\\)).\n\n\n\nb\nSince the data is supposed to be normally distributed, the sampling distribution of the sample mean follows t-distribution. The t-test statistic is calculated as follows:\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nThe t-test statistic is calculated as follows:\n\ndata2_4 &lt;- c(22, 23, 21, 24, 22, 20, 25, 19, 23, 22)\n\nx_bar &lt;- mean(data2_4)\n\ns &lt;- sd(data2_4)\n\nt &lt;- (x_bar - 20) / (s / sqrt(length(data2_4)))\n\nt\n\n[1] 3.705882\n\n\nThe t-test statistic is 3.705882…\n\n\nc\nUsing pt() function, the p-value is calculated as follows:\n\np_value &lt;- 2 * pt(-t, df = 9)\np_value\n\n[1] 0.004875954\n\n\nThe p-value is approximately 0.1. Since the p-value is less than 0.1, we reject the null hypothesis.\nTherefore, there is sufficient evidence to conclude that the population mean is not equal to 20 which means the environmental differences have altered the fish’s average length.\n\n\nd\nUsing qt() function to find the critical value for a two-tailed test with 90% confidence level:\n\nt_critical &lt;- qt(0.95, df = 9)\nt_critical\n\n[1] 1.833113\n\n\nThe critical value for a two-tailed test with 90% confidence level is about 1.833113.\nSince the t-test statistic 3.705882 is greater than the critical value 1.833113, which is in the critical region. Therefore, we reject the null hypothesis.\nAlso, we could use confidence interval to verify the result. The 90% confidence interval for the population mean is calculated as follows:\n\nci_4 &lt;- c(x_bar - t_critical * s / sqrt(10), x_bar + t_critical * s / sqrt(10))\nci_4\n\n[1] 21.06124 23.13876\n\n\nSo the 90% confidence interval for the population mean is about (21.1, 23.2).\nThe confidence interval does not contain the hypothesized value 20. Therefore, we reject the null hypothesis.\nTherefore, there is sufficient evidence to conclude that the population mean is not equal to 20 which means the environmental differences have altered the fish’s average length."
  },
  {
    "objectID": "mth1113.html#example-of-unpaired-t-test",
    "href": "mth1113.html#example-of-unpaired-t-test",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Example of unpaired t-test",
    "text": "Example of unpaired t-test\nSuppose \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown but assumed equal. We want to test the null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) against the alternative hypothesis \\(H_1: \\mu_1 \\ne \\mu_2\\). The test statistic is given by\n\\[\nt = \\frac{\\bar x_1 - \\bar x_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nwhere \\(s_p\\) is the pooled sample standard deviation, given by\n\\[\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\]"
  },
  {
    "objectID": "mth1113.html#example-of-one-sample-variance-test",
    "href": "mth1113.html#example-of-one-sample-variance-test",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Example of one-sample Variance Test",
    "text": "Example of one-sample Variance Test\n\\(\\chi^2\\)-test for variance. Suppose \\(X_1, \\cdots, X_n\\) are i.i.d. normal random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We want to test the null hypothesis \\(H_0: \\sigma^2 = \\sigma_0^2\\) against the alternative hypothesis \\(H_1: \\sigma^2 \\ne \\sigma_0^2\\). The test statistic is given by\n\\[\n\\chi^2 = \\frac{(n - 1)s^2}{\\sigma^2}\n\\]"
  },
  {
    "objectID": "mth1113.html#example-of-two-sample-variance-test",
    "href": "mth1113.html#example-of-two-sample-variance-test",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Example of two-sample Variance Test",
    "text": "Example of two-sample Variance Test\n\n\n\n\n\n\n\n\n\n\\(H_0\\)\nTest Statistic\n\\(H_1\\)\nRejection Region\n\n\n\n\n\\(\\sigma_1^2=\\sigma_2^2\\)\n\\(f=\\frac{s_1^2}{s_2^2}\\)\n\\(\\sigma_1^2&lt;\\sigma_2^2\\)\n\\(f&lt;f_\\alpha\\left(\\nu_1, \\nu_2\\right)\\)\n\n\n\n\n\\(\\sigma_1^2&gt;\\sigma_2^2\\)\n\\(f&gt;f_{1-\\alpha}\\left(\\nu_1, \\nu_2\\right)\\)\n\n\n\n\\(\\sigma_1^2 \\neq \\sigma_2^2\\)\n\\(f&lt;f_{\\alpha / 2}\\left(\\nu_1, \\nu_2\\right)\\) or \\(f&gt;f_{1-\\alpha / 2}\\left(\\nu_1, \\nu_2\\right)\\)\n\n\n\n\\(\\nu_1=n_1-1\\) and \\(\\nu_2=n_2-1\\) are two degree of freedom.\n\n\n\n\n\n\n\\[\nF=\\frac{\\frac{\\left(n_1-1\\right) S_1^2}{\\sigma_1^2} /\\left(n_1-1\\right)}{\\frac{\\left(n_2-1\\right) S_2^2}{\\sigma_2^2} /\\left(n_2-1\\right)}=\\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2}\n\\]\nIf \\(\\sigma_1^2=\\sigma_2^2\\), we have \\[\nF=\\frac{S_1^2}{S_2^2} \\sim F_{n_1-1, n_2-1}\n\\]"
  },
  {
    "objectID": "mth1113.html#anova-analysis-of-variance",
    "href": "mth1113.html#anova-analysis-of-variance",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "ANOVA– Analysis of Variance",
    "text": "ANOVA– Analysis of Variance\n\none-way ANOVA\n\nwe need to test the null hypothesis that the group population means are all the same against the alternative that at least one group population mean differs from the others. That is,\n\\(H_0: \\mu_1=\\mu_2=\\cdots=\\mu_k\\) against \\(H_1: \\text { at least one } \\mu_i \\text { differs from the others.}\\)\nANOVA Table\n\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSum Sq\nMean Sq\nF value\np value\n\n\n\n\nFactor\nm-1\n11.84 (SS between)\n2.9587 (MSB)\n8.074 (MSB/MSW)\n\\(5.38 \\mathrm{e}-05\\) (p-value)\n\n\nError\nn-m\n16.49 (SS Within)\n0.3664 (MSW)\n\n\n\n\nTotal\nn-1\n28.33 (SS Total)\n\n\n\n\n\n\nSource means “the source of the variation in the data.” the possible sources for a one-factor study are Factor, Residuals, and Total.\nFactor means “the variability due to the factor of interest.” In the drug example, the factor was the different drug. In the learning example on the previous page, the factor was the method of learning. Sometimes the row heading is labeled as Between.\nError (or Residuals) means “the variability within the groups” or “unexplained random error.” Sometimes the row heading is labeled as Within.\nTotal means “the total variation in the data from the grand mean”.\nDF means “the degrees of freedom in the source.”\nSum Sq means “the sum of squares due to the source.”\nMean Sq means “the mean sum of squares due to the source.”\nF value means “the F-statistic.”\nP value means “the P-value.”\nSS(Total)=SS(Between)+SS(Within), where\nSS(Between) is the sum of squares between the group means and the grand mean. As the name suggests, it quantifies the variability between the groups of interest.\nSS(Within) is the sum of squares between the data and the group means. It quantifies the variability within the groups of interest.\nSS(Total) is the sum of squares between the n data points and the grand mean. As the name suggests, it quantifies the total variability in the observed data.\n\ntwo-way ANOVA\n\nWe can extend the idea of a one-way ANOVA, which tests the effects of one factor on a response variable, to a two-way ANOVA which tests the effects of two factors and their interaction on a response variable.\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSum Sq\nMSW\nF\n\n\n\n\nCells\n\\(a b-1\\)\n\\(\\sum_{i=1}^a \\sum_{j=1}^b n\\left(\\bar{X}_{i j}-\\bar{X}_{\\ldots . .}\\right)^2\\)\n\n\n\n\nA\na-1\nbn \\(\\sum_{i=1}^a\\left(\\bar{X}_{i . .}-\\bar{X}_{\\ldots .}\\right)^2\\)\nSS(A)\nMS(Error\n\n\nB\nb-1\nan \\(\\sum_{j=1}^b\\left(\\bar{X}_{. j .}-\\bar{X}_{\\ldots .}\\right)^2\\)\nSS(B)\nMS(B)\n\n\n\\(\\mathrm{A} \\times \\mathrm{B}\\)\n\\((a-1)(b-1)\\)\nSS(Cells)-SS(A)-SS(B)\nSS(AB)\nMS(Error\n\n\n\n\n\nDF(A \\(\\times\\) B)\nMS(Error\n\n\nError\n\\(a b(n-1)\\)\n\\(\\sum_{i=1}^a \\sum_{j=1}^b \\sum_{l=1}^n\\left(X_{i j l}-\\bar{X}_{i j} .\\right)^2\\)\nSS(Error)\n\n\n\nTotal\n\\(a b n-1\\)\n\\(\\sum_{i=1}^a \\sum_{j=1}^b n\\left(X_{i j l}-\\bar{X}_{\\ldots .}\\right)^2\\)\n\n\n\n\n\n\n\\(F=\\frac{\\mathrm{MS}(\\mathrm{A})}{\\mathrm{MS}(\\text { Error })}\\), for \\(H_0\\) : no effect of factor A on response variable,\n\\(F=\\frac{\\mathrm{MS}(\\mathrm{B})}{\\mathrm{MS}(\\text { Error) }}\\), for \\(H_0\\) : no effect of factor B on response variable,\n\\(F=\\frac{\\mathrm{MS}(\\mathrm{A} \\times \\mathrm{B})}{\\mathrm{MS}(\\text { Error })}\\), for \\(H_0\\) : no effect of interaction on response variable.\n\nWe reject any \\(H_0\\) if \\(F \\geq F_{\\text {critical }}\\); otherwise, we do not reject \\(H_0\\).\n\nExample of two-way ANOVA\nTwo-way ANOVA. In this question, we will use the built-in R data set ToothGrowth to perform two-way ANOVA test. ToothGrowth includes information from a study on the effects of vitamin C on tooth growth in Guinea pigs. The response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC). Assuming the data satisfy the assumptions of normality and equal variance, please address the following using a significance level of 0.05\n\na\n\nThe effects of vitamin C on tooth growth in guinea pigs:\nnull hypothesis: \\(H_0\\): mean tooth growth for all doses of vitamin C are equal\nalternative hypothesis: \\(H_1\\): at least one of the means of all doses of vitamin C is different from the others\nThe effects of delivery method on tooth growth in guinea pigs:\nnull hypothesis: \\(H_0\\): mean tooth growth for the delivery method of orange juice and ascorbic acid are equal.\nalternative hypothesis \\(H_1\\): mean tooth growth for the delivery method of orange juice and ascorbic acid are different.\nThe interaction effects of the dose of vitamin C and delivery method on tooth growth in guinea pigs:\nnull hypothesis: \\(H_0:\\) there is no interaction between the dose of vitamin C and delivery method on tooth growth in guinea pigs, meaning that the relationship between vitamin C and tooth growth is the same for both delivery methods (similarly, the relationship between delivery method and tooth growth is the same for all doses of vitamin C).\nalternative hypothesis: \\(H_1\\): there is an interaction between the dose vitamin C and delivery method on tooth growth in guinea pigs, meaning that the relationship between vitamin C and tooth growth is different for both delivery methods (similarly, the relationship between delivery method and tooth growth depends on the dose of vitamin C).\n\n\n\nb\nWe can plot the relationship one by one using two plots\n\nlibrary(ggplot2)\ndata(ToothGrowth)\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n# potential effects of vitamin C on tooth growth.\n\nggplot(ToothGrowth, aes(x = factor(dose), y = len)) +\n  geom_boxplot() +\n  labs(x = \"Dose (mg/day)\", y = \"Tooth Growth (len)\", title = \"Tooth Growth by Dose of vitamin C\")\n\n\n\n\n\n\n\n# potential effects of delivery method on tooth growth.\n\nggplot(ToothGrowth, aes(x = supp, y = len)) +\n  geom_boxplot() +\n  labs(x = \"Delivery Method\", y = \"Tooth Growth (len)\", title = \"Tooth Growth by Delivery Method\") \n\n\n\n\n\n\n\n\nor just one:\n\nlibrary(ggplot2)\n\n# potential effects of vitamin C and delivery method. \n\n# OJ represents orange juice and VC represents ascorbic acid.\n\nggplot(ToothGrowth, aes(x = factor(dose), y = len, fill = supp)) +\n  geom_boxplot() +\n  labs(x = \"Dose (mg/day)\", y = \"Tooth Growth (len)\", title = \"Tooth Growth by Dose and Delivery Method\") \n\n\n\n\n\n\n\n\n\n\nc\n\n# Perform two-way ANOVA\n\nanova_result &lt;- aov(len ~ supp * dose, data = ToothGrowth)\n\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsupp         1  205.4   205.4  12.317 0.000894 ***\ndose         1 2224.3  2224.3 133.415  &lt; 2e-16 ***\nsupp:dose    1   88.9    88.9   5.333 0.024631 *  \nResiduals   56  933.6    16.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince all p-values are less than 0.05, we reject all null hypotheses. Therefore, there is sufficient evidence to conclude that the dose of vitamin C, delivery method, and their interaction have significant effects on tooth growth in guinea pigs.\n\n\nd\n\nlibrary(car)\n\nLoading required package: carData\n\nqqPlot(anova_result$residuals, main = \"QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n[1] 22 50"
  },
  {
    "objectID": "mth1113.html#non-parametric-tests",
    "href": "mth1113.html#non-parametric-tests",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\n\nApplication of testing the goodness of fit\nTesting whether there is a “good fit” between the observed data and the assumed probability model amounts to testing:\n\nConstruction of test statistics with an example of 2 categories\nPopulation is 60% female and 40% male. Then, if a sample of 100 students yields 53 females and 47 males, can we conclude that the sample is (random and) representative of the population? That is, how “good” do the data “fit” the assumed probability model of 60% female and 40% male?\nHere, let \\(Y_1\\) denote the number of females selected, \\(Y_1 \\sim B(n,p_1)\\) and let \\(Y_2\\) denote males selected, \\(Y_2 = (n-Y_1)\\sim B(n,p_2)=B(n,1-p_1)\\).\nfor samples satisfying the general rule of thumb (the expected number of successes must be at least 5 and the expected number of failures must be at least 5), we can use the normal approximation to the binomial distribution. The test statistic is given by\n\\[\nZ = \\frac{Y_1 - np_1}{\\sqrt{np_1(1-p_1)}}\\sim N(0,1)\n\\]\nwhich is at least approximately normally distributed.\nand \\[\nZ^2 =Q_1= \\frac{(Y_1 - np_1)^2}{np_1(1-p_1)}\\sim \\chi^2(1)\n\\]\nwhich is an approximate chi-square distribution with one degree of freedom.\nNow we can multiply \\(Q_1\\) by 1 = \\((1-p_1)+p_1\\) to get\n\\[\nQ_1 = \\frac{(Y_1 - np_1)^2(1-p_1)}{np_1(1-p_1)} + \\frac{(Y_1 - np_1)^2p_1}{np_1(1-p_1)}\\sim \\chi^2(1)\n\\]\nSince \\(Y_1=n-Y_2\\) and \\(p_1=1-p_2\\), after simplifying, we have\n\\[\nQ_1 = \\frac{(Y_1-np_1)^2}{np_1} + \\frac{(-(Y_2-np_2))^2}{np_2}\\sim \\chi^2(1)\n\\]\nwhich is \\(Q_1=\\sum_{i=1}^2 \\frac{\\left(Y_i-n p_i\\right)^2}{n p_i}=\\sum_{i=1}^2 \\frac{(\\text { Observed }- \\text { Expected })^2}{\\text { Expected }}\\sim \\chi^2(1)\\)\nHence, it is observed that if the observed counts are very different from the expected counts, then the test statistic will be large. So we reject the null hypothesis if \\(Q_1\\) is large and how large is large is determined by the critical value of the chi-square distribution with one degree of freedom.\nThe statistics \\(Q_1\\) is called the chi-square goodness of fit statistic.\nGoing back to the example,\n\n\\(H_0\\):\\(p_F = 0.6\\)\n\\(H_1\\):\\(p_F \\ne 0.6\\)\n\nwe can calculate the test statistic using a significant level of \\(\\alpha = 0.05\\) (\\(\\chi^2_{0.05,1}=3.84\\))as follows:\n\\[\nQ_1 = \\frac{(53-60)^2}{60} + \\frac{(47-40)^2}{40} = 2.04\n\\]\nSince \\(Q_1=2.04&lt;3.84\\), we do not reject the null hypothesis. Therefore, we conclude that the sample is (random and) representative of the population.\nThis can be extended to k categories\n\n\nConstruction of test statistics with an example of k categories\nFor categories more than 2, i.e.\n\n\n\n\n\n\n\n\n\n\n\nCategories\n1\n2\n\\(\\cdots\\)\n\\(k-1\\)\n\\(k\\)\n\n\n\n\nObserved\n\\(Y_1\\)\n\\(Y_2\\)\n\\(\\cdots\\)\n\\(Y_{k-1}\\)\n\\(n-Y_1-Y_2-\\cdots-Y_{k-1}\\)\n\n\nExpected\n\\(n p_1\\)\n\\(n p_2\\)\n\\(\\cdots\\)\n\\(n p_{k-1}\\)\n\\(n p_k\\)\n\n\n\nKarl Pearson showed that the chi-square statistic \\(Q_{k-1}\\) defined as: \\[\nQ_{k-1}=\\sum_{i=1}^k \\frac{\\left(Y_i-n p_i\\right)^2}{n p_i}\n\\] follows approximately a chi-square random variable with \\(k-1\\) degrees of freedom. Let’s try it out on an example.\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategories\nBrown\nYellow\nOrange\nGreen\nCoffee\nTotal\n\n\n\n\nObserved \\(y_i\\)\n224\n119\n130\n48\n59\n580\n\n\nAssumed \\(H_0\\left(p_i\\right)\\)\n0.4\n0.2\n0.2\n0.1\n0.1\n1.0\n\n\nExpected \\(n p_i\\)\n232\n116\n116\n58\n58\n580\n\n\n\n\\(Q_4=\\frac{(224-232)^2}{232}+\\frac{(119-116)^2}{116}+\\frac{(130-116)^2}{116}+\\frac{(48-58)^2}{58}+\\frac{(59-58)^2}{58}=3.784\\)\nBecause there are \\(k=5\\) categories, we have to compare our chisquare statistic \\(Q_4\\) to a chi-square distribution with \\(k-1=5-1=4\\) degrees of freedom: \\[\nQ_4=3.784&lt;\\chi_{4,0.05}^2=9.488\n\\] we fail to reject the null hypothesis.\n\n\n\nApplication of testing for homogeneity\nThis is to look at a method for testing whether two or more multinomial distributions are equal.\n\nExample:\n\nTest the hypothesis that the acceptances of males and females are ditributed equally among the four schools,\n\n\n\n\n\n\n\n\n\n\n\n(Acceptances)\nBus\nEng\nL Arts\nSci\n(FIXED) Total\n\n\n\n\nMale\n240 (20%)\n480 (40%)\n120 (10%)\n360 (30%)\n1200\n\n\nFemale\n240 (30%)\n80 (10%)\n320 (40%)\n160 (20%)\n800\n\n\nTotal\n480 (24%)\n560 (28%)\n440 (22%)\n520 (26%)\n2000\n\n\n\nHere,\n\\(H_0: p_{M B}=p_{F B}, p_{M E}=p_{F E}, p_{M L}=p_{F L}\\), and \\(p_{M S}=p_{F S}\\)\n\\(H_1: p_{M B} \\neq p_{F B}\\) or \\(p_{M E} \\neq p_{F E}\\) or \\(p_{M L} \\neq p_{F L}\\), or \\(p_{M S} \\neq p_{F S}\\)\nwhere:\n\n\\(p_{M j}\\) is the proportion of males accepted into school \\(j=B, E, L, S\\).\n\\(p_{F j}\\) is the proportion of females accepted into school \\(j=B, E, L, S\\).\n\nIn conducting such a hypothesis test, we’re comparing the proportions of two multinomial distributions.\n\n\n\n\n\n\n\n\n\n\n\n#(Acc)\nBus ( \\(j=1\\) )\nEng ( \\(j=2\\) )\nL Arts ( \\(j=3\\) )\nSci \\((j=4)\\)\n(FIXED) Total\n\n\n\n\n\\(\\mathrm{M}(i=1)\\)\n\\(y_{11}\\left(\\hat{p}_{11}\\right)\\)\n\\(y_{12}\\left(\\hat{p}_{12}\\right)\\)\n\\(y_{13}\\left(\\hat{p}_{13}\\right)\\)\n\\(y_{14}\\left(\\hat{p}_{14}\\right)\\)\n\\(n_1=\\sum_{j=1}^k y_{1 j}\\)\n\n\nF \\((i=2)\\)\n\\(y_{21}\\left(\\hat{p}_{21}\\right)\\)\n\\(y_{22}\\left(\\hat{p}_{22}\\right)\\)\n\\(y_{23}\\left(\\hat{p}_{23}\\right)\\)\n\\(y_{24}\\left(\\hat{p}_{24}\\right)\\)\n\\(n_2=\\sum_{j=1}^k y_{2 j}\\)\n\n\nTotal\n\\(y_{11}+y_{21}\\left(\\hat{p}_1\\right)\\)\n\\(y_{12}+y_{22}\\left(\\hat{p}_2\\right)\\)\n\\(y_{13}+y_{23}\\left(\\hat{p}_3\\right)\\)\n\\(y_{14}+y_{24}\\left(\\hat{p}_4\\right)\\)\n\\(n_1+n_2\\)\n\n\n\nThe chi-square test statistic for testing the equality of two multinomial distributions: \\[\nQ=\\sum_{i=1}^2 \\sum_{j=1}^k \\frac{\\left(y_{i j}-n_i \\hat{p}_j\\right)^2}{n_i \\hat{p}_j}\n\\] follows an approximate chi-square distribution with \\(k-1\\) degrees of freedom. Reject the null hypothesis of equal proportions if \\(Q\\) is large (since if male and female distributed nearly equally, the expected number of each should be \\(n_i\\hat p_j\\)): \\[\nQ \\geq \\chi_{\\alpha, k-1}^2\n\\]\n(omit the derive of the above Q)\nGenerally,\n\\[\nQ=\\sum_{i=1}^h \\sum_{j=1}^k \\frac{\\left(y_{i j}-n_i \\hat{p}_j\\right)^2}{n_i \\hat{p}_j}\\sim \\chi^2_{(h-1)(k-1)}\n\\]\n\nFurther example\nThe head of a surgery department at a university medical center was concerned that surgical residents in training applied unnecessary blood transfusions at a different rate than the more experienced attending physicians. Therefore, he ordered a study of the 49 Attending Physicians and 71 Residents in Training with privileges at the hospital. For each of the 120 surgeons, the number of blood transfusions prescribed unnecessarily in a one-year period was recorded. Based on the number recorded, a surgeon was identified as either prescribing unnecessary blood transfusions Frequently, Occasionally, Rarely, or Never. Here’s a summary table (or “contingency table”) of the resulting data:\n\n\n\nPhysician\nFrequent\nOccasionally\nRarely\nNever\nTotal\n\n\n\n\nAttending\n6.942\n12.658\n22.05\n7.35\n49\n\n\nResident\n10.058\n18.342\n31.95\n10.65\n71\n\n\nTotal\n17\n31\n54\n18\n120\n\n\n\nHere,\n\\(H_0: p_{R F}=p_{A F}, p_{R O}=p_{A O}, p_{R R}=p_{A R}\\), and \\(p_{R N}=p_{A N}\\)\n\\(H_1: p_{R F} \\neq p_{A F}\\) or \\(p_{R O} \\neq p_{A O}\\) or \\(p_{R R} \\neq p_{A R}\\), or \\(p_{R N} \\neq p_{A N}\\)\nWe should also calculate the expected counts under the null hypothesis. The expected counts are calculated as follows:\n\n\n\nPhysician\nFrequent\nOccasionally\nRarely\nNever\nTotal\n\n\n\n\nAttending\n6.942\n12.658\n22.05\n7.35\n49\n\n\nResident\n10.058\n18.342\n31.95\n10.65\n71\n\n\nTotal\n17\n31\n54\n18\n120\n\n\n\nwhere, for example, \\(6.942=\\frac{17}{120} \\times 49\\) and \\(10.058=\\frac{17}{120} \\times 71\\). Now that we have the observed and expected counts, calculating the chisquare statistic is a straightforward exercise: \\[\nQ=\\frac{(2-6.942)^2}{6.942}+\\cdots+\\frac{(5-10.65)^2}{10.65}=31.88\n\\]\nThe chi-square test tells us to reject the null hypothesis, at the 0.05 level, if \\(Q\\) is greater than a chi-square random variable with 3 degrees of freedom, that is, if \\(Q=31.88&gt;7.815\\), we reject the null hypothesis.\n\n\n\nApplication of testing for independence\nThis is to look at whether two or more categorical variables are independent.\n(previously,the the sampling scheme involves: Taking two random (and therefore independent) samples with n1 and n2 fixed in advance and observing into which of the k categories the first random samples fall, and observing into which of the k categories the second random samples fall. )\nlets consider a different example to illustrate an alternative sampling scheme. Suppose 395 people are randomly selected, and are “cross-classified” into one of eight cells, depending into which age category they fall and whether or not they support legalizing marijuana:\n(the sampling scheme involves: Taking one random sample of size n, with n fixed in advance, and then “cross-classifying” each subject into one and only one of the mutually exclusive and exhaustive \\(A_i\\cap B_j\\) cells.)\n\n\n\n\n\n\n\n\n\n\n\n\nMarijuana Support\n\nVariable B (Age)\n\n\n\n\n\n\n\n\nVariable A\nOBSERVED\n\\((18-24) B_1\\)\n(25-34) \\(B_1 2\\)\n(35-49) \\(B_3\\)\n\\((50-64) B_4\\)\nTotal\n\n\n\n(YES) \\(A_1\\)\n60\n54\n46\n41\n201\n\n\n\n(NO) \\(A_2\\)\n40\n44\n53\n57\n194\n\n\n\nTotal\n100\n98\n99\n98\n\\(n=395\\)\n\n\n\nHere,\nH0 : Variable A is independent of variable B, that is \\(P(A_i\\cap B_j)=PA_i \\times B_j\\) for all i and j\nH1: Variable A is not independent of variable B.\nGenerally,\nSuppose we have \\(k\\) (column) levels of Variable B indexed by the letter \\(j\\), and \\(h\\) (row) levels of Variable \\(A\\) indexed by the letter \\(i\\). Then, we can summarize the data and probability model in tabular format, as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable B\n\n\n\n\n\n\n\n\n\n\nVariable A\n\\(B_1(j=1)\\)\n\\(B_2(j=2)\\)\n\\(B_3(j=3)\\)\n\\(B_4(j=4)\\)\nTotal\n\n\n\n\\(A_1(i=1)\\)\n\\(Y_{11}\\left(p_{11}\\right)\\)\n\\(Y_{12}\\left(p_{12}\\right)\\)\n\\(Y_{13}\\left(p_{13}\\right)\\)\n\\(Y_{14}\\left(p_{14}\\right)\\)\n\\(\\left(p_{1 .}\\right)\\)\n\n\n\n\\(A_2(i=2)\\)\n\\(Y_{21}\\left(p_{21}\\right)\\)\n\\(Y_{22}\\left(p_{22}\\right)\\)\n\\(Y_{23}\\left(p_{23}\\right)\\)\n\\(Y_{24}\\left(p_{24}\\right)\\)\n\\(\\left(p_{2 .}\\right)\\)\n\n\n\nTotal\n\\(\\left(p_{.1}\\right)\\)\n\\(\\left(p_{.2}\\right)\\)\n\\(\\left(p_{.3}\\right)\\)\n\\(\\left(p_{.4}\\right)\\)\n\\(n\\)\n\n\n\n\nwhere \\(p_{i j}=Y_{i j} / n, p_i .=\\sum_{j=1}^k p_{i j}\\), and \\(p_{. j}=\\sum_{i=1}^h p_{i j}\\)\n\\(Q=\\sum_{j=1}^k \\sum_{i=1}^h \\frac{\\left(y_{i j}-\\frac{y_i \\cdot y_{\\cdot j}}{n}\\right)^2}{\\frac{y_i \\cdot y_{\\cdot j}}{n}}\\sim \\chi^2_{(h-1)(k-1)}\\)\n\nAre chi-square statistic for homogeneity and the chi-square statistic for independence equivalent?\nAlthough their chi-square statistics are equivalent, the two tests are not equivalent since their sampling experiment designs are different.\nHere’s the table of expected counts:\n\n\n\n\n\n\n\n\n\n\n\n\nBicycle Riding Interest\n\nVariable B (Age)\n\n\n\n\n\n\n\n\nVariable A\nEXPECTED\n18-24\n25-34\n35-49\n50-64\nTotal\n\n\n\nYES\n50.886\n49.868\n50.377\n49.868\n201\n\n\n\nNO\n49.114\n48.132\n48.623\n48.132\n194\n\n\n\nTotal\n100\n98\n99\n98\n395\n\n\n\n\\[\nQ=\\frac{(60-50.886)^2}{50.886}+\\cdots+\\frac{(57-48.132)^2}{48.132}=8.006\n\\]\nThe chi-square test tells us to reject the null hypothesis, at the 0.05 level, since \\(Q\\) is greater than a chi-square random variable with 3 degrees of freedom, that is, \\(Q=8.006&gt;7.815\\).\n\n\nSummary\nParametric tests make assumptions that aspects of the data follow some sort of theoretical probability distribution. Non-parametric tests or distribution free methods do not, and are used when the distributional assumptions for a parametric test are not met. While this is an advantage, it often comes at a cost of power (in the sense they are less likely to be able to detect a difference when a true difference exists).\nMost non-parametric tests are just hypothesis tests; there is no estimation of a confidence interval.\nMost non-parametric methods are based on ranking the values of a variable in ascending order and then calculating a test statistic based on the sums of these ranks.\nNon-parametric tests include:\n\nTwo-sample independent t-test - Wilcoxon rank-sum test or Mann-Whitney U test\nPaired t-test - Wilcoxon signed-rank test\nOne-way ANOVA - Kruskal-Wallace Test\nNormality tests - Shapiro-Wilk test and Kolmogorov-Smirnov test"
  },
  {
    "objectID": "mth1113.html#simple-linear-regression",
    "href": "mth1113.html#simple-linear-regression",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nBrief introduction\nSome people think simple methods is bad and like complicated methods , but actually simple is very good–SLR works very well in lots of situations.\nSLR is used to answer:\nis there a relationship between..\nHow strong the relationship is\nwhich variable contribute to this relationship\nHow accurate could we predict the response variable\nIs the relationship linear?\nIs there a synergy among independent variables?\nThis is a model with two random variables, X and Y, where we are trying to predict Y from X. Here are the model’s assumptions:\n\nThe distribution of X is arbitrary, possibly is even non-random;\nIf X=x, then \\(Y = \\beta_0+\\beta_1x+\\epsilon\\) for some constants \\(\\beta_0, \\beta_1\\) and some random noise variable \\(\\epsilon\\)\n\\(\\epsilon\\) has mean 0, a constant variance \\(\\sigma^2\\), and is uncorrelated with X and uncorrelated across observations Cov\\((\\epsilon_i, \\epsilon_j)=0\\) for \\(i \\ne j\\)\n\nUsing Least Squares, we can estimate \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\), which are unbiased estimates of \\(\\beta_0\\) and \\(\\beta_1\\).\n\nGaussian-Noise Simple Linear Regression Model\n\nNow we further assume that the distribution of \\(\\epsilon\\) is normal, i.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\), independent of X.\nThey tell us, exactly, the probability distribution for Y given X, and so will let us get exact distributions for predictions and for other inferential statistics.\n\n\nMaximum Likelihood Estimation (MLE)\n\nIntroduction to MLE\nLikelihood is a fundamental concept in statistics that measures how well a particular set of parameters (e.g., the mean of a distribution) explains observed data. Think of it as a “score” that tells you which parameter values make your data most plausible.\nCompared to probability, which answers: “What’s the chance of seeing this data if we assume specific parameters?” , likelihood answers: “Given this data, how plausible are these parameters?”\nIf the parameters are \\(b_0, b_1, s^2\\) (reserving the Greek letters for their true values), then \\(Y \\mid X=x \\sim N\\left(b_0+b_1 x, s^2\\right)\\), and \\(Y_i\\) and \\(Y_j\\) are independent given \\(X_i\\) and \\(X_j\\), so the overall likelihood is \\[\n\\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi s^2}} e^{-\\frac{\\left(y_i-\\left(b_0+b_i x_i\\right)\\right)^2}{2 s^2}}\n\\]\nAs usual, we work with the log-likelihood, which gives us the same information but replaces products with sums: \\[\nL\\left(b_0, b_1, s^2\\right)=-\\frac{n}{2} \\ln \\left(2 \\pi s^2\\right)-\\frac{1}{2 s^2} \\sum_{i=1}^n\\left(y_i-\\left(b_0+b_1 x_i\\right)\\right)^2\n\\]\nmaximize it:\n\\(\\begin{aligned} \\frac{\\partial L}{\\partial b_0} & =-\\frac{1}{2 s^2} \\sum_{i=1}^n 2\\left(y_i-\\left(b_0+b_1 x_i\\right)\\right)(-1) \\\\ \\frac{\\partial L}{\\partial b_1} & =-\\frac{1}{2 s^2} \\sum_{i=1}^n 2\\left(y_i-\\left(b_0+b_1 x_i\\right)\\right)\\left(-x_i\\right)\\end{aligned}\\)\n\n\nSame result of MLE as least squares in linear regression\nNotice that when we set these derivatives to zero, all the multiplicative constants - in particular, the prefactor of \\(1 / 2 s^2\\) - go away. We are left with \\[\n\\begin{aligned}\n\\sum_{i=1}^n\\left(y_i-\\left(\\hat{\\beta_0}+\\hat{\\beta_1} x_i\\right)\\right) & =0 \\\\\n\\sum_{i=1}^n\\left(y_i-\\left(\\hat{\\beta_0}+\\hat{\\beta_0} x_i\\right)\\right) x_i & =0\n\\end{aligned}\n\\]\nThese are, up to a factor of \\(1 / n\\), exactly the equations we got from the method of least squares. That means that the least squares solution is the maximum likelihood estimate under the Gaussian noise model.\nMaximum likelihood estimates of the regression curve coincide with least-squares estimates when the noise around the curve is additive, Gaussian, of constant variance, and both independent of \\(X\\) and of other noise terms. If any of those assumptions fail, maximum likelihood and least squares estimates can diverge.\n\n\n\nHypothesis testing for estimates with unknown \\(\\sigma^2\\)\n\nResidual sum of squares (RSS) and t-statistics construction in linear regression\nIt can be shown that (the proof is beyond the scope of this course) \\[\n\\frac{R S S}{\\sigma^2}=\\frac{(n-2) \\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi_{n-2}^2\n\\]\nThis allows us to construct a \\(t\\)-value \\[\nt=\\frac{\\hat{\\beta}-\\beta}{s_{\\hat{\\beta}}} \\sim t_{n-2}\n\\]\nUnder the normality assumption of the error terms, the estimator of the slope coefficient will itself be normally distributed with mean \\(\\beta_i\\) and variance \\(\\operatorname{Var}\\left[\\beta_i\\right]\\) For \\(\\hat{\\beta_1}\\), its mean is \\(\\beta_1\\) and its variance is \\(\\sigma^2 / \\sum\\left(x_i-\\bar{x}\\right)^2\\). When \\(\\sigma^2\\) is known, we know \\[\n\\frac{\\hat{\\beta}_1-\\beta_1}{\\frac{\\sigma}{\\sqrt{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}}}\n\\] follows standard normal distribution. However, in practice, \\(\\sigma^2\\) is often unknown. We then divide this standard normal distributed term by \\[\n\\sqrt{\\frac{(n-2) \\hat{\\sigma}^2}{(n-2) \\sigma^2}}=\\frac{\\hat{\\sigma}}{\\sigma}\n\\]\nTherefore, when we write \\[\ns_{\\hat{\\beta_1}}=\\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}}\n\\] we construct a \\(t\\)-statistic for \\(\\hat{\\beta_1}\\) with degrees of freedom \\(n-2\\). This then allows us to construct a \\(100(1-\\alpha) \\%\\) confidence interval for \\(\\beta_1\\) : \\[\n\\hat{\\beta_1} \\pm t_{n-2, \\alpha / 2} \\times \\boldsymbol{s}_{\\hat{\\beta_1}}\n\\]\nWe can also do similar calculation to get the \\(t\\)-statistic and confidence interval for \\(\\beta_0\\).\n\n\nHyphothesis Testing\n\\[\nt =\\hat {\\beta}-\\beta / s_{\\hat{\\beta_1}} \\sim t_{n-2}\n\\] \\(H_0: \\beta_i =0\\)\n\\(H_1: \\beta_i \\ne 0\\)\n\n\nCodes of linear regression with CI\n\nlmodel &lt;- lm(Petal.Length ~ Petal.Width, data=iris)\nsummary(lmodel)\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33542 -0.30347 -0.02955  0.25776  1.39453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08356    0.07297   14.85   &lt;2e-16 ***\nPetal.Width  2.22994    0.05140   43.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4782 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\nconfint(lmodel)\n\n                2.5 %   97.5 %\n(Intercept) 0.9393664 1.227750\nPetal.Width 2.1283752 2.331506\n\nconf_interval &lt;- predict(lmodel, data=iris, interval='confidence', level=0.95)\nplot(iris$Petal.Width, iris$Petal.Length,\n     xlab='Petal.Width', ylab='Petal.Length',\n     main='Simple Linear Regression')\nabline(lmodel, col='lightblue')\nmatlines(iris$Petal.Width, conf_interval[,2:3], col='blue', lty=2)\n\n\n\n\n\n\n\n# Using ggplot2\nlibrary(ggplot2)\nggplot(iris, aes(x=Petal.Width, y=Petal.Length)) +\n  geom_point() +\n  geom_smooth(method=stats::lm, se=T, level=0.95)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression and ANOVA\nfev_dat &lt;- read.table('fev_dat.txt', header=T)\nfev_dat_subset &lt;- fev_dat[fev_dat$age &gt;= 6 & fev_dat$age &lt;= 10,]\nggplot(fev_dat_subset, aes(x=age, y=FEV)) +\n  geom_point() +\n  geom_smooth(method=stats::lm, se=T, level=0.95)\nsummary(aov(FEV ~ age, data=fev_dat_subset))\nsummary(lm(FEV ~ age, data=fev_dat_subset))\nanova(lm(FEV ~ age, data=fev_dat_subset))\n\n\n\n\\(R^2\\)–the fraction of variability explained by the regression\n\\[\nR^2 =1-\\frac{SSR}{SSTO}\n\\]"
  },
  {
    "objectID": "mth1113.html#multiple-linear-regression-mlr",
    "href": "mth1113.html#multiple-linear-regression-mlr",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\n\\(y=X\\beta+\\epsilon\\), where y is a n × 1 row vector, X is a n × (k + 1) matrix, and β is a (k + 1) × 1 column vector for all n observations.\n\nA potential problem in practice –multicollinearity\nWhen multicollinearity exists, any of the following pitfalls can be exacerbated:\n\nThe estimated regression coefficient of any one variable depends on which other predictors are included in the model\nThe precision of the estimated regression coefficients decreases as more predictors are added to the model\nThe marginal contribution of any one predictor variable in reducing the error sum of squares depends on which other predictors are already in the model\nTypothesis tests for βj = 0 may yield different conclusions depending on which predictors are in the model\n\n\nPerfect multicollinearity\nPerfect multicollinearity refers to a situation where the predictive variables have an exact linear relationship. When there is perfect collinearity, the design matrix X has less than full rank, and therefore the moment matrix X′X cannot be inverted. In this situation, the parameter estimates of the regression are not well-defined, as the system of equations has infinitely many solutions.\n\n\nImperfect multicollinearity\nImperfect multicollinearity refers to a situation where the predictive variables have a nearly exact linear relationship.\n\\(R^2=r^2\\) where r is the Pearson correlation coefficient.\n\n\n\nAdjusted R-squared\nAdjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a model. It provides a more accurate measure of the model’s explanatory power, penalizing for the addition of irrelevant predictors. This helps in comparing models with different numbers of predictors."
  },
  {
    "objectID": "mth1113.html#odds",
    "href": "mth1113.html#odds",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Odds",
    "text": "Odds\nOdds are another way of quantifying the probability of an event, commonly used in gambling (and logistic regression).\nFor some event \\(E\\), \\[\n\\operatorname{odds}(E)=\\frac{P(E)}{P\\left(E^c\\right)}=\\frac{P(E)}{1-P(E)}\n\\]\nThe odds ratio (OR) is the ratio of the odds of an event occurring in one group to the odds of it occuring in another group. If the event in each of the groups are \\(p_1\\) (first group) and \\(p_2\\) (second group), then the odds ratio is: \\[\n\\mathrm{OR}=\\frac{p_1 /\\left(1-p_1\\right)}{p_2 /\\left(1-p_2\\right)}\n\\]"
  },
  {
    "objectID": "mth1113.html#random-component",
    "href": "mth1113.html#random-component",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Random Component",
    "text": "Random Component\nThe random component specifies a distribution for the outcome variable (conditional on \\(X\\) ). In the case of linear regression, we assume that \\(Y \\mid X \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\), for some mean \\(\\mu\\) and variance \\(\\sigma^2\\). In the case of logistic regression, we assume that \\(Y \\mid X \\sim \\operatorname{Bern}(p)\\) for some probability \\(p\\).\nIn a generalized model, we are allowed to assume that \\(Y \\mid X\\) has a probability density function or probability mass function of the form \\[\nf(y ; \\theta, \\phi)=\\exp \\left(\\frac{y \\theta-b(\\theta)}{a(\\phi)}+c(y, \\phi)\\right)\n\\]\nHere \\(\\theta, \\phi\\) are parameters, and \\(a, b, c\\) are functions. Any density of the above form is called an exponential family density. The parameter \\(\\theta\\) is called the natural parameter, and the parameter \\(\\phi\\) the dispersion parameter."
  },
  {
    "objectID": "mth1113.html#exponential-family",
    "href": "mth1113.html#exponential-family",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Exponential Family",
    "text": "Exponential Family\nExponential families include many of the most common distributions. For example: - Exponential \\[\nf(y ; \\lambda)=\\lambda e^{-\\lambda y}=\\exp (-y \\lambda+\\ln \\lambda)\n\\] where \\(\\theta=-\\lambda, \\phi=1, b(\\theta)=\\ln \\lambda, a(\\phi)=1\\), and \\(c(y, \\phi)=0\\) - Poisson \\[\nf(y ; \\lambda)=\\frac{e^{-\\lambda} \\lambda^y}{y!}=\\exp (y \\ln \\lambda-\\lambda-\\ln (y!))\n\\] where \\(\\theta=\\ln \\lambda, \\phi=1, b(\\theta)=e^\\theta=\\lambda, a(\\phi)=1\\), and \\(c(y, \\phi)=-\\lambda-\\ln (y!)\\)"
  },
  {
    "objectID": "mth1113.html#systematic-component-and-link-component",
    "href": "mth1113.html#systematic-component-and-link-component",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Systematic Component and Link Component",
    "text": "Systematic Component and Link Component\nThe systematic component relates a parameter \\(\\eta\\) to the predictors \\(X\\). In a GLM, this is always done via \\[\n\\eta=X \\beta=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_k X_k\n\\]\nWe will denote the expectation of the distribution in random component as \\(\\mu\\), i.e., \\(\\mathbb{E}[Y \\mid X]=\\mu\\). It will be our goal to estimate \\(\\mu\\). Finally, the link component connects the random and systematic components, via a link function \\(g\\). In particular, this link function provides a connection between \\(\\mu\\) and \\(\\eta\\), as in \\[\ng(\\mu)=\\eta \\quad \\text { or } \\quad \\mu=g^{-1}(\\eta)\n\\]"
  },
  {
    "objectID": "mth1113.html#example-1",
    "href": "mth1113.html#example-1",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Example",
    "text": "Example\n\nGaussian-noise Linear Regression\n\nRandom Component: \\(Y \\mid X \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\) and \\(\\mathbb{E}[Y \\mid X]=\\mu\\)\nSystematic Component: \\(\\eta=X \\beta\\)\nLink Component: \\(g(\\mu)=\\mu\\), so that \\(\\mu=\\eta=X \\beta\\)\n\n\n\nBernoulli\nSuppose that \\(Y \\in\\{0,1\\}\\), and we model the distribution of \\(Y \\mid X\\) as Bernoulli with success probability \\(p\\). Then the probability mass function (not a density, since \\(Y\\) is discrete) is \\[\nf(y)=p^y(1-p)^{1-y}\n\\]\nWe can rewrite to fit the exponential family form as \\[\n\\begin{aligned}\nf(y) & =\\exp (y \\log p+(1-y) \\log (1-p)) \\\\\n& =\\exp (y \\log (p /(1-p))+\\log (1-p))\n\\end{aligned}\n\\]\n\\[\nf(y ; \\theta, \\phi)=\\exp \\left(\\frac{y \\theta-b(\\theta)}{a(\\phi)}+c(y, \\phi)\\right)\n\\]\nHere we would identify \\(\\theta=\\log (p /(1-p))\\) as the natural parameter. Note that the mean here is \\(\\mu=p\\), and using the inverse of the above relationship, we can directly write the mean \\(p\\) as a function of \\(\\theta\\), as in \\(p=e^\\theta /\\left(1+e^\\theta\\right)\\). Hence \\(b(\\theta)=\\log (1-p)=-\\log \\left(1+e^\\theta\\right)\\). There is no dispersion parameter, so we can set \\(a(\\phi)=1\\). Also, \\(c(y, \\phi)=0\\)."
  },
  {
    "objectID": "mth1113.html#some-definations",
    "href": "mth1113.html#some-definations",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Some definations",
    "text": "Some definations\nHazard ratios; ratios of hazard functions between different groups (e.g., exposed vs. unexposed) while adjusting for confounders.\ncensoring - which occurs when the survival time is only partially known\n\nFixed type I censoring occurs when a study is designed to end after C years of follow-up. In this case, everyone who does not have an event observed during the course of the study is censored at C years.\nIn random type I censoring, the study is designed to end after C years, but censored subjects do not all have the same censoring time. This is the main type of right-censoring we will be concerned with.\nIn type II censoring, a study ends when there is a pre-specified number of events."
  },
  {
    "objectID": "mth1113.html#kaplan-meier-estimate",
    "href": "mth1113.html#kaplan-meier-estimate",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Kaplan-Meier estimate",
    "text": "Kaplan-Meier estimate\n\nlibrary(ISLR2)\nnames(BrainCancer)\n\n[1] \"sex\"       \"diagnosis\" \"loc\"       \"ki\"        \"gtv\"       \"stereo\"   \n[7] \"status\"    \"time\"     \n\nattach(BrainCancer)\ntable(status)\n\nstatus\n 0  1 \n53 35 \n\nlibrary(survival)\nfit.surv &lt;- survfit(Surv(time, status) ~ 1)\nplot(fit.surv, xlab = \"Months\",\n    ylab = \"Estimated Probability of Survival\")"
  },
  {
    "objectID": "mth1113.html#cox-proportional-harzards-model",
    "href": "mth1113.html#cox-proportional-harzards-model",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Cox-proportional harzards model",
    "text": "Cox-proportional harzards model\nS(t) = P(T&gt;t)=1-F(t)\n\\(h(t)=\\lim _{\\Delta t \\rightarrow 0} \\frac{P(t&lt;T \\leq t+\\Delta t \\mid T&gt;t)}{\\Delta t}\\)\n\nfit.all &lt;- coxph(\nSurv(time, status) ~ sex + diagnosis + loc + ki + gtv +\n   stereo)\nfit.all\n\nCall:\ncoxph(formula = Surv(time, status) ~ sex + diagnosis + loc + \n    ki + gtv + stereo)\n\n                       coef exp(coef) se(coef)      z        p\nsexMale             0.18375   1.20171  0.36036  0.510  0.61012\ndiagnosisLG glioma  0.91502   2.49683  0.63816  1.434  0.15161\ndiagnosisHG glioma  2.15457   8.62414  0.45052  4.782 1.73e-06\ndiagnosisOther      0.88570   2.42467  0.65787  1.346  0.17821\nlocSupratentorial   0.44119   1.55456  0.70367  0.627  0.53066\nki                 -0.05496   0.94653  0.01831 -3.001  0.00269\ngtv                 0.03429   1.03489  0.02233  1.536  0.12466\nstereoSRT           0.17778   1.19456  0.60158  0.296  0.76760\n\nLikelihood ratio test=41.37  on 8 df, p=1.776e-06\nn= 87, number of events= 35 \n   (1 observation deleted due to missingness)\n\nmodaldata &lt;- data.frame(\n     diagnosis = levels(diagnosis),\n     sex = rep(\"Female\", 4),\n     loc = rep(\"Supratentorial\", 4),\n     ki = rep(mean(ki), 4),\n     gtv = rep(mean(gtv), 4),\n     stereo = rep(\"SRT\", 4)\n     )\nsurvplots &lt;- survfit(fit.all, newdata = modaldata)\nplot(survplots, xlab = \"Months\",\n    ylab = \"Survival Probability\", col = 2:5)\nlegend(\"bottomleft\", levels(diagnosis), col = 2:5, lty = 1)"
  },
  {
    "objectID": "mth1113.html#calculation",
    "href": "mth1113.html#calculation",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Calculation",
    "text": "Calculation\n\nlog(exp(1)) #  base `e` is the default (log(e) is not defined)\n\n[1] 1"
  },
  {
    "objectID": "mth1113.html#vectors",
    "href": "mth1113.html#vectors",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Vectors",
    "text": "Vectors\n\nx &lt;- c(1,2,3,4)\nclass(x)\n\n[1] \"numeric\"\n\nx %*%x # scalar (\"inner\") product (but default in R as an 1*1 matrix)\n\n     [,1]\n[1,]   30\n\nrep(c('a','b'),3)\n\n[1] \"a\" \"b\" \"a\" \"b\" \"a\" \"b\"\n\nrep(c(2, 4, 8), each = 3)\n\n[1] 2 2 2 4 4 4 8 8 8\n\ny &lt;- seq(from = 1, to = 4, by =1)\nclass(x)\n\n[1] \"numeric\"\n\nstr(x)\n\n num [1:4] 1 2 3 4\n\nx &lt;- c(-5:5)\nstr(x)\n\n int [1:11] -5 -4 -3 -2 -1 0 1 2 3 4 ...\n\n1:4 # c(1,2,3,4) and 1:4 are the same in.R\n\n[1] 1 2 3 4\n\nseq(1,5, length.out=11)\n\n [1] 1.0 1.4 1.8 2.2 2.6 3.0 3.4 3.8 4.2 4.6 5.0\n\n# vac&lt;-c((1,2,3),(3,4,5)) is wrong, but the below is true\nvec1 &lt;- c(1,2,3)\nvec2 &lt;- c(4,5,6)\nvec3 &lt;- c(vec1, vec2)\nvec3[1] == vec1[1]\n\n[1] TRUE\n\nvec3[3:5];vec3[c(2,3)]\n\n[1] 3 4 5\n\n\n[1] 2 3\n\nvec3[-1] # everything but the first element\n\n[1] 2 3 4 5 6\n\nvec3[-2*c(1,2)]\n\n[1] 1 3 5 6\n\nx &lt;- -5:5\n\nabs(-5:5)\n\n [1] 5 4 3 2 1 0 1 2 3 4 5\n\nx &lt;- c(1, 2, 3, 4, 5,6)\ny &lt;- c(10,11)\nresult &lt;- x + y \nprint(result)\n\n[1] 11 13 13 15 15 17\n\n# x * y \nc(1,2)*c(2,3)\n\n[1] 2 6\n\nc(1,2)%*%c(2,3)\n\n     [,1]\n[1,]    8\n\n#####Application\n# Calculate the sample(var(x)) and population variance\nx &lt;- c(1, 2, 3, 4, 5)\nn &lt;- length(x)\n# Calculate the sample variance using R's var() function\nsample_variance &lt;-var(x)\nsample_variance\n\n[1] 2.5\n\npopulation_variance &lt;- sample_variance * (n - 1) / n\npopulation_variance\n\n[1] 2"
  },
  {
    "objectID": "mth1113.html#rmd-knowledge",
    "href": "mth1113.html#rmd-knowledge",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Rmd knowledge",
    "text": "Rmd knowledge\n{r, echo = FALSE} –Hidden Code\n{r, eval = FALSE} –Do not run this code\n{r, message = FALSE} –Do not show the message\n{r, warning = FALSE} –Do not show the warning\n{r,results=‘hide’} –Do not show the results"
  },
  {
    "objectID": "mth1113.html#numeric",
    "href": "mth1113.html#numeric",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Numeric",
    "text": "Numeric"
  },
  {
    "objectID": "mth1113.html#integer",
    "href": "mth1113.html#integer",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Integer",
    "text": "Integer\n\ny &lt;- 42L\nclass(y)\n\n[1] \"integer\""
  },
  {
    "objectID": "mth1113.html#character",
    "href": "mth1113.html#character",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Character",
    "text": "Character\n\nx &lt;- \"123\"\nclass(x)\n\n[1] \"character\"\n\n# [1] \"character\"\n\nx &lt;- as.numeric(x)\nclass(x)\n\n[1] \"numeric\"\n\n# [1] \"numeric\""
  },
  {
    "objectID": "mth1113.html#logical",
    "href": "mth1113.html#logical",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Logical",
    "text": "Logical\n\n# Note that logical elements are NOT in quotes.\nz = c(\"TRUE\", \"FALSE\", \"TRUE\", \"FALSE\")\nclass(z)\n\n[1] \"character\"\n\nas.logical(z)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n# TRUE = 1 and FALSE = 0. sum() and mean() work on logical vectors\n\n# remember:\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nTRUE | FALSE\n\n[1] TRUE"
  },
  {
    "objectID": "mth1113.html#factor",
    "href": "mth1113.html#factor",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Factor",
    "text": "Factor\n\ny &lt;- c('B','B','A','A','C')\nz &lt;- factor(y)\nstr(z)\n\n Factor w/ 3 levels \"A\",\"B\",\"C\": 2 2 1 1 3\n\nas.numeric(z)\n\n[1] 2 2 1 1 3\n\nlevels(z)\n\n[1] \"A\" \"B\" \"C\"\n\nz &lt;- factor(z,                       # vector of data levels to convert \n            levels=c('B','A','C'),   # Order of the levels\n            labels=c(\"B Group\", \"A Group\", \"C Group\")) # Pretty labels to use\nz\n\n[1] B Group B Group A Group A Group C Group\nLevels: B Group A Group C Group\n\n#####Application\n### eg of use in the plot's x-axis name lable\n\niris$Species &lt;- factor(iris$Species,\n                       levels = c('versicolor','setosa','virginica'),\n                       labels = c('Versicolor','Setosa','Virginica'))\n#boxplot(Sepal.Length ~ Species, data=iris)\n\n### another eg\n#age_category &lt;- ifelse(ages &gt;= 18, \"Adult\", \"Minor\")\n#age_factor &lt;- factor(age_category, levels = c(\"Minor\", \"Adult\"))\n#age_factor\n\n### transform a continuous numerical vector into a factor\n\nx &lt;- 1:10\ncut(x, breaks = c(0, 2.5, 5.0, 7.5, 10))\n\n [1] (0,2.5]  (0,2.5]  (2.5,5]  (2.5,5]  (2.5,5]  (5,7.5]  (5,7.5]  (7.5,10]\n [9] (7.5,10] (7.5,10]\nLevels: (0,2.5] (2.5,5] (5,7.5] (7.5,10]\n\nx&lt;-cut(x, breaks=3, labels=c('Low','Medium','High'))\nstr(x)\n\n Factor w/ 3 levels \"Low\",\"Medium\",..: 1 1 1 1 2 2 2 3 3 3"
  },
  {
    "objectID": "mth1113.html#date-and-time",
    "href": "mth1113.html#date-and-time",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Date and time",
    "text": "Date and time\nThe following symbols can be used with the format() function to print dates.\n\n%d day as a number (0-31) 01-31\n%a abbreviated weekday Mon\n%A unabbreviated weekday Monday\n%m month (00-12) 00-12\n%b abbreviated month Jan\n%B unabbreviated month January\n%y 2-digit year 07\n%Y 4-digit year 2007\n\n\nmydates &lt;- as.Date(c(\"2023-04-07\", \"2023-01-01\"))\nmydates\n\n[1] \"2023-04-07\" \"2023-01-01\"\n\ndays &lt;- mydates[1] - mydates[2]; days\n\nTime difference of 96 days\n\ntoday &lt;- Sys.Date()\nformat(today, format=\"%B %d %Y\")\n\n[1] \"August 27 2025\""
  },
  {
    "objectID": "mth1113.html#data-frame",
    "href": "mth1113.html#data-frame",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Data frame",
    "text": "Data frame\n\nData_Frame &lt;- data.frame(Tr =c(\"1\",\"2\",\"3\"),\n                         Pu =c(11,21,32),\n                         Dur=c(22,222,1))\nData_Frame\n\n  Tr Pu Dur\n1  1 11  22\n2  2 21 222\n3  3 32   1\n\nsummary(Data_Frame)\n\n      Tr                  Pu             Dur        \n Length:3           Min.   :11.00   Min.   :  1.00  \n Class :character   1st Qu.:16.00   1st Qu.: 11.50  \n Mode  :character   Median :21.00   Median : 22.00  \n                    Mean   :21.33   Mean   : 81.67  \n                    3rd Qu.:26.50   3rd Qu.:122.00  \n                    Max.   :32.00   Max.   :222.00  \n\n### table and data frame\ntable(mpg$class)\n\n\n   2seater    compact    midsize    minivan     pickup subcompact        suv \n         5         47         41         11         33         35         62 \n\ndf &lt;- as.data.frame(table(mpg$class))\ndf\n\n        Var1 Freq\n1    2seater    5\n2    compact   47\n3    midsize   41\n4    minivan   11\n5     pickup   33\n6 subcompact   35\n7        suv   62"
  },
  {
    "objectID": "mth1113.html#list",
    "href": "mth1113.html#list",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "List",
    "text": "List\n\n# List ---Can hold vectors, strings, matrices, models, list of other list, lists upon lists!\n\nmylist &lt;- list(letters=c(\"a\",\"b\",\"c\"),\n               numbers=1:3,matrix(1:25,ncol=5))\nhead(mylist)\n\n$letters\n[1] \"a\" \"b\" \"c\"\n\n$numbers\n[1] 1 2 3\n\n[[3]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    6   11   16   21\n[2,]    2    7   12   17   22\n[3,]    3    8   13   18   23\n[4,]    4    9   14   19   24\n[5,]    5   10   15   20   25\n\n# Can reference data using $ (if the elements are named), or using [], or [[]]\n\nmylist[1] # list\n\n$letters\n[1] \"a\" \"b\" \"c\"\n\nmylist[\"letters\"] # list\n\n$letters\n[1] \"a\" \"b\" \"c\"\n\nmylist[[1]] # vector\n\n[1] \"a\" \"b\" \"c\"\n\nmylist$letters == mylist[[\"letters\"]]\n\n[1] TRUE TRUE TRUE\n\nmylist[[3]][1:2,1:2]\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n\nclass(mylist[[3]][1:2,1:2])\n\n[1] \"matrix\" \"array\" \n\nx = c(0, 2, 2, 3, 4); 2 %in% x\n\n[1] TRUE\n\n# eg of using list\nx &lt;- c(5.1, 4.9, 5.6, 4.2, 4.8, 4.5, 5.3, 5.2)   # some toy data\nresults &lt;- t.test(x, alternative='less', mu=5)   # do a t-test\nstr(results)    \n\nList of 10\n $ statistic  : Named num -0.314\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 7\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 0.381\n $ conf.int   : num [1:2] -Inf 5.25\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num 4.95\n  ..- attr(*, \"names\")= chr \"mean of x\"\n $ null.value : Named num 5\n  ..- attr(*, \"names\")= chr \"mean\"\n $ stderr     : num 0.159\n $ alternative: chr \"less\"\n $ method     : chr \"One Sample t-test\"\n $ data.name  : chr \"x\"\n - attr(*, \"class\")= chr \"htest\"\n\nresults$p.value\n\n[1] 0.3813385"
  },
  {
    "objectID": "mth1113.html#matrix",
    "href": "mth1113.html#matrix",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Matrix",
    "text": "Matrix\n\n# Matrices\n\nn=1:9\nmat = matrix(n,nrow=3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\ny &lt;- diag(n)\n\n# use %*% as the product of matrices\n\n## Eigenvalue and Eigenvector\n\nA &lt;- matrix(c(13, -4, 2, -4, 11, -2, 2, -2, 8), 3, 3, byrow=TRUE)\nev &lt;- eigen(A)\n\n(values &lt;- ev$values)\n\n[1] 17  8  7\n\n(vectors &lt;- ev$vectors)\n\n           [,1]       [,2]      [,3]\n[1,]  0.7453560  0.6666667 0.0000000\n[2,] -0.5962848  0.6666667 0.4472136\n[3,]  0.2981424 -0.3333333 0.8944272\n\n## Data selection --row then column\n\nmat[1, 1]\n\n[1] 1\n\nmat[1,]\n\n[1] 1 4 7\n\nmat[,1] \n\n[1] 1 2 3\n\nclass(mat[1, ]) # Note that the class of the returned object is no longer a matrix\n\n[1] \"integer\""
  },
  {
    "objectID": "mth1113.html#example-1-1",
    "href": "mth1113.html#example-1-1",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Example 1",
    "text": "Example 1\n\ndb_data &lt;- list(\n  drugs = list(\n    general_information = data.frame(\n      drugbank_id = c(\"DB001\", \"DB002\", \"DB003\", \"DB004\", \"DB005\"),\n      name = c(\"Aspirin\", \"Ibuprofen\", \"Paracetamol\", \"Insulin\", \"Morphine\"),\n      type = c(\"small molecule\", \"small molecule\", \"small molecule\", \"biotech\", \"small molecule\"),\n      created = as.Date(c(\"2020-01-01\", \"2020-02-01\", \"2020-03-01\", \"2020-04-01\", \"2020-05-01\")),\n      stringsAsFactors = FALSE\n    ),\n    drug_classification = data.frame(\n      drugbank_id = c(\"DB001\", \"DB002\", \"DB003\", \"DB004\", \"DB005\"),\n      classification = c(\"Analgesic\", \"Anti-inflammatory\", \"Analgesic\", \"Hormone\", \"Analgesic\"),\n      stringsAsFactors = FALSE\n    ),\n    experimental_properties = data.frame(\n      drugbank_id = c(\"DB001\", \"DB002\", \"DB003\", \"DB004\", \"DB005\", \"DB001\", \"DB002\", \"DB003\", \"DB004\", \"DB005\"),\n      kind = c(\"logP\", \"logP\", \"logP\", \"logP\", \"logP\", \"Molecular Weight\", \"Molecular Weight\", \"Molecular Weight\", \"Molecular Weight\", \"Molecular Weight\"),\n      value = c(\"1.2\", \"1.5\", \"0.8\", \"2.1\", \"1.8\", \"180.1\", \"206.3\", \"151.2\", \"5800.0\", \"281.5\"),\n      stringsAsFactors = FALSE\n    )\n  )\n)\ndb_data\n\n$drugs\n$drugs$general_information\n  drugbank_id        name           type    created\n1       DB001     Aspirin small molecule 2020-01-01\n2       DB002   Ibuprofen small molecule 2020-02-01\n3       DB003 Paracetamol small molecule 2020-03-01\n4       DB004     Insulin        biotech 2020-04-01\n5       DB005    Morphine small molecule 2020-05-01\n\n$drugs$drug_classification\n  drugbank_id    classification\n1       DB001         Analgesic\n2       DB002 Anti-inflammatory\n3       DB003         Analgesic\n4       DB004           Hormone\n5       DB005         Analgesic\n\n$drugs$experimental_properties\n   drugbank_id             kind  value\n1        DB001             logP    1.2\n2        DB002             logP    1.5\n3        DB003             logP    0.8\n4        DB004             logP    2.1\n5        DB005             logP    1.8\n6        DB001 Molecular Weight  180.1\n7        DB002 Molecular Weight  206.3\n8        DB003 Molecular Weight  151.2\n9        DB004 Molecular Weight 5800.0\n10       DB005 Molecular Weight  281.5\n\ngeneral_information &lt;- db_data$drugs$general_information\n\nprint(general_information)\n\n  drugbank_id        name           type    created\n1       DB001     Aspirin small molecule 2020-01-01\n2       DB002   Ibuprofen small molecule 2020-02-01\n3       DB003 Paracetamol small molecule 2020-03-01\n4       DB004     Insulin        biotech 2020-04-01\n5       DB005    Morphine small molecule 2020-05-01\n\n# 20. Number of drugs in the general_information dataframe\ngeneral_information &lt;- db_data$drugs$general_information\nnrow(general_information)\n\n[1] 5\n\n# 21. Filter drugs of type \"biotech\"\ngeneral_information[general_information$type == 'biotech',]\n\n  drugbank_id    name    type    created\n4       DB004 Insulin biotech 2020-04-01\n\n# 22. Sort by the created column and display the first 5 rows\ngeneral_information$created &lt;- as.Date(general_information$created)\nsorted_df &lt;- general_information[order(general_information$created), ]\nhead(sorted_df, 5)\n\n  drugbank_id        name           type    created\n1       DB001     Aspirin small molecule 2020-01-01\n2       DB002   Ibuprofen small molecule 2020-02-01\n3       DB003 Paracetamol small molecule 2020-03-01\n4       DB004     Insulin        biotech 2020-04-01\n5       DB005    Morphine small molecule 2020-05-01\n\n# 23. Subset with specific columns and display the first 5 rows\nsubset_df &lt;- general_information[, c(\"drugbank_id\", \"name\")]\nhead(subset_df, 5)\n\n  drugbank_id        name\n1       DB001     Aspirin\n2       DB002   Ibuprofen\n3       DB003 Paracetamol\n4       DB004     Insulin\n5       DB005    Morphine\n\n# 24. Merge dataframes and count rows\ndrug_classification &lt;- db_data$drugs$drug_classification\nmerged_df &lt;- merge(general_information, drug_classification, by = \"drugbank_id\")\nnrow(merged_df)\n\n[1] 5\n\n# 25. Count unique experimental properties (kind)\nexperimental_properties &lt;- db_data$drugs$experimental_properties\nunique_kinds &lt;- unique(experimental_properties$kind)\nlength(unique_kinds)\n\n[1] 2\n\n# 26. Filter for kind \"logP\" and count rows\nlogP_df &lt;- experimental_properties[experimental_properties$kind == \"logP\", ]\nnrow(logP_df)\n\n[1] 5\n\n# 27. Convert value column to numeric and calculate mean\nlogP_df$value &lt;- as.numeric(logP_df$value)\nmean(logP_df$value, na.rm = TRUE)\n\n[1] 1.48\n\n# 28. Calculate summary statistics for logP values\nsummary(logP_df$value)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.80    1.20    1.50    1.48    1.80    2.10 \n\nsd(logP_df$value, na.rm = TRUE)\n\n[1] 0.5069517\n\n# 29. Create a histogram of molecular weight values\nmolecular_weight &lt;- experimental_properties[experimental_properties$kind == \"Molecular Weight\", ]\nmolecular_weight$value &lt;- as.numeric(molecular_weight$value)\n# clean based on 3 sigma rule\nmolecular_weight_clean &lt;- molecular_weight[\n  abs(molecular_weight$value - mean(molecular_weight$value, na.rm = TRUE)) &lt;= 3 * sd(molecular_weight$value, na.rm = TRUE),]\nhist(molecular_weight_clean$value, main = \"Histogram of Molecular Weight\", xlab = \"Molecular Weight\", ylab = \"Frequency\", col = \"lightblue\",breaks = 20)\n\n\n\n\n\n\n\n# 30. Filter for kind \"Water Solubility\" and count unique values\nwater_solubility_df &lt;- experimental_properties[experimental_properties$kind == \"Water Solubility\", ]\nlength(unique(water_solubility_df$value))\n\n[1] 0"
  },
  {
    "objectID": "mth1113.html#box-plots",
    "href": "mth1113.html#box-plots",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Box plots",
    "text": "Box plots\n\nlibrary(ggplot2)\ndata(iris)\nboxplot(iris$Sepal.Length ~ iris$Species)\n\n\n\n\n\n\n\nggplot(mpg, aes(x=class, y=hwy)) + \n  geom_boxplot() +\n  scale_y_continuous(breaks = seq(10, 45, by=5))  #---diy scale in y-axis \n\n\n\n\n\n\n\n# ggplot(mpg, aes(x=class, y=hwy)) + geom_boxplot() +scale_y_continuous(breaks = seq(10, 45, by=5), minor_breaks = NULL)"
  },
  {
    "objectID": "mth1113.html#histogram-plots",
    "href": "mth1113.html#histogram-plots",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Histogram plots",
    "text": "Histogram plots\n\nhist(iris$Sepal.Length)\n\n\n\n\n\n\n\nplot(Petal.Length ~ Sepal.Length, data=iris)\nabline(lm(Petal.Length ~ Sepal.Length, data=iris), col=\"red\")\n\n\n\n\n\n\n\ndata(mpg, package='ggplot2')\nggplot(data=mpg, aes(x=class)) +\n  geom_bar()\n\n\n\n\n\n\n\n# By default, the geom_bar() just counts the number of cases and displays how many observations were in each factor level. If we have a data frame that we have already summarized, geom_col will allow us to set the height of the bar by a y column.\ntable(mpg$class)\n\n\n   2seater    compact    midsize    minivan     pickup subcompact        suv \n         5         47         41         11         33         35         62 \n\ndf &lt;- as.data.frame(table(mpg$class))\ndf\n\n        Var1 Freq\n1    2seater    5\n2    compact   47\n3    midsize   41\n4    minivan   11\n5     pickup   33\n6 subcompact   35\n7        suv   62\n\nggplot(df, aes(Var1, Freq)) +\n  geom_col()\n\n\n\n\n\n\n\nggplot(mpg,aes(x=hwy)) +geom_histogram(binwidth = 2)"
  },
  {
    "objectID": "mth1113.html#density-plots",
    "href": "mth1113.html#density-plots",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Density plots",
    "text": "Density plots\n\np1 &lt;- ggplot(mpg, aes(x=hwy, y=after_stat(density))) + \n  geom_histogram(bins=8, fill=\"blue\", alpha=0.5) +\n  labs(title=\"Histogram of Highway MPG density\")\np2 &lt;- ggplot(mpg, aes(x=hwy)) + \n  geom_density(fill='red', alpha=0.5) +\n  labs(title=\"Density Plot of Highway MPG\")\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nmutiple plots in one figure\n\n\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_density(aes(fill = Species,color=Species), alpha = 0.5) +\n  labs(title = \"Density plot\") +\n  labs(x = \"Sepal Length\", y = \"Density\") +\n  labs(fill = \"area\",color=\"line\")  # fill is the area,color is the line or dot"
  },
  {
    "objectID": "mth1113.html#scatter-plots",
    "href": "mth1113.html#scatter-plots",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Scatter plots",
    "text": "Scatter plots\n\nmtcars$cyl &lt;- factor(mtcars$cyl) \n\nggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_point() +\n  labs(title='Weight vs Miles per Gallon') +\n  labs(x=\"Weight in tons (2000 lbs)\", y=\"Miles per Gallon (US)\" ) +\n  labs(color=\"Cylinders\") + # color is dot or line\n  scale_color_manual(values=c('blue', 'darkmagenta', 'aquamarine')) # diy color"
  },
  {
    "objectID": "mth1113.html#scatter-plots-with-regression-line",
    "href": "mth1113.html#scatter-plots-with-regression-line",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Scatter plots with regression line",
    "text": "Scatter plots with regression line\n\nggplot(data=iris, aes(x=Sepal.Length, y=Petal.Length,color=Species))+\n  geom_point()+# Anything set inside an aes() command will be of the form attribute=Column_Name and will change based on the data. Anything set outside an aes() command will be in the form attribute=value and will be fixed.\n   geom_smooth(method=\"lm\") #By default, geom_smooth(method=\"lm\") fits a linear regression line for each Species separately because Species are mapped to colors, and geom_smooth automatically draws a line for each category.\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data=iris, aes(x=Sepal.Length, y=Petal.Length)) +\n  geom_point(aes(color=Species,shape=Species))+\n  geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Zooming in/out--Danger!  This removes the data points first!\n#ggplot(trees, aes(x=Girth, y=Volume)) + \n  #geom_point() +\n  #geom_smooth(method='lm') +\n  #xlim( 8, 19 ) + ylim(0, 60)\n\n\nlibrary(palmerpenguins)\nggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g)) + geom_point(aes(color=bill_depth_mm, shape=species), na.rm=T) + geom_smooth(na.rm=T, se=F) + scale_color_gradient2(low='yellow', mid='green', high='blue', midpoint = 17) + labs(x='Flipper length (millimeters)', y='Body mass (grams)', color='Bill depth (millimeters)') + theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "mth1113.html#heat-map",
    "href": "mth1113.html#heat-map",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Heat map",
    "text": "Heat map\n\n# data\nmine.table &lt;- data.frame(\n  Sample.name = rep(paste0(\"Sample\", 1:5), each = 3),\n  Class = rep(c(\"Class1\", \"Class2\", \"Class3\"), times = 5),\n  Abundance = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5),\n  Depth = c(0.5,0.5,0.5, 0.6,0.6,0.6, 0.7,0.7,0.7, 0.8,0.8,0.8, 0.9,0.9,0.9)\n)\nprint(mine.table)\n\n   Sample.name  Class Abundance Depth\n1      Sample1 Class1       0.1   0.5\n2      Sample1 Class2       0.2   0.5\n3      Sample1 Class3       0.3   0.5\n4      Sample2 Class1       0.4   0.6\n5      Sample2 Class2       0.5   0.6\n6      Sample2 Class3       0.6   0.6\n7      Sample3 Class1       0.7   0.7\n8      Sample3 Class2       0.8   0.7\n9      Sample3 Class3       0.9   0.7\n10     Sample4 Class1       1.0   0.8\n11     Sample4 Class2       1.1   0.8\n12     Sample4 Class3       1.2   0.8\n13     Sample5 Class1       1.3   0.9\n14     Sample5 Class2       1.4   0.9\n15     Sample5 Class3       1.5   0.9\n\nmine.heatmap &lt;- ggplot(data = mine.table, mapping = aes(x = Sample.name, y = Class, fill = Abundance)) + \n  geom_tile() + # create the heatmap with tiles+\n  scale_y_discrete(limits = rev(levels(factor(mine.table$Class)))) +  # reverse the order of the y-axis --class1 to class 3 (if not have this:class 3 to class 1)\n  scale_fill_gradient(low = \"white\", high = \"blue\") +  # color\n  theme_minimal() +  # control the theme of the plot\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate the X-axis label for better display\n  labs(x = \"Sample Name\",  # x-axis label\n       y = \"Class\",  # y-axis label\n       fill = \"Abundance\")+ # fill legend label by \"Abundance\"\n       ggtitle(\"Heatmap of Abundance by Sample and Class\")+ # add title\n       geom_text(aes(label = round(Abundance, 2)), color = \"black\", size = 3) # add text labels to the tiles\n\n\nprint(mine.heatmap)\n\n\n\n\n\n\n\n\nCreate heat map using facet_grid to show the data in different panels by depth\n\nmine.heatmap &lt;- ggplot(data = mine.table, mapping = aes(x = Sample.name, y = Class, fill = Abundance)) + \n  geom_tile() + # create the heatmap with tiles+\n  facet_grid(~ Depth,switch = 'x', scales='free', space='free')+ # facet_grid to show the data in different panels by depth\n  scale_y_discrete(limits = rev(levels(factor(mine.table$Class)))) +  # reverse the order of the y-axis --class1 to class 3 (if not have this:class 3 to class 1)\n  scale_fill_gradient(low=\"#FFFFFF\", high=\"#012345\")+  # color\n  theme_minimal() +  # control the theme of the plot\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate the X-axis label for better display\n  labs(x = \"Sample Name\",  # x-axis label\n       y = \"Class\",  # y-axis label\n       fill = \"Abundance\")+ # fill legend label by \"Abundance\"\n       ggtitle(\"Heatmap of Abundance by Sample and Class\")+ # add title\n       geom_text(aes(label = round(Abundance, 2)), color = \"black\", size = 3) # add text labels to the tiles\n  \n       \n\n\nprint(mine.heatmap)"
  },
  {
    "objectID": "mth1113.html#faceting-make-many-panels-of-graphics-where-each-panel-represents-the-same-relationship-between-variables-but-something-changes-between-each-pane",
    "href": "mth1113.html#faceting-make-many-panels-of-graphics-where-each-panel-represents-the-same-relationship-between-variables-but-something-changes-between-each-pane",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Faceting (make many panels of graphics where each panel represents the same relationship between variables, but something changes between each pane)",
    "text": "Faceting (make many panels of graphics where each panel represents the same relationship between variables, but something changes between each pane)\n\nggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +\n  geom_point() +\n  facet_grid(.~Species) #or facet_grid(Species~.)--Categorical variables of species will be vertical\n\n\n\n\n\n\n\n\n\nAnother example\n\n\nlibrary(reshape)\n\n\nAttaching package: 'reshape'\n\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\n\nThe following object is masked from 'package:Matrix':\n\n    expand\n\ndata(tips, package='reshape')\nhead(tips, 3)\n\n  total_bill  tip    sex smoker day   time size\n1      16.99 1.01 Female     No Sun Dinner    2\n2      10.34 1.66   Male     No Sun Dinner    3\n3      21.01 3.50   Male     No Sun Dinner    3\n\nggplot(tips, aes(x = total_bill, y = tip / total_bill)) +\n  geom_point() +\n  facet_grid( smoker ~ day )\n\n\n\n\n\n\n\n# 'free_y' means the scale of different panels are adjusted by themselves\nggplot(tips, aes(x = total_bill, y = tip / total_bill)) +\n  geom_point() +\n  facet_wrap( ~ day, scales='free_y')\n\n\n\n\n\n\n\n# log scales ---a wrapper of  scale_y_continuous() function , trans_new() function\n# ggplot(ACS, aes(x=Age, y=Income)) + geom_point() +\n# scale_y_log10(breaks=c(1, 10, 100),\n#            minor=c(1:10,\n#                 seq(10, 100, by=10 ),\n#                seq(100, 1000, by=100))) +\n#  ylab('Income (1000s of dollars)')\n\n\nMulti-plot\n\n\np1 &lt;- ggplot(ChickWeight, aes(x=Time, y=weight, colour=Diet, group=Chick)) +\n    geom_line() +\n    ggtitle(\"Growth curve for individual chicks\")\n# Second plot\np2 &lt;- ggplot(ChickWeight, aes(x=Time, y=weight, colour=Diet)) +\n    geom_point(alpha=.3) +\n    geom_smooth(alpha=.2, linewidth=1) +\n    ggtitle(\"Fitted growth curve per diet\")\n# Third plot \np3 &lt;- ggplot(subset(ChickWeight, Time==21), aes(x=weight, colour=Diet)) +\n    geom_density() +\n    ggtitle(\"Final weight, by diet\")\n\n\n# to realize:\n# plot1 plot2 plot2\n# plot1 plot2 plot2\n# plot1 plot3 plot3\n\nmy.layout = cbind( c(1,1,1), c(2,2,3), c(2,2,3) ) # each c represents a column in a matrix and 1,2,3 represents p1,p2,p3\nlibrary(Rmisc)\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:reshape':\n\n    rename, round_any\n\n\nThe following object is masked from 'package:mosaic':\n\n    count\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\nRmisc::multiplot( p1, p2, p3, layout=my.layout)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# OR library(ggpubr) https://rpkgs.datanovia.com/ggpubr/). \n\nlibrary(ggpubr)\n\n\nAttaching package: 'ggpubr'\n\n\nThe following object is masked from 'package:plyr':\n\n    mutate\n\n# Box plot (bp)\nbxp &lt;- ggboxplot(ToothGrowth, x = \"dose\", y = \"len\",\n                 color = \"dose\", palette = \"jco\")\n# Dot plot (dp)\ndp &lt;- ggdotplot(ToothGrowth, x = \"dose\", y = \"len\",\n                 color = \"dose\", palette = \"jco\", binwidth = 1)\nmtcars$name &lt;- rownames(mtcars)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nbp &lt;- ggbarplot(mtcars, x = \"name\", y = \"mpg\",\n          fill = \"cyl\",               # change fill color by cyl\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"asc\",           # Sort the value in ascending order\n          sort.by.groups = TRUE,      # Sort inside each group\n          x.text.angle = 90           # Rotate vertically x axis texts\n          ) + font(\"x.text\", size = 8)\n# Scatter plots (sp)\nsp &lt;- ggscatter(mtcars, x = \"wt\", y = \"mpg\",\n                add = \"reg.line\",               # Add regression line\n                conf.int = TRUE,                # Add confidence interval\n                color = \"cyl\", palette = \"jco\", # Color by groups \"cyl\"\n                shape = \"cyl\"                   # Change point shape by groups \"cyl\"\n                ) + \n  stat_cor(aes(color = cyl), label.x = 3)       # Add correlation coefficient\n\nggarrange(bxp, dp, bp + rremove(\"x.text\"),\n          labels = c(\"A\", \"B\", \"C\"),\n          ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n# Themes\n# Rmisc::multiplot( p1 + theme_bw(),          # Black and white\n#                   p1 + theme_minimal(),   \n#                   p1 + theme_dark(),        \n#                   p1 + theme_light(),\n#                   cols=2 )\n\n#ggsave('p1.png', width=6, height=3, dpi=350)"
  },
  {
    "objectID": "mth1113.html#apply",
    "href": "mth1113.html#apply",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "apply",
    "text": "apply\n\n# apply\n# Summarize each column by calculating the mean.\napply(iris[,-5],        # what object do we want to apply the function to\n      MARGIN=1,    # rows = 1, columns = 2, (same order as [rows, cols]\n      FUN=mean     # what function do we want to apply     \n     ) %&gt;% head(10)\n\n [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400\n\naverage &lt;- apply( \n  iris[,-5],        # what object do we want to apply the function to\n  MARGIN=2,    # rows = 1, columns = 2, (same order as [rows, cols]\n  FUN=mean     # what function do we want to apply\n)\niris &lt;- rbind(iris[,-5], average)\niris %&gt;% head(3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n\n\nThere are several variants of the apply() function, and the most frequently used ones are lapply() and sapply(). These two functions apply a given function to each element of a list or vector and returns a corresponding list or vector of results.\n\n#lapply\nx &lt;- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))\nx\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$beta\n[1]  0.04978707  0.13533528  0.36787944  1.00000000  2.71828183  7.38905610\n[7] 20.08553692\n\n$logic\n[1]  TRUE FALSE FALSE  TRUE\n\nlapply(x, quantile, probs = 1:3/4) # list \n\n$a\n 25%  50%  75% \n3.25 5.50 7.75 \n\n$beta\n      25%       50%       75% \n0.2516074 1.0000000 5.0536690 \n\n$logic\n25% 50% 75% \n0.0 0.5 1.0 \n\nsapply(x, quantile, probs = 1:3/4) # matrix\n\n       a      beta logic\n25% 3.25 0.2516074   0.0\n50% 5.50 1.0000000   0.5\n75% 7.75 5.0536690   1.0"
  },
  {
    "objectID": "mth1113.html#tibbles",
    "href": "mth1113.html#tibbles",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Tibbles",
    "text": "Tibbles\nA tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they don t change variable names or types, and don t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\n\ndata &lt;- data.frame(a = 1:3, b = letters[1:3], c = Sys.Date() - 1:3)\ndata\n\n  a b          c\n1 1 a 2025-08-26\n2 2 b 2025-08-25\n3 3 c 2025-08-24\n\nas_tibble(data)\n\n# A tibble: 3 × 3\n      a b     c         \n  &lt;int&gt; &lt;chr&gt; &lt;date&gt;    \n1     1 a     2025-08-26\n2     2 b     2025-08-25\n3     3 c     2025-08-24"
  },
  {
    "objectID": "mth1113.html#section",
    "href": "mth1113.html#section",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "%>%",
    "text": "%&gt;%\nThe pipe operator %&gt;% is used to pass the result of one function to the next function in a chain, making the code more readable and concise. For example, if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h()."
  },
  {
    "objectID": "mth1113.html#select",
    "href": "mth1113.html#select",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "select",
    "text": "select\n\n# Correct usage of select() within a pipeline\nstarwars %&gt;% select(-ends_with('color'))"
  },
  {
    "objectID": "mth1113.html#filter",
    "href": "mth1113.html#filter",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "filter",
    "text": "filter\n\nlibrary(dplyr)\n\n# Filter rows where species is \"Droid\" and mass is greater than or equal to 100\nfiltered_data &lt;- starwars %&gt;% filter(species == \"Droid\", mass &lt; 100)\nprint(filtered_data)\n\n# A tibble: 3 × 14\n  name  height  mass hair_color skin_color  eye_color birth_year sex   gender   \n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n1 C-3PO    167    75 &lt;NA&gt;       gold        yellow           112 none  masculine\n2 R2-D2     96    32 &lt;NA&gt;       white, blue red               33 none  masculine\n3 R5-D4     97    32 &lt;NA&gt;       white, red  red               NA none  masculine\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;"
  },
  {
    "objectID": "mth1113.html#slice",
    "href": "mth1113.html#slice",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "slice",
    "text": "slice\nThis function is used to select rows by their position in the data frame. It can be used to select specific rows or a range of rows.\nfilter rows based on row number:\n\nstarwars %&gt;% slice(2:4)\n\n# A tibble: 3 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n2 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n3 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;"
  },
  {
    "objectID": "mth1113.html#arrange",
    "href": "mth1113.html#arrange",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "arrange",
    "text": "arrange\nThis function is used to sort the rows of a data frame by one or more columns. The default sorting of the number in the dataset is in ascending order, but you can use the desc() function to sort in descending order.\n\nstarwars %&gt;% arrange(desc(name)) #The default sorting is in ascending order\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Zam Wes…    168    55 blonde     fair, gre… yellow            NA fema… femin…\n 2 Yoda         66    17 white      green      brown            896 male  mascu…\n 3 Yarael …    264    NA none       white      yellow            NA male  mascu…\n 4 Wilhuff…    180    NA auburn, g… fair       blue              64 male  mascu…\n 5 Wicket …     88    20 brown      brown      brown              8 male  mascu…\n 6 Wedge A…    170    77 brown      fair       hazel             21 male  mascu…\n 7 Watto       137    NA black      blue, grey yellow            NA male  mascu…\n 8 Wat Tam…    193    48 none       green, gr… unknown           NA male  mascu…\n 9 Tion Me…    206    80 none       grey       black             NA male  mascu…\n10 Taun We     213    NA none       grey       black             NA fema… femin…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars %&gt;% arrange(desc(height)) %&gt;% head(3)\n\n# A tibble: 3 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Yarael P…    264    NA none       white      yellow            NA male  mascu…\n2 Tarfful      234   136 brown      brown      blue              NA male  mascu…\n3 Lama Su      229    88 none       grey       black             NA male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\ndd &lt;- data.frame(\n  Trt = factor(c(\"High\", \"Med\", \"High\", \"Low\"),        \n               levels = c(\"Low\", \"Med\", \"High\")), # level\n  y = c(8, 3, 9, 9),      \n  z = c(1, 1, 1, 2)) \ndd %&gt;% arrange(Trt, desc(y))\n\n   Trt y z\n1  Low 9 2\n2  Med 3 1\n3 High 9 1\n4 High 8 1"
  },
  {
    "objectID": "mth1113.html#mutate",
    "href": "mth1113.html#mutate",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "mutate",
    "text": "mutate\nThis function is used to create new columns or modify existing columns in a data frame. It allows you to perform calculations and transformations on the data.\n# select using the old columns\nstarwars$bmi = starwars$mass / ((starwars$height / 100) ^ 2)\nstarwars %&gt;% select(name, bmi) %&gt;% head(3)\n\n# mutate avoids all the starwars$\n\nstarwars$bmi &lt;- NULL\nstarwars %&gt;% \n  mutate(bmi = mass / ((height / 100) ^ 2)) %&gt;%  \n  select(name, bmi) %&gt;% head(3)\n\n# mutate_at() and mutate_if() allow us to apply a function to a particular column and save the output.\n\nsubset &lt;- starwars %&gt;% \n  mutate(square_height = (height / 100) ^ 2,\n         bmi = mass / square_height) %&gt;%\n  select(name, square_height, bmi)\nsubset %&gt;% head(3)\n\nsubset %&gt;% mutate_if(is.numeric, round, digits=0) # here, is.numeric is the condition\n\nsubset %&gt;% mutate_at(2:3, round, digits=0) %&gt;% head() # column 2 3\n\n# Apply the transformation to columns 2 and 3 for rows 1 to 3\nresult &lt;- subset %&gt;% \n  mutate_at(2:3, ~ifelse(row_number() %in% 1:3, round(., digits = 0), .))\n\nsubset %&gt;% mutate(avg.example = select(., square_height:bmi) %&gt;% rowMeans())"
  },
  {
    "objectID": "mth1113.html#summarise",
    "href": "mth1113.html#summarise",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "summarise",
    "text": "summarise\nThis function is used to create a summary table. It reduces the data frame to a single row containing summary statistics.\nstarwars %&gt;% summarise(mean.height=mean(height, na.rm=T), sd.height=sd(height, na.rm=T))\n\n# apply the same statistic to each column\nstarwars %&gt;% select(height:mass) %&gt;% summarise_all(list(min=min, max=max), na.rm=T)\nstarwars %&gt;% summarise_if(is.numeric, list(min=min, max=max), na.rm = T)"
  },
  {
    "objectID": "mth1113.html#group_by",
    "href": "mth1113.html#group_by",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "group_by",
    "text": "group_by\nThis function is used to group the data frame by one or more columns. It is often used in combination with summarise() to calculate summary statistics for each group.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\ntable(penguins$sex, penguins$species)\npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;%\n  group_by(sex, species) %&gt;%           \n  summarise(n = n(), \n            mean.flipper = mean(flipper_length_mm),\n            sd.flipper = sd(flipper_length_mm),\n            .groups='keep') %&gt;%\n  head(3)"
  },
  {
    "objectID": "mth1113.html#examples",
    "href": "mth1113.html#examples",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "examples",
    "text": "examples\nFind the flight with the longest departure delay among flights from the same origin and destination (use filter()). Relocate the origin, destination, and departure delay to the first three columns and sort by origin and dest.\nflights %&gt;% \n  filter(!is.na(dep_delay)) %&gt;% \n  group_by(origin, dest) %&gt;% \n  filter(dep_delay == max(dep_delay)) %&gt;% \n  relocate(origin, dest, dep_delay) %&gt;% \n  arrange(origin, dest)\nFind the flight with the longest departure delay among flights from the same origin and destination (use top_n() or slice_max()). Relocate the origin, destination, and departure delay to the first three columns and sort by origin and dest.\nflights %&gt;% \n  filter(!is.na(dep_delay)) %&gt;% \n  group_by(origin, dest) %&gt;% \n  top_n(1, dep_delay) %&gt;%  # or using slice_max(dep_delay) %&gt;% \n  relocate(origin, dest, dep_delay) %&gt;% \n  arrange(origin, dest)\nHow do departure delays vary at different times of the day? Summarize the averaged departure delays by hours and create an new column named as dep_delay_level which cut() the averaged departure delays into three levels (low, median, and high).\nflights %&gt;% \n  group_by(hour) %&gt;% \n  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;%\n  mutate(dep_delay_level = cut(avg_dep_delay, breaks=3, c('low', 'median', 'high')))\nHow do departure delays vary at different times of the day? Illustrate your answer with a geom_smooth() plot.\nflights %&gt;% \n  group_by(hour) %&gt;% \n  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = hour, y = avg_dep_delay)) + geom_smooth()\n# ways to deleter the blanks\nstudents %&gt;%\n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  ) %&gt;% head(3)\n\n\nread_csv(\n  \"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\"\n)\n\nlibrary(tidyr)\ngrade.book &lt;- rbind(\n  data.frame(name='Alison',  HW.1=8, HW.2=5, HW.3=8, HW.4=4),\n  data.frame(name='Brandon', HW.1=5, HW.2=3, HW.3=6, HW.4=9),\n  data.frame(name='Charles', HW.1=9, HW.2=7, HW.3=9, HW.4=10))\ngrade.book\n\n     name HW.1 HW.2 HW.3 HW.4\n1  Alison    8    5    8    4\n2 Brandon    5    3    6    9\n3 Charles    9    7    9   10\n\ntidy.scores &lt;- grade.book %&gt;%\n  pivot_longer(\n    cols = starts_with(\"HW\"),\n    names_to = \"Homework\",\n    values_to = \"Score\"\n  )\ntidy.scores\n\n# A tibble: 12 × 3\n   name    Homework Score\n   &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n 1 Alison  HW.1         8\n 2 Alison  HW.2         5\n 3 Alison  HW.3         8\n 4 Alison  HW.4         4\n 5 Brandon HW.1         5\n 6 Brandon HW.2         3\n 7 Brandon HW.3         6\n 8 Brandon HW.4         9\n 9 Charles HW.1         9\n10 Charles HW.2         7\n11 Charles HW.3         9\n12 Charles HW.4        10\n\ntidy.scores %&gt;% pivot_wider(names_from=Homework, values_from=Score)\n\n# A tibble: 3 × 5\n  name     HW.1  HW.2  HW.3  HW.4\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Alison      8     5     8     4\n2 Brandon     5     3     6     9\n3 Charles     9     7     9    10\n\n# table joins\n\nFish.Data &lt;- tibble(\n  Lake_ID = c('A','A','B','B','C','C'), \n  Fish.Weight=rnorm(6, mean=260, sd=25) ) # make up some data\nFish.Data\n\n# A tibble: 6 × 2\n  Lake_ID Fish.Weight\n  &lt;chr&gt;         &lt;dbl&gt;\n1 A              237.\n2 A              247.\n3 B              262.\n4 B              245.\n5 C              300.\n6 C              278.\n\nLake.Data &lt;- tibble(\n  Lake_ID = c('B','C','D'),   \n  Lake_Name = c('Lake Elaine', 'Mormon Lake', 'Lake Mary'),   \n  pH=c(6.5, 6.3, 6.1),\n  area = c(40, 210, 240),\n  avg_depth = c(8, 10, 38))\nLake.Data\n\n# A tibble: 3 × 5\n  Lake_ID Lake_Name      pH  area avg_depth\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 B       Lake Elaine   6.5    40         8\n2 C       Mormon Lake   6.3   210        10\n3 D       Lake Mary     6.1   240        38\n\nfull_join(Fish.Data, Lake.Data)\n\nJoining with `by = join_by(Lake_ID)`\n\n\n# A tibble: 7 × 6\n  Lake_ID Fish.Weight Lake_Name      pH  area avg_depth\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 A              237. &lt;NA&gt;         NA      NA        NA\n2 A              247. &lt;NA&gt;         NA      NA        NA\n3 B              262. Lake Elaine   6.5    40         8\n4 B              245. Lake Elaine   6.5    40         8\n5 C              300. Mormon Lake   6.3   210        10\n6 C              278. Mormon Lake   6.3   210        10\n7 D               NA  Lake Mary     6.1   240        38\n\nleft_join(Fish.Data, Lake.Data)\n\nJoining with `by = join_by(Lake_ID)`\n\n\n# A tibble: 6 × 6\n  Lake_ID Fish.Weight Lake_Name      pH  area avg_depth\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 A              237. &lt;NA&gt;         NA      NA        NA\n2 A              247. &lt;NA&gt;         NA      NA        NA\n3 B              262. Lake Elaine   6.5    40         8\n4 B              245. Lake Elaine   6.5    40         8\n5 C              300. Mormon Lake   6.3   210        10\n6 C              278. Mormon Lake   6.3   210        10\n\ninner_join(Fish.Data, Lake.Data)\n\nJoining with `by = join_by(Lake_ID)`\n\n\n# A tibble: 4 × 6\n  Lake_ID Fish.Weight Lake_Name      pH  area avg_depth\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 B              262. Lake Elaine   6.5    40         8\n2 B              245. Lake Elaine   6.5    40         8\n3 C              300. Mormon Lake   6.3   210        10\n4 C              278. Mormon Lake   6.3   210        10\n\n\nmutate(\n    week = parse_number(week)\n  )\nhow many data points are in the data set\ngender_year &lt;- Survey %&gt;% \n  filter(!is.na(Year)) %&gt;% \n  group_by(Sex, Year) %&gt;% \n  count() %&gt;% \n  rename(nu=n)\ngender_year\ngender_year %&gt;% pivot_wider(names_from = Year, values_from = nu)\nwho2 %&gt;% \n  head(3)\nwho2 &lt;- who2 %&gt;% \n  pivot_longer(\n    cols = !(country:year), \n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\"\n  ) %&gt;% \n  filter(!is.na(count))\nwho2\nleft_join(feb14_VX, airports, by=c('dest'='faa'))\n\nlibrary(psych)\ndrug_prop &lt;- drug_prop %&gt;% \n  filter(class == 'Carboxylic acids and derivatives') \ndrug_prop %&gt;% \n  select(logP, logS, water_solubility) %&gt;% \n  pairs.panels()"
  },
  {
    "objectID": "mth1113.html#psthe-comparison-between-whether-to-use-or-not",
    "href": "mth1113.html#psthe-comparison-between-whether-to-use-or-not",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "ps:the comparison between whether to use %>% or not",
    "text": "ps:the comparison between whether to use %&gt;% or not\n# %&gt;% \npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;%\n  group_by(sex, species) %&gt;%           \n  mutate(Sum.Sq.Cells = (flipper_length_mm - mean(flipper_length_mm))^2)  %&gt;%  \n  select(sex, species, flipper_length_mm, Sum.Sq.Cells) %&gt;% head()\n\n# not use %&gt;% \nhead(\n  select(mutate(group_by(filter(penguins, !is.na(sex)), sex, species),\n                Sum.Sq.Cells = (flipper_length_mm - mean(flipper_length_mm))^2),\n       sex, species, flipper_length_mm, Sum.Sq.Cells))\nlibrary(nycflights13)\nstr(nycflights13::flights)\n# the order of group_by and summarize matters\nflights %&gt;% \n  group_by(carrier) %&gt;% \n  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% \n  arrange(desc(avg_dep_delay))"
  },
  {
    "objectID": "mth1113.html#while-loop",
    "href": "mth1113.html#while-loop",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "while loop",
    "text": "while loop\nx &lt;- 1\nwhile (x &lt; 10) {\n  print(x)\n  x &lt;- x + 1\n}"
  },
  {
    "objectID": "mth1113.html#for-loop",
    "href": "mth1113.html#for-loop",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "for loop",
    "text": "for loop\n\nFibonacci sequence\n\nF &lt;- rep(0, 10)        \nF[1] &lt;- 0             \nF[2] &lt;- 1              \ncat('F = ', F, '\\n') \n\nF =  0 1 0 0 0 0 0 0 0 0 \n\nfor( n in 3:10 ){\n  F[n] &lt;- F[n-1] + F[n-2]\n  cat('F = ', F, '\\n')    \n}\n\nF =  0 1 1 0 0 0 0 0 0 0 \nF =  0 1 1 2 0 0 0 0 0 0 \nF =  0 1 1 2 3 0 0 0 0 0 \nF =  0 1 1 2 3 5 0 0 0 0 \nF =  0 1 1 2 3 5 8 0 0 0 \nF =  0 1 1 2 3 5 8 13 0 0 \nF =  0 1 1 2 3 5 8 13 21 0 \nF =  0 1 1 2 3 5 8 13 21 34 \n\n\n\n\nbootstrap estimate of a sampling distribution\n\nbootstrap estimate of a sampling distribution\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nSampDist &lt;- data.frame()\n\nfor (i in 1:1000){\n  SampDist &lt;- trees %&gt;% \n    slice_sample(n=30, replace =TRUE) %&gt;% \n    dplyr::summarise(xbar=mean(Height)) %&gt;% \n    rbind(SampDist)\n}\nggplot(SampDist,aes(x=xbar)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "mth1113.html#functions-condtruction",
    "href": "mth1113.html#functions-condtruction",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Functions condtruction",
    "text": "Functions condtruction\ncopy of x\n\nk &lt;- 3\nexample.func &lt;- function(x){\n  x &lt;- sort(x)\n  if (k &gt; 1){\n    print(x)\n  }\n}\nx &lt;- c(3,1,5,4,2)\nexample.func(x)\n\n[1] 1 2 3 4 5\n\nx # x is changed inside the function but not outsied the function\n\n[1] 3 1 5 4 2"
  },
  {
    "objectID": "mth1113.html#ellipses",
    "href": "mth1113.html#ellipses",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Ellipses",
    "text": "Ellipses\n\n# a function that draws the regression line and confidence interval\n# notice it doesn't return anything, all it does is draw a plot\nshow.lm &lt;- function(m, interval.type='confidence', fill.col='light grey', ...){\n  x &lt;- m$model[,2]       # extract the predictor variable\n  y &lt;- m$model[,1]       # extract the response\n  pred &lt;- predict(m, interval=interval.type)\n  plot(x, y, ...)\n  polygon( c(x,rev(x)),                         # draw the ribbon defined\n           c(pred[,'lwr'], rev(pred[,'upr'])),  # by lwr and upr - polygon\n           col='light grey')                    # fills in the region defined by            \n  lines(x, pred[, 'fit'])                       # a set of vertices, need to reverse            \n  points(x, y)                                  # the uppers to make a nice figure\n}"
  },
  {
    "objectID": "mth1113.html#ellipses-1",
    "href": "mth1113.html#ellipses-1",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Ellipses",
    "text": "Ellipses\n\n# a function that draws the regression line and confidence interval\n# notice it doesn't return anything, all it does is draw a plot\nshow.lm &lt;- function(m, interval.type='confidence', fill.col='light grey', ...){\n  x &lt;- m$model[,2]       # extract the predictor variable\n  y &lt;- m$model[,1]       # extract the response\n  pred &lt;- predict(m, interval=interval.type)\n  plot(x, y, ...)\n  polygon( c(x,rev(x)),                         # draw the ribbon defined\n           c(pred[,'lwr'], rev(pred[,'upr'])),  # by lwr and upr - polygon\n           col='light grey')                    # fills in the region defined by            \n  lines(x, pred[, 'fit'])                       # a set of vertices, need to reverse            \n  points(x, y)                                  # the uppers to make a nice figure\n}"
  },
  {
    "objectID": "mth1113.html#part-1",
    "href": "mth1113.html#part-1",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Part 1",
    "text": "Part 1\n\nLoad the bloodpress.txt\n\n\nbloodpress &lt;- read.table(\"bloodpress.txt\", header=T)\nbloodpress\n\n   Pt  BP Age Weight  BSA  Dur Pulse Stress\n1   1 105  47   85.4 1.75  5.1    63     33\n2   2 115  49   94.2 2.10  3.8    70     14\n3   3 116  49   95.3 1.98  8.2    72     10\n4   4 117  50   94.7 2.01  5.8    73     99\n5   5 112  51   89.4 1.89  7.0    72     95\n6   6 121  48   99.5 2.25  9.3    71     10\n7   7 121  49   99.8 2.25  2.5    69     42\n8   8 110  47   90.9 1.90  6.2    66      8\n9   9 110  49   89.2 1.83  7.1    69     62\n10 10 114  48   92.7 2.07  5.6    64     35\n11 11 114  47   94.4 2.07  5.3    74     90\n12 12 115  49   94.1 1.98  5.6    71     21\n13 13 114  50   91.6 2.05 10.2    68     47\n14 14 106  45   87.1 1.92  5.6    67     80\n15 15 125  52  101.3 2.19 10.0    76     98\n16 16 114  46   94.5 1.98  7.4    69     95\n17 17 106  46   87.0 1.87  3.6    62     18\n18 18 113  46   94.5 1.90  4.3    70     12\n19 19 110  48   90.5 1.88  9.0    71     99\n20 20 122  56   95.7 2.09  7.0    75     99\n\n\n\nUse pairs.panels() function from psych pacakge to draw scatterplots, histograms, and calculate correlations between variables.\n\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:gtools':\n\n    logit\n\n\nThe following objects are masked from 'package:mosaic':\n\n    logit, rescale\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\npairs.panels(bloodpress[, -1])\n\n\n\n\n\n\n\n\n\nFit a simple linear regression model of BP vs Stress. Is Stress significant?\n\n\nmodel.1 &lt;- lm(BP ~ Stress, data=bloodpress)\nsummary(model.1)\n\n\nCall:\nlm(formula = BP ~ Stress, data = bloodpress)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6394 -3.3014  0.0722  2.2181  9.9287 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 112.71997    2.19345  51.389   &lt;2e-16 ***\nStress        0.02399    0.03404   0.705     0.49    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.502 on 18 degrees of freedom\nMultiple R-squared:  0.02686,   Adjusted R-squared:  -0.0272 \nF-statistic: 0.4969 on 1 and 18 DF,  p-value: 0.4899\n\n\n\nFit a simple linear regression model of BP vs Weight.\n\n\nmodel.2 &lt;- lm(BP ~ Weight, data=bloodpress)\nsummary(model.2)\n\n\nCall:\nlm(formula = BP ~ Weight, data = bloodpress)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6933 -0.9318 -0.4935  0.7703  4.8656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.20531    8.66333   0.255    0.802    \nWeight       1.20093    0.09297  12.917 1.53e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.74 on 18 degrees of freedom\nMultiple R-squared:  0.9026,    Adjusted R-squared:  0.8972 \nF-statistic: 166.9 on 1 and 18 DF,  p-value: 1.528e-10\n\n\n\nFit a simple linear regression model of BP vs BSA.\n\n\nmodel.3 &lt;- lm(BP ~ BSA, data=bloodpress)\nsummary(model.3)\n\n\nCall:\nlm(formula = BP ~ BSA, data = bloodpress)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.314 -1.963 -0.197  1.934  4.831 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.183      9.392   4.811  0.00014 ***\nBSA           34.443      4.690   7.343 8.11e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.79 on 18 degrees of freedom\nMultiple R-squared:  0.7497,    Adjusted R-squared:  0.7358 \nF-statistic: 53.93 on 1 and 18 DF,  p-value: 8.114e-07\n\n\n\nFit a multiple linear regression model of BP vs Weight + BSA. Is BSA still significant? Why?\n\n\nmodel.4 &lt;- lm(BP ~ Weight + BSA, data=bloodpress)\nsummary(model.4)\n\n\nCall:\nlm(formula = BP ~ Weight + BSA, data = bloodpress)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8932 -1.1961 -0.4061  1.0764  4.7524 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.6534     9.3925   0.602    0.555    \nWeight        1.0387     0.1927   5.392 4.87e-05 ***\nBSA           5.8313     6.0627   0.962    0.350    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.744 on 17 degrees of freedom\nMultiple R-squared:  0.9077,    Adjusted R-squared:  0.8968 \nF-statistic: 83.54 on 2 and 17 DF,  p-value: 1.607e-09\n\n\n\nPredict BP for Weight=92 and BSA=2 for the two simple linear regression models and the multiple linear regression model, by hand and by predict() function.\n\n\n2.20531 + 1.20093 * 92\n\n[1] 112.6909\n\npredict(model.2,\n        newdata=data.frame(Weight=92))\n\n      1 \n112.691 \n\n45.183 + 34.443 * 2\n\n[1] 114.069\n\npredict(model.3,\n        newdata=data.frame(BSA=2))\n\n       1 \n114.0689 \n\n5.6534 + 1.0387 * 92 + 5.8313 * 2\n\n[1] 112.8764\n\npredict(model.4,\n        newdata=data.frame(Weight=92, BSA=2))\n\n       1 \n112.8794 \n\n\n\nFit a multiple linear regression model of BP vs Age + Weight. Set argument x and y as TRUE. Save the output of lm() as model.5. How do we interpret each estimated coefficients?\n\n\nmodel.5 &lt;- lm(BP ~ Age + Weight, data=bloodpress, x=TRUE, y=TRUE)\nsummary(model.5)\n\n\nCall:\nlm(formula = BP ~ Age + Weight, data = bloodpress, x = TRUE, \n    y = TRUE)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89968 -0.35242  0.06979  0.35528  0.82781 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -16.57937    3.00746  -5.513 3.80e-05 ***\nAge           0.70825    0.05351  13.235 2.22e-10 ***\nWeight        1.03296    0.03116  33.154  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5327 on 17 degrees of freedom\nMultiple R-squared:  0.9914,    Adjusted R-squared:  0.9904 \nF-statistic: 978.2 on 2 and 17 DF,  p-value: &lt; 2.2e-16\n\n\n\nUse the plot_ly function in the plotly package to create a 3D scatterplot of the data with the fitted plane for a multiple linear regression model of BP vs Age + Weight.\n\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, mutate, rename, summarise\n\n\nThe following object is masked from 'package:reshape':\n\n    rename\n\n\nThe following object is masked from 'package:mosaic':\n\n    do\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nplot_ly(x=bloodpress$Age, y=bloodpress$Weight, z=bloodpress$BP, type='scatter3d', mode='markers', color=bloodpress$BP)\n\n\n\n\n\n\nExtract the matrix x and y of model.5 and assign it to a new object X and y. Remember, if you save the output of lm() as an object, this object contains many elements. After we set x=TRUE and y=TRUE in question 8, we can find x and y in this list.\n\n\nX &lt;- model.5$x\ny &lt;- model.5$y\n\n\nCalculate \\(X^{T}X\\), \\(X^{T}y\\), \\((X^{T}X)^{-1}\\), and \\((X^{T}X)^{-1}X^{T}y\\). Use t() for transpose, %*% for matrix multiplication, and solve() for inverse of matrix. For the last one, is your result same as the estimated values you obtained in question 7? –Of course!\n\n\nt(X) %*% X\n\n            (Intercept)     Age   Weight\n(Intercept)        20.0   972.0   1861.8\nAge               972.0 47358.0  90566.6\nWeight           1861.8 90566.6 173665.4\n\nt(X) %*% y\n\n                [,1]\n(Intercept)   2280.0\nAge         110978.0\nWeight      212666.1\n\nsolve(t(X) %*% X)\n\n            (Intercept)          Age       Weight\n(Intercept)  31.8748075 -0.267669593 -0.202127676\nAge          -0.2676696  0.010092130 -0.002393468\nWeight       -0.2021277 -0.002393468  0.003420885\n\nsolve(t(X) %*% X) %*% (t(X) %*% y)\n\n                   [,1]\n(Intercept) -16.5793694\nAge           0.7082515\nWeight        1.0329611\n\n\n\nUse the anova function to display the ANOVA table with sequential (type I) sums of squares for the model.5.\n\n$SS_{} = *{i=1}^n (* - {y})^2 $\n\\(\\hat{y}_{\\text{Variable}, i}\\) is the model including only varible i.\n$ F = $\nIf (F \\(\\approx\\) 1) : It indicates that the sizes of MS_Variable and MS_Residuals are approximately the same, suggesting that the explanatory power of the independent variable for the dependent variable is comparable to the random error, and the null hypothesis ((H_0)) cannot be rejected.\nIf (F \\(\\gg\\) 1) : It indicates that MS_Variable is significantly greater than MS_Residuals, suggesting that the independent variable has a significant influence on the dependent variable, and the null hypothesis ((H_0)) can be rejected.\n\nanova(model.5)\n\nAnalysis of Variance Table\n\nResponse: BP\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nAge        1 243.266 243.266  857.29 5.481e-16 ***\nWeight     1 311.910 311.910 1099.20 &lt; 2.2e-16 ***\nResiduals 17   4.824   0.284                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# remark\nsum((model.5$y-mean(model.5$y))^2) == 243.266+311.910+4.824\n\n[1] TRUE\n\n\n$SS_{} = SS_{} + SS_{} + SS_{} $\n\nUse the residuals element in fitted model or residuals() function to extract the fitted residuals. Calculate the sum of square of these residual values. Extract the df.residual element in fitted model and use the above elements to calculate the MSE. Is your result same as the anova() output?\n\n\nsum((model.5$residuals)^2)/model.5$df.residual\n\n[1] 0.2837604\n\n\n\nFit a multiple linear regression model of BP vs Age + Weight + Pulse. Save the output of lm() as model.6.\n\n\nmodel.6 &lt;- lm(BP ~ Age + Weight + Pulse, data=bloodpress)\nsummary(model.6)\n\n\nCall:\nlm(formula = BP ~ Age + Weight + Pulse, data = bloodpress)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71174 -0.45422 -0.01909  0.41745  0.88743 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -16.69000    2.93761  -5.681 3.40e-05 ***\nAge           0.75018    0.06074  12.350 1.36e-09 ***\nWeight        1.06135    0.03695  28.722 3.40e-15 ***\nPulse        -0.06566    0.04852  -1.353    0.195    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5201 on 16 degrees of freedom\nMultiple R-squared:  0.9923,    Adjusted R-squared:  0.9908 \nF-statistic: 684.7 on 3 and 16 DF,  p-value: &lt; 2.2e-16\n\n\n\nUse anova() function to obtain the ANOVA table for model.6. We may consider model.6 as full model, and model.5 as reduced model in this question. Based on the obtained ANOVA table and the output of question 11, calculate the F-statistic for testing the reduced model by hand. You may use the Residuals Sum sq and the corresponding Residuals Df from both tables. Then, calculate the p-value using \\(pf()\\) function, don’t forget about the lower.tail.\n\n\nanova(model.6)\n\nAnalysis of Variance Table\n\nResponse: BP\n          Df  Sum Sq Mean Sq   F value    Pr(&gt;F)    \nAge        1 243.266 243.266  899.2446 1.726e-15 ***\nWeight     1 311.910 311.910 1152.9909 2.433e-16 ***\nPulse      1   0.496   0.496    1.8319    0.1947    \nResiduals 16   4.328   0.271                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFstat &lt;- (4.824-4.328)/(17-16) / (4.328/16)\nFstat\n\n[1] 1.833641\n\npf(Fstat, 1, 16, lower.tail = F)\n\n[1] 0.1945157\n\n\n\nUse anova() function to do the F-test on model.5 and model.6. Compare the output with your answers of question 14. What is the conclustion of the F-test?\n\n$ = _{i=1}^n (y_i - _i)^2 $\n$ = - $ (It represents the variation in the interpretation of the dependent variable by the newly added variable Pulse.)\n\\[ F = \\frac{\\text{Sum of Sq} / \\text{Df}}{\\text{RSS(Model 2)} / \\text{Res.Df(Model 2)}} \\\\F = \\frac{0.49557}{0.270525} \\approx 1.8319\\]\n\nanova(model.5, model.6)\n\nAnalysis of Variance Table\n\nModel 1: BP ~ Age + Weight\nModel 2: BP ~ Age + Weight + Pulse\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     17 4.8239                           \n2     16 4.3284  1   0.49557 1.8319 0.1947\n\n\n\nPlot the qqPlot for residuals of model.5. What is the x-axis and y-axis of the qqPlot? What can we say about the qqPlot?\n\n\nlibrary(car)\nqqPlot(model.5$residuals)\n\n\n\n\n\n\n\n\n[1] 19 10\n\n\n\nPlot the residual vs fitted plot of model.5. You may extract fitted.values from model.5 and use it as x in plot().\n\n\nplot(x=model.5$fitted.values, y=model.5$residuals)\n\n\n\n\n\n\n\n\n\nDirectly use plot() function on model.5.\n\n\nplot(model.5)"
  },
  {
    "objectID": "mth1113.html#part-2",
    "href": "mth1113.html#part-2",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Part 2",
    "text": "Part 2\n\nLoad the hospital_infct.txt data and select observations with Stay &lt;= 14.\n\n\ninfectionrisk &lt;- read.table(\"/Users/luyu/Desktop/APH——course/NOTEsAPH101/hospital_infct.txt\", header=T)\ninfectionrisk &lt;- infectionrisk[infectionrisk$Stay&lt;=14,]\ninfectionrisk\n\n     ID  Stay  Age InfctRsk Culture  Xray Beds MedSchool Region Census Nurses\n1     1  7.13 55.7      4.1     9.0  39.6  279         2      4    207    241\n2     2  8.82 58.2      1.6     3.8  51.7   80         2      2     51     52\n3     3  8.34 56.9      2.7     8.1  74.0  107         2      3     82     54\n4     4  8.95 53.7      5.6    18.9 122.8  147         2      4     53    148\n5     5 11.20 56.5      5.7    34.5  88.9  180         2      1    134    151\n6     6  9.76 50.9      5.1    21.9  97.0  150         2      2    147    106\n7     7  9.68 57.8      4.6    16.7  79.0  186         2      3    151    129\n8     8 11.18 45.7      5.4    60.5  85.8  640         1      2    399    360\n9     9  8.67 48.2      4.3    24.4  90.8  182         2      3    130    118\n10   10  8.84 56.3      6.3    29.6  82.6   85         2      1     59     66\n11   11 11.07 53.2      4.9    28.5 122.0  768         1      1    591    656\n12   12  8.30 57.2      4.3     6.8  83.8  167         2      3    105     59\n13   13 12.78 56.8      7.7    46.0 116.9  322         1      1    252    349\n14   14  7.58 56.7      3.7    20.8  88.0   97         2      2     59     79\n15   15  9.00 56.3      4.2    14.6  76.4   72         2      3     61     38\n16   16 11.08 50.2      5.5    18.6  63.6  387         2      3    326    405\n17   17  8.28 48.1      4.5    26.0 101.8  108         2      4     84     73\n18   18 11.62 53.9      6.4    25.5  99.2  133         2      1    113    101\n19   19  9.06 52.8      4.2     6.9  75.9  134         2      2    103    125\n20   20  9.35 53.8      4.1    15.9  80.9  833         2      3    547    519\n21   21  7.53 42.0      4.2    23.1  98.9   95         2      4     47     49\n22   22 10.24 49.0      4.8    36.3 112.6  195         2      2    163    170\n23   23  9.78 52.3      5.0    17.6  95.9  270         1      1    240    198\n24   24  9.84 62.2      4.8    12.0  82.3  600         2      3    468    497\n25   25  9.20 52.2      4.0    17.5  71.1  298         1      4    244    236\n26   26  8.28 49.5      3.9    12.0 113.1  546         1      2    413    436\n27   27  9.31 47.2      4.5    30.2 101.3  170         2      1    124    173\n28   28  8.19 52.1      3.2    10.8  59.2  176         2      1    156     88\n29   29 11.65 54.5      4.4    18.6  96.1  248         2      1    217    189\n30   30  9.89 50.5      4.9    17.7 103.6  167         2      2    113    106\n31   31 11.03 49.9      5.0    19.7 102.1  318         2      1    270    335\n32   32  9.84 53.0      5.2    17.7  72.6  210         2      2    200    239\n33   33 11.77 54.1      5.3    17.3  56.0  196         2      1    164    165\n34   34 13.59 54.0      6.1    24.2 111.7  312         2      1    258    169\n35   35  9.74 54.4      6.3    11.4  76.1  221         2      2    170    172\n36   36 10.33 55.8      5.0    21.2 104.3  266         2      1    181    149\n37   37  9.97 58.2      2.8    16.5  76.5   90         2      2     69     42\n38   38  7.84 49.1      4.6     7.1  87.9   60         2      3     50     45\n39   39 10.47 53.2      4.1     5.7  69.1  196         2      2    168    153\n40   40  8.16 60.9      1.3     1.9  58.0   73         2      3     49     21\n41   41  8.48 51.1      3.7    12.1  92.8  166         2      3    145    118\n42   42 10.72 53.8      4.7    23.2  94.1  113         2      3     90    107\n43   43 11.20 45.0      3.0     7.0  78.9  130         2      3     95     56\n44   44 10.12 51.7      5.6    14.9  79.1  362         1      3    313    264\n45   45  8.37 50.7      5.5    15.1  84.8  115         2      2     96     88\n46   46 10.16 54.2      4.6     8.4  51.5  831         1      4    581    629\n48   48 10.90 57.2      5.5    10.6  71.9  593         2      2    446    211\n49   49  7.67 51.7      1.8     2.5  40.4  106         2      3     93     35\n50   50  8.88 51.5      4.2    10.1  86.9  305         2      3    238    197\n51   51 11.48 57.6      5.6    20.3  82.0  252         2      1    207    251\n52   52  9.23 51.6      4.3    11.6  42.6  620         2      2    413    420\n53   53 11.41 61.1      7.6    16.6  97.9  535         2      3    330    273\n54   54 12.07 43.7      7.8    52.4 105.3  157         2      2    115     76\n55   55  8.63 54.0      3.1     8.4  56.2   76         2      1     39     44\n56   56 11.15 56.5      3.9     7.7  73.9  281         2      1    217    199\n57   57  7.14 59.0      3.7     2.6  75.8   70         2      4     37     35\n58   58  7.65 47.1      4.3    16.4  65.7  318         2      4    265    314\n59   59 10.73 50.6      3.9    19.3 101.0  445         1      2    374    345\n60   60 11.46 56.9      4.5    15.6  97.7  191         2      3    153    132\n61   61 10.42 58.0      3.4     8.0  59.0  119         2      1     67     64\n62   62 11.18 51.0      5.7    18.8  55.9  595         1      2    546    392\n63   63  7.93 64.1      5.4     7.5  98.1   68         2      4     42     49\n64   64  9.66 52.1      4.4     9.9  98.3   83         2      2     66     95\n65   65  7.78 45.5      5.0    20.9  71.6  489         2      3    391    329\n66   66  9.42 50.6      4.3    24.8  62.8  508         2      1    421    528\n67   67 10.02 49.5      4.4     8.3  93.0  265         2      2    191    202\n68   68  8.58 55.0      3.7     7.4  95.9  304         2      3    248    218\n69   69  9.61 52.4      4.5     6.9  87.2  487         2      3    404    220\n70   70  8.03 54.2      3.5    24.3  87.3   97         2      1     65     55\n71   71  7.39 51.0      4.2    14.6  88.4   72         2      2     38     67\n72   72  7.08 52.0      2.0    12.3  56.4   87         2      3     52     57\n73   73  9.53 51.5      5.2    15.0  65.7  298         2      3    241    193\n74   74 10.05 52.0      4.5    36.7  87.5  184         1      1    144    151\n75   75  8.45 38.8      3.4    12.9  85.0  235         2      2    143    124\n76   76  6.70 48.6      4.5    13.0  80.8   76         2      4     51     79\n77   77  8.90 49.7      2.9    12.7  86.9   52         2      1     37     35\n78   78 10.23 53.2      4.9     9.9  77.9  752         1      2    595    446\n79   79  8.88 55.8      4.4    14.1  76.8  237         2      2    165    182\n80   80 10.30 59.6      5.1    27.8  88.9  175         2      2    113     73\n81   81 10.79 44.2      2.9     2.6  56.6  461         1      2    320    196\n82   82  7.94 49.5      3.5     6.2  92.3  195         2      2    139    116\n83   83  7.63 52.1      5.5    11.6  61.1  197         2      4    109    110\n84   84  8.77 54.5      4.7     5.2  47.0  143         2      4     85     87\n85   85  8.09 56.9      1.7     7.6  56.9   92         2      3     61     61\n86   86  9.05 51.2      4.1    20.5  79.8  195         2      3    127    112\n87   87  7.91 52.8      2.9    11.9  79.5  477         2      3    349    188\n88   88 10.39 54.6      4.3    14.0  88.3  353         2      2    223    200\n89   89  9.36 54.1      4.8    18.3  90.6  165         2      1    127    158\n90   90 11.41 50.4      5.8    23.8  73.0  424         1      3    359    335\n91   91  8.86 51.3      2.9     9.5  87.5  100         2      3     65     53\n92   92  8.93 56.0      2.0     6.2  72.5   95         2      3     59     56\n93   93  8.92 53.9      1.3     2.2  79.5   56         2      2     40     14\n94   94  8.15 54.9      5.3    12.3  79.8   99         2      4     55     71\n95   95  9.77 50.2      5.3    15.7  89.7  154         2      2    123    148\n96   96  8.54 56.1      2.5    27.0  82.5   98         2      1     57     75\n97   97  8.66 52.8      3.8     6.8  69.5  246         2      3    178    177\n98   98 12.01 52.8      4.8    10.8  96.9  298         2      1    237    115\n99   99  7.95 51.8      2.3     4.6  54.9  163         2      3    128     93\n100 100 10.15 51.9      6.2    16.4  59.2  568         1      3    452    371\n101 101  9.76 53.2      2.6     6.9  80.1   64         2      4     47     55\n102 102  9.89 45.2      4.3    11.8 108.7  190         2      1    141    112\n103 103  7.14 57.6      2.7    13.1  92.6   92         2      4     40     50\n104 104 13.95 65.9      6.6    15.6 133.5  356         2      1    308    182\n105 105  9.44 52.5      4.5    10.9  58.5  297         2      3    230    263\n106 106 10.80 63.9      2.9     1.6  57.4  130         2      3     69     62\n107 107  7.14 51.7      1.4     4.1  45.7  115         2      3     90     19\n108 108  8.02 55.0      2.1     3.8  46.5   91         2      2     44     32\n109 109 11.80 53.8      5.7     9.1 116.9  571         1      2    441    469\n110 110  9.50 49.3      5.8    42.0  70.9   98         2      3     68     46\n111 111  7.70 56.9      4.4    12.2  67.9  129         2      4     85    136\n113 113  9.41 59.5      3.1    20.6  91.7   29         2      3     20     22\n    Facilities\n1         60.0\n2         40.0\n3         20.0\n4         40.0\n5         40.0\n6         40.0\n7         40.0\n8         60.0\n9         40.0\n10        40.0\n11        80.0\n12        40.0\n13        57.1\n14        37.1\n15        17.1\n16        57.1\n17        37.1\n18        37.1\n19        37.1\n20        77.1\n21        17.1\n22        37.1\n23        57.1\n24        57.1\n25        57.1\n26        57.1\n27        37.1\n28        37.1\n29        37.1\n30        37.1\n31        57.1\n32        54.3\n33        34.3\n34        54.3\n35        54.3\n36        54.3\n37        34.3\n38        34.3\n39        54.3\n40        14.3\n41        34.3\n42        34.3\n43        34.3\n44        54.3\n45        34.3\n46        74.3\n48        51.4\n49        11.4\n50        51.4\n51        51.4\n52        71.4\n53        51.4\n54        31.4\n55        31.4\n56        51.4\n57        31.4\n58        51.4\n59        51.4\n60        31.4\n61        31.4\n62        68.6\n63        28.6\n64        28.6\n65        48.6\n66        48.6\n67        48.6\n68        48.6\n69        48.6\n70        28.6\n71        28.6\n72        28.6\n73        48.6\n74        68.6\n75        48.6\n76        28.6\n77        28.6\n78        68.6\n79        48.6\n80        45.7\n81        65.7\n82        45.7\n83        45.7\n84        25.7\n85        45.7\n86        45.7\n87        65.7\n88        65.7\n89        45.7\n90        45.7\n91        25.7\n92        25.7\n93         5.7\n94        25.7\n95        25.7\n96        45.7\n97        45.7\n98        45.7\n99        42.9\n100       62.9\n101       22.9\n102       42.9\n103       22.9\n104       62.9\n105       42.9\n106       22.9\n107       22.9\n108       22.9\n109       62.9\n110       22.9\n111       62.9\n113       22.9\n\n\n\nCreate new dummy/indicator columns (i1, i2, i3, i4) for regions using ifelse() function. For example, i1 = 1 when Region = 1 and i1 = 0 when Region is not equal to 1; i2 = 1 when Region = 2 and i2 = 0 when Region is not equal to 2; …\n\n\ninfectionrisk$i1 &lt;- ifelse(infectionrisk$Region == 1, 1, 0)\ninfectionrisk$i2 &lt;- ifelse(infectionrisk$Region == 2, 1, 0)\ninfectionrisk$i3 &lt;- ifelse(infectionrisk$Region == 3, 1, 0)\ninfectionrisk$i4 &lt;- ifelse(infectionrisk$Region == 4, 1, 0)\n\n\nFit a multiple linear regression model of InfctRsk on Stay + Xray + i2 + i3 + i4.\n\n\nmodel.7 &lt;- lm(InfctRsk ~ Stay + Xray + i2 + i3 + i4, data=infectionrisk)\nsummary(model.7)\n\n\nCall:\nlm(formula = InfctRsk ~ Stay + Xray + i2 + i3 + i4, data = infectionrisk)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.66492 -0.65420  0.04265  0.64034  2.51391 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.134259   0.877347  -2.433  0.01668 *  \nStay         0.505394   0.081455   6.205 1.11e-08 ***\nXray         0.017587   0.005649   3.113  0.00238 ** \ni2           0.171284   0.281475   0.609  0.54416    \ni3           0.095461   0.288852   0.330  0.74169    \ni4           1.057835   0.378077   2.798  0.00612 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.036 on 105 degrees of freedom\nMultiple R-squared:  0.4198,    Adjusted R-squared:  0.3922 \nF-statistic: 15.19 on 5 and 105 DF,  p-value: 3.243e-11\n\n\n\nCan we include i1 + i2 + i3 + i4 in this multiple linear regression? Why?\n\nNo. In the context of using dummy variables for categorical data in regression analysis, it’s essential to designate a reference category. This reference category is represented by a coefficient of zero, while the coefficients for the other categories are interpreted as deviations from this reference point.\n\nmodel.8 &lt;- lm(InfctRsk ~ Stay + Xray + i1 + i2 + i3 + i4, data=infectionrisk)\nsummary(model.8)\n\n\nCall:\nlm(formula = InfctRsk ~ Stay + Xray + i1 + i2 + i3 + i4, data = infectionrisk)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.66492 -0.65420  0.04265  0.64034  2.51391 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.076424   0.721361  -1.492  0.13864    \nStay         0.505394   0.081455   6.205 1.11e-08 ***\nXray         0.017587   0.005649   3.113  0.00238 ** \ni1          -1.057835   0.378077  -2.798  0.00612 ** \ni2          -0.886551   0.339887  -2.608  0.01042 *  \ni3          -0.962374   0.323365  -2.976  0.00362 ** \ni4                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.036 on 105 degrees of freedom\nMultiple R-squared:  0.4198,    Adjusted R-squared:  0.3922 \nF-statistic: 15.19 on 5 and 105 DF,  p-value: 3.243e-11\n\n\n\nConduct an F-test (use anova() function) to see if at least one of i2, i3, and i4 are useful.\n\n\nmodel.9 &lt;- lm(InfctRsk ~ Stay + Xray, data=infectionrisk)\nanova(model.7, model.9)\n\nAnalysis of Variance Table\n\nModel 1: InfctRsk ~ Stay + Xray + i2 + i3 + i4\nModel 2: InfctRsk ~ Stay + Xray\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    105 112.71                              \n2    108 123.56 -3   -10.849 3.3687 0.02135 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "mth1113.html#example-2",
    "href": "mth1113.html#example-2",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "example",
    "text": "example\n\nLung Cancer Classification (https://www.kaggle.com/datasets/mysarahmadbhat/lung-cancer?select=survey+lung+cancer.csv)\nThe effectiveness of cancer prediction system helps the people to know their cancer risk with low cost and it also helps the people to take the appropriate decision based on their cancer risk status. The data is collected from the website online lung cancer prediction system.\nTotal no. of attributes: 16 No. of instances: 284\nAttribute information:\nGender: M(male), F(female) Age: Age of the patient Smoking: YES=2, NO=1. Yellow fingers: YES=2, NO=1. Anxiety: YES=2, NO=1. Peer_pressure: YES=2, NO=1. Chronic Disease: YES=2, NO=1. Fatigue: YES=2, NO=1. Allergy: YES=2, NO=1. Wheezing: YES=2, NO=1. Alcohol: YES=2, NO=1. Coughing: YES=2, NO=1. Shortness of Breath: YES=2, NO=1. Swallowing Difficulty: YES=2, NO=1. Chest pain: YES=2, NO=1. Lung Cancer: YES, NO.\nGoal: It is your job to classify Lung Cancer using other variables\n\nExample Code\n\n#Load the dataset \nlibrary(readr)\ndata = read_csv('/Users/luyu/Desktop/APH——course/survey_lung_cancer.csv', show_col_types = FALSE)\ndata$LUNG_CANCER &lt;- ifelse(data$LUNG_CANCER==\"YES\", 1, 0)\nsummary(data)\n\n    GENDER               AGE           SMOKING      YELLOW_FINGERS\n Length:309         Min.   :21.00   Min.   :1.000   Min.   :1.00  \n Class :character   1st Qu.:57.00   1st Qu.:1.000   1st Qu.:1.00  \n Mode  :character   Median :62.00   Median :2.000   Median :2.00  \n                    Mean   :62.67   Mean   :1.563   Mean   :1.57  \n                    3rd Qu.:69.00   3rd Qu.:2.000   3rd Qu.:2.00  \n                    Max.   :87.00   Max.   :2.000   Max.   :2.00  \n    ANXIETY      PEER_PRESSURE   CHRONIC DISEASE    FATIGUE     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.498   Mean   :1.502   Mean   :1.505   Mean   :1.673  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n    ALLERGY         WHEEZING     ALCOHOL CONSUMING    COUGHING    \n Min.   :1.000   Min.   :1.000   Min.   :1.000     Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000     1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000     Median :2.000  \n Mean   :1.557   Mean   :1.557   Mean   :1.557     Mean   :1.579  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000     3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000     Max.   :2.000  \n SHORTNESS OF BREATH SWALLOWING DIFFICULTY   CHEST PAIN     LUNG_CANCER    \n Min.   :1.000       Min.   :1.000         Min.   :1.000   Min.   :0.0000  \n 1st Qu.:1.000       1st Qu.:1.000         1st Qu.:1.000   1st Qu.:1.0000  \n Median :2.000       Median :1.000         Median :2.000   Median :1.0000  \n Mean   :1.641       Mean   :1.469         Mean   :1.557   Mean   :0.8738  \n 3rd Qu.:2.000       3rd Qu.:2.000         3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :2.000       Max.   :2.000         Max.   :2.000   Max.   :1.0000  \n\n\nlibrary(ggplot2)\nggplot(data, aes(x = factor(SMOKING), fill = factor(LUNG_CANCER))) +\ngeom_bar(position = \"fill\") +\nscale_fill_manual(values = c(\"0\" = \"lightblue\", \"1\" = \"salmon\")) +\ntheme_minimal()\n\n### Data SAMPLING ####\nlibrary(caret)\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nThe following object is masked from 'package:mosaic':\n\n    dotPlot\n\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nset.seed(101)\nsplit = createDataPartition(data$LUNG_CANCER, p = 0.80, list = FALSE)\ntrain_data = data[split,]\ntest_data = data[-split,]\nnrow(train_data)\n\n[1] 248\n\nnrow(test_data)\n\n[1] 61\n\n\n\n#error metrics -- Confusion Matrix\nerr_metric=function(CM)\n{\n  TN =CM[1,1]\n  TP =CM[2,2]\n  FP =CM[1,2]\n  FN =CM[2,1]\n  precision =(TP)/(TP+FP)\n  recall_score =(TP)/(TP+FN)\n  f1_score=2*((precision*recall_score)/(precision+recall_score))\n  accuracy_model  =(TP+TN)/(TP+TN+FP+FN)\n  False_positive_rate =(FP)/(FP+TN)\n  False_negative_rate =(FN)/(FN+TP)\n  print(paste(\"Precision value of the model: \",round(precision,2)))\n  print(paste(\"Accuracy of the model: \",round(accuracy_model,2)))\n  print(paste(\"Recall value of the model: \",round(recall_score,2)))\n  print(paste(\"False Positive rate of the model: \",round(False_positive_rate,2)))\n  print(paste(\"False Negative rate of the model: \",round(False_negative_rate,2)))\n  print(paste(\"F1 score of the model: \",round(f1_score,2)))\n}\n\n\n# Logistic regression\nlogit_m =glm(formula = LUNG_CANCER ~ ., data = train_data, family = 'binomial')\nsummary(logit_m)\n\n\nCall:\nglm(formula = LUNG_CANCER ~ ., family = \"binomial\", data = train_data)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -32.13449    6.57393  -4.888 1.02e-06 ***\nGENDERM                  -0.84716    0.82793  -1.023 0.306203    \nAGE                       0.01537    0.03490   0.440 0.659756    \nSMOKING                   1.10495    0.82246   1.343 0.179119    \nYELLOW_FINGERS            1.10971    0.82005   1.353 0.175982    \nANXIETY                   2.08645    1.08610   1.921 0.054725 .  \nPEER_PRESSURE             1.94159    0.74009   2.623 0.008704 ** \n`CHRONIC DISEASE`         3.91378    1.12190   3.489 0.000486 ***\nFATIGUE                   2.62906    0.89032   2.953 0.003148 ** \nALLERGY                   1.42702    0.83764   1.704 0.088454 .  \nWHEEZING                  1.07933    0.90761   1.189 0.234357    \n`ALCOHOL CONSUMING`       2.41116    0.98495   2.448 0.014365 *  \nCOUGHING                  3.14783    1.22386   2.572 0.010110 *  \n`SHORTNESS OF BREATH`    -0.22681    0.84245  -0.269 0.787757    \n`SWALLOWING DIFFICULTY`   2.24613    1.24347   1.806 0.070865 .  \n`CHEST PAIN`              0.89356    0.73930   1.209 0.226791    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 198.230  on 247  degrees of freedom\nResidual deviance:  77.353  on 232  degrees of freedom\nAIC: 109.35\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n# Logistic regression\nlogit_m2 =glm(formula = LUNG_CANCER ~ ANXIETY+PEER_PRESSURE+`CHRONIC DISEASE`+FATIGUE+ALLERGY+`ALCOHOL CONSUMING`+COUGHING+`SWALLOWING DIFFICULTY`, data = train_data, family = 'binomial')\nsummary(logit_m2)\n\n\nCall:\nglm(formula = LUNG_CANCER ~ ANXIETY + PEER_PRESSURE + `CHRONIC DISEASE` + \n    FATIGUE + ALLERGY + `ALCOHOL CONSUMING` + COUGHING + `SWALLOWING DIFFICULTY`, \n    family = \"binomial\", data = train_data)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -27.3830     5.5206  -4.960 7.05e-07 ***\nANXIETY                   2.5514     0.8659   2.947 0.003213 ** \nPEER_PRESSURE             2.1822     0.7245   3.012 0.002593 ** \n`CHRONIC DISEASE`         3.5120     0.9696   3.622 0.000292 ***\nFATIGUE                   2.4939     0.6979   3.573 0.000353 ***\nALLERGY                   2.0104     0.7428   2.707 0.006796 ** \n`ALCOHOL CONSUMING`       2.5084     0.8653   2.899 0.003744 ** \nCOUGHING                  3.2220     0.9540   3.377 0.000732 ***\n`SWALLOWING DIFFICULTY`   2.5273     1.0096   2.503 0.012309 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 198.230  on 247  degrees of freedom\nResidual deviance:  83.501  on 239  degrees of freedom\nAIC: 101.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nlibrary(dplyr)\nlogit_P_prob = predict(logit_m, newdata = select(test_data, -LUNG_CANCER), type = 'response')\nlogit_P_prob[1:3]\n\n        1         2         3 \n0.9912600 0.9997708 0.9980692 \n\nlogit_P &lt;- ifelse(logit_P_prob &gt; 0.5, 1, 0) # Probability check\nlogit_P[1:3]\n\n1 2 3 \n1 1 1 \n\n\n\nCM = table(test_data$LUNG_CANCER, logit_P)\nprint(CM)\n\n   logit_P\n     0  1\n  0  3  2\n  1  1 55\n\n\n\nerr_metric(CM)\n\n[1] \"Precision value of the model:  0.96\"\n[1] \"Accuracy of the model:  0.95\"\n[1] \"Recall value of the model:  0.98\"\n[1] \"False Positive rate of the model:  0.4\"\n[1] \"False Negative rate of the model:  0.02\"\n[1] \"F1 score of the model:  0.97\"\n\n\n\n#ROC-curve using pROC library\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following object is masked from 'package:colorspace':\n\n    coords\n\n\nThe following objects are masked from 'package:mosaic':\n\n    cov, var\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nroc_score=roc(test_data$LUNG_CANCER, logit_P_prob) #AUC score\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_score, main = \"ROC curve -- Logistic Regression\")"
  },
  {
    "objectID": "mth1113.html#case-1",
    "href": "mth1113.html#case-1",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "case 1",
    "text": "case 1\n\\(M_x\\) may not exist. When it exists in a neighborhood of 0, using talor\n\\[e^{tx}=1+tX+(tX)^2/2+...\\]\n\\[M_x(t)=1+t\\mu+t^2 \\mu/2+...\\]\n\\(\\mu_j = E(X^i)\\) is the j-th moment of X. Therefore,\n\\[E(X^i)=M^{(j)}(0)\\]\neg. Then we could also get the variance by take the 2nd order derivatives"
  },
  {
    "objectID": "mth1113.html#case-2",
    "href": "mth1113.html#case-2",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "case 2",
    "text": "case 2\n\\(\\int\\)\neg. normal\n\nX~N(0,1)\n\nidea: try to write an integral of a certain distribution’s pdf and we get the result of this integral as 1. here, we get the pdf of the N(t,1) and finally we get \\(e^{t^2/2}\\)\n\nX~N(\\(\\mu, \\sigma^2\\))\n\nThen X=\\(\\mu+\\sigma Z\\) where Z~N(0,1)\n\\[M_x(t)=E[e^{tx}]=e^{\\mu t} E[e^{\\sigma t Z}]=e^{\\mu t} M_Z(\\sigma t)=e^{\\mu t +\\sigma^2 t^2/2}\\]\n\nGamma disteibution\nthe family of gamma distributions generalizes the family of exponential distributions. The gamma distribution with shape r and rate \\(\\lambda\\)\n\naddition rule\nr +\n\\(\\lambda\\) stays"
  },
  {
    "objectID": "mth1113.html#why-mgf-is-useful-to-determine-if-two-random-variables-have-the-identical-cdf-to-prove-the-addition-property-of-distributions",
    "href": "mth1113.html#why-mgf-is-useful-to-determine-if-two-random-variables-have-the-identical-cdf-to-prove-the-addition-property-of-distributions",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "why MGF is useful? – to determine if two random variables have the identical CDF / to prove the addition property of distributions",
    "text": "why MGF is useful? – to determine if two random variables have the identical CDF / to prove the addition property of distributions\nMGF includes all characteristics of a distribution, from whom we could get pdf, cdf, expectation, variance\n\nThm If X and Y are random variables with the same MGF, which is finite on [-t, t ] for some t &gt;0 then X and Y have the same distribution\n\nA gamma distribution with shape r =1 is an exponential distribution\nA more general function than MGF is the characteristic function.\n\\(\\phi_X (t) = E(e^{itX})\\)"
  },
  {
    "objectID": "mth1113.html#十堂极简概率课-中信出版-diaconis",
    "href": "mth1113.html#十堂极简概率课-中信出版-diaconis",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "十堂极简概率课 中信出版 diaconis",
    "text": "十堂极简概率课 中信出版 diaconis"
  },
  {
    "objectID": "mth1113.html#心理统计",
    "href": "mth1113.html#心理统计",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "心理统计",
    "text": "心理统计"
  },
  {
    "objectID": "mth1113.html#the-lady-tasting-tea",
    "href": "mth1113.html#the-lady-tasting-tea",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "The lady tasting tea",
    "text": "The lady tasting tea"
  },
  {
    "objectID": "mth1113.html#概率论与数理统计第三版-峁诗松等老师编著",
    "href": "mth1113.html#概率论与数理统计第三版-峁诗松等老师编著",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "概率论与数理统计（第三版） 峁诗松等老师编著",
    "text": "概率论与数理统计（第三版） 峁诗松等老师编著"
  },
  {
    "objectID": "mth1113.html#probability.",
    "href": "mth1113.html#probability.",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Probability….",
    "text": "Probability…."
  },
  {
    "objectID": "mth1113.html#background",
    "href": "mth1113.html#background",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "background",
    "text": "background\n\nRegression to the mean (Galton’s thinking)"
  },
  {
    "objectID": "mth1113.html#it-is-not-stable-to-predict-the-data-outside-our-data-sample",
    "href": "mth1113.html#it-is-not-stable-to-predict-the-data-outside-our-data-sample",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "it is not stable to predict the data outside our data sample’",
    "text": "it is not stable to predict the data outside our data sample’"
  },
  {
    "objectID": "mth1113.html#residual-plot-should-have-no-pattern",
    "href": "mth1113.html#residual-plot-should-have-no-pattern",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Residual plot should have no pattern",
    "text": "Residual plot should have no pattern\nacross the whole range, it could not be showing a certain trend or a specific shape.\npositive and negative points sperate averagely .\n\n\n\n# 数据\nx &lt;- c(50, 55, 50, 79, 44, 37, 70, 45, 49)  # Rock surface area\ny &lt;- c(152, 48, 22, 35, 38, 171, 13, 185, 25)  # Algae colony density\n\n# (a) 计算最小二乘回归方程\nmodel &lt;- lm(y ~ x)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-65.53 -63.91 -14.47  46.99  84.39 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  232.258     92.390   2.514   0.0402 *\nx             -2.926      1.690  -1.731   0.1271  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63.32 on 7 degrees of freedom\nMultiple R-squared:  0.2998,    Adjusted R-squared:  0.1997 \nF-statistic: 2.997 on 1 and 7 DF,  p-value: 0.1271\n\n# 获取回归系数\nintercept &lt;- coef(model)[1]\nslope &lt;- coef(model)[2]\ncat(\"最小二乘回归方程: y =\", intercept, \"+\", slope, \"* x\\n\")\n\n最小二乘回归方程: y = 232.2575 + -2.925507 * x\n\n# (b) 计算 R^2 值并解释\nr_squared &lt;- summary(model)$r.squared\ncat(\"R^2 值:\", r_squared, \"\\n\")\n\nR^2 值: 0.2997552 \n\ncat(\"解释: R^2 表示了\", round(r_squared * 100, 2), \"% 的 y 的变异可以通过 x 来解释。\\n\")\n\n解释: R^2 表示了 29.98 % 的 y 的变异可以通过 x 来解释。\n\n# (c) 计算残差标准误差 s_e\nse &lt;- summary(model)$sigma\ncat(\"残差标准误差 s_e:\", se, \"\\n\")\n\n残差标准误差 s_e: 63.31527 \n\ncat(\"解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\\n\")\n\n解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\n\n# (d) 判断线性关系的方向和强度\ncorrelation &lt;- cor(x, y)\ncat(\"相关系数 r:\", correlation, \"\\n\")\n\n相关系数 r: -0.547499 \n\nif (correlation &gt; 0) {\n  direction &lt;- \"正相关\"\n} else {\n  direction &lt;- \"负相关\"\n}\n\nif (abs(correlation) &gt; 0.7) {\n  strength &lt;- \"强相关\"\n} else if (abs(correlation) &gt; 0.3) {\n  strength &lt;- \"中等相关\"\n} else {\n  strength &lt;- \"弱相关\"\n}\n\ncat(\"线性关系:\", direction, \"且为\", strength, \"\\n\")\n\n线性关系: 负相关 且为 中等相关 \n\n\n\n# 数据\nquality_rating &lt;- c(111, 113, 93, 130, 170, 87, 83, 117, 135, 109)\nsatisfaction_rating &lt;- c(832, 845, 794, 854, 836, 842, 877, 745, 797, 795)\n\n# 计算相关系数\ncorrelation_coefficient &lt;- cor(quality_rating, satisfaction_rating)\nprint(paste(\"相关系数 r:\", correlation_coefficient))\n\n[1] \"相关系数 r: -0.115403519735578\"\n\n# 绘制散点图\nplot(quality_rating, satisfaction_rating,\n     main = \"Scatterplot of Quality Rating vs. Satisfaction Rating\",\n     xlab = \"Quality Rating\",\n     ylab = \"Satisfaction Rating\",\n     pch = 19, col = \"blue\")\nabline(lm(satisfaction_rating ~ quality_rating), col = \"red\")  # 添加回归线"
  },
  {
    "objectID": "mth1113.html#z-scores",
    "href": "mth1113.html#z-scores",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "z Scores",
    "text": "z Scores\neg. height(among women or men (come from different populations)instead of just comparing the height itself)"
  },
  {
    "objectID": "mth1113.html#intro",
    "href": "mth1113.html#intro",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Intro",
    "text": "Intro\nstat is a large field in math involving the collection, organization, analysis,interpretation, and presentation of data(a collection of observations on one or more variables(A characteristic whose value may change from one observation to another))\nStatistics is the scientific discipline that provides methods to help us make sense of data.\nIt is important to be able to:\n1 Extract information from tables, charts, and graphs.\n2 Follow numerical arguments.\n3 Understand the basics of how data should be gathered, summarized, and analysed to draw statistical conclusions.\nThe Data Analysis Process\n1 Understanding the nature of the research problem or goals.\n2 Deciding what to measure and how.\n3 Collecting data.\n4 Data summarization and preliminary analysis.\n5 Formal Data Analysis (Statistical Methods).\n6 Interpretation of the results."
  },
  {
    "objectID": "mth1113.html#populations-and-samples",
    "href": "mth1113.html#populations-and-samples",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "populations and samples",
    "text": "populations and samples\npopulation: The entire collection of individuals or objects about which information is desired\nsample: A sample is a subset of the population, selected for study.\nthen select the sample\nthen we could summarize it using 2 branches of stat.— Decriptive stat.(methods for organizing and summarizing data.) or inferential stat.(generalizing from a sample(incomplete information) to the population from which the sample was selected and assessing the reliability of such generalizations.So we run the risk(An important aspect of statistics and making statistics inferences involves quantifying the chance of making an incorrect conclusions.))\n\ndescriptive stat\n\n\ninferential stat\nsample"
  },
  {
    "objectID": "mth1113.html#types-of-data",
    "href": "mth1113.html#types-of-data",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Types of data",
    "text": "Types of data\n\nuni data set and bivariate and multivariate\n\n\ncategorical and numerical(discrete and continuous) with plot using excel (data analysis) or rstudio plot (ggplot2)\nfor categorial data we could use a bar chart which is a graph of a frequency distribution for categorical data.\nfor a small numerical data we could use dotplot\n\ndiscrete\n\n\nlibrary(ggplot2)\n\n# creat data：Wechat number\ndiscrete_data &lt;- data.frame(value = c(30, 15, 20,30,60))\n\n# plot\nggplot(discrete_data, aes(x = value)) +\n  geom_dotplot(binwidth = 1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Discrete Data (Number of Wechats)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ncontinuous\n\n\nall_athletes &lt;- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball &lt;- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 设置画布的高度，以便将两个图绘制在同一页面上\nplot.new()\nplot.window(xlim = c(0, 100), ylim = c(0.5, 2.5))\n\n# 绘制 Basketball 数据的 dotplot\nstripchart(basketball, method = \"stack\", at = 2, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 绘制 All Athletes 数据的 dotplot\nstripchart(all_athletes, method = \"stack\", at = 1, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 添加 X 轴\naxis(1, at = seq(10, 100, by = 10), labels = seq(10, 100, by = 10))\n\n# 添加标签\ntext(-5, 2, \"Basketball\", xpd = TRUE, adj = 1)\ntext(-5, 1, \"All Athletes\", xpd = TRUE, adj = 1)\n\n# 添加横线\nabline(h = 1.5, col = \"black\", lwd = 2)\n\n# 添加 X 轴标签\ntitle(xlab = \"Graduation rates (%)\")\n\n\n\n\n\n\n\n\n\n# creat data--time spent in minutes\ncontinuous_data &lt;- data.frame(value = c(6, 5.25, 3.62,1,2,3.1,3.2,4,5,6,7,4,10))\n\n# dotplot\nggplot(continuous_data, aes(x = value)) +\n  geom_dotplot(binwidth = 0.1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Continuous Data (Time Spent in Minutes)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\n# 毕业率数据\nschool &lt;- 33:68\nall_athletes &lt;- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball &lt;- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 创建数据框\ndata &lt;- data.frame(school, all_athletes, basketball)\n\n# 画图\nggplot() +\n  geom_dotplot(data = data, aes(x = all_athletes, y = \"All Athletes\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5) +\n  geom_dotplot(data = data, aes(x = basketball, y = \"Basketball\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5, color = \"red\") +\n  xlab(\"Graduation rates (%)\") +\n  ylab(\"\") +\n  theme_minimal() +\n  ggtitle(\"Dotplot of Graduation Rates for All Athletes and Basketball Players\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nhistogram excel plot"
  },
  {
    "objectID": "mth1113.html#collect-data-sensibly",
    "href": "mth1113.html#collect-data-sensibly",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "collect data sensibly",
    "text": "collect data sensibly"
  },
  {
    "objectID": "mth1113.html#two-types-of-studies-observational-studies-and-experiments.",
    "href": "mth1113.html#two-types-of-studies-observational-studies-and-experiments.",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Two types of studies: Observational studies and Experiments.",
    "text": "Two types of studies: Observational studies and Experiments.\n\nObservational\nA study in which the investigator observes characteristics of a sample selected from one or more existing populations. The goal is to draw conclusions about the corresponding population or about differences between two or more populations.\nIn an observational study, it is impossible to draw clear cause-and-effect conclusions\n\n\nExperiments\nA study in which the investigator observes how a response variable behaves when one or more explanatory variables, also called factors, are manipulated.\nA well-designed experiment can result in data that provide evidence for a cause-and-effect relationship.\n\n\nExperimental conditions: Any particular combination of values for the explanatory variables, which are also called treatments.\n\n\n\ncomparison\n\nBoth observational studies and experiments can be used to compare groups, but in an experiment the researcher controls who is in which group, whereas this is not the case in an observational study.\nIn an observational study, it is impossible to draw clear cause-and\u0002effect conclusions\n\n\n\nconfounding vars\nA variable that is related to both how the experimental groups were formed and the response variable of interest.\n\nTwo methods for data collection: Sampling and Experimentation.\ndistinguish between selection bias, measurement or response bias, and non-response bias.\nselect a simple random sample from a given population.\ndistinguish between simple random sampling, stratified random sampling, cluster sampling, systematic sampling, and convenience sampling"
  },
  {
    "objectID": "mth1113.html#variable",
    "href": "mth1113.html#variable",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "variable",
    "text": "variable\n\nresponse variable–y\nThe response variable is the focus of a question in a study or experiment.\n\n\nexplanotory variable–x\nAn explanatory variable is one that explains for changes in the response variable.\n\n\nexperiments and obeservational study\n\n\nbias\nselection bias：When the way the sample is selected systematically excludes some part of the population of interest.\nmeasurement or response bias\neg: survey question/scale(The scale or a machine used for measurements is not calibrated properly)\nNon-response Bias:When responses are not obtained from all individuals selected for inclusion in the sample.\nnon-response bias can distort results if those who respond differ in important ways from those who do not respond (e.g. laziness a confounding vari\u0002able).\n\n\nrandom sampling\ndef: A sample that is selected from a population in a way that ensures that every different possible sample of size n has the same chance of being selected.\nthe same chance to be selected\ncounter eg:\nConsider 100 students in a classroom, 60 females and 40 males. If we randomly sample 6 females, and 4 males, then each female has a 6/60 = 0.1 chance of being selected. Same for males, 4/40=0.1. However, not every group of 10 students is equally likely to be selected. This is not simple random sampling\nThe random selection process allows us to be confident that the sample adequately reflects the population, even when the sample consists of only a small fraction of the population.\neg.Voting Sample Size in a country\n\n\nstratified and cluster\n\nstratified random sampling:In stratified random sampling, separate simple random samples are independently selected from each subgroup. Each subgroup is called a strata.\n\nIn general, it is much easier to produce relatively accurate es\u0002timates of characteristics of a homogeneous group than of a heterogeneous group.\nstratified: according to certain characteristic\neg.Even with a small sample, it is possible to obtain an accurate estimate of the average grade point average (GPA) of students graduating with high honours from a university (Similar high grades, homogenous, thus only sample a few students). On the other hand, producing a reasonably accurate estimate of the average GPA of all seniors at the university, a much more diverse group of GPAs, is a more difficult task. Not only does this ensure that students at each GPA level are represented, it also allows for a more accurate estimate of the overall average GPA.\n\ncluster reflect general characteristic about the whole entire population\n\ncluster: randomly groups\nCluster sampling involves dividing the population of interest into non-overlapping subgroups, called clusters. Clusters are then selected at random, and then all individuals in the selected clusters are included in the sample.\n\n\nsystematic sampling\nA value k is specified (e.g. k = 50 or k = 200). Then one of the first k individuals is selected at random, after which every k-th individual in the sequence is included in the sample. A sample selected in this way is called a 1 in k systematic sample.\nIn the case of large samples, it can ensure that the sample is evenly distributed in the population."
  },
  {
    "objectID": "mth1113.html#random-variable-r.v.",
    "href": "mth1113.html#random-variable-r.v.",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Random Variable (R.V.)",
    "text": "Random Variable (R.V.)\nA numerical variable whose value depends on the outcome of a chance experiment. A random variable associates a numerical value with each outcome of a chance experiment. (Think of it as a rule that translates each result of a chance event into a number.)\nIn shorta: random variables convert random events into numbers.\nA real-valued random variable X is a function: X : S → \\(\\mathbb R\\), where S is the sample space of a chance experiment.\nContinuous random variable X: S–&gt; R is continuous if its set of possible values includes an entire interval on the number line(measurement), which could not be count.\nDiscrete random variable: X: S–&gt; R if its set of possible value is a collection of isolated points along the number line.(counting)\n\neg\nExamples: Coin Tossing (Discrete Random Variable) If we flip a coin 5 times, let X be the number of heads we get. Possible values of X {0,1,2,3,4,5} (where 0 means no heads, and 5 means all heads). Here, X turns each outcome of multiple coin tosses into a count of heads.\nDeparture Time (Continuous Random Variable) Imagine tracking when people leave a subway station between 10 PM and 11 PM. Let Y represent the time (in hours) someone leaves, so Y can be any number from 10 to 11. Here, Y assigns each departure time to a point in the range [10,11]."
  },
  {
    "objectID": "mth1113.html#sample-space-of-multivariables",
    "href": "mth1113.html#sample-space-of-multivariables",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "sample space of multivariables",
    "text": "sample space of multivariables\nG:gender; F: year—corresponding to a certain student:S\n(G, H) : S → \\(\\mathbb R^2\\), S={1,2,3,4}\n…"
  },
  {
    "objectID": "mth1113.html#probability-mass-function-and-cumulative-distribution-function-for-discrete-random-variables",
    "href": "mth1113.html#probability-mass-function-and-cumulative-distribution-function-for-discrete-random-variables",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Probability Mass Function and Cumulative Distribution Function for Discrete Random Variables",
    "text": "Probability Mass Function and Cumulative Distribution Function for Discrete Random Variables\nProbability Mass Function (PMF): \\(p_x (x) := P(X = x) ,\\forall x\\)\nCumulative Distribution Function (CDF): \\(F_x (x) := P(X \\leq x) ,\\forall x\\)\n\n# 定义每个事件的概率和对应的 X 值\noutcomes &lt;- c(\"GGGG\", \"EGGG\", \"GEGG\", \"GGEG\", \"GGGE\", \n              \"EEGG\", \"EGEG\", \"EGGE\", \"GEEG\", \"GEGE\", \n              \"GGEE\", \"GEEE\", \"EEEG\", \"EEGE\", \"EEEE\")\nprobabilities &lt;- c(0.1296, 0.0864, 0.0864, 0.0864, 0.0864, \n                   0.0576, 0.0576, 0.0576, 0.0576, 0.0576, \n                   0.0384, 0.0384, 0.0384, 0.0384, 0.0256)\nX_values &lt;- c(0, 1, 1, 1, 1, \n              2, 2, 2, 2, 2,\n              3, 3, 3, 3, 4)\n\n# 计算每个 X 值的 PMF 通过分组和求和\npmf &lt;- tapply(probabilities, X_values, sum)\n\n# 定义可能的 X 值\nX_values_unique &lt;- sort(unique(X_values))\n\n# 计算 CDF\ncdf &lt;- cumsum(pmf)\n\n# 确保 CDF 在 x &gt; 4 时为 1\ncdf &lt;- c(cdf, 1)\n\n# 更新 X 值以包括 x &gt; 4 的情况\nX_values_unique &lt;- c(X_values_unique, \"&gt;4\")\n\n# 创建数据框显示 PMF 和 CDF\ntable &lt;- data.frame(\n  X = X_values_unique,\n  `PMF P(X=x)` = c(pmf, 1-0.1296-0.3456-0.2880-0.1536-0.0256),  # PMF 没有对应的 x &gt; 4 值，填 NA\n  `CDF F(X&lt;=x)` = cdf\n)\n\n# 打印表格\nprint(table)\n\n   X PMF.P.X.x. CDF.F.X..x.\n0  0     0.1296      0.1296\n1  1     0.3456      0.4752\n2  2     0.2880      0.7632\n3  3     0.1536      0.9168\n4  4     0.0256      0.9424\n  &gt;4     0.0576      1.0000\n\n\n\nNote that the domain of a cdf is (−∞, ∞)\n\n\\(\\mathrm{pmf} \\Longrightarrow \\mathrm{cdf}\\) \\[\nF_x(x)=\\sum_{y \\leq x} p_x(y)\n\\] cdf \\(\\Longrightarrow\\) pmf Suppose \\(X\\) takes ordered values \\(x_1, x_2, x_3, \\cdots\\), then \\[\n\\begin{aligned}\np_X\\left(x_i\\right) & =P\\left(X=x_i\\right)=P\\left(x_{i-1}&lt;X \\leq x_i\\right) \\\\\n& =P\\left(X \\leq x_i\\right)-P\\left(X \\leq x_{i-1}\\right) \\\\\n& =F\\left(x_i\\right)-F\\left(x_{i-1}\\right)\n\\end{aligned}\n\\]\n\nremark: The probability of a discrete distribution varies depending on the inclusion and exclusion of the boundary values."
  },
  {
    "objectID": "mth1113.html#expectation-and-variance-for-discrete-random-variables",
    "href": "mth1113.html#expectation-and-variance-for-discrete-random-variables",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Expectation and Variance for Discrete Random Variables",
    "text": "Expectation and Variance for Discrete Random Variables\n\\[\n\\frac{0 \\cdot f_0+1 \\cdot f_1+2 \\cdot f_2+\\cdots+n \\cdot f_n}{N}=\\frac{1}{N} \\sum_{i=0}^n i \\cdot f_i\n\\]\nNote that in \\(\\frac{1}{N} \\sum_{i=0}^n i \\cdot f_i\\), \\[\n\\lim _{N \\rightarrow \\infty} \\frac{f_i}{N}=P(X=i)\n\\]\nSo the average number will be \\[\n\\sum_{i=0}^n i \\cdot P(X=i)\n\\]\n\nDefinition: Expectation Given a discrete random variable \\(X\\), the expectation of \\(X\\) is \\[\nE[X]=\\sum_x x \\cdot p_X(x)\n\\]\nProperties of Expectation\nIf \\(c\\) is a constant, then \\(E[c]=c\\).\nIf \\(X \\geq 0\\) then \\(E[X] \\geq 0\\).\nIf \\(a \\leq X \\leq b\\) then \\(a \\leq E[X] \\leq b\\).\nProof of 3: First show \\(E[X] \\geq a\\), then show \\(E[X] \\leq b\\), \\[\n\\begin{aligned}\nE[X] & =\\sum_x x p_X(x) \\geq \\sum_x a p_X(x), \\\\\n& =a \\sum_x p_x(x)=a .\n\\end{aligned}\n\\]\n\nSimilarly, \\(E[X] \\leq b\\).\n\nSuppose \\(X\\) is a discrete random variable and \\(Y=g(X)\\), then \\[\n\\begin{aligned}\nE[Y] & =\\sum_y y p_Y(y)=\\sum_y y P(Y=y) \\\\\n& =\\sum_y y \\sum_{\\{x: g(x)=y\\}} P(X=x) \\\\\n& =\\sum_y \\sum_{\\{x: g(x)=y\\}} y P(X=x) \\\\\n& =\\sum_y \\sum_{\\{x: g(x)=y\\}} g(x) P(X=x) \\\\\n& =\\sum_x g(x) P(X=x)\n\\end{aligned}\n\\]\nFirst moment of \\(X\\) (mean): \\[\nE[X]=\\sum_x x p_X(x) .\n\\]\nSecond moment of \\(X\\) : \\[\nE\\left[X^2\\right]=\\sum_x x^2 p_x(x) .\n\\]\nIn general, \\(E[g(X)] \\neq g(E[X])\\). For example, let \\(g(x)=x^2\\), and consider \\(X\\) such that \\[\np_X(x)= \\begin{cases}0.5, & \\text { for } x=-1 \\\\ 0.5, & \\text { for } x=1\\end{cases}\n\\]\n\nThen clearly \\(E\\left[X^2\\right]=1 \\neq 0=(E[X])^2\\).\n\nThere are exceptions (e.g. when g is linear)!\nLinearity of Expectation\n\nE[aX + b] = aE[X] + b\nproof:\nSuppose \\(g(x)=a x+b\\). Then \\[\n\\begin{aligned}\nE[g(X)] & =\\sum_x g(x) p_X(x), \\\\\n& =\\sum_x(a x+b) p_x(x), \\\\\n& =\\sum_x a x p_x(x)+\\sum_x b p_x(x), \\\\\n& =a \\sum_x x p_x(x)+b \\sum_x p_x(x), \\\\\n& =a E[X]+b=g(E[X]),\n\\end{aligned}\n\\] this implies \\[\nE[a X+b]=a E[X]+b\n\\]\n\nRemark: Apart from this case, always assume \\(E[g(X)] \\neq g(E[X])\\)."
  },
  {
    "objectID": "mth1113.html#independence-expectationsmean-and-variance",
    "href": "mth1113.html#independence-expectationsmean-and-variance",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "independence, expectations(mean) and variance",
    "text": "independence, expectations(mean) and variance\nIndependence and Expectations If \\(X\\) and \\(Y\\) are independent, then \\[\nE[X Y]=E[X] E[Y] .\n\\]\nProof: We use \\(E[g(X, Y)]\\) where \\(g(x, y)=x y\\). \\[\n\\begin{aligned}\nE[X Y] & =\\sum_x \\sum_y x y p_{X, Y}(x, y) \\\\\n& =\\sum_x \\sum_y x y p_X(x) p_Y(y), \\quad(\\text { by inde } \\\\\n& =\\left(\\sum_x x p_X(x)\\right)\\left(\\sum_y y p_Y(y)\\right)=E[X] E[Y] .\n\\end{aligned}\n\\] (by independence)\nSimilarly, if \\(X\\) and \\(Y\\) are independent, then \\[\nE[g(X) h(Y)]=E[g(X)] E[h(Y)]\n\\]\n\nIndependence and Variances\n\nIt is always true that \\[\n\\operatorname{Var}(a X)=a^2 \\operatorname{Var}(X), \\quad \\text { and } \\quad \\operatorname{Var}(X+a)=\\operatorname{Var}(X)\n\\]\nIn general, when we have a sum of random variables \\(X\\) and \\(Y\\) \\[\n\\operatorname{Var}(X+Y) \\neq \\operatorname{Var}(X)+\\operatorname{Var}(Y) .\n\\]\nIt is only true if \\(X\\) and \\(Y\\) are independent\n\nSum of Variance for independent R.V. If two random variables \\(X\\) and \\(Y\\) are independent then \\[\n\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)\n\\]\nSum of Variance for independent R.V.\n\nIf two random variables \\(X\\) and \\(Y\\) are independent then \\[\n\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y) .\n\\]\nProof: Independence implies \\(E[X Y]=E[X] E[Y]\\). Thus \\[\n\\begin{aligned}\n& \\operatorname{Var}(X+Y)=E\\left[(X+Y-(E[X]+E[Y]))^2\\right], \\\\\n& =E\\left[(X-E[X])^2+2(X-E[X])(Y-E[Y])+(Y-E[Y])^2\\right], \\\\\n& =\\operatorname{Var}(X)+\\operatorname{Var}(Y)+2 E[(X-E[X])(Y-E[Y])]\n\\end{aligned}\n\\]\nAs \\(X\\) is indep to \\(Y\\), then \\(X-\\mu_X\\) is indep to \\(Y-\\mu_Y\\) so \\[\nE[(X-E[X])(Y-E[Y])]=E[X-E[X]] E[Y-E[Y]]=0\n\\]\nExample: Assume independence, \\(\\operatorname{Var}(3 X-5 Y)=\\) \\[\n\\operatorname{Var}(3 X)+\\operatorname{Var}(-5 Y)=9 \\operatorname{Var}(X)+25 \\operatorname{Var}(Y)\n\\]\n\nExample. Independence, mean and variance\n\nLet \\(Y\\) be the random variable denoting the total number of heads by tossing a coin \\(n\\) times. Find the mean and variance of \\(Y\\).\nLet \\[\nY_i= \\begin{cases}1, & \\text { if the } i^{\\text {th }} \\text { toss gets a head } \\\\ 0, & \\text { otherwise. }\\end{cases}\n\\]\nThen \\(Y=Y_1+Y_2+\\cdots+Y_n\\) where \\(Y_1, Y_2, \\cdots, Y_n\\) are independent. For any \\(i=1,2, \\cdots, n\\), we have \\[\n\\begin{array}{ccc}\ny & 0 & 1 \\\\\nP_{Y_i}(y) & 1 / 2 & 1 / 2\n\\end{array}\n\\]\nAs \\(E\\left[Y_i\\right]=\\frac{1}{2}\\) and \\(\\operatorname{Var}\\left(Y_i\\right)=\\frac{1}{4}\\) for \\(i=1,2, \\cdots, n\\) \\[\n\\begin{gathered}\nE[Y]=E\\left[Y_1\\right]+E\\left[Y_2\\right]+\\cdots+E\\left[Y_n\\right]=\\frac{n}{2} \\\\\n\\operatorname{Var}(Y)=\\operatorname{Var}\\left(Y_1\\right)+\\operatorname{Var}\\left(Y_2\\right)+\\cdots+\\operatorname{Var}\\left(Y_n\\right)=\\frac{n}{4}\n\\end{gathered}\n\\] (for not independent cases we have the same result as E but not Var because Var is not linear)"
  },
  {
    "objectID": "mth1113.html#definition.-probability-density-function-pdf",
    "href": "mth1113.html#definition.-probability-density-function-pdf",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Definition. Probability Density Function (pdf)",
    "text": "Definition. Probability Density Function (pdf)\nFor a continuous random variable X, the Probability Density Function (PDF) of X is f(x) where P(X = x) = 0 for all x and for any a ≤ b"
  },
  {
    "objectID": "mth1113.html#discrete-and-continuous",
    "href": "mth1113.html#discrete-and-continuous",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "discrete and continuous",
    "text": "discrete and continuous\n\nDensity function’s value could be greater than 1 because the integral of it in a very tiny interval could be very small, which represents the probability(which could not be greater than 1 and since the interval could be very tiny this condition is satisfied!).(This reminder notices that the value of pdf is not the probability since the integration is the probability which is totally 1)"
  },
  {
    "objectID": "mth1113.html#wait-for-reviewing..",
    "href": "mth1113.html#wait-for-reviewing..",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "wait for reviewing…..",
    "text": "wait for reviewing….."
  },
  {
    "objectID": "mth1113.html#bernoulli-is-composed-by-binomial",
    "href": "mth1113.html#bernoulli-is-composed-by-binomial",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Bernoulli is composed by binomial",
    "text": "Bernoulli is composed by binomial"
  },
  {
    "objectID": "mth1113.html#hypergeometric-distribution-could-be-approximated-by-binomial-when-samples-are-large",
    "href": "mth1113.html#hypergeometric-distribution-could-be-approximated-by-binomial-when-samples-are-large",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Hypergeometric distribution could be approximated by Binomial when samples are large",
    "text": "Hypergeometric distribution could be approximated by Binomial when samples are large"
  },
  {
    "objectID": "mth1113.html#poisson-distribution-as-the-limit-of-binomial-distribution-when-the-number-of-trials-is-large-and-the-probabiliy-of-success-of-each-trial-is-inverse-proportional-to-the-number-of-trials",
    "href": "mth1113.html#poisson-distribution-as-the-limit-of-binomial-distribution-when-the-number-of-trials-is-large-and-the-probabiliy-of-success-of-each-trial-is-inverse-proportional-to-the-number-of-trials",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Poisson distribution as the limit of Binomial distribution when the number of trials is large and the probabiliy of success of each trial is inverse-proportional to the number of trials",
    "text": "Poisson distribution as the limit of Binomial distribution when the number of trials is large and the probabiliy of success of each trial is inverse-proportional to the number of trials\n\nThe Poisson distribution is a discrete probability distribution that applies to occurrences of some event over a specified interval. The random variable x is the number of occurrences of the event in an interval.The probability of the event occurring x times over an interval is given by \\[P(x)=\\frac{u^x\\cdot e^{-\\mu}}{x!}\\] where the random variable x is the number of occurences of an event over some interval and the ovvurrences must be random and independent of each other.\n\n中心极限定理：当泊松分布的参数 λ 较大时，泊松分布的形状会接近正态分布。这是因为中心极限定理指出，大量独立随机变量的和趋向于正态分布，而泊松分布可以看作是大量伯努利试验成功次数的分布，当试验次数足够多时，其和可以用正态分布来近似\n\n???The occurrences must be uniformly distributed over the interval being used\nThe mean is \\(\\mu\\)\nThe standard deviation is \\(\\sigma= \\sqrt \\mu\\)\n\neg of Poisson distribution: (describing the behavior of rare events(with small probabilities). radioactive decay, arrivals of people in a line, eagles nesting in a region, patients arriving at an emergency room(the local hospital experiences a mean of 2.3 patients arriving at the emergency room during 10-11 P.M. on Fri. is known, we can find the probability that for a randomly selected Fri. between 10-11 P.M., exactly four patients arrive), Internet users logging onto a Web site )\nComparison between Binomial:\n\nBinomal distribution is affected by yhe smaple size n and the probability p, whereas the Poisson distribution is affected only by mean \\(\\mu\\)\nA binomial distribution has a limit of possible values but a Poisson distribution has a possible values x without upper bound.\nSuppose the \\(X_n,n\\geq1\\) is a sequence of random variables such that \\(X_n\\)~Bin(\\(n,p_n\\)), where \\(p_n\\)~\\(\\lambda/n\\) as \\(n-&gt;\\infty\\) \\(lim_{n-&gt;\\infty}(np_0)=\\lambda\\), intuitively we can observe that \\(\\lambda\\) is the mean of\ngiven the well-known limit \\(\\lim _{n \\rightarrow \\infty}\\left(1-\\frac{\\lambda}{n}\\right)^n=e^{-\\lambda}\\), and \\[\n\\begin{aligned}\n& \\frac{n}{n} \\frac{n-1}{n} \\ldots \\frac{n-k+1}{n}=\\prod_{i=0}^{k-1}\\left(1-\\frac{i}{n}\\right) \\\\\n& \\lim _{n \\rightarrow \\infty} \\prod_{i=0}^{k-1}\\left(1-\\frac{i}{n}\\right)=1 \\\\\n& P\\left(X_n=k\\right)=\\binom{n}{k} p_n^k\\left(1-p_n\\right)^{n-k} \\\\\n&= \\frac{n \\cdots(n-k+1)}{k!} p_n^k\\left(1-p_n\\right)^n\\left(1-p_n\\right)^{-k} \\\\\n&= \\frac{1}{k!}(\\underbrace{n p_n}_{\\rightarrow \\lambda})^k \\underbrace{\\frac{n}{n} \\frac{n-1}{n} \\cdots \\frac{n-k+1}{n}}_{\\rightarrow 1} \\underbrace{\\left(1-p_n\\right)^n}_{\\rightarrow \\mathrm{e}^{-\\lambda}} \\underbrace{\\left(1-p_n\\right)^{-k}}_{\\rightarrow 1} \\\\\n& \\rightarrow \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda} \\underbrace{}_{\\text {as } n \\rightarrow \\infty .}\n\\end{aligned}\n\\]\nCheck that \\(p_x(k), k=0,1,2, \\cdots\\) defines a probability mass function: given the Taylor Series of \\(e^\\lambda\\) around \\(\\lambda=0\\) is given by \\(f(x)=\\) \\(\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!} x^n\\) and \\(e^\\lambda=\\sum_{n=0}^{\\infty} \\frac{\\lambda^n}{n!}\\). Hence, \\[\n\\begin{aligned}\n\\sum_{k=0}^{\\infty} p_X(k) & =\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda} \\\\\n& =\\mathrm{e}^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\\\\n& =\\mathrm{e}^{-\\lambda} \\cdot \\mathrm{e}^\\lambda \\\\\n& =1 .\n\\end{aligned}\n\\] ## Geometric distribution (and Geometric series–powerful!fantastic series)\n\nThe probability of the first happening\nwell-defined \\(\\begin{aligned} \\sum_{k=1}^{\\infty} p_X(k) & =\\sum_{k=1}^{\\infty}(1-p)^{k-1} p \\\\ & =p \\sum_{k=0}^{\\infty}(1-p)^k \\\\ & =p \\cdot \\frac{1}{1-(1-p)} \\\\ & =1 .\\end{aligned}\\)\nTail probability of the Geometric distribution\n\nLet \\(X \\sim \\operatorname{Geom}(p)\\) then \\[\n\\begin{aligned}\nP(X&gt;n) & =P(X=n+1)+P(X=n+2)+P(X=n+3)+\\cdots \\\\\n& =(1-p)^n p+(1-p)^{n+1} p+(1-p)^{n+2} p+\\cdots \\\\\n& =(1-p)^n p\\left(1+(1-p)+(1-p)^2+\\cdots\\right) \\\\\n& =(1-p)^n p \\frac{1}{1-(1-p)} \\\\\n& =(1-p)^n\n\\end{aligned}\n\\] for any \\(n=0,1,2, \\ldots\\)\n\nMemoryless property of Geometric distribution\n\nSuppose that \\(X \\sim \\operatorname{Geom}(p)\\) and \\(n \\in\\{1,2,3, \\cdots\\}\\). Then \\[\nP(X-n=k \\mid X&gt;n)=P(X=k) \\quad, k=1,2,3, \\cdots\n\\]\nThat is, the distribution of \\(X-n\\) under the probability function \\(P(\\cdot \\mid X&gt;n)\\) is the same as the distribution of \\(X\\)\nthe memoryless property is saying that given the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success (independent)\n\nExpectation\n\nSuppose that \\(X \\sim \\operatorname{Geom}(p)\\). Then \\[\n\\begin{aligned}\nE(X)=\\sum_{k=1}^{\\infty} k P(X=k) & =\\sum_{k=1}^{\\infty} k(1-p)^{k-1} p \\\\\n& =p \\sum_{k=1}^{\\infty}\\left[-\\frac{\\mathrm{d}}{\\mathrm{~d} p}(1-p)^k\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\sum_{k=0}^{\\infty}(1-p)^k\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\frac{1}{1-(1-p)}\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\frac{1}{p}\\right] \\\\\n& =\\frac{1}{p}\n\\end{aligned}\n\\] — variance\nLikewise, \\[\n\\begin{aligned}\nE(X(X-1)) & =\\sum_{k=1}^{\\infty} k(k-1) P(X=k) \\\\\n& =\\sum_{k=2}^{\\infty} k(k-1)(1-p)^{k-1} p \\\\\n& =p(1-p) \\sum_{k=2}^{\\infty} k(k-1)(1-p)^{k-2} \\\\\n& =p(1-p) \\frac{\\mathrm{d}^2}{\\mathrm{~d} p^2}\\left[\\sum_{k=0}^{\\infty}(1-p)^k\\right] \\\\\n& =p(1-p) \\frac{\\mathrm{d}^2}{\\mathrm{~d} p^2} \\frac{1}{p} \\\\\n& =p(1-p) \\cdot \\frac{2}{p^3} \\\\\n& =\\frac{2(1-p)}{p^2} .\n\\end{aligned}\n\\] \\[\\begin{aligned} \\operatorname{Var}(X) & =E(X(X-1))+E(X)-(E(X))^2 \\\\ & =\\frac{2(1-p)}{p^2}+\\frac{1}{p}-\\frac{1}{p^2} \\\\ & =\\frac{1-p}{p^2}\\end{aligned}\\]"
  },
  {
    "objectID": "mth1113.html#geometric-distribution",
    "href": "mth1113.html#geometric-distribution",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Geometric distribution",
    "text": "Geometric distribution\n\nThe irrelevance of past events to the probability of future independent events\nGiven the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success.\nX~Geom(p) and n$${1,2,3,….}\nP(X-n=k|X&gt;n)=P(X=k), k=1,2,3,…\nTail probability for the probability calculation of more/higher than…."
  },
  {
    "objectID": "mth1113.html#addition",
    "href": "mth1113.html#addition",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Addition",
    "text": "Addition\nIndependent random variable with the same distribution allows the addition law"
  },
  {
    "objectID": "mth1113.html#interpretation-1",
    "href": "mth1113.html#interpretation-1",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "interpretation:",
    "text": "interpretation:\n\nStandard normally distributed sth. z=1.58(corresponding to 0.9429):\nthe probability of randomly selecting sth. with a value less than 1.58(unit) is equal to the area(probability) of 0.9429.\n(Or: 94.29% of sth, will have a value below 1.58(unit))"
  },
  {
    "objectID": "mth1113.html#sampling-distributions-and-estimators",
    "href": "mth1113.html#sampling-distributions-and-estimators",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Sampling distributions and Estimators",
    "text": "Sampling distributions and Estimators\nWe are beginning to embark a jourfa d dney that allows us to learn about populations by obtaining data from samples since it is rare that we know all values in an entire population.\nSampling distribution of a statistic is the probability distribution of a sample statistics (such as mean/proportion which tend to target the population mean/proportion), with all samples having the same sample size. This concept is important to understand. The behavior of a statistic can be known by understanding its distribution( (The random variable in this case is the value of that sample statistics)). Under certain condition, the distribution of sampling mean/proportion approximates a normal distribution.\nThough statistics does not depend on unknown parameters, the distribution of it depend on unknown parameters.(eg. Normal distribution of sample means depends on population mean(an unknow parameter) and standard deviation)\n(ps: the advantage of sampling with replacement:\nwhen selecting a relatively small sample from a large population, it makes no significant difference whether we sample with or without replacement.\nSampling with replacement results in independent events that are unaffected by previous outcomes, and independent events are easier to analyze and they result in simpler formulas.)\nFor a fixed sample size, the mean of all possible sample means is equal to the mean of population though sample means vary(sampling variability)"
  },
  {
    "objectID": "mth1113.html#unbiased-estimators-and-biased-estimators",
    "href": "mth1113.html#unbiased-estimators-and-biased-estimators",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Unbiased estimators and biased estimators",
    "text": "Unbiased estimators and biased estimators\nStatistics that target population parameters: Mean, variance, proportion\nStatistics that do target population parameters: Median, Range, Standard Deviation\n(the bias is relatively small then sampling standard deviation in large samples so s is ofent used to estimate \\(\\sigma\\))"
  },
  {
    "objectID": "mth1113.html#using-the-normal-distribution-as-an-approximation-to-the-binomial-distribution",
    "href": "mth1113.html#using-the-normal-distribution-as-an-approximation-to-the-binomial-distribution",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Using the Normal distribution as an approximation to the binomial distribution",
    "text": "Using the Normal distribution as an approximation to the binomial distribution\nrequirement: np, n(1-p) \\(\\geq\\) 5\nBe careful: adjust x for continuity by + or - 0.5(eg. at least 99, choose 98.5)"
  },
  {
    "objectID": "mth1113.html#to-hypothesis-testing",
    "href": "mth1113.html#to-hypothesis-testing",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "to hypothesis testing",
    "text": "to hypothesis testing\na question (eg):\nIn a test of a gender-selection technique assume that 100 couples using a particular treatment give birth to 52 girls (and 48 boys). If the technique has no effect, then the probability of a girl is approximately 0.5. If the probability of a girl is 0.5, find the probability that among 100 newborn babies, exactly 52 are girls. Based on the result, is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?\nit is a binomial distribution with np=nq=100*0.5=50\\(\\geq 5\\)\nso we use the normal distribution with mean of 50 and \\(\\sigma = \\sqrt {npq} = 5\\)as an approximation to the binomial distribution\nto answer”is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?“, we need to calculate more than 52(x successes among n trials is an unusually high number of successes if P(\\(x\\geq a\\)) is very small)\n原因是因为如果只看52这个数字肯定概率很小，因为任何单个数字发生的可能性概率都很小\nSo if the answer of P(\\(x\\geq 52\\)) is small we could conclude that the gender selection is useful\n(总结：如果是0。5概率来看的话52以上本来就不是难事，as indicated by the such large probability of P(\\(x\\geq 52\\)) 所以我们没有充分证据拒绝“not effective”这个假设前提)\n此处还没有引入假设检验所以都是用using probability to determine when results are unusual这个思想来思考问题的：\n\nUnusually low： x successes among n trails is an unually low number of successes if P(x or fewer) is very small\n\n\nInterpretation for another example of gender selection(using only unusual results to explain): Because the probability of more than 13 girls, which is 0.001 is so low, we conclude that it is unusual to get 13 girls among 14 babies (using binomial to calculate it). This suggests that the technique of gender selection appears to be effective since it is highly unlikely that the result of 13 girls among 14 births happened by chance."
  },
  {
    "objectID": "mth1113.html#normal-distribution-2",
    "href": "mth1113.html#normal-distribution-2",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "Normal distribution",
    "text": "Normal distribution\nIf a variable is the superposition result of a large number of small independent random factors, then the variable must obey the normal distribution of variables\ne.g. measure error\n\nAssessing Normality\nIn general, quantile plots can be used to assess any probability distribution.\nFor a normal quantile plot(or normal probability plot), it is a graph of points(x,y) where each x vaue is from the original set of sample data and each y value is the cooresponding z score that is a quantile value expected from the standard normal distribution\n\nProcedures\n\nHistogram(not helpful for small data set)\noutliers: reject normality if there is more than 1 outlier present(not helpful for small data set)\nnormal quantile plot:\n\nsort data from lowest to highest\n\nSampling distribution of pˆ when population is infinite"
  },
  {
    "objectID": "mth1113.html#inferences-from-2-samples-introduces-the-differences-between-two-populaton-means-using-matched-pairs-but-correlaition-and-regression-analyze-the-association-between-the-2-variables-and-if-such-an-association-exists-we-wnat-to-describe-it-with-an-equation-that-can-be-used-for-predictions",
    "href": "mth1113.html#inferences-from-2-samples-introduces-the-differences-between-two-populaton-means-using-matched-pairs-but-correlaition-and-regression-analyze-the-association-between-the-2-variables-and-if-such-an-association-exists-we-wnat-to-describe-it-with-an-equation-that-can-be-used-for-predictions",
    "title": "APH101 Biostatistics And R + MTH113 Intro to Probability and Statistics+APH003 Exploring the World Through Data",
    "section": "inferences from 2 samples introduces the differences between two populaton means using matched pairs but correlaition and regression analyze the association between the 2 variables and if such an association exists we wnat to describe it with an equation that can be used for predictions",
    "text": "inferences from 2 samples introduces the differences between two populaton means using matched pairs but correlaition and regression analyze the association between the 2 variables and if such an association exists we wnat to describe it with an equation that can be used for predictions\npaired sampled data(or called bivariate data)\n\na correlation exists between two variables when one of them is related to the other in some way.\nthe linear correlation coefficient r measures the strength of the linear association between the paired x- and y-quantitative values in a sample. Its value is computed by using the formula(Pearson(1857-1937) product moment correlation coefficient)\n\n……otherwise there is not sufficient evidence to support the conclusion of a significant linear equation\n!!!: interpreting r: explained variation: the value of \\(r^2\\) is the proportion of the variation in y that is explained by the linear association between x and y.(and the other percentage is explained by factors other thanx such as characteristics not included in the study)\n\ncorrelation does not imply causality,just the association\naverage suppress individual variation and may inflate the correlation coefficient(the linear correlation coefficient became higher when reginal averages were used)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math is charismatic",
    "section": "",
    "text": "Many many and many…math is always expressing the secret in our magic nature…\n\n\nThe math teachers I have learned from are teaching math in an interesting and sophisticated way with different charismatic philosophy of themselves.\n\n\nI want to log the profound and inspiring things that I learned from my math teachers, including notes, philosophy and the like.\n\nAdvanced Linear Algebra and Linear Algebra\nAnalysis\nPDE intro\nmulti-variable calculus(geometric guys, sequence, …)\nmodeling knowledge\ninteresting problems"
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Math is charismatic",
    "section": "",
    "text": "The math teachers I have learned from are teaching math in an interesting and sophisticated way with different charismatic philosophy of themselves.\n\n\nI want to log the profound and inspiring things that I learned from my math teachers, including notes, philosophy and the like.\n\nAdvanced Linear Algebra and Linear Algebra\nAnalysis\nPDE intro\nmulti-variable calculus(geometric guys, sequence, …)\nmodeling knowledge\ninteresting problems"
  },
  {
    "objectID": "JavaCPT105.html",
    "href": "JavaCPT105.html",
    "title": "JavaCPT105",
    "section": "",
    "text": "The first section here is the knowledge that attracts most of my interests on this module, followed by the lecture notes I tapped when I am on the journey of this module."
  },
  {
    "objectID": "JavaCPT105.html#motivation",
    "href": "JavaCPT105.html#motivation",
    "title": "JavaCPT105",
    "section": "Motivation",
    "text": "Motivation\n\nGod is in the details. –Today’s agile world needs the role of architecture with patient to internalize the idea of “small things matter”, especially in software development.\nCraftmanship is important in these two aspects:\n\nKnowledge of principles, patterns\nWork hard and practic/sweat over it and watch myself fail.\n\n\nSo, please grind the knowledge with forever practice!\n\nCode represents the details of the requirements.\ndefend the code with equal passion\nA programmer with “code-sense” will look at messy module and see options and variations, helping choose the best variation and guide to plot a sequence of behavior preserving transformations to get from here to there.\nA programmer who writes clean code is an artist who can take a blank screen through a series of transformations until it is an elegantly coded system"
  }
]