[
  {
    "objectID": "eng pde.html#what-is-a-number",
    "href": "eng pde.html#what-is-a-number",
    "title": "A Kun’s PDE Lecture",
    "section": "what is a number",
    "text": "what is a number\n\nQ, rational number q/p (ratio)\n\n有理数用直尺画，根号用圆规–算术平均大于几何平均\n什么是理性\n古巴比伦用宗教解释不理解的事情\n古希腊人（大自然是可理解的）带给现代人最珍贵的礼物就是理性\n爱因斯坦：大自然最不可理解的地方是大自然竟然是可理解的\n古希腊的传人\n理性的工具是数学，希腊人对数学的重视\n科技（建筑）和科学（要有主张和实验）\n\\(\\mathbb R\\)\nE.Steim\n分离变数法–傅立叶级数—分析\n微积分\n\n傅立叶 analysis\ncomplex analysis\nreal analysis\nfunctional analysis"
  },
  {
    "objectID": "eng pde.html#geometry",
    "href": "eng pde.html#geometry",
    "title": "A Kun’s PDE Lecture",
    "section": "geometry",
    "text": "geometry\nthe most important thing in geometry is about measure."
  },
  {
    "objectID": "eng pde.html#algebra",
    "href": "eng pde.html#algebra",
    "title": "A Kun’s PDE Lecture",
    "section": "algebra",
    "text": "algebra\nsimplize authority独断权威"
  },
  {
    "objectID": "eng pde.html#analysis",
    "href": "eng pde.html#analysis",
    "title": "A Kun’s PDE Lecture",
    "section": "analysis",
    "text": "analysis\ndemocracy\nNewton\nanatomy\nf(x)约等于 \\(\\Sigma a_nx^n\\)—-Talor series\nbe patient to learn analysis"
  },
  {
    "objectID": "eng pde.html#reformation",
    "href": "eng pde.html#reformation",
    "title": "A Kun’s PDE Lecture",
    "section": "Reformation",
    "text": "Reformation"
  },
  {
    "objectID": "eng pde.html#renaissance",
    "href": "eng pde.html#renaissance",
    "title": "A Kun’s PDE Lecture",
    "section": "Renaissance",
    "text": "Renaissance"
  },
  {
    "objectID": "eng pde.html#enlightenment",
    "href": "eng pde.html#enlightenment",
    "title": "A Kun’s PDE Lecture",
    "section": "Enlightenment",
    "text": "Enlightenment"
  },
  {
    "objectID": "eng pde.html#industrial-revolution",
    "href": "eng pde.html#industrial-revolution",
    "title": "A Kun’s PDE Lecture",
    "section": "Industrial Revolution",
    "text": "Industrial Revolution\nNo dynasty lasts over 300 years, except Song dynasty which lasted over 300 years, with scholars serving as prime ministers. Su Shi snored."
  },
  {
    "objectID": "eng pde.html#wave-equation",
    "href": "eng pde.html#wave-equation",
    "title": "A Kun’s PDE Lecture",
    "section": "Wave Equation",
    "text": "Wave Equation\nVibration of a string.\nTaylor, d’Alembert, Daniel Bernoulli, Jacob Bernoulli (Euler), Jacob Bernoulli (distribution).\n\\[\n\\frac{d^2x}{dt^2}\n\\]\nThe truths of this world have all been discovered by Newton.\n\\[\nu(x,t) \\text{: amplitude, } [u] = L\n\\]\n$$ [p] = M/L\n$$\n\\[\np(x,t) \\text{: density (1-dimensional)}\n\\]\n[T] = [ma] = [m][a] = ML/t^2\n\\(T=T_1=T_2\\) tension\n(0-L)\nF = Tsin() - Tsin\nm = \\(\\rho\\) x\n2nd law: \\(\\rho\\) x"
  },
  {
    "objectID": "eng pde.html#linear-transformation",
    "href": "eng pde.html#linear-transformation",
    "title": "A Kun’s PDE Lecture",
    "section": "linear transformation",
    "text": "linear transformation\nKeep the parallelogram diagonal 保持平行四边形的对角线 x+y\nkeep a straight line 保持直线—a\n向量空间 vector space\n如果是从二维映射到三维怎么办。2dim—3dim？\n代数结构\n保留加法 keep the addition\nhomomorphism\nmorphism–形象"
  },
  {
    "objectID": "eng pde.html#da..-bernulli-separation-of-variables",
    "href": "eng pde.html#da..-bernulli-separation-of-variables",
    "title": "A Kun’s PDE Lecture",
    "section": "Da.. Bernulli Separation of Variables",
    "text": "Da.. Bernulli Separation of Variables\n\\(u(x,\\theta)=\\phi(x)T(t)\\). (Gassian integral formula derivation also use this method)\n(A continuous function can be approximated by a polynomial, which is a super polynomial)\n(the defination of “density”)\n(Seperation variable: God to God, Caesar to Caesar—-x to x, t to t)\n分离变数\n\\[\n\\frac{T^{''}(t)}{C^2 T(t)} = -\\frac{\\phi^{''}(x)}{\\phi(x)}\n\\]\n\\[\n\\begin{aligned}\n&\\text{PDE:} \\quad \\text{分离变量法,separete variables} \\rightarrow \\text{ODE} \\\\\n&\\phi'' + \\lambda \\phi = 0 \\\\\n&T'' + \\lambda c^2 T = 0 \\\\\n&\\text{(B.C.) 边界条件：} \\\\\n&u(0,t) = \\phi(0) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(0) = 0 \\quad \\text{(trivial 非平凡解,boring)} \\\\\n&u(L,t) = \\phi(L) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(L) = 0\n\\end{aligned}\n\\]\n\\[\n\\lambda &lt; 0 \\quad , \\quad \\phi = e^{mx} \\quad \\text{(不可能，从0到0。0--0， impossible)}\n\\]\n\\[\n\\lambda = 0 \\quad , \\quad \\phi = Ax + B \\quad \\text{直线运动， straight move}\n\\]\n\\[\n\\lambda &gt; 0 \\quad , \\quad \\phi = c_1 \\cos(\\sqrt{\\lambda} x) + c_2 \\sin(\\sqrt{\\lambda} x)\n\\] ### 解的推导\n考虑方程： \\[\n\\phi'' + \\lambda \\phi = 0\n\\] 其中 () 是一个常数，解的形式取决于 () 的值。\n\n1. 当 (&lt; 0)\n当 (&lt; 0) 时，我们令 (= -^2) ，于是方程变为： \\[\n\\phi'' - \\mu^2 \\phi = 0\n\\] 其通解为： \\[\n\\phi(x) = A e^{\\mu x} + B e^{-\\mu x}\n\\] 这种解形式表示指数发散或衰减，通常是不稳定解。\n\n\n2. 当 (= 0)\n当 (= 0) 时，方程变为： \\[\n\\phi'' = 0\n\\] 这是一个线性方程，其通解为： \\[\n\\phi(x) = A x + B\n\\] 这表示直线运动。\n\n\n3. 当 (&gt; 0)\n当 (&gt; 0) 时，我们令 (= ^2) ，方程变为： \\[\n\\phi'' + \\mu^2 \\phi = 0\n\\] 其解为： \\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x) ---wave\n\\] 这种解表示周期性波动。\n\n推导\n对于形如 ( \\(\\phi'' + \\mu^2 \\phi = 0\\) ) 的常微分方程，我们可以猜测它的解是指数形式，即：\n\\[\n\\phi(x) = e^{rx}\n\\]\n这里 ( r ) 是需要确定的参数。\n\n\n\n代入微分方程：\n将 ( (x) = e^{rx} ) 代入原方程，得到：\n\\[\nr^2 e^{rx} + \\mu^2 e^{rx} = 0\n\\]\n由于 ( e^{rx} )，可以消掉这个项，剩下的是特征方程：\n\\[\nr^2 + \\mu^2 = 0\n\\] 这个特征方程的解为：\n\\[\nr = \\pm i \\mu\n\\]\n这是一个虚数解。\n当特征根为纯虚数时，方程的通解可以写成正弦和余弦的组合形式，根据欧拉公式 ( e^{ix} = (x) + i (x) )，我们得到通解为：\n\\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x)\n\\]\n其中，( A ) 和 ( B ) 是待定常数，它们可以通过边界条件或初始条件来确定。\ncharacteristic\n\\[\n\\phi(x) = C_1 \\cos(\\sqrt{\\lambda}x) + C_2 \\sin(\\sqrt{\\lambda}x)\n\\]\n边界条件： \\[\n\\phi(0) = 0 \\Rightarrow C_1 = 0\n\\]\n\\[\n\\phi(L) = C_2 \\sin(\\sqrt{\\lambda}L) = 0\n\\]\n因此， \\[\n\\sqrt{\\lambda_n}L = n\\pi \\quad (n = 1, 2, 3, \\dots)\n\\]\n得到本征值： \\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n对应的本征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] Sturm-Liouville Problem:\n\\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n因此，特征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] \\[\nT_n(t) = C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right)\n\\]\n\\[\nu_n(x,t) = T_n(t) \\phi_n(x)\n\\]\n代入展开： \\[\nu(x,t) = \\sum_{n=1}^{\\infty} C_n \\sin \\left( \\frac{n\\pi x}{L} \\right) \\left[ C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right) \\right]\n\\]"
  },
  {
    "objectID": "eng pde.html#law-of-large-number",
    "href": "eng pde.html#law-of-large-number",
    "title": "A Kun’s PDE Lecture",
    "section": "law of large number",
    "text": "law of large number\neg. The reason the casino makes money is because even though the people who go there have a little more than a half chance of winning, the casino has more money than you, and if you stay long enough, you will win, so don’t go to the casino, because you can’t go deeper than the casino"
  },
  {
    "objectID": "eng pde.html#does-math-have-elements",
    "href": "eng pde.html#does-math-have-elements",
    "title": "A Kun’s PDE Lecture",
    "section": "Does math have elements?",
    "text": "Does math have elements?\n             ---Paul Halmos\n             Geometric Series\n\n1-ab and 1-ba —- invertible\nI-AB invertible\n— I-BA invertible (hint: \\(I+B(I-AB)^{-1}A\\))\ndo not 杀鸡用牛刀—-\nprove the harmonic series is divergent:\nidea: geometric series:\n1+1/2+(1/3+1/4)+….\n1+(1/2+1/3)+(1/4+..+1/9)+(1/10+…+1/27)+…\\(\\geq\\) 1+(1/3)\nwhich is relevant to fuliyejishu\n学数学不要兵来将挡水来土掩，要有一个整套的思想，不要背书 idea: geometric series\n(1-ba){-1}=1/1-ba=1+ba+baba+..=1+b(1+ab+abab+..)a=1+b(1-ab){-1}a\n(1-ba)[1+b(1-ab)^{-1}a]\n1+b(1-ab){-1}a-ba-bab(1-ab){-1}a=(commutive law)=1-ba+b(1-ab){-1}a-bab(1-ab){-1}a=1-ba+b(1-ab)(1-ab)^{-1}a\n=1-ba+b1a\n=1-ba+ba\n=1 —-so do not recite boring things, remember based on understanding.\nDo not read Gassian, read Oral\nfirstly, for matrix, AB \\(\\noequal\\) BA—-f(g) is not equal to g(f)\n学学问要有感觉\nchat with different famous … 西方的没落–fu springer\nliangzilixue hysenber—max born\nthe true meaning of matrix is the linear transformation(fun f)"
  },
  {
    "objectID": "eng pde.html#p-series",
    "href": "eng pde.html#p-series",
    "title": "A Kun’s PDE Lecture",
    "section": "p-series",
    "text": "p-series\nIntegral test with dimensional analysis to know p&gt;1 for convergence\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}  \\approx  \\int_1^\\infty 1/x^p \\, dx\\approx 1/[x]^p*[x]=[x]^{1-p} ([x]--&gt;\\infty)---&gt;1-p\\leq 0\\)\nagain, geometric series,\n1/12+(1/22+1/32)+(1/42+..)\n\\(\\leq 1+(1/2^2+1/2^2)+\\)\n= 1+2/2^2 +4/42+8/82z=…=2\n\nEuler\nEuler idea: Viete fumula\nrelation of root and coefficient\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}=\\pi^2/6\\)\n(x-a)(x-b)=x^2-(a+b)x+ab\n(1-x/a)(1-x/b)=1-(1/a+1/b)x+1/ab*x^2\n(1-x)(1+x)\n(1-x)(1+x)(1-x/2)(1+x/2)\n(1-x)(1+x)(1-x/2)(1+x/2)…(1-x/n)(1+x/n)+…\n=1-(1/12+1/22+…+1/n2)x2+…\nQ: find a function whose roots are +-1, +-2, +-3,….\nso Euler changed this Q:\nrewrite:\n(1-x/)(1+x/)(1-x/2)(1+x/2)…\n= 1-(1/12+1/22+1/n2+…)1/2x^2+…\n先猜答案\nguess sinx = (1-x/)(1+x/)…(1-x/n)(1+x/n)\nbut 0 is a solution\nso change to sinx/x\nsinx/x =k…..\nk=limsinx/x=1\n1/x is 振幅\nsinx/x=1-(1/12+1/22+…+1/n2+…)x2/^2+…\n=1/x\n天才是创意\ncreate a thery of math\nand six/x = 1/x(x-x3/3!+…)=1-x3/6\nso we know"
  },
  {
    "objectID": "APH103.html",
    "href": "APH103.html",
    "title": "APH103 Survey Sampling",
    "section": "",
    "text": "\\(X_1, X_2,..., X_n \\sim^{iid} F(\\mu, \\sigma^2)\\), \\(Y_n=\\sqrt n \\frac{\\bar x- \\mu}{\\sigma} --&gt;N(0,1)\\)\n(p.s. Apart from this, CLT also says that When the sample size is large enough (n \\(\\geq\\) 30), the mean distribution of the sample is normal. But here we just prove the above thing)\nProof: (based on MGF)\n\\(z_i =(x_i-\\mu)/ \\sigma\\) where \\(Y_n=\\sqrt n \\bar z\\) and \\(z_i \\sim ^{iid} F(0,1)\\)\n\\[\nE(e^{tZ}) = m(t) \\implies\n\\begin{cases}\nm(0) = 1 \\\\\nm'(0) = 0 = E(Z) \\\\\nm''(0) = 1 + 0^2 = 1 = E(Z^2)\n\\end{cases}\n\\]\n\\[m(t) = m(0) + m'(0)t + \\frac{m''(\\xi)}{2}t^2, \\quad 0 &lt; \\xi &lt; t\\]\n\\[M_{Y_n}(t) = E\\left[ \\exp\\left\\{ t \\sum_{i=1}^n \\frac{Z_i}{\\sqrt{n}} \\right\\} \\right] = \\prod_{i=1}^n E\\left[ \\exp\\left\\{ \\frac{t Z_i}{\\sqrt{n}} \\right\\} \\right].\\]\n\\[= \\left[ m\\left( \\frac{t}{\\sqrt{n}} \\right) \\right]^n = \\left[ 1 + \\frac{m''(\\xi) t^2}{2n} \\right]^n, \\quad 0 &lt; \\xi &lt; \\frac{t}{\\sqrt{n}}.\\]\n\\(n \\to \\infty, \\quad \\frac{t}{\\sqrt{n}} \\to 0 \\implies \\xi \\to 0 \\implies m''(\\xi) \\to m''(0) = 1.\\)\n\\(M_{Y_n}(t) \\to \\left[ 1 + \\frac{t^2}{2n} \\right]^n \\to e^{\\frac{t^2}{2}} = E(e^{tZ})\\)\n\\(\\left[ 1 + \\frac{t}{n} \\right]^n \\to e^t \\quad \\text{Y}_n \\xrightarrow{d} N(0,1)\\)\n\\(\\textbf{Hajek}\\) Let \\(m_N = \\max_{1 \\leq i \\leq n} (y_i - \\bar{y})\\). Then if\n\\(\\frac{1}{\\min(n, N-n)} \\frac{m_N}{S_y^2} \\to 0, \\quad \\frac{\\bar{y} - Y}{\\sqrt{\\text{Var}(\\bar y)}} \\xrightarrow{d} N(0,1)\\)\n\n\nWold-weorwilz Thm\n\\(\\{a_{N_1}, \\ldots, a_{N_n}\\}\\), \\(\\{X_{n_1}, \\ldots, X_{n_N}\\}\\),\n\\[\n\\frac{\\frac{1}{N} \\sum (a_{N_i} - \\overline{a_N})^r}{\\left[ \\frac{1}{N} \\sum (a_{N_i} - \\overline{a_N})^2 \\right]^{r/2}} = O(1), \\quad \\overline{a_N} = \\frac{1}{N} \\sum a_{N_i}.\n\\]\n\\[\n\\frac{\\frac{1}{N} \\sum (X_{N_i} - \\overline{X_N})^r}{\\left[ \\frac{1}{N} \\sum (X_{N_i} - \\overline{X_N})^2 \\right]^{r/2}} = O(1) \\quad X_N = \\frac{1}{N} \\sum X_{N_i}.\n\\] \\(X_1,X_2,...X_N\\) uniformly distributio from \\(X_{N_1},...\\)\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) i.i.d. \\(F(\\mu, \\sigma^2)\\). Define \\(Y_n = \\sqrt{n} \\frac{\\overline{X} - \\mu}{\\sigma} \\xrightarrow{d} N(0,1)\\).\nLet \\(L_N = \\sum a_{N_i} X_i\\), \\(E[L_N] = N \\overline{a_N} \\overline{X}\\).\n\\[\n\\text{Var}(L_N) = \\frac{1}{N-1} \\left[ \\sum (a_{N_i} - \\overline{a_N})^2 \\right] \\left[ \\sum (X_{N_i} - \\overline{X})^2 \\right]\n\\]\nAs \\(N \\to \\infty\\),\n\\[\nP\\left\\{ \\frac{L_N - E(L_N)}{\\sqrt{\\text{Var}(L_N)}} \\leq z \\right\\} \\to \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z e^{-\\frac{t^2}{2}} dt.\n\\]"
  },
  {
    "objectID": "APH103.html#week-4",
    "href": "APH103.html#week-4",
    "title": "APH103 Survey Sampling",
    "section": "week 4",
    "text": "week 4\nrandom sampling\nsufficient sample size \\(n\\ge 30\\) finite population variance independence\nB=\\(2\\sqrt {\\hat V (\\bar y)}=2\\sqrt {(1-n/N)S^2/n}\\)\n\nSample without replacement is better in the aspect of …\n\n有无放回都ok？ interpretation of character statistics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math is charismatic",
    "section": "",
    "text": "Many many and many…math is always expressing the secret in our magic nature…\n\n\nThe math teachers I have learned from are teaching math in an interesting and sophisticated way with different charismatic philosophy of themselves.\n\n\nI want to log the profound and inspiring things that I learned from my math teachers, including notes, philosophy and the like.\n\nAdvanced Linear Algebra and Linear Algebra\nAnalysis\nPDE intro\nmulti-variable calculus(geometric guys, sequence, …)\nmodeling knowledge\ninteresting problems"
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Math is charismatic",
    "section": "",
    "text": "The math teachers I have learned from are teaching math in an interesting and sophisticated way with different charismatic philosophy of themselves.\n\n\nI want to log the profound and inspiring things that I learned from my math teachers, including notes, philosophy and the like.\n\nAdvanced Linear Algebra and Linear Algebra\nAnalysis\nPDE intro\nmulti-variable calculus(geometric guys, sequence, …)\nmodeling knowledge\ninteresting problems"
  },
  {
    "objectID": "Simple_ODEs (1).html",
    "href": "Simple_ODEs (1).html",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "",
    "text": "To solve the linear ODE $ ’ = A $ and visualize how different initial conditions affect the trajectories, we can use a saddle point matrix $ A $ with eigenvalues of opposite signs. This setup results in varied trajectory shapes such as diverging, converging, and hyperbolic paths depending on the initial conditions."
  },
  {
    "objectID": "Simple_ODEs (1).html#what-is-a-number",
    "href": "Simple_ODEs (1).html#what-is-a-number",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "what is a number",
    "text": "what is a number\n\nQ, rational number q/p (ratio)\n\n有理数用直尺画，根号用圆规–算术平均大于几何平均\n什么是理性\n古巴比伦用宗教解释不理解的事情\n古希腊人（大自然是可理解的）带给现代人最珍贵的礼物就是理性\n爱因斯坦：大自然最不可理解的地方是大自然竟然是可理解的\n古希腊的传人\n理性的工具是数学，希腊人对数学的重视\n科技（建筑）和科学（要有主张和实验）\n\\(\\mathbb R\\)\nE.Steim\n分离变数法–傅立叶级数—分析\n微积分\n\n傅立叶 analysis\ncomplex analysis\nreal analysis\nfunctional analysis"
  },
  {
    "objectID": "Simple_ODEs (1).html#geometry",
    "href": "Simple_ODEs (1).html#geometry",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "geometry",
    "text": "geometry\nthe most important thing in geometry is about measure."
  },
  {
    "objectID": "Simple_ODEs (1).html#algebra",
    "href": "Simple_ODEs (1).html#algebra",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "algebra",
    "text": "algebra\nsimplize authority独断权威"
  },
  {
    "objectID": "Simple_ODEs (1).html#analysis",
    "href": "Simple_ODEs (1).html#analysis",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "analysis",
    "text": "analysis\ndemocracy\nNewton\nanatomy\nf(x)约等于 \\(\\Sigma a_nx^n\\)—-Talor series\nbe patient to learn analysis"
  },
  {
    "objectID": "Simple_ODEs (1).html#reformation",
    "href": "Simple_ODEs (1).html#reformation",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Reformation",
    "text": "Reformation"
  },
  {
    "objectID": "Simple_ODEs (1).html#renaissance",
    "href": "Simple_ODEs (1).html#renaissance",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Renaissance",
    "text": "Renaissance"
  },
  {
    "objectID": "Simple_ODEs (1).html#enlightenment",
    "href": "Simple_ODEs (1).html#enlightenment",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Enlightenment",
    "text": "Enlightenment"
  },
  {
    "objectID": "Simple_ODEs (1).html#industrial-revolution",
    "href": "Simple_ODEs (1).html#industrial-revolution",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Industrial Revolution",
    "text": "Industrial Revolution\nNo dynasty lasts over 300 years, except Song dynasty which lasted over 300 years, with scholars serving as prime ministers. Su Shi snored."
  },
  {
    "objectID": "Simple_ODEs (1).html#wave-equation",
    "href": "Simple_ODEs (1).html#wave-equation",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Wave Equation",
    "text": "Wave Equation\nVibration of a string.\nTaylor, d’Alembert, Daniel Bernoulli, Jacob Bernoulli (Euler), Jacob Bernoulli (distribution).\n\\[\n\\frac{d^2x}{dt^2}\n\\]\nThe truths of this world have all been discovered by Newton.\n\\[\nu(x,t) \\text{: amplitude, } [u] = L\n\\]\n$$ [p] = M/L\n$$\n\\[\np(x,t) \\text{: density (1-dimensional)}\n\\]\n[T] = [ma] = [m][a] = ML/t^2\n\\(T=T_1=T_2\\) tension\n(0-L)\nF = Tsin() - Tsin\nm = \\(\\rho\\) x\n2nd law: \\(\\rho\\) x"
  },
  {
    "objectID": "Simple_ODEs (1).html#linear-transformation",
    "href": "Simple_ODEs (1).html#linear-transformation",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "linear transformation",
    "text": "linear transformation\nKeep the parallelogram diagonal 保持平行四边形的对角线 x+y\nkeep a straight line 保持直线—a\n向量空间 vector space\n如果是从二维映射到三维怎么办。2dim—3dim？\n代数结构\n保留加法 keep the addition\nhomomorphism\nmorphism–形象"
  },
  {
    "objectID": "Simple_ODEs (1).html#da..-bernulli-separation-of-variables",
    "href": "Simple_ODEs (1).html#da..-bernulli-separation-of-variables",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Da.. Bernulli Separation of Variables",
    "text": "Da.. Bernulli Separation of Variables\n\\(u(x,\\theta)=\\phi(x)T(t)\\). (Gassian integral formula derivation also use this method)\n(A continuous function can be approximated by a polynomial, which is a super polynomial)\n(the defination of “density”)\n(Seperation variable: God to God, Caesar to Caesar—-x to x, t to t)\n分离变数\n\\[\n\\frac{T^{''}(t)}{C^2 T(t)} = -\\frac{\\phi^{''}(x)}{\\phi(x)}\n\\]\n\\[\n\\begin{aligned}\n&\\text{PDE:} \\quad \\text{分离变量法,separete variables} \\rightarrow \\text{ODE} \\\\\n&\\phi'' + \\lambda \\phi = 0 \\\\\n&T'' + \\lambda c^2 T = 0 \\\\\n&\\text{(B.C.) 边界条件：} \\\\\n&u(0,t) = \\phi(0) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(0) = 0 \\quad \\text{(trivial 非平凡解,boring)} \\\\\n&u(L,t) = \\phi(L) T(t) = 0 \\quad \\Rightarrow \\quad \\phi(L) = 0\n\\end{aligned}\n\\]\n\\[\n\\lambda &lt; 0 \\quad , \\quad \\phi = e^{mx} \\quad \\text{(不可能，从0到0。0--0， impossible)}\n\\]\n\\[\n\\lambda = 0 \\quad , \\quad \\phi = Ax + B \\quad \\text{直线运动， straight move}\n\\]\n\\[\n\\lambda &gt; 0 \\quad , \\quad \\phi = c_1 \\cos(\\sqrt{\\lambda} x) + c_2 \\sin(\\sqrt{\\lambda} x)\n\\] ### 解的推导\n考虑方程： \\[\n\\phi'' + \\lambda \\phi = 0\n\\] 其中 () 是一个常数，解的形式取决于 () 的值。\n\n1. 当 (&lt; 0)\n当 (&lt; 0) 时，我们令 (= -^2) ，于是方程变为： \\[\n\\phi'' - \\mu^2 \\phi = 0\n\\] 其通解为： \\[\n\\phi(x) = A e^{\\mu x} + B e^{-\\mu x}\n\\] 这种解形式表示指数发散或衰减，通常是不稳定解。\n\n\n2. 当 (= 0)\n当 (= 0) 时，方程变为： \\[\n\\phi'' = 0\n\\] 这是一个线性方程，其通解为： \\[\n\\phi(x) = A x + B\n\\] 这表示直线运动。\n\n\n3. 当 (&gt; 0)\n当 (&gt; 0) 时，我们令 (= ^2) ，方程变为： \\[\n\\phi'' + \\mu^2 \\phi = 0\n\\] 其解为： \\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x) ---wave\n\\] 这种解表示周期性波动。\n\n推导\n对于形如 ( \\(\\phi'' + \\mu^2 \\phi = 0\\) ) 的常微分方程，我们可以猜测它的解是指数形式，即：\n\\[\n\\phi(x) = e^{rx}\n\\]\n这里 ( r ) 是需要确定的参数。\n\n\n\n代入微分方程：\n将 ( (x) = e^{rx} ) 代入原方程，得到：\n\\[\nr^2 e^{rx} + \\mu^2 e^{rx} = 0\n\\]\n由于 ( e^{rx} )，可以消掉这个项，剩下的是特征方程：\n\\[\nr^2 + \\mu^2 = 0\n\\] 这个特征方程的解为：\n\\[\nr = \\pm i \\mu\n\\]\n这是一个虚数解。\n当特征根为纯虚数时，方程的通解可以写成正弦和余弦的组合形式，根据欧拉公式 ( e^{ix} = (x) + i (x) )，我们得到通解为：\n\\[\n\\phi(x) = A \\cos(\\mu x) + B \\sin(\\mu x)\n\\]\n其中，( A ) 和 ( B ) 是待定常数，它们可以通过边界条件或初始条件来确定。\ncharacteristic\n\\[\n\\phi(x) = C_1 \\cos(\\sqrt{\\lambda}x) + C_2 \\sin(\\sqrt{\\lambda}x)\n\\]\n边界条件： \\[\n\\phi(0) = 0 \\Rightarrow C_1 = 0\n\\]\n\\[\n\\phi(L) = C_2 \\sin(\\sqrt{\\lambda}L) = 0\n\\]\n因此， \\[\n\\sqrt{\\lambda_n}L = n\\pi \\quad (n = 1, 2, 3, \\dots)\n\\]\n得到本征值： \\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n对应的本征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] Sturm-Liouville Problem:\n\\[\n\\lambda_n = \\left( \\frac{n\\pi}{L} \\right)^2, \\quad n = 1, 2, 3, \\dots\n\\]\n因此，特征函数为： \\[\n\\phi_n(x) = \\sin \\left( \\frac{n\\pi x}{L} \\right)\n\\] \\[\nT_n(t) = C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right)\n\\]\n\\[\nu_n(x,t) = T_n(t) \\phi_n(x)\n\\]\n代入展开： \\[\nu(x,t) = \\sum_{n=1}^{\\infty} C_n \\sin \\left( \\frac{n\\pi x}{L} \\right) \\left[ C_1 \\cos \\left( \\frac{n\\pi c t}{L} \\right) + C_2 \\sin \\left( \\frac{n\\pi c t}{L} \\right) \\right]\n\\]"
  },
  {
    "objectID": "Simple_ODEs (1).html#law-of-large-number",
    "href": "Simple_ODEs (1).html#law-of-large-number",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "law of large number",
    "text": "law of large number\neg. The reason the casino makes money is because even though the people who go there have a little more than a half chance of winning, the casino has more money than you, and if you stay long enough, you will win, so don’t go to the casino, because you can’t go deeper than the casino"
  },
  {
    "objectID": "Simple_ODEs (1).html#does-math-have-elements",
    "href": "Simple_ODEs (1).html#does-math-have-elements",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "Does math have elements?",
    "text": "Does math have elements?\n             ---Paul Halmos\n             Geometric Series\n\n1-ab and 1-ba —- invertible\nI-AB invertible\n— I-BA invertible (hint: \\(I+B(I-AB)^{-1}A\\))\ndo not 杀鸡用牛刀—-\nprove the harmonic series is divergent:\nidea: geometric series:\n1+1/2+(1/3+1/4)+….\n1+(1/2+1/3)+(1/4+..+1/9)+(1/10+…+1/27)+…\\(\\geq\\) 1+(1/3)\nwhich is relevant to fuliyejishu\n学数学不要兵来将挡水来土掩，要有一个整套的思想，不要背书 idea: geometric series\n(1-ba){-1}=1/1-ba=1+ba+baba+..=1+b(1+ab+abab+..)a=1+b(1-ab){-1}a\n(1-ba)[1+b(1-ab)^{-1}a]\n1+b(1-ab){-1}a-ba-bab(1-ab){-1}a=(commutive law)=1-ba+b(1-ab){-1}a-bab(1-ab){-1}a=1-ba+b(1-ab)(1-ab)^{-1}a\n=1-ba+b1a\n=1-ba+ba\n=1 —-so do not recite boring things, remember based on understanding.\nDo not read Gassian, read Oral\nfirstly, for matrix, AB \\(\\noequal\\) BA—-f(g) is not equal to g(f)\n学学问要有感觉\nchat with different famous … 西方的没落–fu springer\nliangzilixue hysenber—max born\nthe true meaning of matrix is the linear transformation(fun f)"
  },
  {
    "objectID": "Simple_ODEs (1).html#p-series",
    "href": "Simple_ODEs (1).html#p-series",
    "title": "Thanks to Dr. Mengqi Hu for offering his code to let us know the ODE better.",
    "section": "p-series",
    "text": "p-series\nIntegral test with dimensional analysis to know p&gt;1 for convergence\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}  \\approx  \\int_1^\\infty 1/x^p \\, dx\\approx 1/[x]^p*[x]=[x]^{1-p} ([x]--&gt;\\infty)---&gt;1-p\\leq 0\\)\nagain, geometric series,\n1/12+(1/22+1/32)+(1/42+..)\n\\(\\leq 1+(1/2^2+1/2^2)+\\)\n= 1+2/2^2 +4/42+8/82z=…=2\n\nEuler\nEuler idea: Viete fumula\nrelation of root and coefficient\n\\(\\sum_{n=1}^{\\infty} \\frac{1}{n^p}=\\pi^2/6\\)\n(x-a)(x-b)=x^2-(a+b)x+ab\n(1-x/a)(1-x/b)=1-(1/a+1/b)x+1/ab*x^2\n(1-x)(1+x)\n(1-x)(1+x)(1-x/2)(1+x/2)\n(1-x)(1+x)(1-x/2)(1+x/2)…(1-x/n)(1+x/n)+…\n=1-(1/12+1/22+…+1/n2)x2+…\nQ: find a function whose roots are +-1, +-2, +-3,….\nso Euler changed this Q:\nrewrite:\n(1-x/)(1+x/)(1-x/2)(1+x/2)…\n= 1-(1/12+1/22+1/n2+…)1/2x^2+…\n先猜答案\nguess sinx = (1-x/)(1+x/)…(1-x/n)(1+x/n)\nbut 0 is a solution\nso change to sinx/x\nsinx/x =k…..\nk=limsinx/x=1\n1/x is 振幅\nsinx/x=1-(1/12+1/22+…+1/n2+…)x2/^2+…\n=1/x\n天才是创意\ncreate a thery of math\nand six/x = 1/x(x-x3/3!+…)=1-x3/6\nso we know"
  },
  {
    "objectID": "mth106.html",
    "href": "mth106.html",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "",
    "text": "Find the solution of the equation\n\\[\n\\frac{dy}{dx} - P(x)y = e^{2x}, \\quad y(0) = 1,\n\\]\nwhere \\[P(x) = \\begin{cases}\n1 & x \\in [0,1), \\\\\n0 & x \\notin [0,1).\n\\end{cases}\\]\n[Solution:] Consider the homogeneous equation\n\\[\n\\frac{dy}{dx} - P(x)y = 0,\n\\]\nby separating the variables we have \\(\\frac{dy}{y} = P(x)dx\\), namely \\(\\ln|y| = \\int_0^x P(t)dt + C_1\\), and we get\n\\[\ny = Ce^{\\int_0^x P(t)dt} = \\begin{cases}\nC & x &lt; 0 \\\\\nCe^x & x \\in [0,1) \\\\\nCe & x \\geq 1.\n\\end{cases}\n\\]\nNow by variation of parameters we suppose \\(u = u(x)\\), and \\(y = u(x)e^{\\int_0^x P(t)dt}\\) is the solution, hence\n\\[\nu'(x) = e^{2x - \\int_0^x P(t)dt} = \\begin{cases}\ne^{2x} & x &lt; 0, \\\\\ne^x & x \\in [0,1), \\\\\ne^{2x-1} & x \\geq 1.\n\\end{cases}\n\\]\n(this is because :\nThe differential equation is given by: \\[\ny' + p(x)y = f(x).\n\\]\nIntegrating factor is: \\[\ne^{\\int p(x)dx}.\n\\]\nMultiplying through by the integrating factor: \\[\n\\frac{d}{dx} \\left[ e^{\\int p(x)dx} y \\right] = f(x) e^{\\int p(x)dx}.\n\\]\nSolving for \\(y\\):\n\\[\ny e^{\\int p(x)dx} = \\int f(x) e^{\\int p(x)dx} dx + C.\n\\]\nThus, the general solution is: \\[\ny = Ce^{-\\int p(x)dx} + e^{-\\int p(x)dx} \\int f(x) e^{\\int p(x)dx} dx,\n\\] where \\(y_c\\) represents the complementary solution \\(Ce^{-\\int p(x)dx}\\) and \\(y_p\\) represents the particular solution \\(e^{-\\int p(x)dx} \\int f(x) e^{\\int p(x)dx}\\).\n)\nand \\(C(0) = 1\\) since \\(y(0) = 1\\).\nWe see that\n\\[\nu(x) = u(0) + \\int_0^x u'(t)dt = \\begin{cases}\n\\frac{e^{2x} + 1}{2} & x &lt; 0 \\\\\ne^x & x \\in [0,1) \\\\\n\\frac{e^{2x-1} + e}{2} & x \\geq 1.\n\\end{cases}\n\\]\nFinally we get\n\\[\ny = \\begin{cases}\n\\frac{e^{2x} + 1}{2} & x &lt; 0 \\\\\ne^{2x} & x \\in [0,1) \\\\\n\\frac{e^{2x} + e^2}{2} & x \\geq 1.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "Calculus.html",
    "href": "Calculus.html",
    "title": "Calculus and Multivariable Calculus",
    "section": "",
    "text": "\\(y'=\\frac{d}{dx}\\cdot y\\)\n\\(y''=\\frac{d}{dx} \\frac{d}{dx}\\cdot y\\)=\\(\\frac{d^2y}{dx^2}\\cdot y\\)"
  },
  {
    "objectID": "Calculus.html#remark-of-derivative",
    "href": "Calculus.html#remark-of-derivative",
    "title": "Calculus and Multivariable Calculus",
    "section": "",
    "text": "\\(y'=\\frac{d}{dx}\\cdot y\\)\n\\(y''=\\frac{d}{dx} \\frac{d}{dx}\\cdot y\\)=\\(\\frac{d^2y}{dx^2}\\cdot y\\)"
  },
  {
    "objectID": "Calculus.html#cylinders",
    "href": "Calculus.html#cylinders",
    "title": "Calculus and Multivariable Calculus",
    "section": "2 cylinders",
    "text": "2 cylinders\nIt is easy\n\nsurface\n\n\nvolume\nuse method of sections directly\n\n\n3 cylinders\n\n\nsurface\n\nsee the photo attached, which is the easiest way for computation:\n\n\n\nvolume\n\nmethod 1—do not draw it(using conditional inferences)\nThanks to Dr.Bohuan Lin to teach me such a clever way which we do not need to draw the picture(only use conditional equations to solve it is interesting and a little difficult to handle it correctly).\nThe picture behind the method is attached to “Steinmetz(mou he fang gai)’s photo behind.png”\nWe have the set ( D ) defined as: \\[\nD \\triangleq \\left\\{(x, y, z) \\mid \\begin{cases}\nx^2 + y^2 \\leq 4 \\\\\ny^2 + z^2 \\leq 4 \\\\\nz^2 + x^2 \\leq 4\n\\end{cases} \\right\\}\n\\]\nFrom \\[x^2+ y^2 \\leq 4 , x^2 + z^2 \\leq 4 \\] we derive: if \\[\n|x| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |z| \\leq \\sqrt{2}\n\\] (and &lt; is the same shape)\nSimilarly:\nif \\[|z| \\geq \\sqrt{2} ,|x| \\leq \\sqrt{2} \\quad \\text{then} \\quad |y| \\leq \\sqrt{2} \\]\nAnd: if \\[\n|z| \\geq \\sqrt{2},|y| \\leq \\sqrt{2} \\quad \\text{then} \\quad |x| \\leq \\sqrt{2}\n\\] Therefore: \\[\nD = D_{\\leq \\sqrt{2}} \\cup \\bar{D}\n\\]\nWhere: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\] \\[\n\\begin{aligned}\n&= \\left\\{ (x, y, z) \\in \\mathbb{R}^3 \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\} \\\\\n&= [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}] \\times [-\\sqrt{2}, \\sqrt{2}]\n\\end{aligned}\n\\]\nAnd: \\[\n\\bar{D} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus: \\[\n\\bar{D} = \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}}\n\\]\n(This also implies: \\[\nD_{\\leq \\sqrt{2}} = \\left\\{(x, y, z) \\in D \\mid |x|, |y|, |z| \\leq \\sqrt{2} \\right\\}\n\\])\nWe derive: \\[\n\\bar{D} = \\left\\{ (x, y, z) \\mid |x| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |y| &gt; \\sqrt{2} \\right\\} \\cup \\left\\{ (x, y, z) \\mid |z| &gt; \\sqrt{2} \\right\\}\n\\]\nThus, combining all components, we get: \\[\nD = \\left( D_{\\leq \\sqrt{2}} \\cup \\bar{D}_{|x| &gt; \\sqrt{2}} \\cup \\bar{D}_{|y| &gt; \\sqrt{2}} \\cup \\bar{D}_{|z| &gt; \\sqrt{2}} \\right)\n\\] So we could decompose it into a cube in the center and 6 common volume. the 6 volume: 6 \\(\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\nIn summary, the whole volume is \\((2 \\sqrt 2)^3 +6\\int_{ \\sqrt{2}}^{2}4(4-x^2) dx\\)\n\n\n\nmethod 2—cross-section method\nThanks to Dr. Haoran Chen for teaching us such method.\nWe suppose z&gt;x, z&gt;y,（then *3） then we could continue decompose it into (1)z~(\\(\\sqrt 2\\),2) and (2)z~(\\(0,\\sqrt 2\\))(based on whether the square is out of the circle)\n\n\n\nmethod 3\nsee the photo attached"
  },
  {
    "objectID": "Calculus.html#integral-problems-with-solution-of-alternating-integral-test",
    "href": "Calculus.html#integral-problems-with-solution-of-alternating-integral-test",
    "title": "Calculus and Multivariable Calculus",
    "section": "integral problems with solution of alternating integral test",
    "text": "integral problems with solution of alternating integral test\nquestion: does \\[\\int_{0}^{\\infty}sinx/x\\,dx\\] converge?\nIt is easy to think of these 2 famous but difficultly proved formula: \\[\\int_{-\\infty}^{\\infty}sinx/x\\,dx=\\pi\n\\] \\[\\int_{-\\infty}^{\\infty}e^{-x^2}\\,dx=\\pi\n\\] However, they are not useful, which is how charasmatic the math is! I love math!\n\\[=\\Sigma_{n=1}^{\\infty}\\int_{2(n-1)\\pi}^{2n\\pi}sinx/x\\,dx\n\\] Then we let \\(x-[2(n-1)\\pi]=y\\)(dx=dy)\nso \\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{2\\pi}siny/(y+[2(n-1)\\pi])\\,dy\\] =\\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}siny/(y+[2(n-1)\\pi])\\,dy+\\Sigma_{n=1}^{\\infty}\\int_{\\pi}^{2\\pi}siny/(y+[2(n-1)\\pi])\\,dy\\]\nThen we let y-\\(\\pi\\)=z(dy=dz)\n\\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}siny/(y+[2(n-1)\\pi])\\,dy+\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}-sinz/(z+[(2n-1)\\pi])\\,dz\\] \\[=\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}sinm/(m+[2(n-1)\\pi])\\,dm+\\Sigma_{n=1}^{\\infty}\\int_{0}^{\\pi}-sinm/(m+[(2n-1)\\pi])\\,dm\\]\nso if \\[a_n=\\int_{0}^{\\pi}sinm/(m+(n-1)\\pi)\\] \\[\\int_{0}^{\\infty}sinx/x\\,dx=a_1-a_2+a_3-a_4+...+a_n\\], which is an alternating series!\nCoincidently, it is decresing and \\(a_n&lt;\\int_{0}^{\\pi}1/(n-1)\\pi\\,dm=1/(n-1)\\), and \\(\\lim_{{n \\to \\infty}} \\left( \\frac{1}{n-1}  \\right) = 0\\) so \\(\\lim_{{n \\to \\infty}}a_n=0\\) So it converges(Alternating series test)."
  },
  {
    "objectID": "Calculus.html#similar-question-2",
    "href": "Calculus.html#similar-question-2",
    "title": "Calculus and Multivariable Calculus",
    "section": "similar question 2",
    "text": "similar question 2\nThanks for Dr.Chi-Kun Lin to teach me this kind of problems!\n\\[\n\\int_0^\\infty \\frac{1}{1 + x^p \\sin^2 x} \\, dx\n\\]\n\n\n\n\\[\n\\sum_{n=0}^\\infty \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n + \\frac{1}{2} \\right)^2 p \\sin^2 t} \\, dt = \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n + \\frac{1}{2} \\right)^2 p \\sin^2 t} \\, dt\n\\]\n\\[\\Sigma\\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n\\pi/2 + t \\right)^ p \\sin^2 t} \\, dt + \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left(( n + 1 \\right)\\pi/2-t)^ p sin^2t} \\, dt\\]\nfor:\n\\[\nx_n = \\Sigma\\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( n\\pi/2 + t \\right)^ p \\sin^2 t} \\, dt + \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left(( n + 1 \\right)\\pi/2-t)^ p sin^2t} \\, dt,\n\\]\nthere has\n\\[2 \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right)^ p t^2} \\, dt \\leq x_n \\leq 2 \\int_0^{\\frac{\\pi}{2}} \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right) ^p \\frac{4}{\\pi^2} t^2} \\, dt\\]\n\nHowever, the integrals on both sides of the inequality can be calculated separately. For example:\n\n\\[\n\\int_0^1 \\frac{1}{1 + \\left( (n + 1) \\frac{\\pi}{2} \\right)^ p t^2} \\, dt = \\frac{1}{\\sqrt{((n + 1) \\frac{\\pi}{2} )^p}} \\arctan \\left( \\sqrt{(n + 1) \\frac{\\pi}{2}}^{2p}\\pi/2 \\right) \\geq \\frac{1}{\\sqrt{(n + 1) \\frac{\\pi}{2} }^{2p}  } \\frac{\\pi}{4}\n\\]"
  },
  {
    "objectID": "Calculus.html#lagrange-multipliers",
    "href": "Calculus.html#lagrange-multipliers",
    "title": "Calculus and Multivariable Calculus",
    "section": "Lagrange Multipliers",
    "text": "Lagrange Multipliers\nThe method of Lagrange multipliers is used to solve optimization problems with constraints. It involves introducing Lagrange multipliers to incorporate constraints into the objective function, converting constrained problems into unconstrained problems. Given an objective function ( f() ) with constraints ( g_i() = 0 ), the Lagrange function is:\n[ (, ) = f() + _{i} _i g_i() ]\nwhere (_i) are the Lagrange multipliers. By setting the derivatives of () to zero, we can find the optimal solution."
  },
  {
    "objectID": "Calculus.html#relationship-between-lasso-and-lagrange-multipliers",
    "href": "Calculus.html#relationship-between-lasso-and-lagrange-multipliers",
    "title": "Calculus and Multivariable Calculus",
    "section": "Relationship Between Lasso and Lagrange Multipliers",
    "text": "Relationship Between Lasso and Lagrange Multipliers\nIn Lasso regression, the L1 norm regularization term (||_1) can be viewed as a constraint. The Lasso problem can be reformulated as a constrained optimization problem:\n[ | - |_2^2 ||_1 t ]\nwhere (t) is a non-negative constant representing the limit on the regularization strength.\n\nUsing Lagrange Multipliers for Lasso\nWe can use Lagrange multipliers to solve this constrained problem. The Lagrange function is defined as:\n[ (, ) = | - |_2^2 + (||_1 - t) ]\nwhere () is the Lagrange multiplier. The optimal solution (^*) satisfies:\n[ = -^( - ) + () = 0 ]\n[ ||_1 t ]\n[ (||_1 - t) = 0 ]"
  },
  {
    "objectID": "Calculus.html#summary",
    "href": "Calculus.html#summary",
    "title": "Calculus and Multivariable Calculus",
    "section": "Summary",
    "text": "Summary\n\nLasso Regression uses L1 regularization to achieve feature selection, and its optimization problem can be transformed into a constrained optimization problem.\nLagrange Multipliers provide a method to handle constraints in optimization problems by converting them into unconstrained problems and introducing multipliers to adjust the regularization strength.\n\nIn Lasso regression, the L1 regularization constraint can be handled using Lagrange multipliers, converting the problem into one with multipliers that adjust the strength of regularization. ### Problem\nFind extreme values of\n\\[ f(x, y) = \\cos x + \\cos y + \\cos (x + y). \\]\n\nSolution\nSince cosine is a periodic function, we can consider the region ( 0 x ), ( 0 y ) (bounded and closed region) to find the maximal and minimal values.\nFirstly, we consider the interior of the region to find stationary points:\n\\[ f_x = -\\sin x - \\sin (x + y) = 0 \\] \\[ f_y = -\\sin y - \\sin (x + y) = 0 \\]\nThis implies:\n\\[ \\sin x = \\sin y \\]\nThen, inside the region (not on boundary), we have three cases:\n\n( y = x )\n( y = - x ) for ( 0 &lt; x &lt; )\n( y = 3- x ) for ( &lt; x &lt; 2)\n\n\nCase 1\n\\[ \\sin x + \\sin (x + y) = \\sin x + 2 \\sin x \\cos x = 0 \\] \\[ \\sin x (2 \\cos x + 1) = 0 \\]\nThis implies:\n\\[ x = \\pi, y = \\pi \\] or \\[ x = \\frac{2\\pi}{3}, y = \\frac{2\\pi}{3} \\] or \\[ x = \\frac{4\\pi}{3}, y = \\frac{4\\pi}{3} \\]\nEvaluating the function at these points:\n\\[ f(\\pi, \\pi) = -1 \\] \\[ f\\left( \\frac{2\\pi}{3}, \\frac{2\\pi}{3} \\right) = -\\frac{3}{2} \\] \\[ f\\left( \\frac{4\\pi}{3}, \\frac{4\\pi}{3} \\right) = -\\frac{3}{2} \\]\n\n\nCase 2\n\\[ \\sin x + \\sin (x + y) = \\sin x + \\sin \\pi = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 0 \\] (on boundary)\n\n\nCase 3\n\\[ \\sin x + \\sin (3\\pi - x) = \\sin x = 0 \\]\nThis implies:\n\\[ x = \\pi, y = 2\\pi \\] (also on boundary)\n\n\n\nOn the Boundary\nDue to periodic property, we consider:\n\\[ x = 0, 0 \\leq y \\leq 2\\pi \\] and \\[ y = 0, 0 \\leq x \\leq 2\\pi \\]\nEvaluating the function at these boundaries:\n\\[ f(0, y) = 1 + 2 \\cos y, \\min = -1, \\max = 3 \\] \\[ f(x, 0) = 1 + 2 \\cos x, \\min = -1, \\max = 3 \\]\nSo,\n\\[ \\max f(x, y) = 3 \\text{ at } (2n\\pi, 2k\\pi) \\text{ for any } n, k \\in \\mathbb{Z} \\] \\[ \\min f(x, y) = -\\frac{3}{2} \\text{ at } \\left( (2n+1)\\pi \\pm \\frac{\\pi}{3}, (2k+1)\\pi \\pm \\frac{\\pi}{3} \\right) \\]"
  },
  {
    "objectID": "mth107.html",
    "href": "mth107.html",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "The first section here is the knowledge that attracts most of my interests on this module, followed by the lecture notes I tapped when I am on the journey of this module.\n\n\nI feel so lucky to learn from Professor Paul-Henry Leemann. He teaches Linear Algebra very well. He not only teahces us how to solve tutorials, but also the real application in the real world such as the application of eigenvectors on Google search. He is also a kind, responsible and humorous teacher. His lecture notes are clean and clear with multiple good examples helping us to understand more effectively.\nProfessor Andrew Lin teaches Linear Algebra very very well and interesting. He teaches me many tips to help me never remember sth., understand totally instead."
  },
  {
    "objectID": "mth107.html#thanks",
    "href": "mth107.html#thanks",
    "title": "MTH107—NOTEs",
    "section": "",
    "text": "I feel so lucky to learn from Professor Paul-Henry Leemann. He teaches Linear Algebra very well. He not only teahces us how to solve tutorials, but also the real application in the real world such as the application of eigenvectors on Google search. He is also a kind, responsible and humorous teacher. His lecture notes are clean and clear with multiple good examples helping us to understand more effectively.\nProfessor Andrew Lin teaches Linear Algebra very very well and interesting. He teaches me many tips to help me never remember sth., understand totally instead."
  },
  {
    "objectID": "mth107.html#eigenvalue-and-eigenspace",
    "href": "mth107.html#eigenvalue-and-eigenspace",
    "title": "MTH107—NOTEs",
    "section": "Eigenvalue and Eigenspace",
    "text": "Eigenvalue and Eigenspace\nAn operator on V has a diagnalizable matrix representation if it has the same number as dimV of eigenvectors .\nHowever, some operators have not enough eigenvectors to span a whole vector space so that the matrix could not be as simple as a diagnalizable matrix."
  },
  {
    "objectID": "mth107.html#generalized-eigenspace",
    "href": "mth107.html#generalized-eigenspace",
    "title": "MTH107—NOTEs",
    "section": "Generalized eigenspace",
    "text": "Generalized eigenspace\n\\(A=PJ_{\\lambda}P^{-1}\\)\nWhen P has linearly independent vectors(eigenvectors), A is diagonalizable\nSo we introduce the Jordan form which is as simple as possible, though not much simple as a diagnal matrix, which need generalized eigenvector to get P—\\((A-\\lambda I)^2x_2=(A-\\lambda I)x_1=0\\)\nSince \\(A=PJ_{\\lambda}P^{-1}\\), P=[\\(x_1,x_2\\)], AP=A[\\(x_1,x_2\\)]=[\\(\\lambda x_1, \\lambda x_2+x_1\\)]=PJ=[\\(x_1,x_2\\)]=\\(\\left[\\begin{array}{ll}\\lambda & 1 \\\\ 0 & \\lambda\\end{array}\\right]\\)\nThe introduce of generalized eigenspaces makes a matrix as simple as possible, though not much simple as a diagnal matrix for computation."
  },
  {
    "objectID": "mth107.html#cayley-hamlton-thm",
    "href": "mth107.html#cayley-hamlton-thm",
    "title": "MTH107—NOTEs",
    "section": "Cayley Hamlton Thm",
    "text": "Cayley Hamlton Thm\nV a finite dimensional vector space then \\(\\chi_T(T)=0_{l(v)}\\)\neg. A=$"
  },
  {
    "objectID": "mth107.html#jordan-form-makes-me-understand-linear-algebra-more-deeply-which-also-introduces-differential-equations-to-me",
    "href": "mth107.html#jordan-form-makes-me-understand-linear-algebra-more-deeply-which-also-introduces-differential-equations-to-me",
    "title": "MTH107—NOTEs",
    "section": "Jordan form makes me understand Linear Algebra more deeply, which also introduces differential equations to me",
    "text": "Jordan form makes me understand Linear Algebra more deeply, which also introduces differential equations to me\nJ = \\(\\lambda I+ N\\), where N is a nilpotent matrix —D-N decomposition A = D +N DN = ND\neg.\\(e^A=e^{D+N} ?= e^De^N\\)\n$e^{tJ_}= e^{tI} $\\(e^{tN}\\)=\\(e^{t\\lambda} [I+N+t^2/2! N^2+.....]\\)\n\nThe characteristic polynomial tells us the eigenvalues and the dimension of each generalized eigenspace, which is the number of times the eigenvalue appears along the diagonal of the Jordan form (also known as the “multiplicity” of λ)\nEach Jordan block with respect to a basis of an eigenvector and others are generalized eigenvectors which could be computed based on the eigenvector and \\((T-\\lambda Id)^m,\\)where m\\(\\leq dimV\\)since the dimension of each eigenspace tells us how many Jordan blocks corresponding to that eigenvalue there are in the Jordan form. This also means that the Jordan form is unique with permutation but the Jordan basis is not unique.\nThe size of the largest Jordan block corresponding to an eigenvalue λ of T is exactly the degree of the (t − λ) term in the minimal polynomial of T .i.e. The exponents of the different terms in the minimal polynomial tell us the sizes of the largest Jordan blocks corresponding to each eigenvalue.\n\nTo kill off a Jordan block of size k, we need the polynomial \\((t-\\lambda)^k\\). Any smaller Jordan block with the same eigenvalue will also be killed off under this polynomial\n\nWe could explain it more clearly\n\n\nFor one simple situation, which is the Jordan form with only 1 block\nSince the minimal polynomial applied by the operator T on V (\\(M_T(T)\\)) must be 0, the Jordan form with only 1 block has the minimal polynomial (t-\\(\\lambda\\))\\(^{dimV}\\)\nMore generally, if we have a Jordan form with 2 blocks B and C, with one same eigenvalue, Since the minimal polynomial applied by the operator T on V (\\(M_T(T)\\)) must be 0, \\((T-\\lambda I)^m\\) in \\(M_T(T)\\) should have m satisfying \\((B-\\lambda I)^m=0\\) and \\((C-\\lambda I)^m=0\\). In this case, since any smaller Jordan block with the same eigenvalue will also be killed off under the \\((T-\\lambda I)^m\\) in \\(M_T(T)\\), when the largest Jordan block is 0, smaller blocks are also 0. What is more, if \\(\\exists\\) a block with dimension greater than the power of the corresponding term in the polynomial, there then is a contradiction to the defination of the minimal polynomial(must be 0). If the largest Jordan block has the dimension strictly less than the power of the corresponding term in the polynomial, there then also is a contradiction to the defination of the minimal polynomial(smallest power)\nEach block has a basis of eigenvector and other basis are generalized eigenvector which are linearly independent.\nMore precisely, each term with power 1, means that the restriction of T to the corresponding generalised eigenspace is diagonalisable.So the minimal polynomial’s each term with the power of 1 means the matrix representation of the operator is diagonalizable, and each eigenvalue has the same number of corresponding power of characteristic polynomial’s term of linearly independent eigenvectors."
  },
  {
    "objectID": "mth107.html#the-relationship-between-the-direct-sumdecomposition-and-diagolizable",
    "href": "mth107.html#the-relationship-between-the-direct-sumdecomposition-and-diagolizable",
    "title": "MTH107—NOTEs",
    "section": "The relationship between the direct sum(decomposition) and diagolizable",
    "text": "The relationship between the direct sum(decomposition) and diagolizable\n\nIf A:V–&gt; V, A is diagonalizable then V=RangeA direct sum NullA (proof idea: clarify the basis of A by the non-zero element and zero element on the diagonal of the diagonal shape matrix of A. Based on them we could find the basis of RangeA and NullA which span V while with the intersection of {0})"
  },
  {
    "objectID": "mth107.html#the-relationship-between-jordan-form-and-diffrential-equation",
    "href": "mth107.html#the-relationship-between-jordan-form-and-diffrential-equation",
    "title": "MTH107—NOTEs",
    "section": "The relationship between Jordan form and diffrential equation",
    "text": "The relationship between Jordan form and diffrential equation"
  },
  {
    "objectID": "mth107.html#diagnalize-2-matrix-at-the-same-time",
    "href": "mth107.html#diagnalize-2-matrix-at-the-same-time",
    "title": "MTH107—NOTEs",
    "section": "Diagnalize 2 matrix at the same time",
    "text": "Diagnalize 2 matrix at the same time"
  },
  {
    "objectID": "mth107.html#linearty-is-everywherelinearty-is-the-easiest-one",
    "href": "mth107.html#linearty-is-everywherelinearty-is-the-easiest-one",
    "title": "MTH107—NOTEs",
    "section": "Linearty is everywhere–Linearty is the easiest one",
    "text": "Linearty is everywhere–Linearty is the easiest one\nmatrix-vector multiplication\ndifferentiation/integration\nODE(mth106)\nRecurrence Relations and Statistical Models"
  },
  {
    "objectID": "mth107.html#comparison",
    "href": "mth107.html#comparison",
    "title": "MTH107—NOTEs",
    "section": "comparison",
    "text": "comparison\n\nReal R space\nA matrix A in this space is a real matrix, which maps vectors in \\(\\mathbb{R^n}\\) to another vector in \\(\\mathbb{R^n}\\) through multiplication: \\[\nA: \\mathbb{R^n} \\to \\mathbb{R^n}\n\\] \\[\nA \\cdot v = w \\in \\mathbb{R^n}\n\\]\nreal matrix\nconcrete\n\n\nAbstract vector space over R or C\nlinear transformation: \\[\nT: V \\to W\n\\] abstract and more general vector space over \\(\\mathbb{R^n}\\) or \\(\\mathbb{C^n}\\)"
  },
  {
    "objectID": "mth107.html#notations",
    "href": "mth107.html#notations",
    "title": "MTH107—NOTEs",
    "section": "Notations",
    "text": "Notations\n\n\\[\na \\in A\n\\] eg. \\[\n3 \\in \\mathbb{Z}\n\\]\n\\(\\emptyset\\)\nTwo sets A and B are equal if and only if they contain the same elements: \\[\nA = B\n\\] if and only if \\[\n\\forall x \\, (x \\in A \\iff x \\in B)\n\\] \\[ A \\subseteq B\\]\nintersection: \\[ A \\cap B\\]\nunion: \\[ A\\cup B \\]"
  },
  {
    "objectID": "mth107.html#maps",
    "href": "mth107.html#maps",
    "title": "MTH107—NOTEs",
    "section": "Maps",
    "text": "Maps\nmaps(functions)\n\\[\nf: A \\to B, \\quad a \\mapsto f(a)\n\\] composition: Given two functions \\(f: B \\to C\\) and \\(g: A \\to B\\), the composition of f and g is denoted as: \\[\n(f \\circ g)(x) = f(g(x)), \\quad \\text{for all} \\, x \\in A\n\\]\n\\[\na \\mapsto c(a \\mapsto b \\mapsto c)\n\\]\n\ninjective–one to one\nif \\(a_1 \\neq a_2\\) then \\(f(a_1) \\neq f(a_2)\\)\nif \\(f(a_1) = f(a_2)\\), then \\(a_1=a_2\\)\n\n\nsurjective–onto\nfor \\(f: A \\to B\\)\n\\[\n\\forall b \\in B, \\, \\exists a \\in A \\, \\text{such that} \\, f(a) = b\n\\]\n\n\nbijetive–both inj and suj\nfor \\(f: A \\to B\\) \\[\n\\forall b \\in B, \\, \\exists! a \\in A \\, \\text{such that} \\, f(a) = b\n\\] （use \\(\\exists!\\)expresses only exist one）\nf: x–&gt;y is a bijection if and only if \\(\\exists\\)g:y—&gt;x, s.t. f(g(y))=\\(id_y\\) and g(f(x))=\\(id_x\\)"
  },
  {
    "objectID": "mth107.html#subsetsinclusions-restrictions",
    "href": "mth107.html#subsetsinclusions-restrictions",
    "title": "MTH107—NOTEs",
    "section": "subsets,inclusions, restrictions",
    "text": "subsets,inclusions, restrictions\n\nsubsets\n\\(A \\subseteq B\\)\n\n\nempty set is a subset of every set B.\n\\[\n\\emptyset \\to B\n\\]\n\n\ninclusion\nif A \\(\\subseteq\\) B, we have a map: \\[\n\\iota: A \\to B\n\\]\n\\[\n\\iota(a) = a\n\\], called the inclusion of A (into B)\n\n\nrestriction\nThat is:\nif A \\(\\subseteq\\) B and \\(f:B \\to C\\), we have a map for all a \\(\\in\\) A: \\[\nf|_A: A \\to C\n\\]:\n\\[\nf|_A(a) = f(a)\n\\] called the restriction of f to A\n\\[\nf|_A = f \\circ \\iota_A\n\\] because \\[\nf \\circ \\iota_A(a)=f(\\iota_A(a))=f(a)=f|_A(a)\n\\]\nthe reason to define it:\n\ndomains differ\n\nFor example, consider a map \\(g|_B: B \\to C\\), where the function takes each element \\(x\\) and maps it to \\(x + 2\\). Let the sets be as follows:\n\n\\(A = \\mathbb{N} \\subseteq B = \\mathbb{R}\\)\n\\(C = \\mathbb{R}\\) is the codomain of the function.\n\nThe map \\(g\\) is defined as: \\[ g: B \\to C, \\quad g(x) = x + 2 \\quad \\text{for all } x \\in B \\]\nNow, consider the restriction of \\(g\\) to \\(A\\), denoted \\(g|_A: A \\to C\\), where: \\[ g|_A: A \\to C, \\quad g|_A(x) = x + 2 \\quad \\text{for all } x \\in A \\]\nAlthough both \\(g: \\mathbb{R} \\to \\mathbb{R}\\) and \\(g|_A: \\mathbb{N} \\to \\mathbb{R}\\) follow the same rule \\(x \\mapsto x + 2\\), they cannot be considered the same map because their domains differ.\n(here, A \\(\\subseteq\\) B)\n\nfocus on the specific area within the whole area\n\n\n\nset of all maps from A to B(f)\nif a and b are sets we define \\(B^A={f:A---&gt;B (map)}\\) the set of all maps from A to B\n\\[\nB^A = \\{ f: A \\to B \\}\n\\] \\(B^\\emptyset = \\{ \\emptyset \\to B \\}\\) has only 1 element even if B=\\(\\emptyset\\)\n\nThe set \\(B^{\\emptyset}\\) contains only the zero function. proof: suppose f,g \\(\\in\\) \\(B^\\emptyset = \\{f: \\emptyset \\to B \\}\\), imagine f != g, we can derive \\(\\exists x \\in \\emptyset\\) s.t. f(x) != g(x), which is absurd, so f=g\n\nsuppose the only one element is sth.:\nsth. should follow the quality of the set, which is:\nsth. + sth. = sth.\n\\(\\lambda\\)sth. =sth.\nso sth. = 0\n\nrelevant to cardinality:\n(Def: let A be a.set, if A containsfinitely many elements, then the number of elements of A is called the cardinality of A, denoted by |A|(|A|\\(\\in Z_{\\geq0}\\))\n|| #\n|A|: number of elements in A(set))\nif A and B are finite sets #A=n #B=m, then \\(|B^A| = m^n\\) (ok)"
  },
  {
    "objectID": "mth107.html#finite-space",
    "href": "mth107.html#finite-space",
    "title": "MTH107—NOTEs",
    "section": "finite space",
    "text": "finite space\nFor finite spaces ,only R^n and C^n.\n(For infinite spaces, there are many, e.g. a continuous set: [0,1] to …)"
  },
  {
    "objectID": "mth107.html#rn-notification",
    "href": "mth107.html#rn-notification",
    "title": "MTH107—NOTEs",
    "section": "R^n notification",
    "text": "R^n notification\n\\[\n\\mathbb{R}^n = \\{ (x_1, x_2, \\dots, x_n) \\mid x_i \\in \\mathbb{R} \\text{ for } i = 1, 2, \\dots, n \\}\n\\] real numbers\n\ndefine 2 oprations on the set R^n\naddition: \\[\n(x_1, x_2, \\dots, x_n) + (y_1, y_2, \\dots, y_n) = (x_1 + y_1, x_2 + y_2, \\dots, x_n + y_n)\n\\]\n\nLeft-hand side: \\((x_1, x_2, \\dots, x_n), (y_1, y_2, \\dots, y_n) \\in \\mathbb{R}^n\\)\nRight-hand side: The result of \\(x_i + y_i\\) is in \\(\\mathbb{R}\\).\n\nscalar multiplication: for lambda \\(\\in\\) R,\n\\[\n\\lambda \\cdot (x_1, x_2, \\dots, x_n) = (\\lambda x_1, \\lambda x_2, \\dots, \\lambda x_n)\n\\]\n\nLeft-hand side: \\(\\lambda \\in \\mathbb{R}, (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n\\).\nRight-hand side: The result \\(\\lambda x_i \\in \\mathbb{R}\\), for each i.\n\nthese 2 oprations generalize the standard operations on R2 and R3\n\n\ngeometric realization\n矢量三角形–addition\n直线上–scalar multiplication\nmention: for n greater than or equal to 4 we cannot visualize vectors in the real world but e can still use them to solove real world problems\nif a and b are finite sets\n\n\nto simplify notations we sometimes use a single letter for vectors:\nx= \\(\\vec x\\) =(x_1,x_2,….)\nR^n as a vector space\nso we have(rn,+, mutiplication notation). the operations satisfies some useful properties that turn rn into a real vector space"
  },
  {
    "objectID": "mth107.html#relation-between-rn-and-.",
    "href": "mth107.html#relation-between-rn-and-.",
    "title": "MTH107—NOTEs",
    "section": "relation between rn and + .",
    "text": "relation between rn and + .\n\\((R^n,+,\\cdot)\\)\nit means that + and \\(\\cdot\\) satisfy the following axioms:\n\n\\(\\forall x,y \\in rn\\): x+y=y+x. commutativity\n\\(\\forall x,y,z \\in rn\\): (x+y)+z=x+(y+z). associativity\nx + 0 = 0 + x neutral element for addition\n\\(x + (-x) = (-x) + x = 0\\)\n\n——inverse for addition -x=y\n\n\\(1 \\cdot x = x\\) Identity Element for Scalar Multiplication\n\\((\\lambda \\mu) \\cdot x = \\lambda \\cdot (\\mu \\cdot x)\\) compatibilily of multiplication\n\\(\\lambda \\cdot (x + y) = \\lambda \\cdot x + \\lambda \\cdot y\\) Distributivity of Scalar Multiplication Over Vector Addition\n\\((\\lambda + \\mu) \\cdot x = \\lambda \\cdot x + \\mu \\cdot x\\) Distributivity of Scalar Addition\n\n\n?\nalso fn—f{1,..,n}\n5st ahead and the other 3（prove？eg tutorial1）. should we prove again or directly use them? tutorial 1 vs. 2 ’ s proof. ask again, sorry: since negative number set satisfies 2 oprations but not 8 axioms I want to ensure if a finite set satisfies the 2 operations it is not satisfy all 1-8 axioms instead of F^n so if the problem is a vector space V, we could directly suppose \\(\\exists u\\in F\\) and then do the scalar multiplication in it?\nis that because of the defination of F-vector space?(abstract vector space-defination)"
  },
  {
    "objectID": "mth107.html#cn-complex-vector-space",
    "href": "mth107.html#cn-complex-vector-space",
    "title": "MTH107—NOTEs",
    "section": "Cn complex Vector Space",
    "text": "Cn complex Vector Space\n\\(i^2 = -1\\)\nDefine \\(\\mathbb {C}^n\\) as the set of all ordered n-tuples of complex numbers: \\[\n\\mathbb{C}^n = \\{ (z_1, z_2, \\dots, z_n) \\mid z_i \\in \\mathbb{C}, \\, i = 1, 2, \\dots, n \\}\n\\]\n\nOperations on C^n\nWe define addition and scalar multiplication on \\(\\mathbb{C}^n\\) in the same way as we did on \\(\\mathbb{R}^n\\), but using complex numbers:\n\nAddition:\nFor two vectors \\((z_1, z_2, \\dots, z_n)\\) and \\((w_1, w_2, \\dots, w_n) \\in \\mathbb{C}^n\\):\n\\[\n(z_1, z_2, \\dots, z_n) + (w_1, w_2, \\dots, w_n) = (z_1 + w_1, z_2 + w_2, \\dots, z_n + w_n)\n\\]\n\n\nScalar Multiplication:\nFor a scalar \\(\\lambda \\in \\mathbb{C}\\) and a vector \\((z_1, z_2, \\dots, z_n) \\in \\mathbb{C}^n\\):\n\\[\n\\lambda \\cdot (z_1, z_2, \\dots, z_n) = (\\lambda z_1, \\lambda z_2, \\dots, \\lambda z_n)\n\\]\nThus, \\((\\mathbb{C}^n, +, \\cdot)\\) is a complex vector space because it satisfies the vector space axioms 1-8, where we replaced \\(\\mathbb{R}^n\\) with \\(\\mathbb{C}^n\\). Many of the results from MTH107 will hold regardless of whether we are using \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\), so we will often use \\(\\mathbb{F}\\) to represent either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\nFor example, \\(\\mathbb{F}^n\\) is an \\(\\mathbb{F}\\)-vector space, where \\(\\mathbb{F}\\) could be either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\).\n\n\n\nGeneralization (Not on the Exam):\n\nFinite Field Example:\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements.\nMany of our results hold in a more general setting, where \\(\\mathbb{F}\\) is a field—a set in which we can perform addition, multiplication, subtraction, and division (except division by zero).\nExamples of fields include:\n\n\\(\\mathbb{R}\\): the real numbers\n\\(\\mathbb{C}\\): the complex numbers\n\\(\\mathbb{Q}\\): the rational numbers\n\\(\\mathbb{F}_2 = \\{0, 1\\}\\): the finite field with two elements"
  },
  {
    "objectID": "mth107.html#abstract-vector-space",
    "href": "mth107.html#abstract-vector-space",
    "title": "MTH107—NOTEs",
    "section": "Abstract Vector Space:",
    "text": "Abstract Vector Space:\nWe can generalize this idea by replacing \\(\\mathbb{F}^n\\) with some abstract space \\(V\\), define addition \\(+\\) and scalar multiplication \\(\\cdot\\), and check if they satisfy the eight vector space axioms.\n\nDefinitions:\nFor a set \\(V\\), addition on \\(V\\) is a map:\n\\[\nV \\times V \\to V\n\\]\nIt maps an element set to their addition. For example, if \\(V = \\mathbb{R}^2\\):\n\\[\n(v, w) \\mapsto v + w\n\\]\nExample: \\((1,2) + (3,4) = (4,6)\\), which is also in \\(V\\).\nA scalar multiplication is a map:\n\\[\nF \\times V \\to V\n\\]\nFor example, \\((\\lambda, v) \\mapsto \\lambda v\\).\n(remark:if V and W are sets, V\\(\\times\\) W = {(v,w)|v\\(\\in\\) V, w\\(\\in\\) W})\nAn \\(F\\)-vector space is a set \\(V\\) with an addition \\(+\\) and scalar multiplication \\(\\cdot\\) by elements of \\(F\\), such that \\((V, +, \\cdot)\\) satisfies the vector space axioms 1-8, where \\(\\mathbb{R}\\) is replaced by \\(F\\), and \\(\\mathbb{R}^n\\) is replaced by \\(V\\).\n\n\nRemarks and Examples:\n\nVectors: Vectors are elements of \\(V\\), denoted as \\(v \\in V\\).\nField \\(F\\): The choice of \\(F\\) matters! For example, we will see later that \\(\\mathbb{C}^n\\) is a complex vector space of dimension \\(n\\), but is also a real vector space of dimension \\(2n\\)."
  },
  {
    "objectID": "mth107.html#prove",
    "href": "mth107.html#prove",
    "title": "MTH107—NOTEs",
    "section": "？prove？",
    "text": "？prove？\nabove field F\n107’s learning need of C\n2n proof???\n\nExamples:\nFor any field \\(F\\),\n\nTrivial Vector Space: the set \\(V = \\{0\\}\\) is a trivial \\(F\\)-vector space with addition \\(0 + 0 = 0\\) and scalar multiplication \\(\\lambda \\cdot 0 = 0\\).\nFinite-Dimensional Vector Space: \\(F^n\\) is an \\(F\\)-vector space, and \\(F^0 = \\{0\\}\\).\nInfinite-Dimensional Vector Space: Let \\(F^\\infty\\) be the space of infinite sequences, where:\n\n\\[\nF^\\infty = \\{ (x_1, x_2, \\dots) \\mid x_i \\in F, \\, i = 1, 2, \\dots \\}\n\\]\nAddition and scalar multiplication are defined component-wise:\n\\[\n(x_1, x_2, \\dots) + (y_1, y_2, \\dots) = (x_1 + y_1, x_2 + y_2, \\dots)\n\\]\n\nFunction Space: Let \\(S\\) be a set, then:\n\n\\[\nF^S = \\{ f: S \\to F \\, \\text{(maps from S to F)} \\}\n\\]\nAddition and scalar multiplication are defined pointwise. For functions \\(f, g \\in F^S\\) and \\(\\lambda \\in F\\):\n\\[\n(f + g)(x) = f(x) + g(x) \\quad \\forall x \\in S\n\\]\n\\[\n(\\lambda \\cdot f)(x) = \\lambda \\cdot f(x) \\quad \\forall x \\in S\n\\]\n\\(F^S\\) is an F-vector space"
  },
  {
    "objectID": "mth107.html#section-1",
    "href": "mth107.html#section-1",
    "title": "MTH107—NOTEs",
    "section": "?",
    "text": "?\ndoes this thing have more explaination except for f：s to F\nis there any quick way to think this kind of question instead of proving 8 axioms one by one? such as geometry meaning like keep straight line and parallelogram in linear transformation\n\nproof of FS\n\\(F^S\\) is a vector space\nProof: we need to check the 8 axioms from the defination\n(core: use the element here to x and then see the equality of the 2 function on the each side of the equality using the quality of \\(\\mathbb F^n\\))\n\nf + g = g+f?\n\nthese 2 functions ((f+g)(x) and (g+f)(x)) are equal if and only if they have values agrees on every \\(s \\in S\\)\n\\[\n(f+g)(x)=f(x)+g(x)\n\\] which is \\(\\in F\\) so it is equal to g(x)+f(x)(addition axiom)=(g+f)(x)\n2)(f+g)+h = f+(g+h)\n[(f+g)+h](x)=(f+g)(x)+h(x)=[f(x)+g(x)]+h(x), which are \\(\\in F\\), so\n=f(x)+[g(x)+h(x)](axiom 3)=f(x)+(g+h)(x)=[f+(g+h)](x)\n\n\\(\\exists 0: 0+f=f\\) for any \\(f \\in F^S\\)\n\nyes, define the zero function \\(0_F:\\) to be the constant function \\(0_F\\), i.e. \\(0_F(x)=0_F\\)\n\nfor \\(f\\in F^S\\), can we find an inverse?\n\n-f is defined by (-f)(x)=-f(x)\ncheck: (f+(-f))(x)=f(x)+(-f(x))=\\(0_F\\)\nso f+(-f)=\\(0_{F^S}\\)\n\n1f=f \\(（1\\cdot f)(x)=1\\cdot f(x)(\\in F)=f(x)\\)\n(ab)f=a(bf)\n\npass\n7)a(f+g)=af+ag\na(f+g)(x)=a(f(x)+g(x))=af(x)+ag(x)=(af+ag)(x)\n8)(a+b)f=af+bf\n(a+b)f(x)=af(x)+bf(x)\n(af+bf)(x)=(af)(x)+(bf)(x)=af(x)+bf(x)\n\n\nSpecial Cases:\n\nEmpty Set: If \\(S = \\emptyset\\), then:\n\n\\[\nF^{\\emptyset} = \\{ 0 \\}\n\\] the proof has been proved above on “set of all maps from A to B(f)”\n\nFinite Set: If \\(S = \\{1, 2, \\dots, n\\}\\), then:\n\n\\[\nF^{\\{1, 2, \\dots, n\\}} = F^n\n\\]\n\nproof of special 2:\nLet \\(F^n\\) represent the n-dimensional vector space over a field \\(F\\), and \\(F^{\\{1, \\dots, n\\}}\\) represent the set of functions from the set \\(\\{1, \\dots, n\\}\\) to \\(F\\), i.e., it assigns a scalar from \\(F\\) to each index in \\(\\{1, \\dots, n\\}\\).\nWe will prove that there exists a linear map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) such that:\n\n\\(T\\) is a bijection (i.e., both injective and surjective).\n\\(T\\) preserves addition: \\[\nT(f + g) = T(f) + T(g)\n\\] for all \\(f, g \\in F^n\\).\n\\(T\\) preserves scalar multiplication: \\[\nT(a f) = a T(f)\n\\] for all \\(f \\in F^n\\) and \\(a \\in F\\).\n\nA map that satisfies these two conditions is called a linear map, and we will learn about it later in class. A linear map that is a bijection is called an isomorphism.\nDefine the Map \\(T\\)\nDefine the map \\(T: F^n \\to F^{\\{1, \\dots, n\\}}\\) as follows:\nFor each vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function \\(T((a_1, a_2, \\dots, a_n)) = f \\in F^{\\{1, \\dots, n\\}}\\) by: \\[\nf(i) = a_i \\quad \\text{for each} \\, i = 1, 2, \\dots, n.\n\\] Thus, \\(T((a_1, a_2, \\dots, a_n)) = (f(1), f(2), \\dots, f(n)) = (a_1, a_2, \\dots, a_n)\\).\n\nInjectivity: \\(T((a_1, a_2, \\dots, a_n)) = T((b_1, b_2, \\dots, b_n))\\). This implies that for each \\(i\\), \\(a_i = b_i\\). Therefore, \\((a_1, a_2, \\dots, a_n) = (b_1, b_2, \\dots, b_n)\\), so \\(T\\) is injective. i.e. if \\(T(a_1,...a_n)=T(b_1,...,b_n)\\), then \\(\\forall i,\\) T(a)(i)=ai=T(b)(i)=bi,so\\(\\forall i,a_i=b_i\\),we need a=b\nSurjectivity: Given any function \\(f \\in F^{\\{1, \\dots, n\\}}\\), we can find a vector \\((a_1, a_2, \\dots, a_n) \\in F^n\\) such that \\(f(i) = a_i\\) for each \\(i = 1, 2, \\dots, n\\). Therefore, \\(T\\) is surjective.\n\ni.e., given \\(f\\in \\mathbb F^{\\{1,..n\\}}\\), then T(f(1),…f(n))=f with (f(1),…f(n))\\(\\in F^n\\)(T is {1,..n}–&gt;F) with (f(1),…f(n))\\(\\in F^n\\)\nSince \\(T\\) is both injective and surjective, it is a bijection.\nProve that \\(T\\) Preserves Addition\nLet \\((a_1, a_2, \\dots, a_n), (b_1, b_2, \\dots, b_n) \\in F^n\\). Then:\n\\[\nT((a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n)) = T((a_1 + b_1, a_2 + b_2, \\dots, a_n + b_n)).\n\\]\n\\[\nf(i) = a_i + b_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\)) the other hand site: \\[\nT((a_1, a_2, \\dots, a_n)) + T((b_1, b_2, \\dots, b_n)) = (a_1, a_2, \\dots, a_n) + (b_1, b_2, \\dots, b_n),\n\\] which results f, also.\nProve that \\(T\\) Preserves Scalar Multiplication\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\) and \\(c \\in F\\). Then: \\[\nT(c \\cdot (a_1, a_2, \\dots, a_n)) = T((c a_1, c a_2, \\dots, c a_n)).\n\\]\n\\[\nf(i) = c a_i, \\quad i = 1, 2, \\dots, n.\n\\] (the definition of \\(T\\))\nthe other hand site: \\[\nc \\cdot T((a_1, a_2, \\dots, a_n)) = c \\cdot (a_1, a_2, \\dots, a_n),\n\\] which also results in \\(f\\)\nSince T is an isomorphism (bijection + linear), then \\(T^{-1}\\) is automatically linear. That is we automatically have \\(T^{-1}(x+y)= T^{-1}(x)+ T^{-1}(y)\\), and the same for scalar multiplication. So they are the same in vector space, too.\nWe have shown that \\(T\\) is a bijection and preserves both addition and scalar multiplication. Therefore, \\(T\\) is a linear isomorphism, and \\(F^n\\) and \\(F^{\\{1, \\dots, n\\}}\\) are isomorphic as vector spaces.\n(Last remark: If \\(T\\) is an isomorphism (i.e., bijective and linear), then \\(T^{-1}\\) is automatically linear. That is, we automatically have \\(T^{-1}(x + y) = T^{-1}(x) + T^{-1}(y)\\), and the same holds for scalar multiplication. Therefore, there is no need to check these properties for \\(T^{-1}\\).)\n(a more detailed one proving bijetive: \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is Equivalent to \\(F^n\\)\nTo prove that \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is equivalent to \\(F^n\\), we show there is a bijection between the two sets, meaning each element in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) corresponds to a unique element in \\(F^n\\), and vice versa.\nBy definition, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) is the set of all functions from \\(\\{1, 2, 3, \\dots, n\\}\\) to \\(F\\). Each function \\(f\\) can be written as:\n\\[\nf = (f(1), f(2), \\dots, f(n)).\n\\]\n\\(F^n\\) is the set of ordered \\(n\\)-tuples \\((a_1, a_2, \\dots, a_n)\\), where each \\(a_i \\in F\\).\nMapping from \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) to \\(F^n\\)—\nFor any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), we define the corresponding tuple in \\(F^n\\) as:\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nMapping from \\(F^n\\) to \\(F^{\\{1, 2, 3, \\dots, n\\}}\\)—-\nFor any tuple \\((a_1, a_2, \\dots, a_n) \\in F^n\\), define the corresponding function in \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) as:\n\\[\nT^{-1}(a_1, a_2, \\dots, a_n)(i) = a_i, \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nLet \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\)–\n\\[\nT(f) = (f(1), f(2), \\dots, f(n)).\n\\]\nthen, define\n\\[\nT^{-1}(f(1), f(2), \\dots, f(n))(i) = f(i), \\quad \\text{for each } i \\in \\{1, 2, \\dots, n\\}.\n\\]\nThus,\n\\[\nT^{-1}(T(f)) = f.\n\\]\nLet \\((a_1, a_2, \\dots, a_n) \\in F^n\\). Applying \\(T^{-1}\\), we get the function \\(f\\) such that \\(f(i) = a_i\\) for each \\(i\\). And then,\n\\[\nT(T^{-1}(a_1, a_2, \\dots, a_n)) = (a_1, a_2, \\dots, a_n).\n\\]\nThus,\n\\[\nT \\circ T^{-1} = \\text{id}_{F^n}.\n\\] This equation means that for any element \\((a_1, a_2, \\dots, a_n) \\in F^n\\), applying \\(T^{-1}\\) to obtain a function \\(f\\), and then applying \\(T\\) to \\(f\\), returns the original tuple:\n\\[\nT^{-1} \\circ T = \\text{id}_{F^{\\{1,2,3,...n\\}}}.\n\\]\nThis equation means that for any function \\(f \\in F^{\\{1, 2, 3, \\dots, n\\}}\\), applying \\(T\\) to obtain a tuple \\((f(1), f(2), \\dots, f(n))\\), and then applying \\(T^{-1}\\) to that tuple, returns the original function:\nSince \\(T\\) and \\(T^{-1}\\) are inverses of each other, we have established a bijection between \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\). Therefore, \\(F^{\\{1, 2, 3, \\dots, n\\}}\\) and \\(F^n\\) are equivalent as sets and vector spaces.)"
  },
  {
    "objectID": "mth107.html#linear-transformation-makes-us-know-everything",
    "href": "mth107.html#linear-transformation-makes-us-know-everything",
    "title": "MTH107—NOTEs",
    "section": "linear transformation makes us know everything",
    "text": "linear transformation makes us know everything\n二维映射到三维的linear transformation’s geometric meaning： if you know a transformation， then you know everything"
  },
  {
    "objectID": "mth107.html#general-properties-of-vector-space",
    "href": "mth107.html#general-properties-of-vector-space",
    "title": "MTH107—NOTEs",
    "section": "General properties of vector space",
    "text": "General properties of vector space\nLet V be a vector space over F\nWe will prove some properties of V using only the defination (axioms 1-8)\n\nProposition\n\nThe zero(additive identity) is unique. That is: \\(\\exists ! 0\\in V s.t. 0+V=V, \\forall v\\in V\\)\n\nproof: Suppose we have 2 zero elements: 0 and 0’\n0=0’+0=0+0’=0’\n\n\\(\\forall v\\in V\\) there exists a unique additive inverse\n\nSupppose w and w’ are 2 inverses for v, w=0+w=(v+w’)+w=v+(w’+w)=v+(w+w’)=(v+w)+w’=0+w’=w’\n\n\\(\\forall v\\in V\\), \\(O_F\\cdot V=O_V\\)\n\n\n\n?\n怎么想到的 why \\(0_F\\cdot V=0_V+w\\) F—&gt;V????\nwhy we could think of let w be the inverse of \\(0_F\\cdot V\\)? I always just could recite I mean remember the process instead of write it down smoothly\n(see 0_f as 0 is also okay)\nproof: \\(0_F\\)\\(\\cdot\\)V= \\((0_F+0_F)\\)\\(\\cdot\\)V=\\(0_Fv+0_Fv\\)\nlet w be the inverse of \\(0_F\\cdot v\\)(use proposition 2)\nthen \\(0_V\\)=\\(0_F\\cdot v+w\\)=\\((0_Fv+0_Fv)+w=0_Fv+(0_Fv+w)=0_Fv\\)\n\n\\(\\forall x\\in F: x\\cdot O_V=O_F\\)\n\nproof: \\(x\\cdot 0_V=x\\cdot (0_V+0_V)=x\\cdot 0_V+x\\cdot 0_V\\)\nso \\(0_V=x\\cdot 0_V\\)\n\n?\ndifference between o_Fv=0_v and xo_v=0_f(3 and 4 proposition)\n\n\n\nProperty\n\\(\\forall v\\in V,(-1)\\cdot V=-V\\)\nproof: V+(-1)V=(1+(-1))\\(\\cdot\\)V=0\\(\\cdot\\)V=0\nso (-1)V is an addictive inverse of V\nso we finish proof(by the addictive inverse)\n\nreminder of computing inverse of a matrix\nmethod1:\ndet(A)\n每一个位置的det构成的矩阵：B\ncofactor matrix:\\((-1)^{n+m}\\)\nC=B\\(\\times\\) cofactor matrix\n\\(A^{-1}=1/det(A)\\) times \\(C^T\\)\nmethod2: work for the tansformation of a matrix"
  },
  {
    "objectID": "mth107.html#motivation",
    "href": "mth107.html#motivation",
    "title": "MTH107—NOTEs",
    "section": "motivation",
    "text": "motivation\n\nif sets have subsets, vector spaces have subspaces.\n\n(Mathematicians study subobjects to understand the big objects better)"
  },
  {
    "objectID": "mth107.html#def",
    "href": "mth107.html#def",
    "title": "MTH107—NOTEs",
    "section": "Def",
    "text": "Def\nLet \\(V(V,+_V,\\cdot_V)\\)(what is the link with restrictions here?: we have \\(U\\subseteq V\\), i.e. U\\(\\times\\)U={(\\(x_1,x_2|x_1,x_2\\in U\\))}\\(\\subseteq V\\times V\\), now we have \\(+_V|_{U\\times U}\\):U\\(\\times U\\)–&gt;V, and scalar multiplication is similar to it. ) be a vector space, a subset U of V is a subspace if and only if \\(U(U,+_V,\\cdot_V)\\) is a vector space(for the link with restrictions: Here, for the condition of U being a vector space, we need\n\\(+_V|_{U\\times U}\\):U\\(\\times U\\)–&gt;U i.e. for \\(u_1,u_2\\in U, u_1+_vu_2 \\in U, \\lambda \\cdot_vu_1\\in U)\\)\n\\(\\cdot_v|_{F\\times U}\\): \\(F\\times U\\)–&gt;U\n\\(0_V\\in U\\)\n, which could replace 2 operations and 8 axioms and we will prove later)\nex: R=U\\(\\subseteq\\)V=C (a C-vector space) is not a subspace because scalar multiplication is not internal(i\\(\\cdot \\pi\\)is not in R—-not satisfying \\(\\cdot_v|_{F\\times U}\\): \\(F\\times U\\)–&gt;U)\n—-so we should explicit \\(\\mathbb F\\) when saying vector spaces"
  },
  {
    "objectID": "mth107.html#section-4",
    "href": "mth107.html#section-4",
    "title": "MTH107—NOTEs",
    "section": "?",
    "text": "?\nC r vector space though i including???\n(ex: \\(\\mathbb C\\)is a R-vector space and \\(\\mathbb R \\subseteq \\mathbb C\\) is a R subspace)"
  },
  {
    "objectID": "mth107.html#propsition",
    "href": "mth107.html#propsition",
    "title": "MTH107—NOTEs",
    "section": "Propsition",
    "text": "Propsition\n\\(U\\subseteq V\\)is a subspace if and only if :\na 0\nb +\nc \\(\\cdot\\)\n\nproof\nfor 1-8 axioms we only need to prove 4) because others are either included by the proposition or share the qualities of them because the elements in U are the elements in V.\n4): there exists only one inverse in U\ni.e. V’s inverse is in U\nlet \\(u\\in U\\), then \\(u\\in V\\) so \\(\\exists v=(-u)\\in V\\), we need to check is -u in U s.t. -u+u=\\(0_v\\):\n-u=(-1)u \\(\\in U\\)(scalar multiplication)"
  },
  {
    "objectID": "mth107.html#eg",
    "href": "mth107.html#eg",
    "title": "MTH107—NOTEs",
    "section": "eg",
    "text": "eg\n\n\\(\\lambda \\in \\mathbb F\\), U:={\\(x_1,x_2,x_3,x_4|x_3=5x_4+\\lambda\\)} is a subset if and only if \\(\\lambda =0\\)\n\nproof:\nside one: 0=(0,0,0,0) is in U… and others satisfy the proposition also.\nside two: if \\(\\lambda =0\\), let x and y \\(\\in U\\), so \\(x_3=5x_4\\),\\(y_3=5y_4\\), so \\(x_3+y_3=5(x_4+y_4)\\), so the addition is satisfied.\nfor c, also\n\n\\(R_\\{geq0}\\) is not a vector space–(-1,2)—&gt; (-1)2=-2\\(\\notin R_{\\geq0}\\)\n\\(C^0\\)([0,1]):={f:[0,1]–&gt;R, continuous} is a subspace \\(\\mathbb R^{[0,1]}\\)"
  },
  {
    "objectID": "mth107.html#proof-of-all-kinds-of-subspaces-of-r2",
    "href": "mth107.html#proof-of-all-kinds-of-subspaces-of-r2",
    "title": "MTH107—NOTEs",
    "section": "proof of all kinds of subspaces of R2",
    "text": "proof of all kinds of subspaces of R2"
  },
  {
    "objectID": "mth107.html#sum-of-subspaces",
    "href": "mth107.html#sum-of-subspaces",
    "title": "MTH107—NOTEs",
    "section": "sum of subspaces",
    "text": "sum of subspaces\n\nfor sets, if \\(A \\subseteq C\\) and \\(B\\subseteq C\\) are 2 subsets then A U B\\(\\subseteq\\)C is still a subset of C and it is the smallest subset of C containing both A and B.\n\nQ: what about subspaces of vector spaces?\nin general if U W are Z subspaces of V then U U W is not a subspace\nex:\n\nin fact U u W is a subspace if and only if \\(U \\subseteq W\\) or \\(W \\subseteq U\\)\n\nQ: How to produce the smallest subspace of V containing both U and W?\n\nDef: ley U_1,…U_n be subspaces of V then their SUM is \\(U_1+...+U_n=\\{ u_1+...+u_n|u_i\\in U_i ,\\forall i\\}\\). This is a subset of V containing all possible sums of elements of the U\n\nex\n\nThm: let U_1,…U_b be subspace of V then U_1+…+U_nis the smallest subspace of V containing each of the \\(U_i\\)\n\nProof: we need to prove the following 3 things:\n\nU:=\\(U_1+...+U_n\\) is a subspace\n\n0=0+..0(U_i are subspaces)\\(\\in U\\)\nif U=sum Ui and V=sum Vi \\(\\in U\\) then U+V=(sum ui)+(sum vi )=(u1+v1)+…\\(\\in U\\)\nif \\(\\lambda \\in F, U\\in U\\)(U is the sum of Ui(these subspaces))\n\nthen lambda(u sum)=(lambda u_1)+…\\(\\in U\\)\n\\(\\forall i: U_i\\subseteq U\\)\n\nlet v\\(\\in U_i\\) hten v=0+0+0(U_{i-1})+v(U_i)+0+0+0\\(\\in U\\), which means \\(U_i\\in U\\)\n\nif W is another subpace of V containing all the \\(U_i\\) then \\(U\\subseteq W\\)\n\nfor any \\(u\\in U\\), U(sum)\\(\\in W\\)(\\(U_i\\) \\(\\in W\\))(sum of elements of W), which means U\\(\\subseteq W\\)\n\n\nRemark:\n\n\\(U\\subseteq W\\) if and only if U+W=W\nproof:\n\nwe can do U+W only if U and W are subspaces of the same space\n\n\nComparison of subsets(A B)/subspaces(U W):\n\n\\(A\\cap B\\) is the biggest subset contained in both A and B\n\\(A \\cup B\\) is the smallest subspace containing both U and W\nA \\(\\sqcup\\) (disjoint union) B: whenever \\(A\\cap B=\\emptyset\\),then |A|+|B|=|\\(A\\cup B\\)|\nbut as the disjoint union for subspaces:\nmaybe ask that \\(U \\cap W\\) is as small as possible, that is L \\(U\\cap W=\\{0\\}\\)\n\nDef Let U1-Un be the subspaces of V we say that U1+Un is a DIRECT SUM if (shuangjiantou) \\(\\forall U\\in U sum\\) there exists a unique way to write U=Usum\nex:\n\nin this case we write U_1 \\(\\oplus\\)… \\(\\oplus\\) U_n\n(if \\(u\\in  U_1 +\\)… \\(+\\) \\(U_n\\), then there always exists \\(U_i\\in U_1 +\\)… \\(+\\) U_n\\(\\in U_n\\), such that U=U_1 \\(+\\)… \\(+\\) \\(U_n\\), the defination is about uniqueness of such elements.)\nex 1).U:={(x,x,y,y)|x,y\\(\\in F\\)}\\(\\subseteq F^4\\)\nW:={(x,x,x,y)|x,y\\(\\in F\\)}\\(\\subseteq F^4\\)\nthen U+W={(x,x,y,z)|x,y,z\\(\\in F\\)}=:A\nproof:\nside1:u+v\\(\\in U+W\\) easy —xxyz\nside2: let (x,x,y,z)\\(\\in A\\) we want to find v=(a,a,b,b), w=(c,c,c,d),s.t. u+w=(x,x,y,z)\nso we need to solve a+c=x, b+c=y,b+d=z,\nb is free, d=z-b,c=y-b,a=x-c=x-y+b\nfor ex for b=0:\nu=\nw=\nso u+w=\nso \\(A\\subseteq U+W\\)\nex 3) : \\(U_i=\\){(0,0,…x(i th coordinate),0,0,0(n th coordinate))|\\(x\\in F\\)}\\(\\subseteq F^n\\)\nthen \\(U_1\\oplus ...\\oplus U_n=F^n\\)\nex 4) : \\(U_1=\\){(x,y,z)|\\(x,y\\in F\\)}\n\\(U_2\\)={(0,0,z)|\\(z\\in F\\)}\n\\(U_3\\)={(0,y,y)|\\(y\\in F\\)}\nthen \\(U_1+U_2+U_3=F^3\\), but not direct sum\neg:\nbut \\(U_1\\odot U_2=F^3\\)\n\\(U_1\\odot U_3=F^3\\)\n\\(U_2\\odot U_3=\\{(0,y,z)|y,z\\in F^3\\}\\)\n```\n\nThm let U1,….Un be subspaces of V then [U1+…+Un is a direct sum] if and only if [if 0=u1+…+un, then u1=…=0]\n\nproof: one side is obvious: \\(0\\in U_1+...+U_n\\)\n   the other side: let v $\\in U_1+...+U_n$ with v= u+ =v+ be 2 decompsition \n\n\n   so o=u-v=(u1+)-(V1+)=(u1-v1)+(u2-v2)\n\n\n   SO 0 = U1-V1-U2-V2=... by assumption\n\n\n   so we can derive: u1=v1...\n\n\n   so the deconposition of v is unique\n\nintersection of subspaces:\n\nlemma: if U W are subspaces aof V then \\(U\\cap W\\) is the biggest subspace contained in both U and W\n\nproof :\nsubspace:\n\n0\n\n\nlambda\n\n\n\n-   already know $\\subseteq$\n\n-   biggest: let $Z\\subseteq V$ be a subspace, contained both in U and W, because $U\\cap W$ is the biggest subset contained both in U and W then $Z\\subseteq U\\cap W$(as subset)\n\nDef if V=U\\(\\oplus\\)W we say that W is COMPLEMENTARY SUBSPACE of U(inside V)\ngive a conunter example to:\nfor U1 U2, W subspaces of V, then\n\nif U1+w=U2+w, then U1=U2:\nif \\(U_1\\oplus W=U_2\\oplus W\\) hten U_1=U_2"
  },
  {
    "objectID": "mth107.html#linear-independence",
    "href": "mth107.html#linear-independence",
    "title": "MTH107—NOTEs",
    "section": "linear independence",
    "text": "linear independence\nfinite????\na list of vectors v,…vi\\(\\in V\\) is\n\nlinearly independent: if and only if the vector equation \\(\\lambda v_1+..=o\\) admits a unique solution: (0,0,)\n\nlinearly dependence: non-trivial solution.\ndet=0\nv,v,…,0,…\nremark\ninfinite\n\nv1,…vn is linearly independent if and only if span(v1)+span(v2)..+span(vn) is a direct sum.\n\nex: a list of 1 vector \\(v\\in V\\) is lin independent if and only if V!=0\nex: \\(u,v\\in V\\) is linearly independent if and only if [\\(u\\notin span(V)\\) and \\(v\\notin span(U)\\)]\n\nex: (0,0), (1,0) but v\\(\\notin span(u)=\\){0}\nif both u. and v !=0 then it is enough to check only one of a or b\n\nex: v1,… linearly dependent iff \\(\\exists i\\) s.t. $v_i$span(v1,….)\n\none side: \\(\\exists \\lambda\\)\nthe other side:\nsmalest span list:\n\none include\nthe other include:\n\nthis lemmma tells us the if a family is lin dependent then we can remove “redundan” vectors without changing the span\n\nTHM: suppose V=span(v1,..vm)=span(w1,..wn). If v1,..vm is linearly independent then \\(m\\leq n\\)\n\nproof:let us consider v1,w2,….,wn\n\nthis is a spannign family for v, \\(v=span(w1,..wn)\\subseteq span(v1,w1,...wn)\\subseteq v\\)\n\n-it is linearly dependent(because v1\\(\\in span(w1,...wn)....\\))\n\nv1 !=0 because v1,…vm is lin independent\n\n\n\nNow we apply the lemma to the family\n\nwhich implies that \\(\\exists j\\) s.t. wj can be replaced by v1\nindeed 0!= \\(\\notin span()\\)={0}\nsee photo in particular, \\(m \\leq n\\). ok\n\namong finite spanning lists, linearly independent are the smallest one\ncorollary: if v=span(v1,…vm)=span(w1,..wn) and (v1,..) and (w1,..) are both lin indep then m=n\nex: \\(\\mathbb F^n\\), any list of \\(m\\geq n+1\\) vectors is lin dependent because (1,0,..),(0,1,…),…(0,…,1) (n elements here) and any of \\(\\leq n-1\\) vecters cannot span \\(\\mathbb {F^n}\\)"
  },
  {
    "objectID": "mth107.html#begin",
    "href": "mth107.html#begin",
    "title": "MTH107—NOTEs",
    "section": "Begin",
    "text": "Begin\n\nDef Let V and W be 2 vector spaces over the same filed F. A maoo T V–&gt;W is a linear map(linear transformation) if and only if \\(\\forall v \\in V\\) T(u+v)=T(u)+T(v), \\(\\forall v \\in V, \\lambda\\in F\\) T(\\(\\lambda \\cdot v\\))= \\(\\lambda\\cdot T(v)\\)\nLemma Let T"
  },
  {
    "objectID": "mth107.html#thm-an-inner-product-space-is-a-normed-space-by-defining.",
    "href": "mth107.html#thm-an-inner-product-space-is-a-normed-space-by-defining.",
    "title": "MTH107—NOTEs",
    "section": "Thm An inner product space is a normed space by defining.",
    "text": "Thm An inner product space is a normed space by defining.\n||a||=\\(\\sqrt{&lt;a,a&gt;}\\)\n|| || V–&gt; \\(R^+\\) length\n\n||a||\\(\\geq0, \\forall a\\in V\\)\n\n||a||=0 iff a=0\n\n||a+b|| \\(\\leq\\) ||a||+||b||\n\npf: &lt;a,a&gt;+&lt;a,b&gt;+&lt;b,a&gt;+&lt;b,b&gt;\n= \\(||a||^2|+2||a|||b||+||b||^2\\)\n\\(\\leq||a||^2+2||a|| ||b||+||b||^2\\)\n= \\((a+b)^2\\)"
  },
  {
    "objectID": "mth107.html#section-5",
    "href": "mth107.html#section-5",
    "title": "MTH107—NOTEs",
    "section": "1",
    "text": "1\nIf V=\\(U\\oplus W\\), then \\(P_u\\in \\mathcal L(V)\\) is a projection\nProposition Let \\(V\\) be a vector space and let \\(P \\in \\mathcal{L}(V)\\) be a projection. Then \\(V=\\operatorname{range}(P) \\oplus \\operatorname{null}(P)\\) and \\(P=P_{\\operatorname{range}(P)}\\).\nProof. It is clear that range \\((P)+\\operatorname{null}(P) \\subseteq V\\). So we need to prove that the sum is direct and that it is equal to \\(V\\). Let \\(v \\in V\\). Then one have \\(v=P(v)+(v-P(v))\\) with \\(P(v) \\in \\operatorname{range}(P)\\) and \\(v-P(v)\\) is in null \\((P)\\). Indeed, \\(P(v-P(v))=P(v)-P^2(v)=\\) \\(P(v)-P(v)=0\\). Now, for the direct sum part. If \\(v \\in \\operatorname{range}(P) \\cap \\operatorname{null}(P)\\) we have \\(v=P(w)\\) and \\(0=P(v)=P^2(w)=P(w)=v\\), so range \\((P) \\cap \\operatorname{null}(P)=0\\) which finishes the proof of range \\((P)+\\operatorname{null}(P) \\subseteq V\\).\nFinally, let \\(v \\in V\\). Then \\(v=P(v)+(v-P(v))\\) is the unique decomposition \\(v=u+w\\) with \\(u \\in \\operatorname{range}(P)\\) and \\(w \\in \\operatorname{null}(P)\\). Therefore, \\(=P_{\\text {range }(P)}(v)=P(v)\\) and we just demonstrated \\(P=P_{\\text {range }(P)}\\).\n???Another way to interpret this proposition is that there is a one-to-one correspondance between projections \\(P \\in \\mathcal{L}(V)\\) and direct sums decomposition \\(V=U \\oplus W\\) where order matter. We conclude this subsection by the following result that is a generalization of this proposition.\n\nThis means that projection is a kind of decomposition—along and onto decompose one vector in a unique way, which is an intuitive understanding of a special case of direct sum."
  },
  {
    "objectID": "mth107.html#extension",
    "href": "mth107.html#extension",
    "title": "MTH107—NOTEs",
    "section": "1 extension",
    "text": "1 extension\nSuppose \\(P_1, \\ldots, P_k \\in \\mathcal{L}(V)\\) are projections such that \\(P_1+\\cdots+P_k=\\operatorname{Id}_V\\) and \\(P_i P_j=0_{\\mathcal{L}(V)}\\) if \\(i \\neq j\\). Then \\(V=\\operatorname{range}\\left(P_1\\right) \\oplus \\cdots \\oplus \\operatorname{range}\\left(P_k\\right)\\).\nProof. Let \\(v\\) be any vector in \\(V\\). Then \\[\nv=\\operatorname{Id}_V v=\\left(P_1+\\cdots+P_k\\right) v=P_1 v+\\cdots+P_k v \\in \\operatorname{range}\\left(P_1\\right)+\\cdots+\\operatorname{range}\\left(P_k\\right)\n\\] and thus \\(V=P_1 v+\\cdots+P_k v \\in \\operatorname{range}\\left(P_1\\right)+\\cdots+\\) range \\(\\left(P_k\\right)\\). We now prove that the sum is direct. Suppose \\(0=v_1+\\cdots+v_k\\) with \\(v_i \\in \\operatorname{range}\\left(P_i\\right)\\) for \\(1 \\leq i \\leq k\\). We want to prove that all the \\(v_i\\) are zero. By definition of the range, there exist \\(w_i\\) such that \\(P_i w_i=v_i\\). Using that \\(P_i\\) is a projection we have \\(P_i v_i=P_i^2 w_i=P_i w_i=v_i\\) while \\(P_i v_j=P_i P_j w_j=0 w_j=0\\) if \\(i \\neq j\\). Now, if we apply \\(P_i\\) to both sides of (3.5) we obtain \\(0=v_i\\). By doing this for all \\(1 \\leq i \\leq k\\) we have that all the \\(v_i\\) are zero as desired, and so the sum is direct.\nNow that we have proven the Theorem, let us discuss why it is a generalization of the defination of a projection onto rangeP alone nullP with just 2 decomposition(direct sum) Let \\(P \\in \\mathcal{L}(V)\\) be a projection, \\(P_1:=P\\) and \\(P_2:=\\mathrm{Id}_V-P\\). One easily check the following properties: - \\(P_2\\) is a projection: \\(P_2^2=\\mathrm{Id}_V^2-\\mathrm{Id}_V P-P \\mathrm{Id}_V+P^2=\\mathrm{Id}_V-P-P+P=P_2\\); - \\(P 1+P_2=\\mathrm{Id}_V\\); - and \\(P_1 P_2=P^2-P=0=P_2 P_1\\)."
  },
  {
    "objectID": "mth107.html#easier-way-leftright-inverse",
    "href": "mth107.html#easier-way-leftright-inverse",
    "title": "MTH107—NOTEs",
    "section": "easier way (left/right inverse)",
    "text": "easier way (left/right inverse)\nAn easy way to create projections is to use left and right inverses.\nLet \\(V\\) and \\(W\\) be two vector spaces and let \\(T \\in \\mathcal{L}(V, W)\\) and \\(S \\in \\mathcal{L}(W, V)\\) be such that \\(T S=\\operatorname{Id}_W\\). Then \\(V=\\operatorname{range}(S) \\oplus \\operatorname{null}(T)\\) and \\(S T \\in \\mathcal{L}(V)\\) is a projection to range \\((S)\\) with nullspace \\(\\operatorname{null}(T)\\). Proof. We first show that \\(S T\\) is an abstract projection: \\((S T)^2=S(T S) T=S \\operatorname{Id}_W T=\\) \\(S T\\).\nIt now remains to show that range \\((S T)=\\operatorname{range}(S)\\) and \\(\\operatorname{null}(S T)=\\operatorname{null}(T)\\). If \\(v\\) is in range \\((S T)\\), then there exists \\(u \\in V\\) such that \\(v=S T(u)=S(T(u)) \\in \\operatorname{range}(S)\\). So range \\((S T) \\subseteq \\operatorname{range}(S)\\). For the other inclusion, let \\(v \\in \\operatorname{range}(S)\\). Thus there exists \\(w \\in W\\) with \\(v=S(w)=S \\operatorname{Id}_W(w)=S T S(w)=S T(T(w))\\) which shows that range \\((S) \\subseteq\\) range \\((S T)\\).\nFor the nullspaces, if \\(v \\in \\operatorname{null}(T)\\), then \\(S T(v)=S(0)=0\\) and hence \\(\\operatorname{null}(T) \\subseteq \\operatorname{null}(S T)\\). For the other inclusion, let \\(v\\) be an element in null \\((S T)\\). We have \\(0_V=S T(v)\\) and by applying \\(T\\) to both sides \\(0_W=T S T(v)=\\operatorname{Id}_W T(v)=T(v)\\), which shows that \\(\\operatorname{null}(S T) \\subseteq \\operatorname{null}(T)\\).\nObserve that for a given \\(T \\in \\mathcal{L}(V, W)\\) there exists a right inverse \\(S\\) such that \\(T S=\\operatorname{Id}_W\\) if and only if \\(T\\) is surjective. If such an inverse exists it is not unique, unless \\(T\\) is an isomorphism. This is related to the fact that given a general subspace \\(U(=\\operatorname{null}(T))\\) of \\(V\\), there exists more than one direct sum complement \\(X(=\\operatorname{range}(S))\\) such that \\(V=U \\oplus X\\). Let us exemplify this.\n(ps:ST may not be invertible. However, if T and S are operator \\(\\in L(V)\\), it is trivial that TS is invertible since T is surjective and S is injetive then they are all invertible so ST is invertible since \\(ST(T^{-1}S^{-1})=Id\\) and \\((T^{-1}S^{-1})ST=Id\\))"
  },
  {
    "objectID": "mth107.html#projection-is-a-useful-tool-to-do-approximation",
    "href": "mth107.html#projection-is-a-useful-tool-to-do-approximation",
    "title": "MTH107—NOTEs",
    "section": "projection is a useful tool to do approximation",
    "text": "projection is a useful tool to do approximation\nThe projection is the approximation and the vector minus projection is the error.\neg. use the inner product on continuous functions to approximate sine function using \\(P_4(R)\\)"
  },
  {
    "objectID": "mth107.html#infinite-here-will-also-be-true",
    "href": "mth107.html#infinite-here-will-also-be-true",
    "title": "MTH107—NOTEs",
    "section": "Infinite here will also be true",
    "text": "Infinite here will also be true"
  },
  {
    "objectID": "mth107.html#least-square-in-regression-analysis-1",
    "href": "mth107.html#least-square-in-regression-analysis-1",
    "title": "MTH107—NOTEs",
    "section": "Least-square in regression analysis",
    "text": "Least-square in regression analysis\n\na simple example"
  },
  {
    "objectID": "mth107.html#problems",
    "href": "mth107.html#problems",
    "title": "MTH107—NOTEs",
    "section": "problems",
    "text": "problems\n\nsummarize1: give opposite example and draw pictures are okay.\n\neg.\neg. of Tutorial 4:\nf: \\(\\mathbb{R}_{\\geq0}\\)—&gt;\\(\\mathbb{R}\\)\nx–&gt;\\(\\sqrt {x}\\)\ng: \\(\\mathbb R\\)—&gt;\\(\\mathbb{R}_{\\geq0}\\)\nx–&gt;\\(x^2\\)\ng(f(x))=\\(x^2\\)=\\(id_{\\mathbb R\\geq0}\\) f(g(x))=|\\(x\\)|is not equal to \\(id_{\\mathbb R}\\) because f(g(x)) is not equal to x.\neg."
  },
  {
    "objectID": "mth107.html#understand-again",
    "href": "mth107.html#understand-again",
    "title": "MTH107—NOTEs",
    "section": "understand again",
    "text": "understand again"
  },
  {
    "objectID": "mth1113.html",
    "href": "mth1113.html",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "",
    "text": "dplyr::filter()"
  },
  {
    "objectID": "mth1113.html#case-1",
    "href": "mth1113.html#case-1",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "case 1",
    "text": "case 1\n\\(M_x\\) may not exist. When it exists in a neighborhood of 0, using talor\n\\[e^{tx}=1+tX+(tX)^2/2+...\\]\n\\[M_x(t)=1+t\\mu+t^2 \\mu/2+...\\]\n\\(\\mu_j = E(X^i)\\) is the j-th moment of X. Therefore,\n\\[E(X^i)=M^{(j)}(0)\\]\neg. Then we could also get the variance by take the 2nd order derivatives"
  },
  {
    "objectID": "mth1113.html#case-2",
    "href": "mth1113.html#case-2",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "case 2",
    "text": "case 2\n\\(\\int\\)\neg. normal\n\nX~N(0,1)\n\nidea: try to write an integral of a certain distribution’s pdf and we get the result of this integral as 1. here, we get the pdf of the N(t,1) and finally we get \\(e^{t^2/2}\\)\n\nX~N(\\(\\mu, \\sigma^2\\))\n\nThen X=\\(\\mu+\\sigma Z\\) where Z~N(0,1)\n\\[M_x(t)=E[e^{tx}]=e^{\\mu t} E[e^{\\sigma t Z}]=e^{\\mu t} M_Z(\\sigma t)=e^{\\mu t +\\sigma^2 t^2/2}\\]\n\nGamma disteibution\nthe family of gamma distributions generalizes the family of exponential distributions. The gamma distribution with shape r and rate \\(\\lambda\\)\n\naddition rule\nr +\n\\(\\lambda\\) stays"
  },
  {
    "objectID": "mth1113.html#why-mgf-is-useful-to-determine-if-two-random-variables-have-the-identical-cdf-to-prove-the-addition-property-of-distributions",
    "href": "mth1113.html#why-mgf-is-useful-to-determine-if-two-random-variables-have-the-identical-cdf-to-prove-the-addition-property-of-distributions",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "why MGF is useful? – to determine if two random variables have the identical CDF / to prove the addition property of distributions",
    "text": "why MGF is useful? – to determine if two random variables have the identical CDF / to prove the addition property of distributions\nMGF includes all characteristics of a distribution, from whom we could get pdf, cdf, expectation, variance\n\nThm If X and Y are random variables with the same MGF, which is finite on [-t, t ] for some t &gt;0 then X and Y have the same distribution\n\nA gamma distribution with shape r =1 is an exponential distribution\nA more general function than MGF is the characteristic function.\n\\(\\phi_X (t) = E(e^{itX})\\)"
  },
  {
    "objectID": "mth1113.html#十堂极简概率课-中信出版-diaconis",
    "href": "mth1113.html#十堂极简概率课-中信出版-diaconis",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "十堂极简概率课 中信出版 diaconis",
    "text": "十堂极简概率课 中信出版 diaconis"
  },
  {
    "objectID": "mth1113.html#心理统计",
    "href": "mth1113.html#心理统计",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "心理统计",
    "text": "心理统计"
  },
  {
    "objectID": "mth1113.html#the-lady-tasting-tea",
    "href": "mth1113.html#the-lady-tasting-tea",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "The lady tasting tea",
    "text": "The lady tasting tea"
  },
  {
    "objectID": "mth1113.html#概率论与数理统计第三版-峁诗松等老师编著",
    "href": "mth1113.html#概率论与数理统计第三版-峁诗松等老师编著",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "概率论与数理统计（第三版） 峁诗松等老师编著",
    "text": "概率论与数理统计（第三版） 峁诗松等老师编著"
  },
  {
    "objectID": "mth1113.html#probability.",
    "href": "mth1113.html#probability.",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Probability….",
    "text": "Probability…."
  },
  {
    "objectID": "mth1113.html#background",
    "href": "mth1113.html#background",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "background",
    "text": "background\n\nRegression to the mean (Galton’s thinking)"
  },
  {
    "objectID": "mth1113.html#it-is-not-stable-to-predict-the-data-outside-our-data-sample",
    "href": "mth1113.html#it-is-not-stable-to-predict-the-data-outside-our-data-sample",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "it is not stable to predict the data outside our data sample’",
    "text": "it is not stable to predict the data outside our data sample’"
  },
  {
    "objectID": "mth1113.html#residual-plot-should-have-no-pattern",
    "href": "mth1113.html#residual-plot-should-have-no-pattern",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Residual plot should have no pattern",
    "text": "Residual plot should have no pattern\nacross the whole range, it could not be showing a certain trend or a specific shape.\npositive and negative points sperate averagely .\n\n\n\n# 数据\nx &lt;- c(50, 55, 50, 79, 44, 37, 70, 45, 49)  # Rock surface area\ny &lt;- c(152, 48, 22, 35, 38, 171, 13, 185, 25)  # Algae colony density\n\n# (a) 计算最小二乘回归方程\nmodel &lt;- lm(y ~ x)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-65.53 -63.91 -14.47  46.99  84.39 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  232.258     92.390   2.514   0.0402 *\nx             -2.926      1.690  -1.731   0.1271  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63.32 on 7 degrees of freedom\nMultiple R-squared:  0.2998,    Adjusted R-squared:  0.1997 \nF-statistic: 2.997 on 1 and 7 DF,  p-value: 0.1271\n\n# 获取回归系数\nintercept &lt;- coef(model)[1]\nslope &lt;- coef(model)[2]\ncat(\"最小二乘回归方程: y =\", intercept, \"+\", slope, \"* x\\n\")\n\n最小二乘回归方程: y = 232.2575 + -2.925507 * x\n\n# (b) 计算 R^2 值并解释\nr_squared &lt;- summary(model)$r.squared\ncat(\"R^2 值:\", r_squared, \"\\n\")\n\nR^2 值: 0.2997552 \n\ncat(\"解释: R^2 表示了\", round(r_squared * 100, 2), \"% 的 y 的变异可以通过 x 来解释。\\n\")\n\n解释: R^2 表示了 29.98 % 的 y 的变异可以通过 x 来解释。\n\n# (c) 计算残差标准误差 s_e\nse &lt;- summary(model)$sigma\ncat(\"残差标准误差 s_e:\", se, \"\\n\")\n\n残差标准误差 s_e: 63.31527 \n\ncat(\"解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\\n\")\n\n解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\n\n# (d) 判断线性关系的方向和强度\ncorrelation &lt;- cor(x, y)\ncat(\"相关系数 r:\", correlation, \"\\n\")\n\n相关系数 r: -0.547499 \n\nif (correlation &gt; 0) {\n  direction &lt;- \"正相关\"\n} else {\n  direction &lt;- \"负相关\"\n}\n\nif (abs(correlation) &gt; 0.7) {\n  strength &lt;- \"强相关\"\n} else if (abs(correlation) &gt; 0.3) {\n  strength &lt;- \"中等相关\"\n} else {\n  strength &lt;- \"弱相关\"\n}\n\ncat(\"线性关系:\", direction, \"且为\", strength, \"\\n\")\n\n线性关系: 负相关 且为 中等相关 \n\n\n\n# 数据\nquality_rating &lt;- c(111, 113, 93, 130, 170, 87, 83, 117, 135, 109)\nsatisfaction_rating &lt;- c(832, 845, 794, 854, 836, 842, 877, 745, 797, 795)\n\n# 计算相关系数\ncorrelation_coefficient &lt;- cor(quality_rating, satisfaction_rating)\nprint(paste(\"相关系数 r:\", correlation_coefficient))\n\n[1] \"相关系数 r: -0.115403519735578\"\n\n# 绘制散点图\nplot(quality_rating, satisfaction_rating,\n     main = \"Scatterplot of Quality Rating vs. Satisfaction Rating\",\n     xlab = \"Quality Rating\",\n     ylab = \"Satisfaction Rating\",\n     pch = 19, col = \"blue\")\nabline(lm(satisfaction_rating ~ quality_rating), col = \"red\")  # 添加回归线"
  },
  {
    "objectID": "mth1113.html#z-scores",
    "href": "mth1113.html#z-scores",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "z Scores",
    "text": "z Scores\neg. height(among women or men (come from different populations)instead of just comparing the height itself)"
  },
  {
    "objectID": "mth1113.html#intro",
    "href": "mth1113.html#intro",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Intro",
    "text": "Intro\nstat is a large field in math involving the collection, organization, analysis,interpretation, and presentation of data(a collection of observations on one or more variables(A characteristic whose value may change from one observation to another))\nStatistics is the scientific discipline that provides methods to help us make sense of data.\nIt is important to be able to:\n1 Extract information from tables, charts, and graphs.\n2 Follow numerical arguments.\n3 Understand the basics of how data should be gathered, summarized, and analysed to draw statistical conclusions.\nThe Data Analysis Process\n1 Understanding the nature of the research problem or goals.\n2 Deciding what to measure and how.\n3 Collecting data.\n4 Data summarization and preliminary analysis.\n5 Formal Data Analysis (Statistical Methods).\n6 Interpretation of the results."
  },
  {
    "objectID": "mth1113.html#populations-and-samples",
    "href": "mth1113.html#populations-and-samples",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "populations and samples",
    "text": "populations and samples\npopulation: The entire collection of individuals or objects about which information is desired\nsample: A sample is a subset of the population, selected for study.\nthen select the sample\nthen we could summarize it using 2 branches of stat.— Decriptive stat.(methods for organizing and summarizing data.) or inferential stat.(generalizing from a sample(incomplete information) to the population from which the sample was selected and assessing the reliability of such generalizations.So we run the risk(An important aspect of statistics and making statistics inferences involves quantifying the chance of making an incorrect conclusions.))\n\ndescriptive stat\n\n\ninferential stat\nsample"
  },
  {
    "objectID": "mth1113.html#types-of-data",
    "href": "mth1113.html#types-of-data",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Types of data",
    "text": "Types of data\n\nuni data set and bivariate and multivariate\n\n\ncategorical and numerical(discrete and continuous) with plot using excel (data analysis) or rstudio plot (ggplot2)\nfor categorial data we could use a bar chart which is a graph of a frequency distribution for categorical data.\nfor a small numerical data we could use dotplot\n\ndiscrete\n\n\nlibrary(ggplot2)\n\n# creat data：Wechat number\ndiscrete_data &lt;- data.frame(value = c(30, 15, 20,30,60))\n\n# plot\nggplot(discrete_data, aes(x = value)) +\n  geom_dotplot(binwidth = 1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Discrete Data (Number of Wechats)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ncontinuous\n\n\nall_athletes &lt;- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball &lt;- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 设置画布的高度，以便将两个图绘制在同一页面上\nplot.new()\nplot.window(xlim = c(0, 100), ylim = c(0.5, 2.5))\n\n# 绘制 Basketball 数据的 dotplot\nstripchart(basketball, method = \"stack\", at = 2, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 绘制 All Athletes 数据的 dotplot\nstripchart(all_athletes, method = \"stack\", at = 1, pch = 16, col = \"orange\", \n           add = TRUE, offset = 0.5, cex = 1.2)\n\n# 添加 X 轴\naxis(1, at = seq(10, 100, by = 10), labels = seq(10, 100, by = 10))\n\n# 添加标签\ntext(-5, 2, \"Basketball\", xpd = TRUE, adj = 1)\ntext(-5, 1, \"All Athletes\", xpd = TRUE, adj = 1)\n\n# 添加横线\nabline(h = 1.5, col = \"black\", lwd = 2)\n\n# 添加 X 轴标签\ntitle(xlab = \"Graduation rates (%)\")\n\n\n\n\n\n\n\n\n\n# creat data--time spent in minutes\ncontinuous_data &lt;- data.frame(value = c(6, 5.25, 3.62,1,2,3.1,3.2,4,5,6,7,4,10))\n\n# dotplot\nggplot(continuous_data, aes(x = value)) +\n  geom_dotplot(binwidth = 0.1, dotsize = 1) +\n  ggtitle(\"Dot Plot of Continuous Data (Time Spent in Minutes)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\n# 毕业率数据\nschool &lt;- 33:68\nall_athletes &lt;- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, \n                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)\nbasketball &lt;- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, \n                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)\n\n# 创建数据框\ndata &lt;- data.frame(school, all_athletes, basketball)\n\n# 画图\nggplot() +\n  geom_dotplot(data = data, aes(x = all_athletes, y = \"All Athletes\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5) +\n  geom_dotplot(data = data, aes(x = basketball, y = \"Basketball\"), binaxis = 'x', stackdir = 'up', dotsize = 0.5, color = \"red\") +\n  xlab(\"Graduation rates (%)\") +\n  ylab(\"\") +\n  theme_minimal() +\n  ggtitle(\"Dotplot of Graduation Rates for All Athletes and Basketball Players\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nhistogram excel plot"
  },
  {
    "objectID": "mth1113.html#collect-data-sensibly",
    "href": "mth1113.html#collect-data-sensibly",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "collect data sensibly",
    "text": "collect data sensibly"
  },
  {
    "objectID": "mth1113.html#two-types-of-studies-observational-studies-and-experiments.",
    "href": "mth1113.html#two-types-of-studies-observational-studies-and-experiments.",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Two types of studies: Observational studies and Experiments.",
    "text": "Two types of studies: Observational studies and Experiments.\n\nObservational\nA study in which the investigator observes characteristics of a sample selected from one or more existing populations. The goal is to draw conclusions about the corresponding population or about differences between two or more populations.\nIn an observational study, it is impossible to draw clear cause-and-effect conclusions\n\n\nExperiments\nA study in which the investigator observes how a response variable behaves when one or more explanatory variables, also called factors, are manipulated.\nA well-designed experiment can result in data that provide evidence for a cause-and-effect relationship.\n\n\nExperimental conditions: Any particular combination of values for the explanatory variables, which are also called treatments.\n\n\n\ncomparison\n\nBoth observational studies and experiments can be used to compare groups, but in an experiment the researcher controls who is in which group, whereas this is not the case in an observational study.\nIn an observational study, it is impossible to draw clear cause-and\u0002effect conclusions\n\n\n\nconfounding vars\nA variable that is related to both how the experimental groups were formed and the response variable of interest.\n\nTwo methods for data collection: Sampling and Experimentation.\ndistinguish between selection bias, measurement or response bias, and non-response bias.\nselect a simple random sample from a given population.\ndistinguish between simple random sampling, stratified random sampling, cluster sampling, systematic sampling, and convenience sampling"
  },
  {
    "objectID": "mth1113.html#variable",
    "href": "mth1113.html#variable",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "variable",
    "text": "variable\n\nresponse variable–y\nThe response variable is the focus of a question in a study or experiment.\n\n\nexplanotory variable–x\nAn explanatory variable is one that explains for changes in the response variable.\n\n\nexperiments and obeservational study\n\n\nbias\nselection bias：When the way the sample is selected systematically excludes some part of the population of interest.\nmeasurement or response bias\neg: survey question/scale(The scale or a machine used for measurements is not calibrated properly)\nNon-response Bias:When responses are not obtained from all individuals selected for inclusion in the sample.\nnon-response bias can distort results if those who respond differ in important ways from those who do not respond (e.g. laziness a confounding vari\u0002able).\n\n\nrandom sampling\ndef: A sample that is selected from a population in a way that ensures that every different possible sample of size n has the same chance of being selected.\nthe same chance to be selected\ncounter eg:\nConsider 100 students in a classroom, 60 females and 40 males. If we randomly sample 6 females, and 4 males, then each female has a 6/60 = 0.1 chance of being selected. Same for males, 4/40=0.1. However, not every group of 10 students is equally likely to be selected. This is not simple random sampling\nThe random selection process allows us to be confident that the sample adequately reflects the population, even when the sample consists of only a small fraction of the population.\neg.Voting Sample Size in a country\n\n\nstratified and cluster\n\nstratified random sampling:In stratified random sampling, separate simple random samples are independently selected from each subgroup. Each subgroup is called a strata.\n\nIn general, it is much easier to produce relatively accurate es\u0002timates of characteristics of a homogeneous group than of a heterogeneous group.\nstratified: according to certain characteristic\neg.Even with a small sample, it is possible to obtain an accurate estimate of the average grade point average (GPA) of students graduating with high honours from a university (Similar high grades, homogenous, thus only sample a few students). On the other hand, producing a reasonably accurate estimate of the average GPA of all seniors at the university, a much more diverse group of GPAs, is a more difficult task. Not only does this ensure that students at each GPA level are represented, it also allows for a more accurate estimate of the overall average GPA.\n\ncluster reflect general characteristic about the whole entire population\n\ncluster: randomly groups\nCluster sampling involves dividing the population of interest into non-overlapping subgroups, called clusters. Clusters are then selected at random, and then all individuals in the selected clusters are included in the sample.\n\n\nsystematic sampling\nA value k is specified (e.g. k = 50 or k = 200). Then one of the first k individuals is selected at random, after which every k-th individual in the sequence is included in the sample. A sample selected in this way is called a 1 in k systematic sample.\nIn the case of large samples, it can ensure that the sample is evenly distributed in the population."
  },
  {
    "objectID": "mth1113.html#random-variable-r.v.",
    "href": "mth1113.html#random-variable-r.v.",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Random Variable (R.V.)",
    "text": "Random Variable (R.V.)\nA numerical variable whose value depends on the outcome of a chance experiment. A random variable associates a numerical value with each outcome of a chance experiment. (Think of it as a rule that translates each result of a chance event into a number.)\nIn shorta: random variables convert random events into numbers.\nA real-valued random variable X is a function: X : S → \\(\\mathbb R\\), where S is the sample space of a chance experiment.\nContinuous random variable X: S–&gt; R is continuous if its set of possible values includes an entire interval on the number line(measurement), which could not be count.\nDiscrete random variable: X: S–&gt; R if its set of possible value is a collection of isolated points along the number line.(counting)\n\neg\nExamples: Coin Tossing (Discrete Random Variable) If we flip a coin 5 times, let X be the number of heads we get. Possible values of X {0,1,2,3,4,5} (where 0 means no heads, and 5 means all heads). Here, X turns each outcome of multiple coin tosses into a count of heads.\nDeparture Time (Continuous Random Variable) Imagine tracking when people leave a subway station between 10 PM and 11 PM. Let Y represent the time (in hours) someone leaves, so Y can be any number from 10 to 11. Here, Y assigns each departure time to a point in the range [10,11]."
  },
  {
    "objectID": "mth1113.html#sample-space-of-multivariables",
    "href": "mth1113.html#sample-space-of-multivariables",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "sample space of multivariables",
    "text": "sample space of multivariables\nG:gender; F: year—corresponding to a certain student:S\n(G, H) : S → \\(\\mathbb R^2\\), S={1,2,3,4}\n…"
  },
  {
    "objectID": "mth1113.html#probability-mass-function-and-cumulative-distribution-function-for-discrete-random-variables",
    "href": "mth1113.html#probability-mass-function-and-cumulative-distribution-function-for-discrete-random-variables",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Probability Mass Function and Cumulative Distribution Function for Discrete Random Variables",
    "text": "Probability Mass Function and Cumulative Distribution Function for Discrete Random Variables\nProbability Mass Function (PMF): \\(p_x (x) := P(X = x) ,\\forall x\\)\nCumulative Distribution Function (CDF): \\(F_x (x) := P(X \\leq x) ,\\forall x\\)\n\n# 定义每个事件的概率和对应的 X 值\noutcomes &lt;- c(\"GGGG\", \"EGGG\", \"GEGG\", \"GGEG\", \"GGGE\", \n              \"EEGG\", \"EGEG\", \"EGGE\", \"GEEG\", \"GEGE\", \n              \"GGEE\", \"GEEE\", \"EEEG\", \"EEGE\", \"EEEE\")\nprobabilities &lt;- c(0.1296, 0.0864, 0.0864, 0.0864, 0.0864, \n                   0.0576, 0.0576, 0.0576, 0.0576, 0.0576, \n                   0.0384, 0.0384, 0.0384, 0.0384, 0.0256)\nX_values &lt;- c(0, 1, 1, 1, 1, \n              2, 2, 2, 2, 2,\n              3, 3, 3, 3, 4)\n\n# 计算每个 X 值的 PMF 通过分组和求和\npmf &lt;- tapply(probabilities, X_values, sum)\n\n# 定义可能的 X 值\nX_values_unique &lt;- sort(unique(X_values))\n\n# 计算 CDF\ncdf &lt;- cumsum(pmf)\n\n# 确保 CDF 在 x &gt; 4 时为 1\ncdf &lt;- c(cdf, 1)\n\n# 更新 X 值以包括 x &gt; 4 的情况\nX_values_unique &lt;- c(X_values_unique, \"&gt;4\")\n\n# 创建数据框显示 PMF 和 CDF\ntable &lt;- data.frame(\n  X = X_values_unique,\n  `PMF P(X=x)` = c(pmf, 1-0.1296-0.3456-0.2880-0.1536-0.0256),  # PMF 没有对应的 x &gt; 4 值，填 NA\n  `CDF F(X&lt;=x)` = cdf\n)\n\n# 打印表格\nprint(table)\n\n   X PMF.P.X.x. CDF.F.X..x.\n0  0     0.1296      0.1296\n1  1     0.3456      0.4752\n2  2     0.2880      0.7632\n3  3     0.1536      0.9168\n4  4     0.0256      0.9424\n  &gt;4     0.0576      1.0000\n\n\n\nNote that the domain of a cdf is (−∞, ∞)\n\n\\(\\mathrm{pmf} \\Longrightarrow \\mathrm{cdf}\\) \\[\nF_x(x)=\\sum_{y \\leq x} p_x(y)\n\\] cdf \\(\\Longrightarrow\\) pmf Suppose \\(X\\) takes ordered values \\(x_1, x_2, x_3, \\cdots\\), then \\[\n\\begin{aligned}\np_X\\left(x_i\\right) & =P\\left(X=x_i\\right)=P\\left(x_{i-1}&lt;X \\leq x_i\\right) \\\\\n& =P\\left(X \\leq x_i\\right)-P\\left(X \\leq x_{i-1}\\right) \\\\\n& =F\\left(x_i\\right)-F\\left(x_{i-1}\\right)\n\\end{aligned}\n\\]\n\nremark: The probability of a discrete distribution varies depending on the inclusion and exclusion of the boundary values."
  },
  {
    "objectID": "mth1113.html#expectation-and-variance-for-discrete-random-variables",
    "href": "mth1113.html#expectation-and-variance-for-discrete-random-variables",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Expectation and Variance for Discrete Random Variables",
    "text": "Expectation and Variance for Discrete Random Variables\n\\[\n\\frac{0 \\cdot f_0+1 \\cdot f_1+2 \\cdot f_2+\\cdots+n \\cdot f_n}{N}=\\frac{1}{N} \\sum_{i=0}^n i \\cdot f_i\n\\]\nNote that in \\(\\frac{1}{N} \\sum_{i=0}^n i \\cdot f_i\\), \\[\n\\lim _{N \\rightarrow \\infty} \\frac{f_i}{N}=P(X=i)\n\\]\nSo the average number will be \\[\n\\sum_{i=0}^n i \\cdot P(X=i)\n\\]\n\nDefinition: Expectation Given a discrete random variable \\(X\\), the expectation of \\(X\\) is \\[\nE[X]=\\sum_x x \\cdot p_X(x)\n\\]\nProperties of Expectation\nIf \\(c\\) is a constant, then \\(E[c]=c\\).\nIf \\(X \\geq 0\\) then \\(E[X] \\geq 0\\).\nIf \\(a \\leq X \\leq b\\) then \\(a \\leq E[X] \\leq b\\).\nProof of 3: First show \\(E[X] \\geq a\\), then show \\(E[X] \\leq b\\), \\[\n\\begin{aligned}\nE[X] & =\\sum_x x p_X(x) \\geq \\sum_x a p_X(x), \\\\\n& =a \\sum_x p_x(x)=a .\n\\end{aligned}\n\\]\n\nSimilarly, \\(E[X] \\leq b\\).\n\nSuppose \\(X\\) is a discrete random variable and \\(Y=g(X)\\), then \\[\n\\begin{aligned}\nE[Y] & =\\sum_y y p_Y(y)=\\sum_y y P(Y=y) \\\\\n& =\\sum_y y \\sum_{\\{x: g(x)=y\\}} P(X=x) \\\\\n& =\\sum_y \\sum_{\\{x: g(x)=y\\}} y P(X=x) \\\\\n& =\\sum_y \\sum_{\\{x: g(x)=y\\}} g(x) P(X=x) \\\\\n& =\\sum_x g(x) P(X=x)\n\\end{aligned}\n\\]\nFirst moment of \\(X\\) (mean): \\[\nE[X]=\\sum_x x p_X(x) .\n\\]\nSecond moment of \\(X\\) : \\[\nE\\left[X^2\\right]=\\sum_x x^2 p_x(x) .\n\\]\nIn general, \\(E[g(X)] \\neq g(E[X])\\). For example, let \\(g(x)=x^2\\), and consider \\(X\\) such that \\[\np_X(x)= \\begin{cases}0.5, & \\text { for } x=-1 \\\\ 0.5, & \\text { for } x=1\\end{cases}\n\\]\n\nThen clearly \\(E\\left[X^2\\right]=1 \\neq 0=(E[X])^2\\).\n\nThere are exceptions (e.g. when g is linear)!\nLinearity of Expectation\n\nE[aX + b] = aE[X] + b\nproof:\nSuppose \\(g(x)=a x+b\\). Then \\[\n\\begin{aligned}\nE[g(X)] & =\\sum_x g(x) p_X(x), \\\\\n& =\\sum_x(a x+b) p_x(x), \\\\\n& =\\sum_x a x p_x(x)+\\sum_x b p_x(x), \\\\\n& =a \\sum_x x p_x(x)+b \\sum_x p_x(x), \\\\\n& =a E[X]+b=g(E[X]),\n\\end{aligned}\n\\] this implies \\[\nE[a X+b]=a E[X]+b\n\\]\n\nRemark: Apart from this case, always assume \\(E[g(X)] \\neq g(E[X])\\)."
  },
  {
    "objectID": "mth1113.html#independence-expectationsmean-and-variance",
    "href": "mth1113.html#independence-expectationsmean-and-variance",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "independence, expectations(mean) and variance",
    "text": "independence, expectations(mean) and variance\nIndependence and Expectations If \\(X\\) and \\(Y\\) are independent, then \\[\nE[X Y]=E[X] E[Y] .\n\\]\nProof: We use \\(E[g(X, Y)]\\) where \\(g(x, y)=x y\\). \\[\n\\begin{aligned}\nE[X Y] & =\\sum_x \\sum_y x y p_{X, Y}(x, y) \\\\\n& =\\sum_x \\sum_y x y p_X(x) p_Y(y), \\quad(\\text { by inde } \\\\\n& =\\left(\\sum_x x p_X(x)\\right)\\left(\\sum_y y p_Y(y)\\right)=E[X] E[Y] .\n\\end{aligned}\n\\] (by independence)\nSimilarly, if \\(X\\) and \\(Y\\) are independent, then \\[\nE[g(X) h(Y)]=E[g(X)] E[h(Y)]\n\\]\n\nIndependence and Variances\n\nIt is always true that \\[\n\\operatorname{Var}(a X)=a^2 \\operatorname{Var}(X), \\quad \\text { and } \\quad \\operatorname{Var}(X+a)=\\operatorname{Var}(X)\n\\]\nIn general, when we have a sum of random variables \\(X\\) and \\(Y\\) \\[\n\\operatorname{Var}(X+Y) \\neq \\operatorname{Var}(X)+\\operatorname{Var}(Y) .\n\\]\nIt is only true if \\(X\\) and \\(Y\\) are independent\n\nSum of Variance for independent R.V. If two random variables \\(X\\) and \\(Y\\) are independent then \\[\n\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)\n\\]\nSum of Variance for independent R.V.\n\nIf two random variables \\(X\\) and \\(Y\\) are independent then \\[\n\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y) .\n\\]\nProof: Independence implies \\(E[X Y]=E[X] E[Y]\\). Thus \\[\n\\begin{aligned}\n& \\operatorname{Var}(X+Y)=E\\left[(X+Y-(E[X]+E[Y]))^2\\right], \\\\\n& =E\\left[(X-E[X])^2+2(X-E[X])(Y-E[Y])+(Y-E[Y])^2\\right], \\\\\n& =\\operatorname{Var}(X)+\\operatorname{Var}(Y)+2 E[(X-E[X])(Y-E[Y])]\n\\end{aligned}\n\\]\nAs \\(X\\) is indep to \\(Y\\), then \\(X-\\mu_X\\) is indep to \\(Y-\\mu_Y\\) so \\[\nE[(X-E[X])(Y-E[Y])]=E[X-E[X]] E[Y-E[Y]]=0\n\\]\nExample: Assume independence, \\(\\operatorname{Var}(3 X-5 Y)=\\) \\[\n\\operatorname{Var}(3 X)+\\operatorname{Var}(-5 Y)=9 \\operatorname{Var}(X)+25 \\operatorname{Var}(Y)\n\\]\n\nExample. Independence, mean and variance\n\nLet \\(Y\\) be the random variable denoting the total number of heads by tossing a coin \\(n\\) times. Find the mean and variance of \\(Y\\).\nLet \\[\nY_i= \\begin{cases}1, & \\text { if the } i^{\\text {th }} \\text { toss gets a head } \\\\ 0, & \\text { otherwise. }\\end{cases}\n\\]\nThen \\(Y=Y_1+Y_2+\\cdots+Y_n\\) where \\(Y_1, Y_2, \\cdots, Y_n\\) are independent. For any \\(i=1,2, \\cdots, n\\), we have \\[\n\\begin{array}{ccc}\ny & 0 & 1 \\\\\nP_{Y_i}(y) & 1 / 2 & 1 / 2\n\\end{array}\n\\]\nAs \\(E\\left[Y_i\\right]=\\frac{1}{2}\\) and \\(\\operatorname{Var}\\left(Y_i\\right)=\\frac{1}{4}\\) for \\(i=1,2, \\cdots, n\\) \\[\n\\begin{gathered}\nE[Y]=E\\left[Y_1\\right]+E\\left[Y_2\\right]+\\cdots+E\\left[Y_n\\right]=\\frac{n}{2} \\\\\n\\operatorname{Var}(Y)=\\operatorname{Var}\\left(Y_1\\right)+\\operatorname{Var}\\left(Y_2\\right)+\\cdots+\\operatorname{Var}\\left(Y_n\\right)=\\frac{n}{4}\n\\end{gathered}\n\\] (for not independent cases we have the same result as E but not Var because Var is not linear)"
  },
  {
    "objectID": "mth1113.html#definition.-probability-density-function-pdf",
    "href": "mth1113.html#definition.-probability-density-function-pdf",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Definition. Probability Density Function (pdf)",
    "text": "Definition. Probability Density Function (pdf)\nFor a continuous random variable X, the Probability Density Function (PDF) of X is f(x) where P(X = x) = 0 for all x and for any a ≤ b"
  },
  {
    "objectID": "mth1113.html#discrete-and-continuous",
    "href": "mth1113.html#discrete-and-continuous",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "discrete and continuous",
    "text": "discrete and continuous\n\nDensity function’s value could be greater than 1 because the integral of it in a very tiny interval could be very small, which represents the probability(which could not be greater than 1 and since the interval could be very tiny this condition is satisfied!).(This reminder notices that the value of pdf is not the probability since the integration is the probability which is totally 1)"
  },
  {
    "objectID": "mth1113.html#wait-for-reviewing..",
    "href": "mth1113.html#wait-for-reviewing..",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "wait for reviewing…..",
    "text": "wait for reviewing….."
  },
  {
    "objectID": "mth1113.html#bernoulli-is-composed-by-binomial",
    "href": "mth1113.html#bernoulli-is-composed-by-binomial",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Bernoulli is composed by binomial",
    "text": "Bernoulli is composed by binomial"
  },
  {
    "objectID": "mth1113.html#hypergeometric-distribution-could-be-approximated-by-binomial-when-samples-are-large",
    "href": "mth1113.html#hypergeometric-distribution-could-be-approximated-by-binomial-when-samples-are-large",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Hypergeometric distribution could be approximated by Binomial when samples are large",
    "text": "Hypergeometric distribution could be approximated by Binomial when samples are large"
  },
  {
    "objectID": "mth1113.html#poisson-distribution-as-the-limit-of-binomial-distribution-when-the-number-of-trials-is-large-and-the-probabiliy-of-success-of-each-trial-is-inverse-proportional-to-the-number-of-trials",
    "href": "mth1113.html#poisson-distribution-as-the-limit-of-binomial-distribution-when-the-number-of-trials-is-large-and-the-probabiliy-of-success-of-each-trial-is-inverse-proportional-to-the-number-of-trials",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Poisson distribution as the limit of Binomial distribution when the number of trials is large and the probabiliy of success of each trial is inverse-proportional to the number of trials",
    "text": "Poisson distribution as the limit of Binomial distribution when the number of trials is large and the probabiliy of success of each trial is inverse-proportional to the number of trials\n\nThe Poisson distribution is a discrete probability distribution that applies to occurrences of some event over a specified interval. The random variable x is the number of occurrences of the event in an interval.The probability of the event occurring x times over an interval is given by \\[P(x)=\\frac{u^x\\cdot e^{-\\mu}}{x!}\\] where the random variable x is the number of occurences of an event over some interval and the ovvurrences must be random and independent of each other.\n\n中心极限定理：当泊松分布的参数 λ 较大时，泊松分布的形状会接近正态分布。这是因为中心极限定理指出，大量独立随机变量的和趋向于正态分布，而泊松分布可以看作是大量伯努利试验成功次数的分布，当试验次数足够多时，其和可以用正态分布来近似\n\n???The occurrences must be uniformly distributed over the interval being used\nThe mean is \\(\\mu\\)\nThe standard deviation is \\(\\sigma= \\sqrt \\mu\\)\n\neg of Poisson distribution: (describing the behavior of rare events(with small probabilities). radioactive decay, arrivals of people in a line, eagles nesting in a region, patients arriving at an emergency room(the local hospital experiences a mean of 2.3 patients arriving at the emergency room during 10-11 P.M. on Fri. is known, we can find the probability that for a randomly selected Fri. between 10-11 P.M., exactly four patients arrive), Internet users logging onto a Web site )\nComparison between Binomial:\n\nBinomal distribution is affected by yhe smaple size n and the probability p, whereas the Poisson distribution is affected only by mean \\(\\mu\\)\nA binomial distribution has a limit of possible values but a Poisson distribution has a possible values x without upper bound.\nSuppose the \\(X_n,n\\geq1\\) is a sequence of random variables such that \\(X_n\\)~Bin(\\(n,p_n\\)), where \\(p_n\\)~\\(\\lambda/n\\) as \\(n-&gt;\\infty\\) \\(lim_{n-&gt;\\infty}(np_0)=\\lambda\\), intuitively we can observe that \\(\\lambda\\) is the mean of\ngiven the well-known limit \\(\\lim _{n \\rightarrow \\infty}\\left(1-\\frac{\\lambda}{n}\\right)^n=e^{-\\lambda}\\), and \\[\n\\begin{aligned}\n& \\frac{n}{n} \\frac{n-1}{n} \\ldots \\frac{n-k+1}{n}=\\prod_{i=0}^{k-1}\\left(1-\\frac{i}{n}\\right) \\\\\n& \\lim _{n \\rightarrow \\infty} \\prod_{i=0}^{k-1}\\left(1-\\frac{i}{n}\\right)=1 \\\\\n& P\\left(X_n=k\\right)=\\binom{n}{k} p_n^k\\left(1-p_n\\right)^{n-k} \\\\\n&= \\frac{n \\cdots(n-k+1)}{k!} p_n^k\\left(1-p_n\\right)^n\\left(1-p_n\\right)^{-k} \\\\\n&= \\frac{1}{k!}(\\underbrace{n p_n}_{\\rightarrow \\lambda})^k \\underbrace{\\frac{n}{n} \\frac{n-1}{n} \\cdots \\frac{n-k+1}{n}}_{\\rightarrow 1} \\underbrace{\\left(1-p_n\\right)^n}_{\\rightarrow \\mathrm{e}^{-\\lambda}} \\underbrace{\\left(1-p_n\\right)^{-k}}_{\\rightarrow 1} \\\\\n& \\rightarrow \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda} \\underbrace{}_{\\text {as } n \\rightarrow \\infty .}\n\\end{aligned}\n\\]\nCheck that \\(p_x(k), k=0,1,2, \\cdots\\) defines a probability mass function: given the Taylor Series of \\(e^\\lambda\\) around \\(\\lambda=0\\) is given by \\(f(x)=\\) \\(\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!} x^n\\) and \\(e^\\lambda=\\sum_{n=0}^{\\infty} \\frac{\\lambda^n}{n!}\\). Hence, \\[\n\\begin{aligned}\n\\sum_{k=0}^{\\infty} p_X(k) & =\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\mathrm{e}^{-\\lambda} \\\\\n& =\\mathrm{e}^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\\\\n& =\\mathrm{e}^{-\\lambda} \\cdot \\mathrm{e}^\\lambda \\\\\n& =1 .\n\\end{aligned}\n\\] ## Geometric distribution (and Geometric series–powerful!fantastic series)\n\nThe probability of the first happening\nwell-defined \\(\\begin{aligned} \\sum_{k=1}^{\\infty} p_X(k) & =\\sum_{k=1}^{\\infty}(1-p)^{k-1} p \\\\ & =p \\sum_{k=0}^{\\infty}(1-p)^k \\\\ & =p \\cdot \\frac{1}{1-(1-p)} \\\\ & =1 .\\end{aligned}\\)\nTail probability of the Geometric distribution\n\nLet \\(X \\sim \\operatorname{Geom}(p)\\) then \\[\n\\begin{aligned}\nP(X&gt;n) & =P(X=n+1)+P(X=n+2)+P(X=n+3)+\\cdots \\\\\n& =(1-p)^n p+(1-p)^{n+1} p+(1-p)^{n+2} p+\\cdots \\\\\n& =(1-p)^n p\\left(1+(1-p)+(1-p)^2+\\cdots\\right) \\\\\n& =(1-p)^n p \\frac{1}{1-(1-p)} \\\\\n& =(1-p)^n\n\\end{aligned}\n\\] for any \\(n=0,1,2, \\ldots\\)\n\nMemoryless property of Geometric distribution\n\nSuppose that \\(X \\sim \\operatorname{Geom}(p)\\) and \\(n \\in\\{1,2,3, \\cdots\\}\\). Then \\[\nP(X-n=k \\mid X&gt;n)=P(X=k) \\quad, k=1,2,3, \\cdots\n\\]\nThat is, the distribution of \\(X-n\\) under the probability function \\(P(\\cdot \\mid X&gt;n)\\) is the same as the distribution of \\(X\\)\nthe memoryless property is saying that given the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success (independent)\n\nExpectation\n\nSuppose that \\(X \\sim \\operatorname{Geom}(p)\\). Then \\[\n\\begin{aligned}\nE(X)=\\sum_{k=1}^{\\infty} k P(X=k) & =\\sum_{k=1}^{\\infty} k(1-p)^{k-1} p \\\\\n& =p \\sum_{k=1}^{\\infty}\\left[-\\frac{\\mathrm{d}}{\\mathrm{~d} p}(1-p)^k\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\sum_{k=0}^{\\infty}(1-p)^k\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\frac{1}{1-(1-p)}\\right] \\\\\n& =-p \\frac{\\mathrm{~d}}{\\mathrm{~d} p}\\left[\\frac{1}{p}\\right] \\\\\n& =\\frac{1}{p}\n\\end{aligned}\n\\] — variance\nLikewise, \\[\n\\begin{aligned}\nE(X(X-1)) & =\\sum_{k=1}^{\\infty} k(k-1) P(X=k) \\\\\n& =\\sum_{k=2}^{\\infty} k(k-1)(1-p)^{k-1} p \\\\\n& =p(1-p) \\sum_{k=2}^{\\infty} k(k-1)(1-p)^{k-2} \\\\\n& =p(1-p) \\frac{\\mathrm{d}^2}{\\mathrm{~d} p^2}\\left[\\sum_{k=0}^{\\infty}(1-p)^k\\right] \\\\\n& =p(1-p) \\frac{\\mathrm{d}^2}{\\mathrm{~d} p^2} \\frac{1}{p} \\\\\n& =p(1-p) \\cdot \\frac{2}{p^3} \\\\\n& =\\frac{2(1-p)}{p^2} .\n\\end{aligned}\n\\] \\[\\begin{aligned} \\operatorname{Var}(X) & =E(X(X-1))+E(X)-(E(X))^2 \\\\ & =\\frac{2(1-p)}{p^2}+\\frac{1}{p}-\\frac{1}{p^2} \\\\ & =\\frac{1-p}{p^2}\\end{aligned}\\]"
  },
  {
    "objectID": "mth1113.html#geometric-distribution",
    "href": "mth1113.html#geometric-distribution",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Geometric distribution",
    "text": "Geometric distribution\n\nThe irrelevance of past events to the probability of future independent events\nGiven the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success.\nX~Geom(p) and n$${1,2,3,….}\nP(X-n=k|X&gt;n)=P(X=k), k=1,2,3,…\nTail probability for the probability calculation of more/higher than…."
  },
  {
    "objectID": "mth1113.html#addition",
    "href": "mth1113.html#addition",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Addition",
    "text": "Addition\nIndependent random variable with the same distribution allows the addition law"
  },
  {
    "objectID": "mth1113.html#interpretation",
    "href": "mth1113.html#interpretation",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "interpretation:",
    "text": "interpretation:\n\nStandard normally distributed sth. z=1.58(corresponding to 0.9429):\nthe probability of randomly selecting sth. with a value less than 1.58(unit) is equal to the area(probability) of 0.9429.\n(Or: 94.29% of sth, will have a value below 1.58(unit))"
  },
  {
    "objectID": "mth1113.html#sampling-distributions-and-estimators",
    "href": "mth1113.html#sampling-distributions-and-estimators",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Sampling distributions and Estimators",
    "text": "Sampling distributions and Estimators\nWe are beginning to embark a jourfa d dney that allows us to learn about populations by obtaining data from samples since it is rare that we know all values in an entire population.\nSampling distribution of a statistic is the probability distribution of a sample statistics (such as mean/proportion which tend to target the population mean/proportion), with all samples having the same sample size. This concept is important to understand. The behavior of a statistic can be known by understanding its distribution( (The random variable in this case is the value of that sample statistics)). Under certain condition, the distribution of sampling mean/proportion approximates a normal distribution.\nThough statistics does not depend on unknown parameters, the distribution of it depend on unknown parameters.(eg. Normal distribution of sample means depends on population mean(an unknow parameter) and standard deviation)\n(ps: the advantage of sampling with replacement:\nwhen selecting a relatively small sample from a large population, it makes no significant difference whether we sample with or without replacement.\nSampling with replacement results in independent events that are unaffected by previous outcomes, and independent events are easier to analyze and they result in simpler formulas.)\nFor a fixed sample size, the mean of all possible sample means is equal to the mean of population though sample means vary(sampling variability)"
  },
  {
    "objectID": "mth1113.html#unbiased-estimators-and-biased-estimators",
    "href": "mth1113.html#unbiased-estimators-and-biased-estimators",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Unbiased estimators and biased estimators",
    "text": "Unbiased estimators and biased estimators\nStatistics that target population parameters: Mean, variance, proportion\nStatistics that do target population parameters: Median, Range, Standard Deviation\n(the bias is relatively small then sampling standard deviation in large samples so s is ofent used to estimate \\(\\sigma\\))"
  },
  {
    "objectID": "mth1113.html#using-the-normal-distribution-as-an-approximation-to-the-binomial-distribution",
    "href": "mth1113.html#using-the-normal-distribution-as-an-approximation-to-the-binomial-distribution",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Using the Normal distribution as an approximation to the binomial distribution",
    "text": "Using the Normal distribution as an approximation to the binomial distribution\nrequirement: np, n(1-p) \\(\\geq\\) 5\nBe careful: adjust x for continuity by + or - 0.5(eg. at least 99, choose 98.5)"
  },
  {
    "objectID": "mth1113.html#to-hypothesis-testing",
    "href": "mth1113.html#to-hypothesis-testing",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "to hypothesis testing",
    "text": "to hypothesis testing\na question (eg):\nIn a test of a gender-selection technique assume that 100 couples using a particular treatment give birth to 52 girls (and 48 boys). If the technique has no effect, then the probability of a girl is approximately 0.5. If the probability of a girl is 0.5, find the probability that among 100 newborn babies, exactly 52 are girls. Based on the result, is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?\nit is a binomial distribution with np=nq=100*0.5=50\\(\\geq 5\\)\nso we use the normal distribution with mean of 50 and \\(\\sigma = \\sqrt {npq} = 5\\)as an approximation to the binomial distribution\nto answer”is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?“, we need to calculate more than 52(x successes among n trials is an unusually high number of successes if P(\\(x\\geq a\\)) is very small)\n原因是因为如果只看52这个数字肯定概率很小，因为任何单个数字发生的可能性概率都很小\nSo if the answer of P(\\(x\\geq 52\\)) is small we could conclude that the gender selection is useful\n(总结：如果是0。5概率来看的话52以上本来就不是难事，as indicated by the such large probability of P(\\(x\\geq 52\\)) 所以我们没有充分证据拒绝“not effective”这个假设前提)\n此处还没有引入假设检验所以都是用using probability to determine when results are unusual这个思想来思考问题的：\n\nUnusually low： x successes among n trails is an unually low number of successes if P(x or fewer) is very small\n\n\nInterpretation for another example of gender selection(using only unusual results to explain): Because the probability of more than 13 girls, which is 0.001 is so low, we conclude that it is unusual to get 13 girls among 14 babies (using binomial to calculate it). This suggests that the technique of gender selection appears to be effective since it is highly unlikely that the result of 13 girls among 14 births happened by chance."
  },
  {
    "objectID": "mth1113.html#normal-distribution-1",
    "href": "mth1113.html#normal-distribution-1",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "Normal distribution",
    "text": "Normal distribution\nIf a variable is the superposition result of a large number of small independent random factors, then the variable must obey the normal distribution of variables\ne.g. measure error\n\nAssessing Normality\nIn general, quantile plots can be used to assess any probability distribution.\nFor a normal quantile plot(or normal probability plot), it is a graph of points(x,y) where each x vaue is from the original set of sample data and each y value is the cooresponding z score that is a quantile value expected from the standard normal distribution\n\nProcedures\n\nHistogram(not helpful for small data set)\noutliers: reject normality if there is more than 1 outlier present(not helpful for small data set)\nnormal quantile plot:\n\nsort data from lowest to highest\n\nSampling distribution of pˆ when population is infinite"
  },
  {
    "objectID": "mth1113.html#inferences-from-2-samples-introduces-the-differences-between-two-populaton-means-using-matched-pairs-but-correlaition-and-regression-analyze-the-association-between-the-2-variables-and-if-such-an-association-exists-we-wnat-to-describe-it-with-an-equation-that-can-be-used-for-predictions",
    "href": "mth1113.html#inferences-from-2-samples-introduces-the-differences-between-two-populaton-means-using-matched-pairs-but-correlaition-and-regression-analyze-the-association-between-the-2-variables-and-if-such-an-association-exists-we-wnat-to-describe-it-with-an-equation-that-can-be-used-for-predictions",
    "title": "APH101+ MTH113 Intro to probability and statistics+APH003–exploring world through data",
    "section": "inferences from 2 samples introduces the differences between two populaton means using matched pairs but correlaition and regression analyze the association between the 2 variables and if such an association exists we wnat to describe it with an equation that can be used for predictions",
    "text": "inferences from 2 samples introduces the differences between two populaton means using matched pairs but correlaition and regression analyze the association between the 2 variables and if such an association exists we wnat to describe it with an equation that can be used for predictions\npaired sampled data(or called bivariate data)\n\na correlation exists between two variables when one of them is related to the other in some way.\nthe linear correlation coefficient r measures the strength of the linear association between the paired x- and y-quantitative values in a sample. Its value is computed by using the formula(Pearson(1857-1937) product moment correlation coefficient)\n\n……otherwise there is not sufficient evidence to support the conclusion of a significant linear equation\n!!!: interpreting r: explained variation: the value of \\(r^2\\) is the proportion of the variation in y that is explained by the linear association between x and y.(and the other percentage is explained by factors other thanx such as characteristics not included in the study)\n\ncorrelation does not imply causality,just the association\naverage suppress individual variation and may inflate the correlation coefficient(the linear correlation coefficient became higher when reginal averages were used)"
  },
  {
    "objectID": "Mathematical Analysis 1.html",
    "href": "Mathematical Analysis 1.html",
    "title": "Analysis1",
    "section": "",
    "text": "The first section here is the knowledge that attracts most of my interests on this module, followed by the lecture notes I tapped when I am on the journey of this module.\n\n\n\n\n\nf(x)+f(y)=f(x+y) (addition in linearity in linear transformation)\n(a line across the original point)\n\n1–&gt;1+1–&gt;1+1+1—&gt;n positive integer(addition is needed/1 is needed)\n0—&gt;0-1—0-1-1—-n positive integer(addition is needed/0 is needed)\n\\(\\frac{p}{q}\\)—- rational number(multiplication is needed)\n\nSince the rational number is countable(|N|=|N\\(\\times\\)N|=|Q|) but irrational number is uncountable, we could not directly go ahead from Q to R\\Q. Fortunately, we have the property of density of irrational number(of course also rational number) in R. In this case we could introduce limit here, and also continuity\n\nThis means that, usually, if a situation in rational number is true, then in the whole real number it is almost true.\nFinally we get a whole line across the original point here.\n\n\n\nIf f(x) is continuous on R. g(x)=f(x) when x is a rational number. Then we have g(x)=f(x) on R.\n\n\nUse sequence to think intuitively and write down the proof rigorously:\nWithout loss of generalization, here we let our R-continuous function to be \\(x^2\\).\nquestion: If f is continuous on [a,b], f(x)=\\(x^2\\) if x \\(\\in\\) R\\Q. Show: f(x)=$x^2, xR $ in [a,b]\nproof:\nFirstly we write down Sequential criterion of limit which we will use to finish the proof:\n(Sequential criterion for limits of functions). Let \\(f: A \\rightarrow \\mathbb{R}\\). The following are equivalent. 1. \\(\\lim _{x \\rightarrow a} f(x)=\\ell\\). 2. For any sequence \\(\\left(a_n\\right)_{n \\in \\mathbb{N}}\\) with \\(a_n \\in A, a_n \\neq a\\) and \\(\\lim _n a_n=a\\), we have \\[\n\\lim _n f\\left(a_n\\right)=\\ell\n\\]\nProof. (1) \\(\\Rightarrow\\) (2): Suppose \\(\\lim _{x \\rightarrow a} f(x)=\\ell\\). Then \\(\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0\\) such that \\(\\forall x\\) with \\(|x-a|&lt;\\delta\\) and \\(x \\neq a\\), we have \\[\n|f(x)-\\ell|&lt;\\varepsilon\n\\]\nLet \\(\\lim _n a_n=a, a_n \\in A\\) and \\(a_n \\neq a\\) for all \\(n \\in \\mathbb{N}\\). Then for the \\(\\delta&gt;0\\) above corresponding to \\(\\varepsilon, \\exists k \\in \\mathbb{N}\\) such that \\(\\forall n \\geq k, n \\in \\mathbb{N}\\), we have \\[\n\\left|a_n-a\\right|&lt;\\delta\n\\] which implies \\[\n\\left|f\\left(a_n\\right)-\\ell\\right|&lt;\\varepsilon .\n\\]\nThat shows that \\(\\lim _n f\\left(a_n\\right)=\\ell\\). \\((2) \\Rightarrow(1)\\) : Suppose for the sake of contradiction that \\(\\lim _{x \\rightarrow a} f(x) \\neq \\ell\\). Then \\(\\exists \\varepsilon_0&gt;0\\) such that \\(\\forall \\delta&gt;0, \\exists x\\) with \\(|x-a|&lt;\\delta\\) and \\(x \\neq a\\) such that \\[\n|f(x)-\\ell| \\geq \\varepsilon_0\n\\]\nWe construct a sequence as follows. Take \\(\\delta=\\frac{1}{n}\\). Then \\(\\exists a_n\\) with \\(\\left|a_n-a\\right|&lt;\\delta=\\frac{1}{n}\\) and \\(a_n \\neq a\\) such that \\(\\left|f\\left(a_n\\right)-\\ell\\right| \\geq \\varepsilon_0 . a_n\\) satisfies \\[\na-\\frac{1}{n}&lt;a_n&lt;a+\\frac{1}{n} .\n\\]\nBy letting \\(n \\rightarrow+\\infty\\) and using the Squeeze Theorem(or we could use another method to finish argument here by constructing only one sequence (choose \\(\\delta\\) to be \\(1/n\\) also and then always find one \\(x_n\\) in the \\(\\delta-\\) neighborhood and this \\(x_n\\) is also convergent to a)), we have \\(\\lim _n a_n=a\\). So by the given condition, \\(\\lim _n f\\left(a_n\\right)=\\ell\\). However, for all \\(n \\in \\mathbb{N}\\), \\[\n\\left|f\\left(a_n\\right)-\\ell\\right| \\geq \\varepsilon_0 \\Rightarrow f\\left(a_n\\right) \\geq \\ell+\\varepsilon_0 \\text { or } f\\left(a_n\\right) \\leq \\ell-\\varepsilon_0 .\n\\] contradicting to f\\((a_n)\\) is convergent.\nNow we begin our proof:\nWe have known that if \\(a_n\\in\\) R/Q f(a\\(_n\\))=\\({a_n}^2\\)\nSince we have the property of “irrational numbers are dense on R” we know that \\(\\exists a_n\\in(a-1/n,a+1/n), \\forall n\\in N, a\\in Q\\). This constructs our sequence \\(a_n\\) such that \\(lim_{n\\rightarrow \\infty} a_n=a, a\\in Q\\)\nThis implies that \\(lim_{n\\rightarrow\\infty}f(a_n)=f(lim_{n\\rightarrow\\infty}a_n)\\)=f(a).\n(The first equality is because of this proposition:(Limit of composition of functions). Let \\(f: A \\rightarrow \\mathbb{R}, g: B \\rightarrow \\mathbb{R}\\) such that \\(R(f) \\subseteq B\\). If \\(\\lim _{x \\rightarrow a} f(x)=\\ell\\) and \\(g(x)\\) is continuous at \\(\\ell\\), i.e. \\(\\lim _{x \\rightarrow \\ell} g(x)=g(\\ell)\\), then \\[\n\\lim _{x \\rightarrow a} g(f(x))=g(\\ell)=g\\left(\\lim _{x \\rightarrow a} f(x)\\right) .\n\\] proof:Proof. \\(\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0\\) such that \\(\\forall y\\) with \\(|y-\\ell|&lt;\\delta\\) and \\(y \\in B\\), we have \\[\n|g(y)-g(\\ell)|&lt;\\varepsilon\n\\]\nFor this \\(\\delta, \\exists \\delta^{\\prime}&gt;0\\) such that \\(\\forall x\\) with \\(0&lt;|x-a|&lt;\\delta^{\\prime}\\) and \\(x \\in A\\), we have \\[\n|f(x)-\\ell|&lt;\\delta\n\\]\nSo if \\(0&lt;|x-a|&lt;\\delta^{\\prime}\\) and \\(x \\in A\\), then \\(|f(x)-\\ell|&lt;\\delta\\) and \\(f(x) \\in B\\), which imply that \\(|g(f(x))-g(\\ell)|&lt;\\varepsilon\\). This proves the proposition. )\nAlso, \\(lim_{n\\rightarrow \\infty}f(a_n)=lim_{n\\rightarrow \\infty}{a_n}^2\\)=\\((lim_{n\\rightarrow \\infty}{a_n})^2=a^2\\)\nSo we have f(a)=\\(a^2\\)."
  },
  {
    "objectID": "Mathematical Analysis 1.html#existence-means-beyond-any-human-efforts-and-objectively-existence.-we-take-the-value-that-we-need-means-a-human-effort-and-because-it-is-arbitrary-we-have-the-freedom-to-choose-one.-dr.-zhang",
    "href": "Mathematical Analysis 1.html#existence-means-beyond-any-human-efforts-and-objectively-existence.-we-take-the-value-that-we-need-means-a-human-effort-and-because-it-is-arbitrary-we-have-the-freedom-to-choose-one.-dr.-zhang",
    "title": "Analysis1 and Analysis 2",
    "section": "‘Existence’ means beyond any human efforts, and objectively existence. We take the value that we need means a human effort, and because it is arbitrary, we have the freedom to choose one. — Dr. Zhang",
    "text": "‘Existence’ means beyond any human efforts, and objectively existence. We take the value that we need means a human effort, and because it is arbitrary, we have the freedom to choose one. — Dr. Zhang"
  },
  {
    "objectID": "Mathematical Analysis 1.html#theorem",
    "href": "Mathematical Analysis 1.html#theorem",
    "title": "Analysis1 and Analysis 2",
    "section": "Theorem",
    "text": "Theorem\nLet \\(O\\subset R\\) be an open set then \\(O=\\cup_{n=1}^\\infty I_n\\) where \\(I_n\\) is an open interval in R"
  },
  {
    "objectID": "Mathematical Analysis 1.html#def",
    "href": "Mathematical Analysis 1.html#def",
    "title": "Analysis1 and Analysis 2",
    "section": "Def",
    "text": "Def\nA collection of open sets covers a set A if \\(A\\subset \\cup O_\\alpha\\). The collection {\\(O_\\alpha\\)} is called an open cover of A."
  },
  {
    "objectID": "Mathematical Analysis 1.html#theorem-1",
    "href": "Mathematical Analysis 1.html#theorem-1",
    "title": "Analysis1 and Analysis 2",
    "section": "Theorem",
    "text": "Theorem\nLet C={\\(O_\\alpha\\)} be a collection of open sets of real numbers then there is a countable subcollection {\\(O_i\\)} of C such that \\(\\cup_{O\\in C}O=\\cup_{n=1}^\\infty O_i\\)\nAny open cover of a set of real numbers contains a countable subcover."
  },
  {
    "objectID": "Mathematical Analysis 1.html#i-have-proved-that",
    "href": "Mathematical Analysis 1.html#i-have-proved-that",
    "title": "Analysis1 and Analysis 2",
    "section": "i have proved that:",
    "text": "i have proved that:\nthe closure of \\(Q\\) = R &lt;–&gt; \\(R\\approx Q\\) &lt;–&gt; \\(\\forall x\\in R, \\exists x_n\\in Q\\) such that\\(x_n--&gt;x\\). Q is a countable, dense subset of R, R is separable."
  },
  {
    "objectID": "Mathematical Analysis 1.html#albel-guass-is-a-fox-走过的地方狐狸尾巴扫掉了",
    "href": "Mathematical Analysis 1.html#albel-guass-is-a-fox-走过的地方狐狸尾巴扫掉了",
    "title": "Analysis1 and Analysis 2",
    "section": "albel: guass is a fox, 走过的地方狐狸尾巴扫掉了",
    "text": "albel: guass is a fox, 走过的地方狐狸尾巴扫掉了"
  },
  {
    "objectID": "Mathematical Analysis 1.html#algebra-of-continuity",
    "href": "Mathematical Analysis 1.html#algebra-of-continuity",
    "title": "Analysis1 and Analysis 2",
    "section": "algebra of continuity",
    "text": "algebra of continuity\n\\(\\exists \\delta = min\\){\\(\\delta_1,\\delta_2\\)}"
  },
  {
    "objectID": "Mathematical Analysis 1.html#history",
    "href": "Mathematical Analysis 1.html#history",
    "title": "Analysis1 and Analysis 2",
    "section": "history",
    "text": "history\ncauchy 1821(decrease indefinitely with those of \\(\\alpha\\))–weierstrass 1874 (\\(\\epsilon-\\delta\\))—–"
  },
  {
    "objectID": "Mathematical Analysis 1.html#understanding-in-a-long-process",
    "href": "Mathematical Analysis 1.html#understanding-in-a-long-process",
    "title": "Analysis1 and Analysis 2",
    "section": "understanding in a long process",
    "text": "understanding in a long process\n\n\\(\\delta\\) decides how good the continuity of the function is."
  },
  {
    "objectID": "Mathematical Analysis 1.html#cauchy-1822",
    "href": "Mathematical Analysis 1.html#cauchy-1822",
    "title": "Analysis1 and Analysis 2",
    "section": "Cauchy 1822",
    "text": "Cauchy 1822\nWhat is a limit?–number\nUntil 1870, the answer to “What is the number” had been known.\n(这样来看，学习数分的一年痛苦就不算什么)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#ordercardinality",
    "href": "Mathematical Analysis 1.html#ordercardinality",
    "title": "Analysis1 and Analysis 2",
    "section": "order–cardinality",
    "text": "order–cardinality\n1，2，3，n 2，4，6，2n\nf(n)=2n\n\nexplaination：无穷大加一个数还是等于无穷大\n\n\\(f:N-&gt;N\\cup\\){\\(a_1,a_2,a_3\\)}\nf(1)=\\(a_1\\) f(2)=\\(a_2\\),..f(4)=1, f(n+3)=n, this function is one-to-one – bijection\n–无穷大加上任意一个实属x还是无穷大\nN x N is countable\n\\(S \\subset N\\) S is countable\nf(m,n)–&gt;\\(2^m3^n\\)（只要取互质的两个数就ok）\nN x N–&gt; N\nf: 1-1\n\\(2^{m_1}3^{n_1}\\)=\\(2^{m_2}3^{n_2}\\)—\\(m_1=m_2,n_1=n_2\\)\nf(N x N)\\(\\subset\\) N\n只要映射到N的subset就可以了\n2维3维n维\n(0,1) is uncountable 反证法\nCantor’s diagonal process\n欧几里得是第一个反证法—有无穷多个质数\n{p1 p2 p3…}\np=p1+p2+pn+…+1\np1|p…pn|p—p is a prime—contradiction to countable\n(0,1)={x1,…x} countable\nx1=01a11a12a13. a1n\\(\\in\\){0,1,2,…9} x2=01a21a22a23. a2n\\(\\in\\){0,1,2,…9}\nconstruct a real number\ny=0.b1b2b3…\nb1 budengyu a11 y budengyu x1\nb2 budengyu a22 y budengyu x2\nb3 budengyu ann y budengyu xn\nbn in {a1,…a}–&gt; y\\(\\in\\)(0,1) but y\\(\\notin\\){\\(x_1,...x_n\\)}\nEx: [0,1) 约等于 (0,1)\nproof: Let{p1,p2,….}=(0,1)\\(\\cap Q\\) 分成有理数和无理数\nf(0)=p1 f(p1)=p2 f(pn+1)=pn f(x)=x \\(x\\in (0,1)-\\){p1,..pn}\n概率 特征函数\nCantor set\n进位的概念"
  },
  {
    "objectID": "Mathematical Analysis 1.html#section",
    "href": "Mathematical Analysis 1.html#section",
    "title": "Analysis1 and Analysis 2",
    "section": "",
    "text": "导数 积分 无穷级数 –都是极限 极限是什么–数数是什么 – Augustin Cauchy(1789-1846)\n\n人文素养\n\ncantor 1872年是不平凡的一年， 有（Weierstrass, Dedekind, Meray, Heine, Cantor）同时提出了实属系统建构的理解 出版著作解决困惑人们2500多年的问题\nWeierstrass 法律 数学 喝酒 干架 退学 在中学里边仍然没有与学术脱节 复变函数 椭圆函数（研究在复数平面上的解析函数， C=\\(R^2\\) a+ib–&gt;(a,b) is an isomorphism） \\(C \\cup {\\infty}\\) 相似于球面 \\(S^2\\) 约等于 \\(R^2\\cup \\infty\\) 变成closed and bounded one-point- compactfication 为什么实数不bounded因为无穷 无穷大这点不是实数（不满足实数性质—无穷大加无穷大还是无穷大；无穷大减无穷大是any number）因为\n任何单调有界的实属序列都有极限存在—dedekind的假设\ndedekind cut\nif rational number\n毕达哥拉斯的年代的万物皆数是有理数\n古希腊哲学 古希腊数学\nThales\n做测量工作创出非欧几何这门学问 真正的天才是能够创出一门学问的\n牛顿是科学第一人 当人们迷迷糊糊的时候 他打开理性的光\n这一切是黑暗的 上帝说让牛顿去吧 于是就有了光\n改编圣经的话–文采\n毕达哥拉斯定理的证明都是非常有创意的东西\n毕达哥拉斯定理十个证明\n读慢一点\n心浮气躁\n数学是需要耐心的\n一个城市的发展不应该是盖房子 应该是好的博物馆音乐厅公园 提高人民素养\nvolume 2 多变数 stolze定理 divergent定理\nlimit sup limit inf –这个东西就是我的实数\nCantor-==柯西序列。距离概念\n\\(|a_m-a_n|--&gt;0\\). d(\\(a_m,a_n\\)–&gt;0) metric\nequivalent equation equivalent class 实数 实数是一个集合\ncantor–集合论\n定义是最难的\n有共通之处–作为定义\n会用定理很重要 不是高斯\n用多了就知道是怎么一回事\n定理给了条件 哪里用到什么条件 怎么用的\n碰到好老师的重要性 姜立夫 量纲自然\ncomplete的完备不是完美 不是 门掉了一个螺丝 找一个螺丝完完全全放上去发挥作用\n认识自己 我是螺丝 完完全全发挥我的作用 把长处 恩赐发挥出来 就是complete\n有理数在实数中有很多孔隙 把空隙。把accumulation point收紧来\nclosure of A = B\n\\(\\forall b\\in B, \\exists\\){\\(a_n\\)} \\(\\subset A\\) such that \\(a_n--&gt;b\\)\n$AB $ not equals to $&gt; 0 $\n实数是唯一一个complete ordered field\n代数结构—分配律\neg.complex number没有order所以我们定义norm\n\n和闭区间套定理极限思想的联系和区别？？？"
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-is-an-infinite-subset-of-n-then-a-ni.e.-a-is-countably-infinite.",
    "href": "Mathematical Analysis 1.html#if-a-is-an-infinite-subset-of-n-then-a-ni.e.-a-is-countably-infinite.",
    "title": "Analysis1 and Analysis 2",
    "section": "If A is an infinite subset of N, then |A| = |N|,i.e. A is countably infinite.",
    "text": "If A is an infinite subset of N, then |A| = |N|,i.e. A is countably infinite.\n\n\nin fact， N could be any countable set"
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-not-empty-set-has-upper-bound-then-it-must-have-the-unique-sup.",
    "href": "Mathematical Analysis 1.html#if-a-not-empty-set-has-upper-bound-then-it-must-have-the-unique-sup.",
    "title": "Analysis1 and Analysis 2",
    "section": "if a not empty set has upper bound, then it must have the unique sup.",
    "text": "if a not empty set has upper bound, then it must have the unique sup.\nProve: if a not empty set has upper bound, then it must have the unique sup.\n\nMethod 1: (Completeness axiom)\n\\[\n\\begin{gathered}\nX \\neq \\phi, \\quad Y=\\{y \\in \\mathbb{R} \\mid \\forall x \\in X(x \\leq y)\\} \\neq \\phi \\\\\n\\\\\n\\forall x \\in X, \\forall y \\in Y, x \\leq y \\\\\n\\\\\n\\Rightarrow \\exists c \\in \\mathbb R, \\forall x \\in X, \\forall y \\in Y, \\\\\n\\\\\nx \\leqslant c \\leqslant y(\\text { Completeness axiom) } \\\\\n\\\\\n\\Rightarrow(c \\in Y) \\wedge(\\{\\forall y \\in Y \\mid y \\geqslant c\\}) \\\\\n\\\\\n\\Rightarrow c=\\min Y\n\\end{gathered}\n\\]\nSo \\(c\\) is the only sup. (uniqueness of minimum element of a set—2 inequalities lead to the equality leading to the only one result).\n\n\nMethod 2: Cantor Nested Interval Property(limit思想).asdfaskfkasdjklsad\nWe choose a random upper bound \\(\\gamma, x \\in E(\\) the set) \\[\n[x, r]=\\left[a_1 b_1\\right] \\supset\\left[a_2, b_2\\right] \\supset \\cdots \\left[a_n, b_n\\right]\n\\] (each time we use the method of bisection to choose one side Including the point in \\(E\\) ) $$\n\\[\\begin{aligned}\n& \\text { Since }\\left[a_1, b_1\\right] \\supset\\left[a_2, b_2\\right] \\cdots, b_n-a_n=\\frac{\\gamma-x}{2^{n-1}} \\rightarrow 0, \\\\\n& \\beta \\in\\left[a_n, b_n\\right],(n=1,2, \\cdots), \\lim _{n \\rightarrow \\infty} a_n=\\lim _{n \\rightarrow \\infty} b_n=\\beta \\text {(limit thinking of the Cantor Nested Interval) } \\\\\n& \\Rightarrow \\forall c \\in E, c \\leqslant b_n \\Rightarrow c \\leqslant \\beta \\text { (E }  \\text {is never on the right of} [a_n,b_n]) \\\\\n& \\Rightarrow \\forall \\varepsilon&gt;0, \\exists d \\in E, d&gt;\\beta-\\varepsilon\\text { (each} [a_n,b_n] \\text { has points in E.(or see the attached picture to see more picisely))}\n\\end{aligned}\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-not-empty-set-has-lower-bound-then-it-must-have-the-unique-inf.",
    "href": "Mathematical Analysis 1.html#if-a-not-empty-set-has-lower-bound-then-it-must-have-the-unique-inf.",
    "title": "Analysis1 and Analysis 2",
    "section": "If a not empty set has lower bound, then it must have the unique inf.",
    "text": "If a not empty set has lower bound, then it must have the unique inf.\n\nMethod 1: same as before.\n\n\nMethod 2: Based on before.\nSuppose We choose \\(m\\) as a lower bound of \\(E\\). \\[\n\\begin{aligned}\n& \\Rightarrow \\forall x \\in E, x \\geqslant m,-x \\leqslant-m \\text {. } \\\\\n& \\text { Let } F=\\{-x \\mid x \\in E\\} \\text {. } \\\\\n& \\Rightarrow \\beta=\\sup F \\\\\n& \\Rightarrow-x \\leqslant \\beta, x \\geqslant-\\beta \\text {. } \\\\\n& \\forall \\varepsilon&gt;0, \\exists-d \\in E,-d&gt;\\beta-\\varepsilon, \\\\\n& \\Rightarrow d&lt;-\\beta+\\varepsilon \\\\\n& \\Rightarrow-\\beta=\\inf E\n\\end{aligned}\n\\] \\[\n\\Rightarrow-\\sup (-E)=\\operatorname{inf} E \\text {. }\n\\]\n\n\nMethod 3 more generalized than Method 2’s conclusion (if c is negative then inf(cA)=csupA)\nIn fact, If \\(c&lt;0\\), then \\(\\sup (cA)=\\operatorname{cinf} A, \\inf (c A)=\\operatorname{csup} A\\). (and in particular. sup \\((-B)=-\\) inf \\(B\\) )\nSince if M=supA, \\(\\forall x\\in A,x\\leq M,cx\\geq cM\\), which indicates that cM is the lower bound of cX.\nSo, \\(cA\\) has lower bound ( \\(s\\) )if and only if \\(A\\) has upper bound(s).(inverse, also true) \\(cA\\) is not empty if and only if A is not empty.\nSo, cA has \\(\\inf (cA)\\) if and only if \\(A\\) has supA \\[\n\\begin{aligned}\n& \\forall c x \\in C A, c x \\geqslant c M \\\\\n& \\text { if } \\exists c M', \\forall c x \\in c A, c x \\geqslant c M^{\\prime}, c M^{\\prime}&gt;c M, \\\\\n& X \\leq M^{\\prime}, M^{\\prime}&lt;M(\\forall x \\in A)\n\\end{aligned}\n\\]\nContradicting to the condition that \\(M=\\sup A\\)\nSo \\(\\nexists C M^{\\prime}\\) So cM is the largest lower bound \\[\n\\begin{aligned}\n& \\text { so } c M=\\inf (c A) \\\\\n& \\text { so } c\\sup A=\\inf (c A)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html#exercise-12s-sup",
    "href": "Mathematical Analysis 1.html#exercise-12s-sup",
    "title": "Analysis1 and Analysis 2",
    "section": "exercise: (1,2]’s sup–",
    "text": "exercise: (1,2]’s sup–\n\nMethod 1\n\n2 is an upper bound of [1,2) (obviously)\nif \\(\\exists \\varepsilon&gt;0\\), s.t \\(2-\\varepsilon\\) is also an upper bound of \\([1,2)\\) \\[\n\\begin{aligned}\n& \\because 1 \\in[1,2) \\\\\n& \\therefore2-\\varepsilon \\geqslant 1\n\\end{aligned}\n\\] choose \\(2-\\frac{\\varepsilon}{2} \\in(2-\\varepsilon, 2)\\) Then \\(2-\\frac{\\varepsilon}{2} \\in[1,2)\\) So \\(\\exists\\left(2-\\frac{\\varepsilon}{2}\\right) \\in[1,2)\\) while \\(\\left(2-\\frac{\\varepsilon}{2}\\right)&gt;(2-\\varepsilon)\\) So \\(2-\\varepsilon\\) is not an upper bound, contradicting to the suppose. So we have proved that 2 is the smallest upper bound, i.e. \\(\\sup [1,2)=2\\).\n\n\n\nMethod 2\n\n2 is an upper bound of [1,2) (obviously)\n\\(\\forall \\varepsilon&gt;0, \\exists b \\in[1,2)\\) with \\(b&gt;2-\\varepsilon\\). We can take \\(b=\\max \\left\\{2-\\frac{\\varepsilon}{2}, 1\\right\\}\\)."
  },
  {
    "objectID": "Mathematical Analysis 1.html#exercise-supcsupabsupasupb-from-professor-a-kun",
    "href": "Mathematical Analysis 1.html#exercise-supcsupabsupasupb-from-professor-a-kun",
    "title": "Analysis1 and Analysis 2",
    "section": "exercise: supC=sup(A+B)=supA+supB from Professor A Kun",
    "text": "exercise: supC=sup(A+B)=supA+supB from Professor A Kun\n\\[\n\\begin{array}{rl}\n\\text { if } A\\subset R,  B\\subset R, \\text { define: } \\\\\nC  :=A+B=\\{z \\in R: z=x+y, x \\in A, y \\in B\\} \\\\\nD  :=A-B=\\{z \\in R: z=x-y, x \\in A, y \\in B\\}\n\\end{array}\n\\] show that \\[\n\\begin{aligned}\n& \\text { (1) } \\sup C=\\sup (A+B)=\\sup A+\\sup B \\\\\n& \\text { (2) } \\sup D=\\sup (A-B)=\\sup A-\\inf B\n\\end{aligned}\n\\] (1) Proof:: Obviously,\\(C\\) has upper bounds if and only if \\(A\\) and \\(B\\) have upper bounds, and \\(C\\) is not empty. So \\(C\\) has sup C if and only if and only if \\(A\\) has sup \\(A\\) and \\(B\\) has sup \\(B\\). (Completeness axiom).\n\nprove: \\(\\sup C \\leqslant \\sin A+\\sup B\\). \\[\n\\because x+y \\leqslant \\sup A+\\sup  B\n\\] \\(\\therefore(\\operatorname{sip} A+\\sup B)\\) is an upper bound of \\(C\\) \\[\n\\therefore \\sin C \\leqslant \\sin A+\\sup B\n\\]\nProve : \\(\\sup C \\geqslant \\operatorname{supA}+\\) sup B \\[\n\\begin{aligned}\n& \\because \\forall \\varepsilon&gt;0, \\exists x \\in A, y \\in B \\text {, s.t. } \\\\\n& \\operatorname{sup} A-\\varepsilon&lt;x, \\operatorname{sup} B-\\varepsilon&lt;y . \\\\\n& \\Rightarrow \\sup A+\\sup B-2 \\varepsilon&lt;x+y \\\\\n& \\text { i.e. }(\\operatorname{sup} A+\\sup B-2 \\varepsilon)_{\\text {max }}&lt;(x+y)_{\\text {max }}\n\\end{aligned}\n\\]\n\nSince \\(x+y \\leq\\) sup c We have \\(\\sup A+\\sup B-2 \\varepsilon&lt;\\sup c, \\forall \\varepsilon&gt;0\\) \\[\n\\begin{aligned}\n& \\text { i.e. } \\operatorname{(sup} A+\\sup B-2 \\varepsilon)_{\\text {max }}&lt;\\sup C \\\\\n\\end{aligned}\n\\]\nWe. As \\(\\varepsilon \\rightarrow 0\\), We have \\(\\operatorname{sup} A+\\sup B \\leqslant \\operatorname{} \\operatorname{sup} C\\) So, we have \\(\\operatorname{Sup} C=\\operatorname{Sup} A+\\operatorname{Sup} B\\). Then, we have \\(\\sup D=\\sup (A-B)\\) \\[\n\\begin{aligned}\n& =\\sup (A+(-B)) \\\\\n& =\\sup A+\\sup (-B) \\\\\n& =\\sup A-\\inf B\n\\end{aligned}\n\\] (We have proved \\(\\sup (-B)=-\\inf B\\) before)\n\nanother example\n Let M ∈ R and A, B be two bounded, negative subsets of R,0 &lt; x, y &lt; M, ∀x ∈ A, y ∈ B\nwhen we are proving sup C = sup(AB) = sup A · sup B\nOn the other hand, from the definition of $a^*$ and $b^*, \\forall \\epsilon&gt;0$ there exist $a \\in A$ and $b \\in B$ such that\n\\[\na^*-\\epsilon&lt;a&lt;a^* \\quad \\text { and } \\quad b^*-\\epsilon&lt;b&lt;b^*\n\\]\nThen \\[\n\\left(a^*-\\epsilon\\right)\\left(b^*-\\epsilon\\right)&lt;a b \\leqslant a^* b^*\n\\] or ignoring \\(\\epsilon^2\\) term (If ε &gt; 0, then ε, 3ε, ε², ε⁵, they all represent “any number greater than zero”, and ε’ also represents any number greater than zero, so they are equivalent, that is, we can say that they are equal to ε’.) \\[\na^* b^*-(a+b) \\epsilon=a^* b^*-\\epsilon^{\\prime}&lt;a b \\leq c^*\n\\]\nThis is true for all \\(\\epsilon^{\\prime}&gt;0\\), so \\[\n\\sup A \\sup B=a^* b^* \\leq c^*=\\sup C\n\\]\nCombining the above two inequalities, we can conclude that \\(\\sup A \\sup B=\\) \\(\\sup C\\). ## an exercise about think good Archimedean number from Professor Andrew Lin(A Kun)\n\nConsider the set \\[\nA=\\left\\{\\left.(-1)^n\\left(1-\\frac{1}{n}\\right) \\right\\rvert\\, n \\in \\mathbb{Z}^{+}\\right\\} .\n\\]\n\n\nShow that 1 is an upper bound for \\(A\\).\nShow that if \\(d\\) is an upper bound for \\(A\\), then \\(d \\geq 1\\).\nUse (a) and (b) to show that \\(\\sup A=1\\). [Solution]:\nWe will show that for any \\(x \\in A, x \\leq 1\\). Since \\(x \\in A\\), then \\(x=(-1)^n(1-\\) \\(1 / n\\) ) for some \\(n \\in \\mathbb{Z}^{+}\\). Since \\(\\frac{1}{n}&gt;0\\), then \\(1-\\frac{1}{n}&lt;1\\). We argue our desired inequality in two cases. If \\(n\\) is even, then \\(x=(-1)^n(1-1 / n)=1-1 / n&lt;1\\). If \\(n\\) is odd, then \\(x=(-1)^n(1-1 / n)=-1+1 / n&lt;0&lt;1\\). In either case, \\(x \\leq 1\\) (in fact, \\(x&lt;1\\) ) and 1 is an upper bound for \\(A\\).\nLet \\(d\\) be an upper bound for \\(A\\). Thus, \\((-1)^n(1-1 / n) \\leq d\\) for all \\(n \\in \\mathbb{Z}^{+}\\). Assume, to the contrary that \\(d&lt;1\\). Thus, \\(1-d&gt;0\\). By the Archimedean Property, there exists an \\(n \\in \\mathbb{Z}^{+}\\)such that \\(1&lt;(1-d) n\\). Since \\(n&gt;0\\), we can rewrite this as \\(\\frac{1}{n}&lt;1-d\\), which is equivalent to \\(d&lt;1-\\frac{1}{n}\\). If \\(n\\) is even, then \\((-1)^n=1\\) and we have that \\[\nd&lt;(-1)^n\\left(1-\\frac{1}{n}\\right) \\in A\n\\] contradicting the fact that \\(d\\) is an upper bound. If \\(n\\) is odd, then consider instead \\(n+1\\), which is even. Then, \\((-1)^{n+1}=1\\) and \\[\nd&lt;1-\\frac{1}{n}&lt;(-1)^{n+1}\\left(1-\\frac{1}{n+1}\\right) \\in A\n\\]\n\nThis again contradicts that \\(d\\) is an upper bound for \\(A\\). Either way, we reach a contradiction and therefore conclude that \\(d \\geq 1\\).\n\nobviously\n\nlower bound + inequaility —smallest upper bound—sup"
  },
  {
    "objectID": "Mathematical Analysis 1.html#another-exercise-about-another-episilon-from-professor-a-kun",
    "href": "Mathematical Analysis 1.html#another-exercise-about-another-episilon-from-professor-a-kun",
    "title": "Analysis1 and Analysis 2",
    "section": "another exercise about another episilon from Professor A Kun",
    "text": "another exercise about another episilon from Professor A Kun\n\nFind the least upper bound for the following set and \\[\nA=\\left\\{\\frac{1}{2}, \\frac{2}{3}, \\frac{3}{4}, \\cdots, \\frac{n}{n+1}, \\cdots\\right\\}\n\\] [Solution]: We note that every element of \\(A\\) is less than 1 since \\[\n\\frac{n}{n+1}&lt;1, \\quad n=1,2,3, \\cdots\n\\]\n\nWe claim that the least upper bound is \\(1, \\sup A=1\\). Assume that 1 is not the least upper bound. Then there is an \\(\\epsilon&gt;0\\) such that \\(1-\\epsilon\\) is also an upper bound. However, we claim that there is a natural number \\(n\\) such that \\[\n1-\\epsilon&lt;\\frac{n}{n+1}\n\\]\nThis inequality is equivalent to the following sequence of inequalities \\[\n1-\\frac{n}{n+1}&lt;\\epsilon \\quad \\Longleftrightarrow \\quad \\frac{1}{\\epsilon}-1&lt;n\n\\]\nReversing the above sequence of inequalities shows that if \\(n&gt;\\frac{1}{\\epsilon}-1\\), then \\(1-\\epsilon&lt;\\frac{n}{n+1}\\) showing that \\(1-\\epsilon\\) is not an upper bound for \\(A\\).\n(if n &gt; 1/ \\(\\epsilon\\)-1, \\(\\nexists \\epsilon\\) s.t. 1-\\(\\epsilon\\) is an upper bound, contradicting to our suppose)\nThis verifies our answer."
  },
  {
    "objectID": "Mathematical Analysis 1.html#prove-cantor-nested-interval-property",
    "href": "Mathematical Analysis 1.html#prove-cantor-nested-interval-property",
    "title": "Analysis1 and Analysis 2",
    "section": "Prove Cantor Nested Interval Property",
    "text": "Prove Cantor Nested Interval Property\nProve:\n\n\\(\\exists c \\in\\) all closed internals\n\ni.e. if the non-empty closed intervals \\(I_1 \\supset I_2 \\supset I_3 \\cdots, \\exists c \\in R\\), s.t. \\(c\\in I_i, \\forall i \\in \\mathbb{N}\\), \\(c \\in\\) \\(\\bigcap_{i=1}^{\\infty} I_i\\)\n\nIf the limit of the lengths of these intervals are 0 then the point is unique\n\ni.e. if\\(\\left|I_n\\right| \\rightarrow 0\\), c is unique\n\n\n\\[\n\\forall \\varepsilon&gt;0, \\exists I_n(|I_n| &lt; \\varepsilon) \\Rightarrow c \\text { ! }\n\\]\nProof:\n\nproof of 1\n\n\\(I_1=[a_1, b_1]\\)\nclaim \\(\\forall I_n=[a_n, b_n], I_m=[a_m, b_m]\\) i.e. \\(a_n \\leq b_m\\)\n(if \\(a_n&gt;b_m\\), then \\(a_m \\leq b_m&lt;a_n&lt;b_n\\), which means they are separate internals without any intersection.) \\[\n\\begin{aligned}\n&\\text { Let } X=\\left\\{a_n\\right\\} (\\text { left endpoint set) } \\\\\n&\\text { Let } Y=\\left\\{b_m\\right\\}(\\text { right endpoint set) } \\\\\n& \\forall a_n \\in X, \\forall b_m \\in Y, a_n \\leq c\\leq b_m\n\\end{aligned}\n\\] \\(\\Rightarrow \\exists c \\in \\mathbb{R}\\), s.t., \\(\\forall a_n \\in X\\) \\(\\forall b_m \\in Y,a_n\\leqslant b_m\\).(completeness axiom) We then let \\(m=n \\Rightarrow c \\in I_n\\)\n\nproof of 2\n\nBased on “Any implication is equivalent to its contrapositive”\n\\[\n\\begin{aligned}\n& \\text { if } \\exists c_1&lt;c_2, \\text { s.t. } c_1, c_2 \\in I_n, \\\\\n& \\forall n \\in \\mathbb{R}, a_n \\leqslant c_1&lt;c_2 \\leqslant b_n, \\\\\n& \\Rightarrow\\left|I_n\\right|=b_n-a_n \\geqslant c_2-c_1\n\\end{aligned}\n\\]\nWe then choose \\(\\varepsilon=\\frac{c_2-c_1}{2}&gt;0\\)\nthen there exists no \\(I_n\\) s.t. \\(\\left|I_n\\right|&lt;\\varepsilon\\), since\n\\(|I_n|\\geqslant c_2-c_1 \\geqslant \\frac{c_2-c_1}{2}\\)\nSo if \\(\\exists I_n s. t .\\left|I_n\\right|&lt;\\varepsilon\\), \\(c!\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#prove-finite-covering-lemma",
    "href": "Mathematical Analysis 1.html#prove-finite-covering-lemma",
    "title": "Analysis1 and Analysis 2",
    "section": "prove finite covering lemma",
    "text": "prove finite covering lemma\nProve：Finite Covering Lemma \\[\n\\begin{gathered}\n\\text { Def: } S:=\\{X\\}, \\text { ( } x \\text { is a set) } \\\\\nY \\subset \\bigcup_{X \\in S} X\n\\end{gathered}\n\\]\nWe say. \\(S\\) is a cover of \\(Y\\). \\[\n\\text { i.e. } \\forall y \\in Y, \\exists X \\in S,(y \\in X)\n\\]\nFinite Covering Lemma:\nIf \\(I:[a, b]\\), \\[\nI \\subset \\bigcup_{n \\in I} U_n, U_n=\\left(\\alpha_n, \\beta_n\\right)\n\\] \\(\\exists U_1, \\cdots U_k\\), s.t., \\(I\\subset \\bigcup_{i=1}^k U_i\\)\n（Summray of Finite Covering Lemma: A closed internal can be covered by finite number of open internals）\nproof:\nsuppose \\(I=[a, b]\\) could not be coverd by finite number open intervals:\nThen we use the method of bisection to separate I,s.t.\n\\[\n\\begin{aligned}\n& I=I_1 \\supset I_2 \\supset I_3 \\cdots I_n \\supset \\cdots \\text { ( all In can not be covered } \\\\\n& \\left|I_n\\right|=\\frac{b-a}{2^n} \\rightarrow 0 \\\\\n& \\text { Gover) } \\\\\n& \\Rightarrow \\exists: C \\in \\bigcap_{i=1}^{\\infty} I_i \\text { (Cantor Nested Interral Property) } \\\\\n& c_i\\in[a_i,b_i]\\\\\n& \\Rightarrow C\\in I\\subset \\bigcup U_i\\text {(based on the given condition of the proof problem)}\\\\\n& \\Rightarrow \\exists U=[\\alpha,\\beta],s.t.c\\in U，let ：\\varepsilon =min[c-\\alpha,\\beta-c]\\\\\n& \\Rightarrow I_n\\subset U\n\\end{aligned}\n\\]\n\nwhich indicates that at least this \\(I_n\\) is covered by U, contradicting to the suppose.\nso the finite covering lemma is true."
  },
  {
    "objectID": "Mathematical Analysis 1.html#statement",
    "href": "Mathematical Analysis 1.html#statement",
    "title": "Analysis1 and Analysis 2",
    "section": "statement",
    "text": "statement\np\n\n—An assertion that is either true or false but not both\ne.g.\n\nNegation of a statement p is a statement which means the opposite of P\n~p—-read “not p”\n\n\nQuantifiers\nall, every, each,no(none)—universal quantifiers\nsome,there exists, there is at least on, etc.—existential quantifiers\nP: some a’s are b’s\n~P: all a’s are not b’s/ no a’s are b’s\nP: some a are not b\n~p: all a are b\n\n\nTruth table: give the truth values of related statements in all possible cases\n(relevant to boolean in computer science)\n# Example of boolean logic in a program\nis_raining = True\nhas_umbrella = False\n\nif is_raining and not has_umbrella: # which is the only situation that implication fails\n    print(\"You need an umbrella!\")\nelse:\n    print(\"You're good to go!\")\ncompound statement: combining several statements via logical operations\nConjunction: p^q. p and q\nDisjunction: p v q–p or q\none of them is true, p v q is true, so we only need to decide if oe of them is true, if it is, the p v q is true\np V (~p) is always true\na statement that is always true is called a tautology"
  },
  {
    "objectID": "Mathematical Analysis 1.html#p101.4",
    "href": "Mathematical Analysis 1.html#p101.4",
    "title": "Analysis1 and Analysis 2",
    "section": "P10,1.4",
    "text": "P10,1.4\nEquivalence of statements: Two statements are logically equivalent, if they have the same truth values in all possible situations.\n\\(A\\equiv B\\)\n‘abstract non-sense’\nThem(De Morgan’s laws)\nA,B\n\\(\\sim (A \\land B) \\equiv (\\sim A) \\lor (\\sim B)\\)\n\\(\\sim (A \\lor B) \\equiv (\\sim A) \\land (\\sim B)\\)\ndraw truth table to look at values\n交的话（and），都T才T；并的话（or），1T则T\nConditional:\nIf p(hypothesis/assumption/condition), then q(conclusion/consequence/result).\nread: p implies q/assume p, then q.\nkey point: the implication is False when the rule is broken\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\nA & B & A \\rightarrow B \\\\\n\\hline\nT & T & T \\\\\nT & F & F \\\\\nF & T & T \\\\\nF & F & T \\\\\n\\hline\n\\end{array}\n\\] Def: p, q,p–&gt;q\n1)Converse: q—&gt;p,i.e. if q, then p\n2)Contrapositive: (q)–&gt;(p), i.e. if not q, then not p\nProp: (p–&gt;q)\\(\\equiv\\) ((q)–&gt;(p))(a statement and its contrapositive are logically equivalent) \\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\np & q & \\sim p & \\sim q & p \\rightarrow q & \\sim q \\rightarrow \\sim p & (p \\rightarrow q) \\equiv (\\sim q \\rightarrow \\sim p) \\\\\n\\hline\nT & T & F & F & T & T & T \\\\\nT & F & F & T & F & F & T \\\\\nF & T & T & F & T & T & T \\\\\nF & F & T & T & T & T & T \\\\\n\\hline\n\\end{array}\n\\]\n(always remember: implication fails only when the rule brokens instead of……(others))\nProof by contradiction: we want to prove p—&gt;q, we prove (q)—-&gt;(p) instead\nproof: Assume ~q, if , then if , then … –&gt; ~p, which is contradict to the original assumption p\n   Hence the assumption ~q is false, i.e. q is true.#\n   \ne.g.: n is a natural number. Prove that if n^2 is divisible by 2(p), then n is divisible by 2(q).\nProof: Assume that n is not divisible by 2(~q),—&gt; n is odd(defination), i.e. n =2k+1,k\\(\\in Z\\)\n—&gt; n^2 =(2k+1)^2(multiplicaiton)=2()+1, which is odd\n—&gt; n^2 is not divisible by 2(~p),\nThus the aassumption that n is not divisible by 2 is false so n is divisible by 2.#\nbiconditional: p,q,\n(p–&gt;q)\\(\\land\\)(q—&gt;p)—p &lt;–&gt; q—-reads‘p if and only if q’.\nstatement converse\n\\[\n\\begin{array}{|c|c|c|c|c|}\n\\hline\np & q & p \\rightarrow q & q \\rightarrow p & p \\leftrightarrow q \\\\\n\\hline\nT & T & T & T & T \\\\\nT & F & F & T & F \\\\\nF & T & T & F & F \\\\\nF & F & T & T & T \\\\\n\\hline\n\\end{array}\n\\]\nThem.: the only case p&lt;–&gt;q is true is then both p and q ture or false"
  },
  {
    "objectID": "Mathematical Analysis 1.html#proposition",
    "href": "Mathematical Analysis 1.html#proposition",
    "title": "Analysis1 and Analysis 2",
    "section": "proposition",
    "text": "proposition\n$x,yQ $, x is less than y, \\(\\exists z\\in Q\\) s.t. x&lt;z&lt;y.\nproof: \\(x,y\\in Q, x&lt;y\\),x=m/n,x=p/q,z=x+y/2\\(\\in\\)(x+x/2,y+y/2),i.e.x&lt;z&lt;y,z=pn+mq/2qn\\(\\in \\mathbb Q\\) by defination."
  },
  {
    "objectID": "Mathematical Analysis 1.html#russel-paradox",
    "href": "Mathematical Analysis 1.html#russel-paradox",
    "title": "Analysis1 and Analysis 2",
    "section": "Russel paradox",
    "text": "Russel paradox\nlet R be the set of all sets that are not a member of themself. i.e.R={S|S\\(\\notin\\)S}\nso if \\(R\\in R\\),R\\(\\notin\\) R\nif \\(R\\notin R\\), R\\(\\in R\\)\nif \\(x\\in A\\),then \\(x\\in B\\) is the subset defination of A \\(\\subseteq\\) B.\nif \\(A\\subset B\\), A is a proper subset of B. i.e. \\(A\\subset B\\) if \\(A\\subseteq B\\) and \\(\\exists x\\in B\\), s.t. \\(x\\notin\\) A\n\\(N\\subset Z \\subset Q \\subset R\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#proposition-1",
    "href": "Mathematical Analysis 1.html#proposition-1",
    "title": "Analysis1 and Analysis 2",
    "section": "Proposition",
    "text": "Proposition\nthe empty set is a subset of any set\nproof:\nmethod1: (truth table)\nmethod2:"
  },
  {
    "objectID": "Mathematical Analysis 1.html#def-1",
    "href": "Mathematical Analysis 1.html#def-1",
    "title": "Analysis1 and Analysis 2",
    "section": "Def",
    "text": "Def\nthe complement of A is denoted by \\(A^c\\)\nthe intersection \\(A \\cap B\\)\nthe union \\(A \\cup B\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#countability-and-bijections",
    "href": "Mathematical Analysis 1.html#countability-and-bijections",
    "title": "Analysis1 and Analysis 2",
    "section": "Countability and Bijections",
    "text": "Countability and Bijections\nCountable Set: A set is said to be countable if it is either finite or has the same size (cardinality) as the set of natural numbers \\(\\mathbb{N}\\). A set is countably infinite if there exists a bijection (a one-to-one correspondence) between that set and \\(\\mathbb{N}\\).\nUncountable Set: A set is uncountable if no such bijection exists, meaning its cardinality is strictly greater than that of \\(\\mathbb{N}\\).\nBijection: A bijection between two sets \\(A\\) and \\(B\\) is a function \\(f: A \\to B\\) that is both injective (one-to-one) and surjective (onto).\n\nThe Problem: Showing that \\(\\mathcal{P}(\\mathbb{N})\\) is Uncountable\n\nmethod 1: Consider the set \\(A = \\{ n \\in \\mathbb{N} \\mid n \\notin f(n) \\}\\), f : N → P(N).\nwe should prove it is not a bijection. so we could prove it is not surjective (P(N) is much bigger than N).\nnow we ask if f is surjective:\nIf yes, there exists \\(n_0\\in N\\) s.t. f\\((n_0)=\\)A, A\\(\\subset\\)N while A \\(\\in\\) P(N)\nnow we ask: Does \\(n_o\\in f(n_0)\\)\n\nif yes, \\(n_0\\notin A=f(n_0)\\)\nif no, \\(n_0\\in A=f(n_0)\\)\n\n(or we discuss the set, we suppose x is in A and based on the condition of being in set A we get A is empty, which is contradicts to “x is in A”)\nso f is not surjective, which means f could not be bijective. so p(N) is uncountable.\n\neg of a bijection between 2 sets(namely 2 sets with the same size)\n\nN-&gt;Z\nn|-&gt;n/2 if n even\nn|-&gt; n+1/2 if n odd\n\n\n\n\nan eg question-Logical Puzzle on Truth of Statements relavent to truth table\nConsider the following 99 statements:\n\n\\(S_1\\): “Among these 99 statements, there is at most one true statement.”\n\\(S_2\\): “Among these 99 statements, there are at most two true statements.”\n…\n\\(S_{99}\\): “Among these 99 statements, there are at most 99 true statements.”\n\nThe goal is to determine which statements among these 99 are true.\n(hint:Consider the chain of implications from statement \\(S_n\\) to \\(S_{n+1}\\). Which statement implies which? )\nsol: we fail the hypothesis of \\(S_{n+1}\\Rightarrow S_n\\)，but because\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\nS_n & S_{n+1} & S_n \\Rightarrow S_{n+1} \\\\\n\\hline\nT & T & T \\\\\nT & F & F \\\\\nF & T & T \\\\\nF & F & T \\\\\n\\hline\n\\end{array}\n\\]\nThus, we conclude that the truth of \\(S_n\\) implies the truth of all subsequent statements \\(S_{n+1}\\).\n)\n\\[\nS_n \\Rightarrow S_{n+1}\n\\] so there exists an \\(S_n\\), after which are true while before which are false.\nso:There are \\(100 - n\\) true statements.\n\\[\n100 - n \\leq n \\implies n \\geq 50\n\\]\nThere are \\(n - 1\\) false statements.\n\\[\nn - 1+1\\leq 100 - n \\implies n \\leq 50\n\\] so The 50th statement \\(S_{50}\\) is true. Statements \\(S_1\\) to \\(S_{49}\\) are false, and statements \\(S_{50}\\) to \\(S_{99}\\) are true."
  },
  {
    "objectID": "Mathematical Analysis 1.html#cardinality",
    "href": "Mathematical Analysis 1.html#cardinality",
    "title": "Analysis1 and Analysis 2",
    "section": "Cardinality",
    "text": "Cardinality\n\nnumebr of elements in a finite set: Def: let A be a set, if A contains finitely many elements, then the number of elements of A is called the cardinality of A, denoted by |A|(|A|\\(\\in Z_{\\geq0}\\)\nA, B have infinitely many elements. If \\(\\exists f:A-&gt;B\\) is a bijective map, then we regard the A, B have the same “cardinality”, i.e.|A|=|B|"
  },
  {
    "objectID": "Mathematical Analysis 1.html#a-x-bcartesian-product",
    "href": "Mathematical Analysis 1.html#a-x-bcartesian-product",
    "title": "Analysis1 and Analysis 2",
    "section": "A x B–Cartesian product",
    "text": "A x B–Cartesian product\n\neg\n|A|=3, |B|=2, |A x B|=6"
  },
  {
    "objectID": "Mathematical Analysis 1.html#map",
    "href": "Mathematical Analysis 1.html#map",
    "title": "Analysis1 and Analysis 2",
    "section": "map",
    "text": "map\nQuestion from the Cardinality:\nWhat happens if A has infinitely many elements?\nDef: LEt A and B be 2 sets, A map(function)f: A–&gt;B assigns each element in A\nthere has 3 kinds of maps(see 107)\n\neg\nthe graph of f(x) in A x B is f: A–&gt; B(x \\(\\in A\\),y\\(\\in B\\))"
  },
  {
    "objectID": "Mathematical Analysis 1.html#inverse-defination",
    "href": "Mathematical Analysis 1.html#inverse-defination",
    "title": "Analysis1 and Analysis 2",
    "section": "inverse defination",
    "text": "inverse defination\nif f is a bijective map, so as \\(f^{-1}\\)."
  },
  {
    "objectID": "Mathematical Analysis 1.html#countable",
    "href": "Mathematical Analysis 1.html#countable",
    "title": "Analysis1 and Analysis 2",
    "section": "countable",
    "text": "countable\n\nA set A is called countable if A iseither a finite set of there exists a bijective map f: A to N.(Otherwise is uncountable)\n\n\neg\n\n\\(B=[x|x=2n,n\\in N\\) is countable\n\nproof: for x|–&gt;x/2 of B–&gt;N i) injective , $x_1 $ not = \\(x_2\\), f(x_1)=x/2, = f(x_2)\n   ii) surjective, $\\forall y\\in N $, $\\exists x\\in B$ s.t. ,f(2y)=x, $2y\\in B$\n   \n   so bijective\n   \n\nZ is countable (负无穷到正无穷范围内的整数)\n\nf:N–&gt;Z(n|–&gt;n/2 if even, 1-n/2 if odd)\nn|–&gt;n/2 if even makes the positive integer possible\n1-n/2 if odd makes negative integer possible\nand both of them are bijections\nso Z is countable"
  },
  {
    "objectID": "Mathematical Analysis 1.html#triangle-inequality-is-a-strong-tool",
    "href": "Mathematical Analysis 1.html#triangle-inequality-is-a-strong-tool",
    "title": "Analysis1 and Analysis 2",
    "section": "triangle inequality is a strong tool",
    "text": "triangle inequality is a strong tool\n\\[\n\\begin{aligned}\n& \\lim _{x \\rightarrow \\infty}\\left|\\frac{\\frac{11}{x}-\\frac{9}{x^2}}{25+\\frac{35}{x}+\\frac{6}{x^2}}\\right| \\\\\n& |25+\\frac{35}{x}+\\frac{10}{x^2}\\left|=\\left|25-\\left(\\frac{35}{-x}-\\frac{10}{x^2}\\right)\\right|\\right. \\\\\n& \\left.\\geqslant|25|-| \\frac{35}{x}+\\frac{10}{x^2} \\right\\rvert\\\\ \\\\\n& \\geqslant 25-\\left|\\frac{35}{x}\\right|-\\left|\\frac{10}{x^2}\\right|\\\\\n& \\forall x&gt;10 \\\\\n\\end{aligned}\n\\] \\[\n25-| \\frac{35}{x}\\left|-\\left|\\frac{10}{x^2}\\right|&gt;25-\\left(\\frac{35}{10}+\\frac{10}{10^2}\\right)=M_1\\right.\n\\] Take \\(M_2=\\frac{22}{M_1 \\varepsilon}, \\forall|x|&gt;\\frac{22}{M_1 \\varepsilon},\\left|\\frac{11}{x}\\right|&lt;\\frac{M_1 \\varepsilon}{2}\\) Take \\(M_3=\\sqrt{\\frac{18}{m_1 \\varepsilon}}, \\forall|x|&gt;\\sqrt{\\frac{18}{m_1 \\varepsilon}},\\left|\\frac{9}{x^2}\\right|&lt;\\frac{m_1 \\varepsilon}{2}\\) \\[\n\\left|\\frac{11}{x}-\\frac{9}{x^2}\\right|&lt;\\left|\\frac{11}{x}\\right|+\\left|\\frac{9}{x^2}\\right|\n\\]\nSo Take \\(M=m \\cdot a x\\left\\{10, \\frac{22}{M_1 a}, \\sqrt{\\frac{18}{m_1 \\varepsilon}}\\right\\}\\), we hame \\(\\forall \\varepsilon&gt;0, \\forall|x|&gt;m\\), such that…&lt;\\(\\epsilon\\)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#understanding-deeplyexplain-by-myself",
    "href": "Mathematical Analysis 1.html#understanding-deeplyexplain-by-myself",
    "title": "Analysis1 and Analysis 2",
    "section": "understanding deeply(explain by myself…)",
    "text": "understanding deeply(explain by myself…)"
  },
  {
    "objectID": "Mathematical Analysis 1.html#proof-of-some-theroem",
    "href": "Mathematical Analysis 1.html#proof-of-some-theroem",
    "title": "Analysis1 and Analysis 2",
    "section": "proof of some theroem",
    "text": "proof of some theroem\n\nboring things(just practivce epsilon-delta language…)\n\n\\(\\lim _{x \\rightarrow a} f(x)=\\ell\\) if and only if \\(\\lim _{x \\rightarrow a^{+}} f(x)=\\lim _{x \\rightarrow a^{-}} f(x)=\\ell\\).\n\nNecessary Condition Let \\(f(x) \\rightarrow l\\) as \\(x \\rightarrow c\\).\nThen from the definition of the limit of a function: \\[\n\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: 0&lt;|x-c|&lt;\\delta \\Longrightarrow|f(x)-l|&lt;\\epsilon\n\\]\nSo for any given \\(\\epsilon\\), there exists a \\(\\delta\\) such that: \\[\n0&lt;|x-c|&lt;\\delta\n\\] implies that: \\[\nl-\\epsilon&lt;f(x)&lt;l+\\epsilon\n\\]\nNow: \\[\n\\begin{array}{ll}\n& 0&lt;|x-c|&lt;\\delta \\\\\n\\leadsto \\quad & -\\delta&lt;-(x-c)&lt;0 \\\\\n& \\vee \\quad 0&lt;(x-c)&lt;\\delta \\\\\n\\leadsto \\quad & c-\\delta&lt;x&lt;c \\\\\n& \\vee \\quad c&lt;x&lt;c+\\delta\n\\end{array}\n\\]\nThat is: \\(\\forall \\epsilon&gt;0: \\exists \\delta&gt;0\\) :\n(1): \\(\\quad c-\\delta&lt;x&lt;c \\Longrightarrow\\|f(x)-l\\|&lt;\\epsilon\\)\n\n: \\(\\quad c&lt;x&lt;c+\\delta \\Longrightarrow\\|f(x)-l\\|&lt;\\epsilon\\)\n\nSo given that particular value of \\(\\epsilon\\), we can find a value of \\(\\delta\\) such that the conditions for both:\n(1): \\(\\quad f\\) tending to the limit \\(l\\) as \\(x\\) tends to \\(c\\) from the left\nand :\n\n: \\(\\quad f\\) tending to the limit \\(l\\) as \\(x\\) tends to \\(c\\) from the right.\n\nThus: \\[\n\\lim _{x \\rightarrow c} f(x)=l\n\\] implies that: \\[\n\\lim _{x \\rightarrow c^{-}} f(x)=l\n\\] and: \\[\n\\lim _{x \\rightarrow c^{+}} f(x)=l\n\\]\nSufficient Condition\nLet \\(f(x) \\rightarrow l\\) as \\(x \\rightarrow c^{-}\\)and \\(f(x) \\rightarrow l\\) as \\(x \\rightarrow c^{+}\\).\nThis means that:\n(1): \\(\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: c-\\delta&lt;x&lt;c \\Longrightarrow|f(x)-l|&lt;\\epsilon\\)\nand :\n(2): \\(\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: c&lt;x&lt;c+\\delta \\Longrightarrow|f(x)-l|&lt;\\epsilon\\)\nIn the same manner as above, the conditions on \\(\\delta\\) give us that: \\[\n\\begin{array}{ll}\n& c-\\delta&lt;x&lt;c \\\\\n\\wedge \\quad & c&lt;x&lt;c+\\delta \\\\\n& 0&lt;|x-c|&lt;\\delta\n\\end{array}\n\\]\nSo: \\[\n\\forall \\epsilon&gt;0: \\exists \\delta&gt;0: 0&lt;|x-c|&lt;\\delta \\Longrightarrow|f(x)-l|&lt;\\epsilon\n\\]\nThus: \\[\n\\lim _{x \\rightarrow c^{-}} f(x)=l\n\\] and: \\[\n\\lim _{x \\rightarrow c^{+}} f(x)=l\n\\] together imply that: \\[\n\\lim _{x \\rightarrow c} f(x)=l\n\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html#decimal-scale",
    "href": "Mathematical Analysis 1.html#decimal-scale",
    "title": "Analysis1 and Analysis 2",
    "section": "decimal scale",
    "text": "decimal scale\n\\(x=0.a_1a_2a_3...\\)\n[0,1]={\\(\\sum_{n=1}^\\infty a_n/10^n,n\\in{0,1,...9}\\)}\n# [0,1]=\\(10^{\\infty}\\)—(uncountable)\n|{\\(x_1\\)}|\\(\\subseteq (x_1-\\epsilon,x_1+\\epsilon), \\forall \\epsilon &gt;0\\) i.e.|{\\(x_1\\)}|=0"
  },
  {
    "objectID": "Mathematical Analysis 1.html#defination",
    "href": "Mathematical Analysis 1.html#defination",
    "title": "Analysis1 and Analysis 2",
    "section": "Defination",
    "text": "Defination"
  },
  {
    "objectID": "Mathematical Analysis 1.html#连续性用sup构造数列",
    "href": "Mathematical Analysis 1.html#连续性用sup构造数列",
    "title": "Analysis1 and Analysis 2",
    "section": "连续性+用sup构造数列",
    "text": "连续性+用sup构造数列"
  },
  {
    "objectID": "Mathematical Analysis 1.html#用sequential-criterion证明连续性取合适的epsilon来说明数列存在",
    "href": "Mathematical Analysis 1.html#用sequential-criterion证明连续性取合适的epsilon来说明数列存在",
    "title": "Analysis1 and Analysis 2",
    "section": "用sequential criterion证明连续性取合适的epsilon来说明数列存在",
    "text": "用sequential criterion证明连续性取合适的epsilon来说明数列存在\neg.\nA={1/2n} where n \\(\\in N\\)\nf(x)=0 when x \\(\\in[0,1]\\) and x is not in A\nf(x)=2x when x \\(\\in A\\)\n\nProve f is continuous at x when x \\(\\in[0,1]\\) and x is not in A\n\n\nnotice!!! Do not choose sequence convergent to a point that we want blindly, since there may exists no such sequence like that! For example, here, by the density of rational/irrational number, there exists no sequence composed by elements in A convergent to a point at [0,1]so we should think of another way to prove this—If b\\(\\in [0,1]\\A\\), then there is an n_0 such that$ 1/2(n_0+1)&lt;b&lt;1/2n_0,$ i.e. b\\(\\in (1/2(n_0+1),1/2n_0).\\) For any sequence $a_n [0,1] $with $ lim_n a_n=b,$ there is an N&gt;0 such that \\(a_n\\in (1/2(n_0+1),1/2n_0).\\) ( It works as the following: We take an \\(\\epsilon=1/2 min{|1/2(n_0+1)-b|, |1/2n_0-b|}, then there exists an N.0 such that |a_n-b|&lt;\\epsilon.\\) ===&gt; when n&gt;N, a_n\\(\\in(b- \\epsilon, b+ \\epsilon)\\subset (1/2(n_0+1),1/2n_0) )\\) Notice that (1/2(n_0+1),1/2n_0) does not intersect with A. Therefore f(a_n)=0. lim_nf(a_n)=0=f(b). Hence f is continuous at b.\nmethod 1 sequential\nmethod 2\n\n\nProve f is not continuous at each point on A\n\n\nmethod 1 limit or \\(\\epsilon-\\delta\\)\nmethod 2 sequential creterion"
  },
  {
    "objectID": "ProfZhu.html",
    "href": "ProfZhu.html",
    "title": "EM",
    "section": "",
    "text": "other unknown parameters\n\n\ncencer\n\n\n\npf\\(_1\\)(X)+(1-p)f\\(_2\\)(X)"
  },
  {
    "objectID": "JavaCPT105.html",
    "href": "JavaCPT105.html",
    "title": "JavaCPT105",
    "section": "",
    "text": "The first section here is the knowledge that attracts most of my interests on this module, followed by the lecture notes I tapped when I am on the journey of this module."
  },
  {
    "objectID": "JavaCPT105.html#motivation",
    "href": "JavaCPT105.html#motivation",
    "title": "JavaCPT105",
    "section": "Motivation",
    "text": "Motivation\n\nGod is in the details. –Today’s agile world needs the role of architecture with patient to internalize the idea of “small things matter”, especially in software development.\nCraftmanship is important in these two aspects:\n\nKnowledge of principles, patterns\nWork hard and practic/sweat over it and watch myself fail.\n\n\nSo, please grind the knowledge with forever practice!\n\nCode represents the details of the requirements.\ndefend the code with equal passion\nA programmer with “code-sense” will look at messy module and see options and variations, helping choose the best variation and guide to plot a sequence of behavior preserving transformations to get from here to there.\nA programmer who writes clean code is an artist who can take a blank screen through a series of transformations until it is an elegantly coded system"
  },
  {
    "objectID": "Mathematical Analysis 1.html#eg-question-from-dr.-chi-kwong-fok-about-weirestrass-extream-value-thm-continuous-function-in-a-compact-set-attains-its-max-and-min",
    "href": "Mathematical Analysis 1.html#eg-question-from-dr.-chi-kwong-fok-about-weirestrass-extream-value-thm-continuous-function-in-a-compact-set-attains-its-max-and-min",
    "title": "Analysis1 and Analysis 2",
    "section": "eg question from Dr. Chi-Kwong Fok about Weirestrass-Extream Value THM (continuous function in a compact set attains its max and min)",
    "text": "eg question from Dr. Chi-Kwong Fok about Weirestrass-Extream Value THM (continuous function in a compact set attains its max and min)\nIf f is a continuous and non-surjective function and the supremum of it in [0,\\(+\\infty\\)] is not in its range. show that $ &gt;0, x &gt; 0, z R z &gt; x, $\nFirstly, negate it: \\(\\exists \\epsilon_0 &gt;0,\\) s.t. \\(\\exists x_0 &gt;0, \\forall z \\in R\\) with z&gt;\\(x_0\\), f(z) $ M-_0, $ where M is sup R(f) —(1)\nSecondly, write down the def of M is sup:\n\\(\\forall \\epsilon &gt;0, \\exists x\\in [0,+ \\infty],\\) s.t. f(x)&gt;\\(M-\\epsilon\\). Now take the global $$ to be \\(\\epsilon_0\\) —(2)\nBy (1) and (2) we know that \\(\\forall \\epsilon &gt;0, \\exists x\\in [0,x_0],\\) s.t. f(x)&gt;\\(M-\\epsilon\\) Now take the global $$ to be \\(\\epsilon_0\\)\nBy Weirestrass-Extream Value THM (continuous function in a compact set attains its max and min) we know it attains its maximum in [0,x_0]. Suppose this maximum is not the sup, then M=supR(f) &lt; its maximum, then by the IVP it must attain sup R(f) contradiction. =–also. —contradiction"
  },
  {
    "objectID": "APH103.html#water",
    "href": "APH103.html#water",
    "title": "APH103 Survey Sampling",
    "section": "water",
    "text": "water"
  },
  {
    "objectID": "APH103.html#formalization-of-sampling-theory",
    "href": "APH103.html#formalization-of-sampling-theory",
    "title": "APH103 Survey Sampling",
    "section": "Formalization of Sampling Theory",
    "text": "Formalization of Sampling Theory"
  },
  {
    "objectID": "APH103.html#limit-distribution",
    "href": "APH103.html#limit-distribution",
    "title": "APH103 Survey Sampling",
    "section": "",
    "text": "Wold-weorwilz Thm\n\\(\\{a_{N_1}, \\ldots, a_{N_n}\\}\\), \\(\\{X_{n_1}, \\ldots, X_{n_N}\\}\\),\n\\[\n\\frac{\\frac{1}{N} \\sum (a_{N_i} - \\overline{a_N})^r}{\\left[ \\frac{1}{N} \\sum (a_{N_i} - \\overline{a_N})^2 \\right]^{r/2}} = O(1), \\quad \\overline{a_N} = \\frac{1}{N} \\sum a_{N_i}.\n\\]\n\\[\n\\frac{\\frac{1}{N} \\sum (X_{N_i} - \\overline{X_N})^r}{\\left[ \\frac{1}{N} \\sum (X_{N_i} - \\overline{X_N})^2 \\right]^{r/2}} = O(1) \\quad X_N = \\frac{1}{N} \\sum X_{N_i}.\n\\] \\(X_1,X_2,...X_N\\) uniformly distributio from \\(X_{N_1},...\\)"
  },
  {
    "objectID": "APH103.html#conclusion",
    "href": "APH103.html#conclusion",
    "title": "APH103 Survey Sampling",
    "section": "",
    "text": "Let \\(X_1, X_2, \\ldots, X_n\\) i.i.d. \\(F(\\mu, \\sigma^2)\\). Define \\(Y_n = \\sqrt{n} \\frac{\\overline{X} - \\mu}{\\sigma} \\xrightarrow{d} N(0,1)\\).\nLet \\(L_N = \\sum a_{N_i} X_i\\), \\(E[L_N] = N \\overline{a_N} \\overline{X}\\).\n\\[\n\\text{Var}(L_N) = \\frac{1}{N-1} \\left[ \\sum (a_{N_i} - \\overline{a_N})^2 \\right] \\left[ \\sum (X_{N_i} - \\overline{X})^2 \\right]\n\\]\nAs \\(N \\to \\infty\\),\n\\[\nP\\left\\{ \\frac{L_N - E(L_N)}{\\sqrt{\\text{Var}(L_N)}} \\leq z \\right\\} \\to \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z e^{-\\frac{t^2}{2}} dt.\n\\]"
  },
  {
    "objectID": "Mathematical Analysis 1.html#all-ways-to-distinguish-whether-it-is-uniformly-continuous",
    "href": "Mathematical Analysis 1.html#all-ways-to-distinguish-whether-it-is-uniformly-continuous",
    "title": "Analysis1 and Analysis 2",
    "section": "All ways to distinguish whether it is uniformly continuous",
    "text": "All ways to distinguish whether it is uniformly continuous\nplease fill this before your final exam!\n\npractice before my mid-term exam (my way to think that of question is like this)\n\\(f(x) = x^4\\) is not uniformly continuous on \\(\\mathbb{R}\\).\nChoose \\(x_n = \\sqrt[4]{n+1}\\), \\(y_n = \\sqrt[4]{n}\\).\nSince \\(x^{1/4}\\) is differentiable on \\([n, n+1]\\), by Lagrange’s MVT,\n\\[|x_n - y_n| = \\frac{1}{4} \\xi^{-\\frac{3}{4}} (n+1-n), \\text{ where } \\xi \\in (n, n+1)\\]\nSo \\(|x_n - y_n| = \\frac{1}{4} \\xi^{-\\frac{3}{4}} &lt; \\frac{1}{4} n^{-\\frac{3}{4}} = \\frac{1}{4 \\sqrt[4]{n^3}}\\)\nSo \\(0 &lt; |x_n - y_n| &lt; \\frac{1}{4 \\sqrt[4]{n^3}}\\)\nSo \\(\\lim_{n \\to \\infty} |x_n - y_n| = 0\\) by squeeze theorem\nSo for \\(\\varepsilon_0 = \\frac{1}{2}\\), by the definition of limit，\\(\\forall \\delta &gt; 0\\), \\(\\exists m &gt; 0\\) s.t. for \\(n_0\\in \\mathbb{N} &gt; m\\), \\(|x_{n_0} - y_{n_0}| &lt; \\delta\\) but \\(|f(x_{n_0}) - f(y_{n_0})| = 1 &gt; \\varepsilon_0 = \\frac{1}{2}\\),\nThis satisfies the negation of the definition of “uniformly continuous”."
  },
  {
    "objectID": "mth106.html#to-study-odes-and-pdes-the-thinking-way-of-linear-algebra-should-be-used",
    "href": "mth106.html#to-study-odes-and-pdes-the-thinking-way-of-linear-algebra-should-be-used",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "To study ODEs and PDEs, the thinking way of Linear Algebra should be used!",
    "text": "To study ODEs and PDEs, the thinking way of Linear Algebra should be used!\n\\(x^{(2)}\\)= x’’ = \\(\\frac {d^2 x}{dy^2}\\)–this notation of the second order’s is actually share the same idea of “division”"
  },
  {
    "objectID": "mth106.html#liniarity",
    "href": "mth106.html#liniarity",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Liniarity",
    "text": "Liniarity\n$a_n(x) + … +a_1(x) +a_0(x)y =g(x) $\n\neg\n\n(y-x)dx+4xdy=0\n\n\\(\\frac {d^2 y}{dx^2}+\\sin y=0\\) is not linear since it has nonlinear function of y"
  },
  {
    "objectID": "mth106.html#solution-of-an-ode",
    "href": "mth106.html#solution-of-an-ode",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "solution of an ODE",
    "text": "solution of an ODE\n\n? why continuous? —\nAn funtion defined on an interval I and possessing at least n derivatives that are continuous on I, which when substituted into an nth-order ordinary differential equation reduces the equation to an identity, is said to be a solution of the equation on the interval.\nWe cannot think solution of an ordinary differential equation without simultaneously thinking interval.\n\n\ntrivial solution –y=0, I\n\n\nSolution Curve SEE CODEs\n\n\nExplicit and implicit solution shape of ODEs\n\n\nSingular solution\n\n\nA solution of a system\nSystem:\n\\[\\cases{^{{\\frac{dx}{dt}}=f(t,x,y)} _{\\frac{dy}{dt}=g(t,x,y)}}\\] Solution: x =\\(\\Phi_1(t)\\), y=\\(\\Phi_2(t)\\) defined on a common interval I that satisfy each equation of the system on this interval."
  },
  {
    "objectID": "mth106.html#existence-and-uniqueness",
    "href": "mth106.html#existence-and-uniqueness",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Existence and Uniqueness",
    "text": "Existence and Uniqueness\nDoes a solution of the problem exist? If a solution exists, is it unique?"
  },
  {
    "objectID": "mth106.html#eg-of-variation-of-parameters-introduced-by-professor-chi-kun-lin",
    "href": "mth106.html#eg-of-variation-of-parameters-introduced-by-professor-chi-kun-lin",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Eg of Variation of Parameters introduced by Professor Chi-Kun Lin:",
    "text": "Eg of Variation of Parameters introduced by Professor Chi-Kun Lin:\nThe method of variation of parameters is a clever and elegant approach in solving non-homogeneous differential equations, especially in the context of physics. When we consider a differential equation, the non-homogeneous term F(x) can be seen as an external force acting on the system. In the absence of this force, the solution space of the homogeneous equation is spanned by a set of linearly independent solutions, such as \\(y_1\\) and \\(y_2\\). These solutions form the basis of the homogeneous solution through linear combinations.\nHowever, when the external force \\(F(x)\\) is introduced, it perturbs the system and necessitates the inclusion of a particular solution that specifically responds to this force. This particular solution still involves the original basis functions \\(y_1\\) and \\(y_2\\), but it introduces new coefficients \\(u_1(x)\\) and \\(u_2(x)\\) that are functions of the independent variable \\(x\\). These coefficients are not constants; instead, they vary in a way that reflects the influence of the external force. The idea is that these coefficients \\(u_1(x)\\) and \\(u_2(x)\\) can be adjusted to account for the systematic changes induced by the physical quantity represented by \\(F(x)\\).\nThe brilliance of the variation of parameters method lies in its ability to construct a particular solution by allowing these coefficients to vary. This method hypothesizes that by introducing these variable coefficients, the particular solution can effectively capture the response of the system to the external force. And indeed, this construction turns out to be effective. It provides a systematic way to find a particular solution that complements the homogeneous solution, thereby giving us the complete solution to the non-homogeneous differential equation. The name “variation of parameters” aptly describes the essence of this method, emphasizing the dynamic adjustment of the parameters (coefficients) in response to the external force."
  },
  {
    "objectID": "mth106.html#eg-of-the-behind-story-of-euler-lagrange-equation",
    "href": "mth106.html#eg-of-the-behind-story-of-euler-lagrange-equation",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Eg of the behind story of “Euler-Lagrange Equation”",
    "text": "Eg of the behind story of “Euler-Lagrange Equation”\n\\(\\frac{d}{dt}\\frac{\\partial L}{\\partial q^{`}\\alpha}-\\frac{\\partial L}{\\partial q \\alpha}\\)\nWhile it’s often said that Lagrange was Euler’s student and that Euler had already discovered some of Lagrange’s work but chose not to publish to allow Lagrange to gain recognition, the reality of their relationship was a bit more nuanced.\nEuler was indeed supportive of Lagrange’s work and recognized his talent. At the age of 19, Lagrange had made significant contributions to mathematics that caught Euler’s attention. Lagrange sent Euler his work on the calculus of variations, and Euler was very supportive. He recognized Lagrange’s talents and helped introduce him to the wider mathematical community.\nEuler held back his own work on the calculus of variations to let Lagrange publish first. When Euler left Berlin for St. Petersburg in 1766, he recommended that Lagrange succeed him as the director of the Berlin Academy. This act was seen as a testament to Euler’s generous spirit and his recognition of Lagrange’s abilities."
  },
  {
    "objectID": "mth106.html#order",
    "href": "mth106.html#order",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Order",
    "text": "Order"
  },
  {
    "objectID": "mth106.html#interval-of-existence-and-uniqueness",
    "href": "mth106.html#interval-of-existence-and-uniqueness",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Interval of Existence and Uniqueness",
    "text": "Interval of Existence and Uniqueness\nSuppose y(x) represents a solution of the first order initial- value problem. The following three sets on the real x-axis may not be the same:\nthe domain of the function y(x), D\nthe interval I over which the solution y(x) is defined or exists,\nand the interval \\(I_0\\) of existence and uniqueness.\n\\(I_0 \\subseteq I \\subseteq D\\)"
  },
  {
    "objectID": "mth106.html#newtons-law",
    "href": "mth106.html#newtons-law",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Newton’s law",
    "text": "Newton’s law"
  },
  {
    "objectID": "mth106.html#properties-of-the-solutions-of-autonomous-de",
    "href": "mth106.html#properties-of-the-solutions-of-autonomous-de",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Properties of the solutions of autonomous DE",
    "text": "Properties of the solutions of autonomous DE\n\nA solution curve"
  },
  {
    "objectID": "mth106.html#translation-propertyonly-autonomous-des-has-this-property",
    "href": "mth106.html#translation-propertyonly-autonomous-des-has-this-property",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "? Translation Property–only autonomous DEs has this property?",
    "text": "? Translation Property–only autonomous DEs has this property?\n\nmeaning: translation?\n\n\nIf y(x) is a solution of an autonomous differential equation dy ∕dx = f(y), then y1(x) = y(x − k) (y of (x-k)), k a constant, is also a solution."
  },
  {
    "objectID": "mth106.html#singular-solution-1",
    "href": "mth106.html#singular-solution-1",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "singular solution",
    "text": "singular solution\n\\(\\frac {dy}{h(y)}=g(x)dx\\). If r is a zero of the function h(y), substituting y=r into dy/dx=g(x)h(y) makes both sides zero (y = r is a constant solution of the differential equation). As a consequence, y = r might not show up in the family of solutions that are obtained after integration and simplification. Such a solution is called a singular solution."
  },
  {
    "objectID": "mth106.html#initial-value-problem",
    "href": "mth106.html#initial-value-problem",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "initial value problem",
    "text": "initial value problem\n\nintergral-defined function\n\n\neg"
  },
  {
    "objectID": "mth106.html#integrating-factor-method",
    "href": "mth106.html#integrating-factor-method",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Integrating factor method",
    "text": "Integrating factor method\n\nsolution\nidea: want to written into \\(\\frac{d}{dx}()=f(x)\\) then we assume after multiplying a factor \\(\\mu(x)\\) we can write \\(\\frac{d}{dx}(\\mu(x)y(x))=\\mu(x)f(x)\\)\nwe get \\(\\mu(x) = ce^{\\int{p(x)dx}{}}\\).\nlet c=1 we get the integrating factor\n\n\nIVP"
  },
  {
    "objectID": "mth106.html#error-function",
    "href": "mth106.html#error-function",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "?Error function",
    "text": "?Error function"
  },
  {
    "objectID": "mth106.html#exact-equations",
    "href": "mth106.html#exact-equations",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Exact equations",
    "text": "Exact equations\nz=f(x,y)=c"
  },
  {
    "objectID": "mth106.html#non-exact",
    "href": "mth106.html#non-exact",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Non-exact",
    "text": "Non-exact"
  },
  {
    "objectID": "mth106.html#thm",
    "href": "mth106.html#thm",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "THM",
    "text": "THM\n\npiecewise continuous function\n\nA function f(x) is piecewise continuous on an interval [a, b] if it is continuous on each subinterval (open interval) of [a, b] and has a finite number of (or jump) discontinuities in the interval. The function can be defined in pieces, and at each piece, it must be continuous."
  },
  {
    "objectID": "mth106.html#thmcondition-for-convergence",
    "href": "mth106.html#thmcondition-for-convergence",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "thm–condition for convergence",
    "text": "thm–condition for convergence\nf, and f’ are piecewise continuous on [-p,p], then for every x $$ [-p,p], the Fourier series of f(x) converges to f(x) at every point x in [-p,p] where f is continuous. If f has a jump discontinuity at x, the Fourier series converges to the average of the left-hand and right-hand limits of f at x."
  },
  {
    "objectID": "mth106.html#homogeneous-linear-system",
    "href": "mth106.html#homogeneous-linear-system",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Homogeneous Linear System",
    "text": "Homogeneous Linear System\nJust follow the idea in \\(y''+my'+ny=0\\), when facing the linear system \\(\\cases{{\\vec {x'} = A\\vec x}\\\\{ay'+by=0}}\\), we also assume that \\(\\vec {x'} = \\vec ke^{\\lambda t}\\) and substitute it into \\(\\vec {x'} = Ax\\)"
  },
  {
    "objectID": "mth106.html#phase-portrait-and-stable-unstable-semi-stable-points",
    "href": "mth106.html#phase-portrait-and-stable-unstable-semi-stable-points",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "phase portrait and stable/ unstable / semi-stable points",
    "text": "phase portrait and stable/ unstable / semi-stable points"
  },
  {
    "objectID": "mth106.html#jordan-form-and-ode-system",
    "href": "mth106.html#jordan-form-and-ode-system",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "? JORDAN FORM AND ODE system?",
    "text": "? JORDAN FORM AND ODE system?\nWhen we solve \\(ay''+by'+cy=0\\) if m is a double root, then \\(y_1=e^{mx}\\), \\(y_2=xe^{mx}\\)\nNaturally, here we assume \\(\\vec x_2 =\\vec k t e^{\\lambda t}\\), substitute \\(\\vec x_2\\) into the system \\(\\vec {x'} = Ax\\);\nLHS = \\(\\vec {x_2'} = \\vec k(e^{\\lambda t }+\\lambda te^{\\lambda t})=A\\vec x_2\\)=\\(A\\vec k t e^{\\lambda t}\\) =RHS\nThen we get \\(\\vec k = \\vec 0\\).\nSo our assumption is not true.\nThere is an extra term"
  },
  {
    "objectID": "mth106.html#对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量",
    "href": "mth106.html#对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量",
    "text": "对称矩阵重复n次的eigenvalue一定能找到n个线性无关的特征向量"
  },
  {
    "objectID": "mth106.html#把一个矩阵化为对角矩阵-gauss--jordan-elimination",
    "href": "mth106.html#把一个矩阵化为对角矩阵-gauss--jordan-elimination",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "把一个矩阵化为对角矩阵 （gauss- Jordan elimination）",
    "text": "把一个矩阵化为对角矩阵 （gauss- Jordan elimination）"
  },
  {
    "objectID": "Mathematical Analysis 1.html#if-a-question-is-not-trivial-we-should-know-where-is-the-difficulty-of-it",
    "href": "Mathematical Analysis 1.html#if-a-question-is-not-trivial-we-should-know-where-is-the-difficulty-of-it",
    "title": "Analysis1 and Analysis 2",
    "section": "If a question is not trivial, we should know where is the difficulty of it",
    "text": "If a question is not trivial, we should know where is the difficulty of it\nHere the difficulty is uncountable\neg. f is a continuous function on [0,1] \\(\\int_0^1 f(x) x^n dx =0\\), prove that f(x)=0 for all x in [0,1].\nwe have \\(\\int_0^1 f(x) [a_nx^n+...+ax+a_0]dx=0\\)\nWe have \\(\\int_0^1 |f(x)|^2 dx=0\\) (Weierstrass Approximation Theorem and Since $ |f(x)|^2$ is not negative, Suppose f(x) is not equal to 0, then we could find a point x0 where f(x0) is not equal to 0, then we could find a small interval around x0 where f(x) is not equal to 0, then this interval’s integral could not be =0, which is a contradiction (the meaning of “continuity” is that if a point is greater than 0, then there is a neighborhood of this point where the function is greater than 0.))\nWe have \\(|f|^2=0\\) a.e. \\(x\\in [0,1]\\)\nSo \\(f(x)=0\\) a.e. \\(x\\in [0,1]\\)\nuncountable is the difficulty of this question (density argument : The closure of Q is R means that \\(\\forall x \\in R\\) \\(\\exists x_n \\in Q\\) such that \\(x_n\\) converges to x)\nSo we use the proof by contradiction:"
  },
  {
    "objectID": "Mathematical Analysis 1.html#epislon-delta-language-is-not-important-the-most-important-thing-is-to-understand-the-essence-of-the-proof-and-the-logic-behind-it.-it-is-better-to-see-everything-intuitively.",
    "href": "Mathematical Analysis 1.html#epislon-delta-language-is-not-important-the-most-important-thing-is-to-understand-the-essence-of-the-proof-and-the-logic-behind-it.-it-is-better-to-see-everything-intuitively.",
    "title": "Analysis1 and Analysis 2",
    "section": "epislon-delta language is not important, the most important thing is to understand the essence of the proof and the logic behind it. It is better to see everything intuitively.",
    "text": "epislon-delta language is not important, the most important thing is to understand the essence of the proof and the logic behind it. It is better to see everything intuitively.\n\neg. The sum of continuous functions\n\n2 functions\nn functions\ninfinite functions —not continuous! –We could not choose the minimum \\(\\delta\\) to make the sum continuous."
  },
  {
    "objectID": "Mathematical Analysis 1.html#easy-to-think-wrongly",
    "href": "Mathematical Analysis 1.html#easy-to-think-wrongly",
    "title": "Analysis1 and Analysis 2",
    "section": "easy to think wrongly",
    "text": "easy to think wrongly\nIf \\(f'(x)\\) exists, \\((f^2)'\\)= \\(2f(x)f'(x)\\) but the counter direction might not be true\n\neg. If \\(f^2\\) is differentialble and \\(f^2 &gt; 0\\), then |f| is differentiable since we could write |f| as some function of f(x) and \\(f^2\\) is differentiable. So we could use the chain rule to show that |f| is differentiable. eg. \\(|f|=e^{1/2lnf^2}\\) or just \\(|f|=\\sqrt {f^2}\\)\neg. If \\(f^2\\) is differentiable, and \\(f^2 &gt; 0\\) \\(f\\) may not differentiable since f itself may NOT continuous at any \\(x\\in \\mathbb{R}\\). f(x) is 1 when x is rational and -1 when x is irrational. f(x) is not continuous at any point in \\(\\mathbb{R}\\), so f(x) is not differentiable at any point in \\(\\mathbb{R}\\)."
  },
  {
    "objectID": "Mathematical Analysis 1.html#integrability",
    "href": "Mathematical Analysis 1.html#integrability",
    "title": "Analysis1 and Analysis 2",
    "section": "Integrability",
    "text": "Integrability\nWhen we consider a continuous functon’s integrability on [a,b], we could divide the function into 2 parts—one is good and the other is bad. For the good part, the uniform continuity guarantes the integrability. For the bad part, we could use the property of boundedness to show that the integral is finite using very thin intervals to cover the bad part. So we could use the property of boundedness to show that the integral is finite using very thin intervals to cover the bad part."
  },
  {
    "objectID": "Mathematical Analysis 1.html#compact--closed-and-bounded-is-just-appropriate-for-the-real-number-set-but-for-the-general-metric-space-is-not-truethe-real-meaning-is-every-open-cover-has-a-finite-subcover",
    "href": "Mathematical Analysis 1.html#compact--closed-and-bounded-is-just-appropriate-for-the-real-number-set-but-for-the-general-metric-space-is-not-truethe-real-meaning-is-every-open-cover-has-a-finite-subcover",
    "title": "Analysis1 and Analysis 2",
    "section": "compact -“closed and bounded” is just appropriate for the real number set, but for the general metric space is not true—the real meaning is “every open cover has a finite subcover”",
    "text": "compact -“closed and bounded” is just appropriate for the real number set, but for the general metric space is not true—the real meaning is “every open cover has a finite subcover”"
  },
  {
    "objectID": "Mathematical Analysis 1.html#gauss-is-a-fox-and-the-foxs-tail-sweeps-away-the-places-he-has-passed-through.-abel",
    "href": "Mathematical Analysis 1.html#gauss-is-a-fox-and-the-foxs-tail-sweeps-away-the-places-he-has-passed-through.-abel",
    "title": "Analysis1 and Analysis 2",
    "section": "Gauss is a fox, and the fox’s tail sweeps away the places he has passed through. —Abel",
    "text": "Gauss is a fox, and the fox’s tail sweeps away the places he has passed through. —Abel\nGauss had the habit of not writing down his proofs, which made it difficult for others to understand his work. He had good habits of writing his daily work in his diary, which was a good way to keep track of his progress."
  },
  {
    "objectID": "Mathematical Analysis 1.html#read-euler-read-euler-he-is-the-master-of-us-all.-laplace",
    "href": "Mathematical Analysis 1.html#read-euler-read-euler-he-is-the-master-of-us-all.-laplace",
    "title": "Analysis1 and Analysis 2",
    "section": "Read Euler, read Euler, he is the master of us all. —Laplace",
    "text": "Read Euler, read Euler, he is the master of us all. —Laplace\nEuler telled us both the true things and the false things in mathematics. He demonstrated the whole thinking process of mathematics.\nNot only did Euler did good mathematics research but also he was a good teacher. He had cultivated lots of students. He wrote a lot of books and papers, which were very helpful for the development of mathematics."
  },
  {
    "objectID": "mth106.html#use-this-idea-we-introduce-the-method-of-variation-of-parameters-of-finding-the-particular-solution-of-the-non-homogeneous-linear-equation-system",
    "href": "mth106.html#use-this-idea-we-introduce-the-method-of-variation-of-parameters-of-finding-the-particular-solution-of-the-non-homogeneous-linear-equation-system",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "use this idea we introduce the method of variation of parameters of finding the particular solution of the non-homogeneous linear equation system",
    "text": "use this idea we introduce the method of variation of parameters of finding the particular solution of the non-homogeneous linear equation system\n…."
  },
  {
    "objectID": "aph101.html#simple-random-sampling",
    "href": "aph101.html#simple-random-sampling",
    "title": "APH101-Biostatistics And R",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nWe can get information about the population by taking a sample from it. The sample should be representative of the population.\nA simple random sample of size n is a sample selected from a population in such a way that every possible sample of size n has the same chance of being selected and without replacement."
  },
  {
    "objectID": "aph101.html#unbiased-estimator",
    "href": "aph101.html#unbiased-estimator",
    "title": "APH101-Biostatistics And R",
    "section": "Unbiased estimator",
    "text": "Unbiased estimator\nAn estimator is a statistic used to estimate a population parameter. An estimator is unbiased if the expected value of the estimator equals the population parameter.\n\\(E(\\hat \\theta) = \\theta\\)\nwhere \\(\\hat \\theta\\) is the estimator and \\(\\theta\\) is the population parameter.\neg.\n\\[\n\\mathbb{E}[\\hat{p}]=\\mathbb{E}\\left[\\frac{X_1+X_2+\\cdots+X_n}{n}\\right]=\\frac{1}{n}\\left(\\mathbb{E}\\left[X_1\\right]+\\cdots+\\mathbb{E}\\left[X_n\\right]\\right)=p\n\\]"
  },
  {
    "objectID": "aph101.html#variance",
    "href": "aph101.html#variance",
    "title": "APH101-Biostatistics And R",
    "section": "Variance",
    "text": "Variance\n\\[\n\\operatorname{Var}[X]=\\mathbb{E}\\left[X^2\\right]-(\\mathbb{E}[X])^2\n\\]\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\hat{p}^2\\right] & =\\mathbb{E}\\left[\\left(\\frac{X_1+X_2+\\cdots+X_n}{n}\\right)^2\\right] \\\\\n& =\\frac{1}{n^2} \\mathbb{E}\\left[X_1^2+\\cdots+X_n^2+2\\left(X_1 X_2+X_1 X_3+\\cdots+X_{n-1} X_n\\right)\\right] \\\\\n& =\\frac{1}{n^2}\\left(n \\mathbb{E}\\left[X_1^2\\right]+2\\binom{n}{2} \\mathbb{E}\\left[X_1 X_2\\right]\\right) \\\\\n& =\\frac{1}{n} \\mathbb{E}\\left[X_1^2\\right]+\\frac{n-1}{n} \\mathbb{E}\\left[X_1 X_2\\right]\n\\end{aligned}\n\\]\n\\[\n\\mathbb{E}\\left[\\hat{p}^2\\right]=\\frac{1}{n} \\mathbb{E}\\left[X_1^2\\right]+\\frac{n-1}{n} \\mathbb{E}\\left[X_1 X_2\\right]\n\\]\nSince \\(X_1\\) is 0 or \\(1, X_1=X_1^2\\). Then \\(\\mathbb{E}\\left[X_1^2\\right]=\\mathbb{E}\\left[X_1\\right]=p\\).\nNotice: \\(X_1\\) and \\(X_2\\) are not independent.\n\\[\n\\mathbb{E}\\left[X_1 X_2\\right]=\\mathbb{P}\\left[X_1=1, X_2=1\\right]=\\mathbb{P}\\left[X_1=1\\right] \\mathbb{P}\\left[X_2=1 \\mid X_1=1\\right]\n\\]\n\\[\n\\mathbb{P}\\left[X_1=1\\right]=p, \\quad \\mathbb{P}\\left[X_2=1 \\mid X_1=1\\right]=\\frac{N p-1}{N-1}\n\\]\n\\[\n\\begin{aligned}\n\\operatorname{Var}[\\hat{p}] & =\\mathbb{E}\\left[\\hat{p}^2\\right]-(\\mathbb{E}[\\hat{p}])^2 \\\\\n& =\\frac{1}{n} p+\\frac{n-1}{n} p\\left(\\frac{N p-1}{N-1}\\right)-p^2 \\\\\n& =\\left(\\frac{1}{n}-\\frac{n-1}{n} \\frac{1}{N-1}\\right) p+\\left(\\frac{n-1}{n} \\frac{N}{N-1}-1\\right) p^2 \\\\\n& =\\frac{N-n}{n(N-1)} p+\\frac{n-N}{n(N-1)} p^2 \\\\\n& =\\frac{p(1-p)}{n} \\frac{N-n}{N-1}=\\frac{p(1-p)}{n}\\left(1-\\frac{n-1}{N-1}\\right)\n\\end{aligned}\n\\] When N is much bigger than n, it is \\(\\frac{p(1-p)}{n}\\), which is like we sample n things with replacement (independently)."
  },
  {
    "objectID": "aph101.html#sampling-distribution",
    "href": "aph101.html#sampling-distribution",
    "title": "APH101-Biostatistics And R",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nThe sampling distribution of a statistic is the probability distribution of that statistic based on a random sample.\n\nSample mean of i.i.d. normals\nSample mean follows a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "aph101.html#simulation",
    "href": "aph101.html#simulation",
    "title": "APH101-Biostatistics And R",
    "section": "Simulation",
    "text": "Simulation\nBelow codes explains the central limit theorem (CLT) and the sampling distribution of the sample proportion.\n\nlibrary(ggplot2)\n\nset.seed(111)\n\npopulation_size &lt;- 12141897\n\np &lt;- 0.54\n\nnum_simulations &lt;- 50\n\nsample_size &lt;- 500\n\nrep()\n\nNULL\n\np_hat_values &lt;- replicate(num_simulations, {\n  # Simulate sampling from the population\n  sample &lt;- sample(c(rep(1,population_size * p), rep(0,population_size*(1-p))),sample_size, replace =FALSE) # replicate(num_simulations, {...})：这个函数会重复执行大括号中的代码num_simulations次，并将每次的结果存储在一个向量中； rep创建一个包含population_size * p个1和population_size * (1-p)个0的向量，模拟总体。\nmean(sample) #calculate the p_hat for each sample\n})\n\n\n  \nhistogram &lt;- ggplot(data.frame(p = p_hat_values), aes(x=p))+\n  geom_histogram(binwidth = 0.01, fill =\"blue\", color = \"black\")+\n  labs(title =\" \",\n       x = \"p_hat\",\n       y= \"Frequency\")+\n  theme_minimal()\n\nprint(histogram)"
  },
  {
    "objectID": "aph101.html#moment-generating-functions",
    "href": "aph101.html#moment-generating-functions",
    "title": "APH101-Biostatistics And R",
    "section": "Moment generating functions",
    "text": "Moment generating functions\nMoment generating function (MGF) and characteristic function are powerful functions that describe the underlying features of a random variable.\n\nDefinition\n\\(M_x(t) = \\mathbb E (e^{tx})\\)\n\n\nTheorems about MGF\n\nIf \\(X\\) and \\(Y\\) are random variables with the same MGF, which is finite on \\(\\left[-t_0, t_0\\right]\\) for some \\(t_0&gt;0\\), then \\(X\\) and \\(Y\\) have the same distribution.\n\nMGFs can be used as a tool to determine if two random variables have the identical CDF.\n\nLet \\(X_1, \\cdots, X_n\\) be independent random variables, with MGFs \\(M_{X_1}, \\cdots, M_{X_n}\\). Then the MGF of their sum is given by \\[\nM_{X_1+\\cdots+X_n}(t)=M_{X_1}(t) \\cdots M_{X_n}(t)\n\\]\n\n\nExample of Gamma and Exponential MGF\nA gamma distribution with shape \\(r=1\\) is an exponential distribution. If \\(X \\sim \\operatorname{Gamma}\\left(r_X, \\lambda\\right)\\) and \\(Y \\sim \\operatorname{Gamma}\\left(r_Y, \\lambda\\right)\\) are independent, then we have \\(X+Y \\sim \\operatorname{Gamma}\\left(r_X+r_Y, \\lambda\\right)\\). As a special case, if \\(X_1, X_2, \\cdots, X_n\\) are i.i.d. with \\(\\operatorname{Exp}(\\lambda)\\) distribution, then \\(X_1+X_2+\\cdots+X_n\\) has \\(\\operatorname{Gamma}(n, \\lambda)\\) distribution.\nSuppose \\(X \\sim \\operatorname{Exp}(\\lambda)\\), for \\(\\lambda&gt;0\\). Then \\[\nM_X(t)=\\mathbb{E}\\left[e^{t X}\\right]=\\int_0^{\\infty} e^{t x} \\lambda e^{-\\lambda x} d x=\\lambda \\int_0^{\\infty} e^{(t-\\lambda) x} d x\n\\]\nSimilar to Gamma MGF, the integral of Exponential MGF converges only for \\(t&lt;\\lambda\\). For \\(t&lt;\\lambda\\), we can integrate: \\[\nM_X(t)=\\lambda\\left[\\frac{e^{(t-\\lambda) x}}{t-\\lambda}\\right]_0^{\\infty}=\\frac{\\lambda}{\\lambda-t}\n\\]\nSince the Gamma MGF is \\(M_X(t)=\\frac{\\lambda^r}{(\\lambda-t)^r}\\) for any \\(t&lt;\\lambda\\) For shape \\(r=1\\), Exponential MGF = Gamma MGF."
  },
  {
    "objectID": "aph101.html#distribution-transformation",
    "href": "aph101.html#distribution-transformation",
    "title": "APH101-Biostatistics And R",
    "section": "Distribution Transformation",
    "text": "Distribution Transformation\n\nUniversality of the Uniform—From uniform you can get everything\nLet u~unif(0,1), F be a CDF(assume F is strictly increased, continuous). Then there comes a theorem: \\[\nx = F^{-1}(u)\n\\] Then \\[\nX \\sim F\n\\] (Proof: \\[\nP(X\\leq x)=P(F^{-1}(u)\\leq x)=P(F(F^{-1}(u))\\leq F(x))=P(u\\leq F(x))=F(x)\n\\])\nYou can convert from the random uniforms to whatever you want to simulate. One example is the simulation of Logistic distribution: F(X)=\\(e^x\\)/(\\(1+e^x\\))\nu~unif(0,1), \\[\nX = F^{-1}(u)=log(u/1-u)\n\\] and X~F\nAnother example is that we could try to use uniform to simulate normal distribution:\nThe Box-Muller transform generates pairs of independent standard normally distributed (zero mean, unit variance) random numbers, given a source of uniformly distributed random numbers.\nGiven two independent random variables \\(U_1\\) and \\(U_2\\) that are uniformly distributed on the interval (0, 1), we can generate two independent standard normal random variables \\(Z_0\\) and \\(Z_1\\) using the following formulas:\n\\[\nZ_0 = \\sqrt{-2 \\ln U_1} \\cos(2 \\pi U_2)\n\\]\n\\[\nZ_1 = \\sqrt{-2 \\ln U_1} \\sin(2 \\pi U_2)\n\\]\nInversely, the example could be: let \\(Z_0\\) and \\(Z_1\\) be standard normal random variables with the following values: \\(Z_0\\) = 0.5 and \\(Z_1\\) = -1.0.\n\nCompute CDF values:\nFor standard normal distribution, the CDF \\[ \\Phi(z) \\] is given by:\n\\[\n\\Phi(z) = \\frac{1}{2} \\left[ 1 + \\text{erf} \\left( \\frac{z}{\\sqrt{2}} \\right) \\right]\n\\]\n(ps:\\[\n\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} \\, dt\n\\])\n\nFor \\[ Z_0 = 0.5 \\]:\n\\[\n\\Phi(0.5) = \\frac{1}{2} \\left[ 1 + \\text{erf} \\left( \\frac{0.5}{\\sqrt{2}} \\right) \\right]\n\\]\nFor \\[ Z_1 = -1.0 \\]:\n\\[\n\\Phi(-1.0) = \\frac{1}{2} \\left[ 1 + \\text{erf} \\left( \\frac{-1.0}{\\sqrt{2}} \\right) \\right]\n\\]\n\nCompute Uniform values:\nSince the CDF values \\[ \\Phi(Z_0) \\] and \\[ \\Phi(Z_1) \\] are in the range [0, 1], we can directly use them as uniform random variables \\(U_0\\) and \\(U_1\\).\n\n\\[U_0 = \\Phi(0.5)\\]\n\\[U_1 = \\Phi(-1.0)\\]\n\nResult:\n\nUsing the error function values:\n\n\\[ \\Phi(0.5) \\approx 0.6915 \\]\n\\[ \\Phi(-1.0) \\approx 0.1587 \\]\n\n\nThus, the corresponding uniform distribution values are:\n\n\\[ U_0 \\approx 0.6915 \\]\n\\[ U_1 \\approx 0.1587 \\]\n\n\nAnother example is that we can create a function that has good quality as F in the theorem, for example, \\[\nu=F(x)=1-e^{-x}          (x&gt;0)\n\\] then we can simulate X~F:X=-ln(1-u)~F\nAlso, if \\[\nX \\sim F\n\\]\nThen, \\[\nF(x) \\sim unif(0,1)\n\\] e.g. let X~F, if\\[\nF(x_0)=1/3\\] then \\[P(F(X)\\leq 1/3)=P(X\\leq x_0)=F(x_0)=1/3 \\] which follows the uniform distribution(0,1) because 1/3 generates 1/3. (Uniform distribution: Probability(CDF) is proportional to length)\n\n\nNormal Distribution\n\nFrom Standard Normal Distribution to Normal Distribution\n\\(f(z)=ce^{-z^2/2}\\) is a function with good qualities such as symmetric……..\nTo generate normalization constant c using CDF=1, instead of using impossible integral methods to compute it we should try to find the area under it. In that case, we transfer the integral shape to the double integral’s multiplication then we get c: \\[\n\\int_{-\\infty}^{\\infty}\\exp(-z^2/2) dz=\\int_{-\\infty}^{\\infty}\\exp(-z^2/2) dz=\\int_{-\\infty}^{\\infty}\\exp(-x^2/2)dx\\int_{-\\infty}^{\\infty}\\exp(-y^2/2)dy\n\\] \\[\n=\\int_{0}^{\\infty}\\exp(-r^2/2)r dr\\int_{0}^{2π} d\\theta\\ =\\sqrt2\\pi\\\n\\] So \\[\nc=1/\\sqrt2\\pi\\\n\\] …….\n\n\n\nFrom…..\n111\n\nExponential distribution\nthe only one parameter is the rate parameter. The probability density function (pdf) of an exponential distribution with rate parameter (&gt; 0) is given by:\n\\[\nf_X(x) =\n\\begin{cases}\n\\lambda e^{-\\lambda x} & \\text{for } x \\geq 0, \\\\\n0 & \\text{for } x &lt; 0.\n\\end{cases}\n\\] The cumulative distribution function (cdf) of an exponential distribution with rate parameter (&gt; 0) is given by:\n\\[\nF_X(x) =\n\\begin{cases}\n1 - e^{-\\lambda x} & \\text{for } x \\geq 0, \\\\\n0 & \\text{for } x &lt; 0.\n\\end{cases}\n\\] 2.Let Y~x, then Y ~Expo(1)\nproof: since \\[\nP(Y\\leq y)=P(X\\leq y/\\lambda)=1-e^{-y}\n\\] (just plot it into x) and we could check that E[X]=Var[X]=1, so X=Y/\\(\\lambda\\) has E[x]=1/\\(\\lambda\\), Var[x]=1/\\(\\lambda^2\\)\ne.g. Memoryless Property(This property implies that the remaining lifetime distribution does not depend on how much time has already elapsed.)(The exponential distribution is the only continuous distribution that has the memoryless property):\\[ P(X\\geq s+t|X\\geq s)=P(X\\geq t)\\] which is actually satisfied by the exponential and we could prove it(though it intuitively makes sence): Here \\[\nP(X\\geq s)=1-P(X\\leq s)=e^{-\\lambda s}\n\\] \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{P(X \\geq s+t \\text { and } X \\geq s)}{P(X \\geq s)}\n\\]\nSince \\(X \\geq s+t\\) implies \\(X \\geq s\\), we can simplify the numerator: \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{P(X \\geq s+t)}{P(X \\geq s)}\n\\]\nNow, substitute the survival function for the exponential distribution: \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{e^{-\\lambda(s+t)}}{e^{-\\lambda s}}\n\\]\nSimplify the expression: \\[\nP(X \\geq s+t \\mid X \\geq s)=\\frac{e^{-\\lambda s} \\cdot e^{-\\lambda t}}{e^{-\\lambda s}}=e^{-\\lambda t}\n\\]\nNotice that \\(e^{-\\lambda t}=P(X \\geq t)\\) : \\[\nP(X \\geq s+t \\mid X \\geq s)=P(X \\geq t)\n\\] usefulness of it: \\[\nX\\sim Expo(\\lambda),E(X|X&gt;a)=a+E(X-a|X&gt;a)=a+1/\\lambda\n\\]\ne.g.2The hazard rate (or failure rate) for an exponential distribution is constant over time. For an exponential random variable \\(X\\) with rate parameter \\(\\lambda\\), the hazard rate is: \\[\nh(t)=\\frac{f_X(t)}{1-F_X(t)}=\\lambda .\n\\]\nA constant hazard rate implies that the event is equally likely to occur at any point in time, which is a reasonable assumption for many processes, such as the lifetime of certain electronic components or the occurrence of certain types of random failures.\ne.g.3 The exponential distribution is closely related to the Poisson process, which is a process that models the occurrence of events happening independently at a constant average rate. If the times between consecutive events in a Poisson process are independent and identically distributed, then these interarrival times follow an exponential distribution. This relationship makes the exponential distribution a natural choice in contexts where events occur randomly over time, such as phone calls arriving at a switchboard or buses arriving at a bus stop.\n\n\nGa\nIf r.v.X~N(0,1), then \\(X^2\\)~\\(Ga\\)(1/2,1/2)\nProof: If r.v. (X N(0,1)), then (X^2 (, ))\nLet (X) be a random variable such that (X N(0,1)).\nThe probability density function (pdf) of (X) is: \\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}, \\quad -\\infty &lt; x &lt; \\infty.\n\\]\nFirst, we find the pdf of (Y = X^2).\nThe cumulative distribution function (CDF) of (Y) is given by: \\[\nF_Y(y) = P(Y \\leq y) = P(X^2 \\leq y).\n\\]\nSince (X^2 ), we only consider (y ): \\[\nF_Y(y) = P(-\\sqrt{y} \\leq X \\leq \\sqrt{y}).\n\\]\nUsing the CDF of the normal distribution, we have: \\[\nF_Y(y) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\, dx.\n\\]\nThe pdf of (Y) is the derivative of the CDF: \\[\nf_Y(y) = \\frac{d}{dy} F_Y(y).\n\\]\n\\[\nF_Y(y) = 2 \\int_{0}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\, dx.\n\\]\n\\[\nF_Y(y) = 2 \\int_{0}^{\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} e^{-u} \\frac{du}{\\sqrt{2u}} = \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\sqrt{y}} e^{-u} \\frac{du}{\\sqrt{2u}}.\n\\]\nThus, \\[\nf_Y(y) = \\frac{d}{dy} \\left( \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\sqrt{y}} e^{-u} \\frac{du}{\\sqrt{2u}} \\right).\n\\]\nDifferentiating with respect to y gives: \\[\nf_Y(y) = \\frac{1}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{y}{2}} \\cdot y^{-\\frac{1}{2}}.\n\\]\nSimplifying, we get: \\[\nf_Y(y) = \\frac{1}{\\sqrt{2\\pi}} y^{-\\frac{1}{2}} e^{-\\frac{y}{2}}.\n\\]\n\\[\nY = X^2 \\sim \\text{Ga}\\left(\\frac{1}{2}, \\frac{1}{2}\\right).\n\\]\n\n\nChi-square\nChi-square is a\n1.Let ( \\(Z_1\\), \\(Z_2\\), , \\(Z_i\\) ) are independent standard normal random variables(i.e. Z~N(0,1)), then the random variable ( X ) defined by\n\\[\nX = Z_1^2 + Z_2^2 + \\cdots + Z_i^2\n\\] (\\(Z_j\\)~iid.N(0,1))\nfollows a chi-square distribution with ( i ) degrees of freedom. So chi-square of 1 is the same thing as Gamma of (1/2,1/2) so chi-square of n is Gamma(n/2,1/2)\n2.If \\[\nZ_i = \\frac{x_i - \\mu}{\\sigma}\n\\] The sum of squared standardized deviations is:\n\\[\n\\sum_{i=1}^n Z_i^2 = \\sum_{i=1}^n \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]\nLet \\(x_1, x_2, \\cdots, x_n\\) be samples of \\(N\\left(\\mu, \\sigma^2\\right)\\) , \\(\\mu\\) is a known constant, find the distribution of statistics: \\[\nT=\\sum_{i=1}^n\\left(x_i-\\mu\\right)^2\n\\]\nsol: \\(y_i=\\left(x_i-\\mu\\right) / \\sigma, i=1,2, \\cdots, n\\), 则 \\(y_1, y_2, \\cdots, y_n\\) are iid r.v. of \\(N(0,1)\\),so\\[\n\\frac{T}{\\sigma^2}=\\sum_{i=1}^n\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2=\\sum_{i=1}^n y_i^2 \\sim \\chi^2(n),\n\\] (i.e.\\[\n\\sum_{i=1}^n Z_i^2 {\\sigma^2}\\sim \\chi^2(n)\\])\nBesides, \\(T\\)’s PDF is \\[\np(t)=\\frac{1}{\\left(2 \\sigma^2\\right)^{n / 2} \\Gamma(n / 2)} \\mathrm{e}^{-\\frac{1}{2 \\sigma^2} t^{\\frac{n}{2}}-1}, \\quad t&gt;0,\n\\]\nwhich is Gamma distribution \\(G a\\left(\\frac{n}{2}, \\frac{1}{2 \\sigma^2}\\right) \\cdot\\)\n3.chi-square is uesful because of theorems below:let \\(x_1, x_2, \\cdots, x_n\\) are samples from \\(N\\left(\\mu, \\sigma^2\\right)\\) , whose sample mean and sample variance are\\[\n\\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i \\text { and } s^2=\\frac{1}{n-1} \\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2,\n\\]\nthen we can get: (1) \\(\\bar{x}\\) and \\(s^2\\) are independent; (2) \\(\\bar{x} \\sim N\\left(\\mu, \\sigma^2 / n\\right)\\); (3) \\(\\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi^2(n-1)\\).\nProof: \\[\np\\left(x_1, x_2, \\cdots, x_n\\right)=\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\mathrm{e}^{-\\sum_{i=1}^n \\frac{\\left(x_i-\\mu\\right)^2}{2 \\sigma^2}}=\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\exp \\left\\{-\\frac{\\sum_{i=1}^n x_i^2-2 n \\bar{x} \\mu+n \\mu^2}{2 \\sigma^2}\\right\\}\n\\]\ndenote\\(\\boldsymbol{X}=\\left(x_1, x_2, \\cdots, x_n\\right)^{\\mathrm{T}}\\), then we create an \\(n\\)-dimension orthogonal \\(\\boldsymbol{A}\\) and every element in the first row is \\(1 / \\sqrt{n}\\), such as \\[\nA=\\left(\\begin{array}{ccccc}\n\\frac{1}{\\sqrt{n}} & \\frac{1}{\\sqrt{n}} & \\frac{1}{\\sqrt{n}} & \\cdots & \\frac{1}{\\sqrt{n}} \\\\\n\\frac{1}{\\sqrt{2 \\cdot 1}} & -\\frac{1}{\\sqrt{2 \\cdot 1}} & 0 & \\cdots & 0 \\\\\n\\frac{1}{\\sqrt{3 \\cdot 2}} & \\frac{1}{\\sqrt{3 \\cdot 2}} & -\\frac{2}{\\sqrt{3 \\cdot 2}} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n\\frac{1}{\\sqrt{n(n-1)}} & \\frac{1}{\\sqrt{n(n-1)}} & \\frac{1}{\\sqrt{n(n-1)}} & \\cdots & -\\frac{n-1}{\\sqrt{n(n-1)}}\n\\end{array}\\right),\n\\] 令 \\(\\boldsymbol{Y}=\\left(y_1, y_2, \\cdots, y_n\\right)^{\\mathrm{T}}=\\boldsymbol{A} \\boldsymbol{X}\\), \\(|Jacobi|=1\\), and we can find thatt\\[\n\\begin{gathered}\n\\bar{x}=\\frac{1}{\\sqrt{n}} y_1, \\\\\n\\sum_{i=1}^n y_i^2=\\boldsymbol{Y}^{\\mathrm{T}} \\boldsymbol{Y}=\\boldsymbol{X}^{\\mathrm{T}} \\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A} \\boldsymbol{X}=\\sum_{i=1}^n x_i^2,\n\\end{gathered}\n\\]\nso\\(y_1, y_2, \\cdots, y_n\\) ’s joint density function is \\[\n\\begin{aligned}\np\\left(y_1, y_2, \\cdots, y_n\\right) & =\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\exp \\left\\{-\\frac{\\sum_{i=1}^n y_i^2-2 \\sqrt{n} y_1 \\mu+n \\mu^2}{2 \\sigma^2}\\right\\} \\\\\n& =\\left(2 \\pi \\sigma^2\\right)^{-n / 2} \\exp \\left\\{-\\frac{\\sum_{i=2}^n y_i^2+\\left(y_1-\\sqrt{n} \\mu\\right)^2}{2 \\sigma^2}\\right\\}\n\\end{aligned}\n\\]\nSo, \\(\\boldsymbol{Y}=\\left(y_1, y_2, \\cdots, y_n\\right)^{\\mathrm{T}}\\) independently distributed as normal distribution and their variances are all equal to\\(\\sigma^2\\), but their means are not all the same because \\(y_2, \\cdots, y_n\\) ’s means are \\(0, y_1\\)’s is \\(\\sqrt{n} \\mu\\), which ends our proof of (2). \\[\n(n-1) s^2=\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2=\\sum_{i=1}^n x_i^2-(\\sqrt{n} \\bar{x})^2=\\sum_{i=1}^n y_i^2-y_1^2=\\sum_{i=2}^n y_i^2,\n\\]\nThis proves conclusion (1). Since \\(y_2, \\cdots, y_n\\) are independently and identically distributed as \\(N\\left(0, \\sigma^2\\right)\\), we have: \\[\n\\frac{(n-1) s^2}{\\sigma^2}=\\sum_{i=2}^n\\left(\\frac{y_i}{\\sigma}\\right)^2 \\sim \\chi^2(n-1) .\n\\]\nTheorem is proved. (similar to the proof above this maybe easier to understand:\\(\\begin{aligned} & i z\\left(Y_1, Y_2, \\cdots, Y_2\\right)^{\\top}=A\\left(X_1, \\cdots, x_n\\right)^{\\top} \\\\ & \\text { then } \\sum_{i=1}^n Y_i^2=\\left(Y_1, \\cdots, Y_n\\right)\\left(Y_1, \\cdots, Y_n\\right)^{\\top} \\\\ & =\\left[A\\left(x_1, \\cdots, x_n\\right)^{\\top}\\right]^{\\top}\\left[A\\left(x_1, \\cdots, X_n\\right)^{\\top}\\right] \\\\ & =\\left(x_1, \\cdots, x_n\\right) A^{\\top} A\\left(x_1, \\cdots, x_n\\right)^{\\top} \\\\ & =\\left(x_1, \\cdots, x_n\\right) E\\left(x_1, \\cdots, x_n\\right)^{\\top}=\\sum_{i=1}^n x_i^2 \\\\ & \\end{aligned}\\)) \\(\\begin{aligned} & \\text { besides } Y_1=\\frac{1}{\\sqrt{n}} x_1+\\cdots+\\frac{1}{\\sqrt{n}} x_n=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i \\\\ & \\text { and } Y_1=\\sqrt{n} \\cdot \\frac{1}{n} \\sum_{i=1}^n X_i=\\sqrt{n} \\bar{X}, \\text { then } \\bar{x}=\\frac{1}{\\sqrt{n}} y_i \\\\ & B S^2=\\frac{1}{n-1} \\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2=\\frac{1}{n-1}\\left[\\sum_{i=1}^n x_i^2-n \\bar{x}^2\\right] \\\\ & =\\frac{1}{n-1}\\left[\\sum_{i=1}^n Y_i^2-Y_1^2\\right]=\\frac{1}{n-1} \\sum_{i=2}^n Y_i^2 \\\\ & 2 \\oplus L=(\\sqrt{2 \\pi} \\sigma)^{-n} \\exp \\left[-\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n\\left(x_i-\\mu\\right)^2\\right] \\text {. } \\\\ & =(\\sqrt{2 \\pi} \\sigma)^{-n} \\exp \\left[-\\frac{1}{2 \\sigma^2}\\left(\\sum_{i=1}^n x_i^2-2 \\mu n \\bar{x}+\\mu^2 n\\right]\\right. \\\\ & =(\\sqrt{2 \\pi} \\sigma)^{-n} \\exp \\left[-\\frac{1}{2 \\sigma^2}\\left(\\sum_{1=1}^n y_i{ }^2-2 \\mu n \\frac{1}{\\sqrt{n}} Y_1+n \\mu^2\\right]\\right. \\\\ & \\end{aligned}\\) \\[=(\\sqrt2\\pi\\sigma)^{-1}exp[-1/2\\sigma^2(Y_1-\\sqrt nu)^2]×(\\sqrt2\\pi\\sigma)^{-1}exp[-1/2\\sigma^2{Y_2}^2]*...*(\\sqrt2\\pi\\sigma)^{-1}exp[-1/2\\sigma^2{Y_n}^2]\n\\]\nSo L is \\(Y_1\\)…\\(Y_n\\)’s joint density function and so they are independent. Besides, we have proved that its mean is \\(1/\\sqrt n\\)\\(Y_1\\) and \\(S^2\\)=\\(1/n-1 \\Sigma{i=2}Y_i^2\\), so the normal distribution’s mean and variance are independent.\nWhen the random variable \\(\\chi^2 \\sim \\chi^2(n)\\), for a given \\(\\alpha\\) (where \\(0&lt;\\) \\(\\alpha&lt;1\\) ), the value \\(\\chi_{1-\\alpha}^2(n)\\) satisfying the probability equation \\(P\\left(\\chi^2 \\leqslant \\chi_{1-\\alpha}^2(n)\\right)=1-\\) \\(\\alpha\\) is called the \\(1-\\alpha\\) quantile of the \\(\\chi^2\\) distribution with \\(n\\) degrees of freedom.\nSuppose the random variables \\(X_1 \\sim \\chi^2(m)\\) and \\(X_2 \\sim \\chi^2(n)\\), and \\(X_1\\) and \\(X_2\\) are independent. Then the distribution of \\(F=\\frac{X_1 / m}{X_2 / n}\\) is called the \\(\\mathrm{F}\\) distribution with \\(m\\) and \\(n\\) degrees of freedom, denoted as \\(F \\sim F(m, n)\\). Here, \\(m\\) is called the numerator degrees of freedom and \\(n\\) the denominator degrees of freedom. We derive the density function of the \\(\\mathrm{F}\\) distribution in two steps. First, we derive the density function of \\(Z=\\frac{X_1}{X_2}\\). Let \\(p_1(x)\\) and \\(p_2(x)\\) be the density functions of \\(\\chi^2(m)\\) and \\(\\chi^2(n)\\) respectively. According to the formula for the distribution of the quotient of independent random variables, the density function of \\(Z\\) is: \\[\n\\begin{gathered}\np_Z(z)=\\int_0^{\\infty} x_2 p_1\\left(z x_2\\right) p_2\\left(x_2\\right) \\mathrm{d} x_2 \\\\\n=\\frac{z^{\\frac{m}{2}-1}}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right) 2^{\\frac{m+n}{2}}} \\int_0^{\\infty} x_2^{\\frac{n}{2}-1} e^{-\\frac{x_2}{2}(1+z)} \\mathrm{d} x_2 .\n\\end{gathered}\n\\]\nUsing the transformation \\(u=\\frac{x_2}{2}(1+z)\\), we get: \\[\np_Z(z)=\\frac{z^{\\frac{m}{2}-1}(1+z)^{-\\frac{m+n}{2}}}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)} \\int_0^{\\infty} u^{\\frac{n}{2}-1} e^{-u} \\mathrm{~d} u\n\\]\nThe final integral is the gamma function \\(\\Gamma\\left(\\frac{n}{2}\\right)\\), so: \\[\np_Z(z)=\\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)} z^{\\frac{m}{2}-1}(1+z)^{-\\frac{m+n}{2}}, \\quad z \\geq 0 .\n\\]\nSecond, we derive the density function of \\(F=\\frac{n}{m} Z\\). Let the value of \\(F\\) be \\(y\\). For \\(y \\geq 0\\), we have: \\[\n\\begin{aligned}\np_F(y) & =p_Z\\left(\\frac{m}{n} y\\right) \\cdot \\frac{m}{n}=\\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)}\\left(\\frac{m}{n} y\\right)^{\\frac{m}{2}-1}\\left(1+\\frac{m}{n} y\\right)^{-\\frac{m+n}{2}} \\cdot \\frac{m}{n} \\\\\n& =\\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right) \\Gamma\\left(\\frac{n}{2}\\right)}\\left(\\frac{m}{n}\\right)\\left(\\frac{m}{n} y\\right)^{\\downarrow \\frac{2}{2}-1}\\left(1+\\frac{m}{n} y\\right)^{-\\frac{m+n}{2}}\n\\end{aligned}\n\\]\nWhen the random variable \\(F \\sim F(m, n)\\), for a given \\(\\alpha\\) (where \\(0&lt;\\alpha&lt;1\\) ), the value \\(F_{1-\\alpha}(m, n)\\) satisfying the probability equation \\(P\\left(F \\leqslant F_{1-\\alpha}(m, n)\\right)=1-\\alpha\\) is called the \\(1-\\alpha\\) quantile of the \\(\\mathrm{F}\\) distribution with \\(m\\) and \\(n\\) degrees of freedom. By the construction of the \\(\\mathrm{F}\\) distribution, if \\(F \\sim F(m, n)\\), then \\(1 / F \\sim F(n, m)\\). Therefore, for a given \\(\\alpha\\) (where \\(0&lt;\\alpha&lt;1\\) ), \\[\n\\alpha=P\\left(\\frac{1}{F} \\leqslant F_\\alpha(n, m)\\right)=P\\left(F \\geqslant \\frac{1}{F_\\alpha(n, m)}\\right) .\n\\]\nThus, \\[\nP\\left(F \\leqslant \\frac{1}{F_\\alpha(n, m)}\\right)=1-\\alpha\n\\]\nThis implies \\[\nF_\\alpha(n, m)=\\frac{1}{F_{1-\\alpha}(m, n)} .\n\\]\nCorollary Suppose \\(x_1, x_2, \\cdots, x_m\\) is a sample from \\(N\\left(\\mu_1, \\sigma_1^2\\right)\\) and \\(y_1, y_2, \\cdots, y_n\\) is a sample from \\(N\\left(\\mu_2, \\sigma_2^2\\right)\\), and these two samples are independent. Let: \\[\ns_x^2=\\frac{1}{m-1} \\sum_{i=1}^m\\left(x_i-\\bar{x}\\right)^2, \\quad s_y^2=\\frac{1}{n-1} \\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2,\n\\] where \\[\n\\bar{x}=\\frac{1}{m} \\sum_{i=1}^m x_i, \\quad \\bar{y}=\\frac{1}{n} \\sum_{i=1}^n y_i\n\\] then \\[\nF=\\frac{s_x^2 / \\sigma_1^2}{s_y^2 / \\sigma_2^2} \\sim F(m-1, n-1) .\n\\]\nIn particular, if \\(\\sigma_1^2=\\sigma_2^2\\), then \\(F=\\frac{s_x}{s_y^2} \\sim F(m-1, n-1)\\). Proof: Since the two samples are independent, \\(s_x^2\\) and \\(s_y^2\\) are independent. According to a Theorem , we have \\[\n\\frac{(m-1) s_x^2}{\\sigma_1^2} \\sim \\chi^2(m-1), \\quad \\frac{(n-1) s_y^2}{\\sigma_2^2} \\sim \\chi^2(n-1) .\n\\]\nBy the definition of the \\(\\mathrm{F}\\) distribution, \\(F \\sim F(m-1, n-1)\\). Corollary: Suppose \\(x_1, x_2, \\cdots, x_n\\) is a sample from a normal distribution \\(N\\left(\\mu, \\sigma^2\\right)\\), and let \\(\\bar{x}\\) and \\(s^2\\) denote the sample mean and sample variance of the sample, respectively. Then \\[\nt=\\frac{\\sqrt{n}(\\bar{x}-\\mu)}{s} \\sim t(n-1) .\n\\]\nProof: From a Theorem we obtain \\[\n\\frac{\\bar{x}-\\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)\n\\]\nThen, \\[\n\\frac{\\sqrt{n}(\\bar{x}-\\mu)}{s}=\\frac{\\frac{\\bar{x}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) s^2 / \\sigma^2}{n-1}}}\n\\]\nSince the numerator is a standard normal variable and the denominator’s square root contains a \\(\\chi^2\\) variable with \\(n-1\\) degrees of freedom divided by its degrees of freedom, and they are independent, by the definition of the \\(t\\) distribution, \\(t \\sim t(n-1)\\). The proof is complete.\nCorollary: In the notation of Corollary , assume \\(\\sigma_1^2=\\sigma_2^2=\\sigma^2\\), and let \\[\ns_w^2=\\frac{(m-1) s_x^2+(n-1) s_y^2}{m+n-2}=\\frac{\\sum_{i=1}^m\\left(x_i-\\bar{x}\\right)^2+\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2}{m+n-2}\n\\]\nThen \\[\n\\frac{(\\bar{x}-\\bar{y})-\\left(\\mu_1-\\mu_2\\right)}{s_w \\sqrt{\\frac{1}{m}+\\frac{1}{n}}} \\sim t(m+n-2)\n\\]\nProof: Since \\(\\bar{x} \\sim N\\left(\\mu_1, \\frac{\\sigma^2}{m}\\right), \\bar{y} \\sim N\\left(\\mu_2, \\frac{\\sigma^2}{n}\\right)\\), and \\(\\bar{x}\\) and \\(\\bar{y}\\) are independent, we have\n\\[\n\\bar{x}-\\bar{y} \\sim N\\left(\\mu_1-\\mu_2,\\left(\\frac{1}{m}+\\frac{1}{n}\\right) \\sigma^2\\right) .\n\\]\nThus, \\[\n\\frac{(\\bar{x}-\\bar{y})-\\left(\\mu_1-\\mu_2\\right)}{\\sigma \\sqrt{\\frac{1}{m}+\\frac{1}{n}}} \\sim N(0,1) .\n\\]\nBy a Theorem , we know that \\(\\frac{(m-1) s_x^2}{\\sigma^2} \\sim \\chi^2(m-1)\\) and \\(\\frac{(n-1) s_y^2}{\\sigma^2} \\sim \\chi^2(n-1)\\), and they are independent. By additivity, we have \\[\n\\frac{(m+n-2) s_w^2}{\\sigma^2}=\\frac{(m-1) s_x^2+(n-1) s_y^2}{\\sigma^2} \\sim \\chi^2(m+n-2) .\n\\]\nSince \\(\\bar{x}-\\bar{y}\\) and \\(s_w^2\\) are independent, by the definition of the \\(\\mathrm{t}\\) distribution, we get the desired result. \\(\\square\\)\nOne interesting example shows the relationship of above distributions used charismatically to solve problems: r.v.: \\(X_1 ， X_2 ， X_3 ， X_4\\) indpendently identically distribute(iid) as \\(N\\left(0 . \\sigma^2\\right)\\). \\(Z=\\left(x_1^2+x_2^2\\right) /\\left(x_1^2+x_2^2+x_3^2+x_4^2\\right)\\) prove: \\(Z \\sim U(0.1)\\). \\[\n\\begin{aligned}\n& \\text { Solution: } Let Y=\\frac{X_3^2+X_4^2}{X_1^2+X_2^2}=\\frac{\\left[\\left(\\frac{X_3}{\\sigma}\\right)^2+\\left(\\frac{X_4}{\\sigma}\\right)^2\\right] / 2}{\\left[\\left(\\frac{X_1}{\\sigma}\\right)^2+\\left(\\frac{X_2}{\\sigma}\\right)^2\\right] / 2} \\sim F(2,2) . \\\\\n& \\text { i.e.  } f_Y(y)=\\frac{1}{(1+y)^2},  y&gt;0 \\\\\n& \\text { then } P(Z \\leq z)=P\\left(\\frac{1}{1+Y} \\leq z\\right)=P\\left(Y \\geqslant \\frac{1}{z}-1\\right) \\\\\n& =\\int_{\\frac{1}{z}-1}^{+\\infty} \\frac{1}{(1+y)^2} d y=z \\quad \\text { H } 0&lt;z&lt;1 . \\\\\n& \\therefore Z \\sim U(0.1)\n\\end{aligned}\n\\] (ps:$\n\\[\\begin{aligned} & f(x)=\\frac{\\Gamma\\left(\\frac{n_1+n_2}{2}\\right)}{\\Gamma\\left(\\frac{n_2}{2}\\right) \\Gamma\\left(\\frac{n_1}{2}\\right)}\\left(\\frac{n_1}{n_2}\\right)\\left(\\frac{n_1}{n_2} x\\right)^{\\frac{n_1}{2}-1}\\left(1+\\frac{n_1}{n_2} x\\right)^{\\frac{-1}{2}\\left(n_1+n_2\\right)} . \\\\ & \\text { of these } x&gt;0 \\text {. and } E(x)=\\frac{n_2}{n_2-2} \\text {, when } n_2&gt;2 \\text {. } \\\\ & \\operatorname{Var}(X)=\\frac{2 n_2^2\\left(n_1+n_2-2\\right)}{n_1\\left(n_2-2\\right)^2\\left(n_2-4\\right)} \\text {when $n_2&gt;4$. } \\\\ & \\end{aligned}\\]"
  },
  {
    "objectID": "aph101.html#pdf-functionss-transformationfrom-books",
    "href": "aph101.html#pdf-functionss-transformationfrom-books",
    "title": "APH101-Biostatistics And R",
    "section": "PDF functions’s transformation(from books)",
    "text": "PDF functions’s transformation(from books)\n\nStandard normal distribution\nPdf : \\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\)\n\n\n\nChi-square distribution\n\n\nFrom standard normal distribution \\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\), we want to get Chi-square distribution \\(f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}\\)\n\nAssuming that \\(Z \\sim N(0, 1)\\)\n\\(W=Z^2\\)\nSo we can deduce the pdf for the variable W \\[\n\\begin{align}\nf_W(w)&=Pr(Z^2=w)\\\\\n& =f_Z(\\sqrt{w}) \\left| \\frac{d(\\sqrt{w})}{dw} \\right| + f_Z(-\\sqrt{w}) \\left| \\frac{d(-\\sqrt{w})}{dw} \\right|\\\\\n&=2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-w / 2} \\cdot \\frac{1}{2\\sqrt{w}} = \\frac{1}{\\sqrt{2\\pi w}} e^{-w / 2}\\\\\n\\end{align}\n\\]\n\\(\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} \\, dt\\)\n\\(\\Gamma(1/2)\\)=\\(\\sqrt(\\pi)\\),we can know that \\(W\\sim \\chi^2(1)\\)\n\nNow, we can introduce the variable Y.\n\\(Y=\\sum_{i=1}^k Z_i^2\\) (\\(Z_i\\) is independent of each other)\nWe want to get the mgf of Y \\[\n\\begin{align}\nM_Y(t)&=E[e^{tY}]\\\\\n& =E[e^{tZ_1^2}e^{tZ_2^2}...e^{tZ_k^2}]\\\\\n& =E[e^{tZ_1^2}]E[e^{tZ_2^2}]...E[e^{tZ_k^2}] \\\\\n& = \\prod_{i=1}^k M_{Z_i^2}(t)\\\\\n&=(1-2t)^{-r1/2}(1-2t)^{-r2/2}...(1-2t)^{-rk/2}\\\\\n& =(1-2t)^{-\\sum_{i=1}^kri/2}\\\\\n\\end{align}\n\\] Because the mgf of Chi-square function is \\((1-2t)^{-r/2}\\)\nr is the degree of freedom of this chi-square function\nSo \\(Y\\sim \\chi^2(r1+r2+...+rk)\\)\nr1,r2…rk represents the degree of freedom of every single sample\nHere, df=1 for every sample\n\n\nStudent- t distribution\n\n\nFrom standard normal distribution \\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\), we want to get Student- t distribution \\(f(t) = \\frac{\\Gamma \\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi} \\, \\Gamma \\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\\), where \\(\\Gamma\\) is the gamma function and \\(\\nu\\) is the degrees of freedom.\n\nGiven two independent variables Z and V (\\(Z \\sim N(0, 1)\\) & \\(V\\sim \\chi^2(\\nu)\\)), then we construct a new variable \\(T=\\frac{Z}{\\sqrt(V/\\nu)}\\).\nThe joint pdf is \\[\ng(z,v)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}\\frac{1}{2^{\\nu/2} \\Gamma(\\nu/2)} v^{\\nu/2 - 1} e^{-v/2}\n\\] Cdf of T is given by \\[\n\\begin{align}\nF(t) & =Pr(\\frac{Z}{\\sqrt(V/\\nu)}\\leq t)\\\\\n& =Pr(Z\\leq {\\sqrt(V/\\nu)}t)\\\\\n& =\\int_{0}^{\\infty} \\int_{-\\infty}^{\\sqrt(V/\\nu)t} g(z,v) \\, dz \\, dv\n\\end{align}\n\\] Simplify F(t) \\[\nF(t)=\\frac{1}{\\sqrt{\\pi}\\Gamma(\\nu/2)}\\int_{0}^{\\infty}[\\int_{-\\infty}^{\\sqrt(V/\\nu)t} \\frac{e^{-z^2/2}}{2^\\frac{\\nu+1}{2}}dz]v^{\\frac{\\nu}{2}-1}e^{-\\frac{v}{2}}dv\n\\]\nTo get pdf, we will differentiate F(t) \\[\n\\begin{align}\nf(t) &=F'(t)=\\frac{1}{\\sqrt{\\pi}\\Gamma(\\nu/2)}\\int_{0}^{\\infty} \\frac{e^{-(v/2)(t^2/\\nu)}}{2^{\\frac{\\nu+1}{2}}}\\sqrt\\frac{v}{\\nu}v^{\\nu/2-1}e^{-\\frac{v}{2}}dv\\\\\n&=\\frac{1}{\\sqrt{\\pi\\nu}\\Gamma(\\nu/2)}\\int_{0}^{\\infty}\\frac{v^{(\\nu+1)/2-1}}{2^{(\\nu+1)/2}}e^{-(\\nu/2)(1+t^2/\\nu)}dv\n\\end{align}\n\\] We need to make the change of variables: \\(y=(1+t^2/\\nu)v\\)\nAnd we need to change dv: \\(\\frac{dv}{dy}=\\frac{1}{1+t^2/\\nu}\\)\n\\[\n\\begin{align}\nf(t)&=\\frac{\\Gamma[(\\nu+1)/2]}{\\sqrt{\\pi\\nu}\\Gamma(\\nu/2)}[\\frac{1}{(1+t^2/\\nu)^{(\\nu+1)/2}}]\\int_{0}^{\\infty}\\frac{y^{(\\nu+1)/2-1}}{\\Gamma[(\\nu+1)/2]2^{(\\nu+1)/2}}e^{-y/2}dy\n\\end{align}\n\\] This part \\(\\int_{0}^{\\infty}\\frac{y^{(\\nu+1)/2-1}}{\\Gamma[(\\nu+1)/2]2^{(\\nu+1)/2}}e^{-y/2}dy\\) is equal to 1, because this part is the whole area under the chi-square distribution with \\(\\nu+1\\) degrees of freedom. So, the pdf for T can be written as follows \\[\nf(t)=\\frac{\\Gamma[(\\nu+1)/2]}{\\sqrt{\\pi\\nu}\\Gamma(\\nu/2)}\\frac{1}{(1+t^2/\\nu)^{(\\nu+1)/2}}\n\\]\n\n\nF-distribution\n\n\nWe will do some trnsformation on chi-square distribution to get F-distribution\nAssuming that we have two independent random variables \\[\nX \\sim \\chi^2(n_1) \\quad and\\quad  Y \\sim \\chi^2(n_2)\\\n\\] Now,we will define a new variable F \\[\nF = \\frac{(X / n_1)}{(Y / n_2)}\n\\] This looks a little complex, so let’s do some simplification. \\[\nU = \\frac{X}{n_1} \\quad \\text{and} \\quad V = \\frac{Y}{n_2}\\\n\\]\nSo, \\(F = \\frac{U}{V}\\).\nBecause, X and Y are independent of each other. Obviously, U and V are also independent of each other.\n\n\\[\nf_{U,V}(u,v) = f_U(u) f_V(v)\n\\] \\[\nf_U(u) = \\frac{(n_1 u)^{n_1/2 - 1} e^{-n_1 u/2}}{2^{n_1/2} \\Gamma(n_1/2)}\n\\] \\[\nf_V(v) = \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)}\n\\] To find the joint density function of F & V, we use Jacobian transformation \\[\nJ =\\left| \\frac{\\partial(U,V)}{\\partial(F,V)} \\right| = \\left| \\begin{matrix}\n\\frac{\\partial U}{\\partial F} & \\frac{\\partial U}{\\partial V} \\\\\n\\frac{\\partial V}{\\partial F} & \\frac{\\partial V}{\\partial V}\n\\end{matrix} \\right| = \\left| \\begin{matrix}\nV & F \\\\\n0 & 1\n\\end{matrix} \\right| = V\n\\] \\[\nf_{F,V}(f,v) = f_{U,V}(u,v) \\left| \\frac{\\partial(u,v)}{\\partial(f,v)} \\right|= f_{U,V}(u,v)v\n\\] Then, we substitute\\(f_U(u)\\) and \\(f_V(v)\\) into \\(f_{F,V}(f,v)\\) \\[\nf_{F,V}(f,v) = \\frac{(n_1 u)^{n_1/2 - 1} e^{-n_1 u/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\n\\] Use the condition u=fv \\[\n\\begin{align}\nf_{F,V}(f,v) &= \\frac{(n_1 fv)^{n_1/2 - 1} e^{-n_1 fv/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\\\\\n& =\\frac{(n_1 f v)^{n_1/2 - 1} e^{-n_1 f v/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\\\\\n& = \\frac{n_1^{n_1/2-1} f^{n_1/2 - 1} v^{n_1/2 - 1} e^{-n_1 f v/2}}{2^{n_1/2} \\Gamma(n_1/2)} \\cdot \\frac{n_2^{n_2/2-1} v^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \\Gamma(n_2/2)} \\cdot v\\\\\n& = \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} v^{(n_1 + n_2)/2 - 1} e^{-(n_1 f + n_2) v/2}}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)}\n\\end{align}\n\\] We will integrate this density function with respect to V \\[\n\\begin{align}\nf_F(f) &= \\int_0^\\infty f_{F,V}(f,v) dv\\\\\n&= \\int_0^\\infty \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} v^{(n_1 + n_2)/2 - 1} e^{-(n_1 f + n_2) v/2}}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)} dv\\\\\n\\end{align}\n\\] Let’s do some substitutions to make it look simpler \\[\n\\begin{align}\nc = \\frac{n_1 f + n_2}{2}\n\\end{align}\n\\] This is the definition of Gamma function \\[\n\\int_0^\\infty v^{(n_1 + n_2)/2 - 1} e^{-c v} dv = \\frac{\\Gamma((n_1 + n_2)/2)}{c^{(n_1 + n_2)/2}}\n\\]\nThen \\[\nf_F(f) = \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1}}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)} \\cdot \\frac{\\Gamma((n_1 + n_2)/2)}{\\left(\\frac{n_1 f + n_2}{2}\\right)^{(n_1 + n_2)/2}}\n\\] Let’s rewrite it in an approximate F-distribution pdf form \\[\nf_F(f) = \\frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} \\Gamma((n_1 + n_2)/2)}{2^{(n_1 + n_2)/2} \\Gamma(n_1/2) \\Gamma(n_2/2)} \\cdot \\left(\\frac{2}{n_1 f + n_2}\\right)^{(n_1 + n_2)/2}\n\\]\n\n\nbeta and Gamma\nWhen considering the product of two Gamma functions \\[(\\Gamma(a) \\Gamma(b))\\], we can write it as two independent integrals:\n\\[\n\\Gamma(a) \\Gamma(b) = \\left( \\int_0^\\infty t^{a-1} e^{-t} \\, dt \\right) \\left( \\int_0^\\infty s^{b-1} e^{-s} \\, ds \\right)\n\\]\nTo convert this product into a double integral, we can use a change of variables. Consider using polar coordinates in the two independent integrals, which allows us to use certain integration techniques to simplify them. First, we transform the integration region from Cartesian coordinates to polar coordinates:\n\\[\nt = r \\cos \\theta, \\quad s = r \\sin \\theta\n\\]\nThe Jacobian determinant is \\[r\\], so the integral can be rewritten as:\n\\[\n\\Gamma(a) \\Gamma(b) = \\int_0^\\infty \\int_0^\\infty t^{a-1} e^{-t} s^{b-1} e^{-s} \\, dt \\, ds\n\\]\nUsing the polar coordinate transformation, the integral becomes:\n\\[\n= \\int_0^\\frac{\\pi}{2} \\int_0^\\infty (r \\cos \\theta)^{a-1} e^{-r \\cos \\theta} (r \\sin \\theta)^{b-1} e^{-r \\sin \\theta} r \\, dr \\, d\\theta\n\\]\nSeparating all \\[r\\] and \\[\\theta\\] related terms:\n\\[\n= \\int_0^\\frac{\\pi}{2} (\\cos \\theta)^{a-1} (\\sin \\theta)^{b-1} \\, d\\theta \\int_0^\\infty r^{a+b-1} e^{-r(\\cos \\theta + \\sin \\theta)} \\, dr\n\\]\nFirst, compute the \\[r\\] integral part:\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r(\\cos \\theta + \\sin \\theta)} \\, dr = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}}\n\\]\nNext, compute the \\[\\theta\\] integral part:\n\\[\n\\int_0^\\frac{\\pi}{2} (\\cos \\theta)^{a-1} (\\sin \\theta)^{b-1} \\, d\\theta = B(a, b)\n\\]\nwhere \\[B(a, b)\\] is the Beta function, defined as:\n\\[\nB(a, b) = \\int_0^1 t^{a-1} (1-t)^{b-1} \\, dt\n\\]\nTherefore, we get:\n\\[\n\\Gamma(a) \\Gamma(b) = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}} \\cdot B(a, b)\n\\]\nSince \\[\\cos \\theta + \\sin \\theta = 1\\] (in polar coordinates), we have:\n\\[\n\\Gamma(a) \\Gamma(b) = \\Gamma(a+b) B(a, b)\n\\]\nThus, the double integral form of the Gamma function product directly comes from the polar transformation and the application of the Beta function. This is an important mathematical technique used to simplify complex integrals and functional relationships.\n\n\nWhy is the integral result as shown?\nThe given integral,\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr,\n\\]\ncan be evaluated using the definition of the Gamma function. The Gamma function \\[\\Gamma(z)\\] is defined as:\n\\[\n\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} \\, dt.\n\\]\nTo see why the integral can be expressed in terms of the Gamma function, let’s rewrite the integral in a form that matches the Gamma function’s definition. The integral has the form:\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr.\n\\]\nLet’s set \\[t = r (\\cos \\theta + \\sin \\theta)\\], then \\[r = \\frac{t}{\\cos \\theta + \\sin \\theta}\\] and \\[dr = \\frac{dt}{\\cos \\theta + \\sin \\theta}\\]. Substituting these into the integral gives:\n\\[\n\\int_0^\\infty \\left( \\frac{t}{\\cos \\theta + \\sin \\theta} \\right)^{a+b-1} e^{-t} \\cdot \\frac{dt}{\\cos \\theta + \\sin \\theta}.\n\\]\nSimplifying inside the integral:\n\\[\n\\int_0^\\infty \\frac{t^{a+b-1}}{(\\cos \\theta + \\sin \\theta)^{a+b}} e^{-t} \\, dt.\n\\]\nSince \\[(\\cos \\theta + \\sin \\theta)^{a+b}\\] is a constant with respect to \\[t\\], it can be factored out of the integral:\n\\[\n\\frac{1}{(\\cos \\theta + \\sin \\theta)^{a+b}} \\int_0^\\infty t^{a+b-1} e^{-t} \\, dt.\n\\]\nThe integral\n\\[\n\\int_0^\\infty t^{a+b-1} e^{-t} \\, dt\n\\]\nis recognized as the Gamma function \\[\\Gamma(a+b)\\]. Thus, we have:\n\\[\n\\frac{1}{(\\cos \\theta + \\sin \\theta)^{a+b}} \\Gamma(a+b).\n\\]\nTherefore,\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}}.\n\\]\nThis demonstrates why the integral is evaluated as shown:\n\\[\n\\int_0^\\infty r^{a+b-1} e^{-r (\\cos \\theta + \\sin \\theta)} \\, dr = \\frac{\\Gamma(a+b)}{(\\cos \\theta + \\sin \\theta)^{a+b}}.\n\\] (method 2 for integral calculation: x=uv,y=u(1-v) J=-u)"
  },
  {
    "objectID": "aph101.html#three-main-sampling-distribution",
    "href": "aph101.html#three-main-sampling-distribution",
    "title": "APH101-Biostatistics And R",
    "section": "Three Main Sampling distribution",
    "text": "Three Main Sampling distribution\n\nChi-square distribution\nSuppose \\(X_1, \\cdots, X_n \\stackrel{\\text { i.i.d. }}{\\sim} \\mathcal{N}(0,1)\\).the distribution of the statistic \\[\nX_1^2+\\cdots+X_n^2\n\\] is called a chi-square distribution with \\(n\\) degrees of freedom, denoted by \\(\\chi^2(n)\\).\nBesides, random variable \\(X_i^2 \\sim \\operatorname{Gamma}\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\) corresponds to the chi-squared distribution with 1 degree of freedom, denoted as \\(\\chi_1^2\\).\nThis is derived by the MGF:\nSince \\[\nM_{X_1^2+\\cdots+X_n^2}(t)=M_{X_1^2}(t) \\times \\cdots \\times M_{X_n^2}(t)= \\begin{cases}\\infty & t \\geq \\frac{1}{2} \\\\ (1-2 t)^{-\\frac{n}{2}} & t&lt;\\frac{1}{2}\\end{cases}\n\\]\nThis is the MGF of the \\(\\operatorname{Gamma}\\left(\\frac{n}{2}, \\frac{1}{2}\\right)\\) distribution, so \\(X_1^2+\\cdots+X_n^2 \\sim \\operatorname{Gamma}\\left(\\frac{n}{2}, \\frac{1}{2}\\right)\\). This is called the chi-squared distribution with \\(\\mathbf{n}\\) degree of freedom, denoted \\(\\chi_n^2\\).\n\nProperties\nThe random variable \\(\\frac{(X-\\mu)^2}{\\sigma^2}\\) follows a \\(\\chi^2\\)-distribution with 1 degree of freedom when \\(X\\) follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\nIf \\(W_1, \\ldots, W_n\\) are independent \\(\\chi^2\\) random variables with, respectively, \\(v_1, \\cdots, v_n\\) degrees of freedom, then the random variable \\(W_1+\\cdots+W_n\\) follows a \\(\\chi^2\\)-distribution with \\(v_1+\\cdots+v_n\\) degree of freedom.\nThe random variable \\(\\frac{(\\bar{X}-\\mu)^2}{\\sigma^2 / n}\\) follows a \\(\\chi^2\\)-distribution with 1 degree of freedom when \\(X\\) follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\n\n\nCode to plot examples of Chi-square distribution\n\nlibrary(ggplot2)\n\n# Create a sequence of values\nx &lt;- seq(0, 20, length.out = 200)\n\n# Calculate the density for different degrees of freedom\ndf4 &lt;- dchisq(x, df = 4)\ndf8 &lt;- dchisq(x, df = 8)\ndf12 &lt;- dchisq(x, df = 12)\n\nchi_data &lt;- data.frame(x, df4, df8, df12)\nggplot(chi_data, aes(x)) +\n  geom_line(aes(y = df4, color = \"df=4\")) +\n  geom_line(aes(y = df8, color = \"df=8\")) +\n  geom_line(aes(y = df12, color = \"df=12\")) +\n  labs(title = \"Chi-Square Distribution\",\n       x = \"Value\",\n       y = \"Density\") +\n  scale_color_manual(name = \"Degrees of Freedom\", values = c(\"df=4\" = \"blue\",\"df=8\" = \"red\", \"df=12\" = \"green\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nApplication\nChi-square distribution is primarily used in testing:\n\nGoodness-of-fit\nIndependence in contingency tables\n\n\n\n\nStudent’s t-distribution\n\nConstruction\nThe statistic \\(T=\\frac{\\bar{X}-\\mu}{S / \\sqrt{n}}\\) follows a \\(t\\)-distribution with \\(v=n-1\\) degrees of freedom when \\(X_1, \\cdots, X_n\\) are i.i.d. normal RVs.\n\\[\n\\bar{X} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\quad \\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0,1) \\quad \\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi_{n-1}^2\n\\]\nIf we know the population variance \\(\\sigma^2\\), we can easily do inference using the statistic \\(\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}}\\). However, \\(\\sigma^2\\) is usually unknown in practice.\n\\[\n\\bar{X} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\quad \\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0,1) \\quad \\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi_{n-1}^2\n\\]\nWe can construct the \\(t\\)-statistic using the sample variance \\(S^2\\) : \\[\nT=\\frac{\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) s^2}{\\sigma^2} /(n-1)}}=\\frac{\\bar{X}-\\mu}{S / \\sqrt{n}}\n\\]\nNotice the sample mean \\(\\bar{X}\\) and the sample variance \\(S^2\\) are independent (the proof is beyond the scope of this course). So the \\(T\\) is now a ratio of a standard normal variable and the square root of a \\(\\chi^2 \\mathrm{RV}\\) divided by its degrees of freedom. This is the definition of a \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\n\n\nProperties\nThe \\(t\\)-distribution is primarily used in contexts where the underlying population is assumed to be normally distributed, especially when the sample size is small. Used extensively in problems that deal with inference about population mean \\(\\mu\\) when population variance \\(\\sigma^2\\) is unknown.\n\nproblems where one is trying to determine if means from two samples are significantly different when population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown.\n\n\n\nCode to plot examples of t-distribution\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a sequence of values\nx &lt;- seq(-4, 4, length.out = 1000)\n\n# Calculate the density for different degrees of freedom\ndf_values &lt;- c(1, 2, 3, 5, 10, 30)\ndistributions &lt;- lapply(df_values, function(df) dt(x, df = df))\n\n# Create a data frame to store the data\ndf_data &lt;- data.frame(x = x)\nfor (i in seq_along(df_values)) {\n  df_data[paste0(\"df\", df_values[i])] &lt;- distributions[[i]]\n}\n\n# Plot the densities\nggplot(df_data, aes(x = x)) +\n  geom_line(aes(y = df1, color = \"df=1\")) +\n  geom_line(aes(y = df2, color = \"df=2\")) +\n  geom_line(aes(y = df3, color = \"df=3\")) +\n  geom_line(aes(y = df5, color = \"df=5\")) +\n  geom_line(aes(y = df10, color = \"df=10\")) +\n  geom_line(aes(y = df30, color = \"df=30\")) +\n  geom_line(aes(y = dnorm(x), color = \"Standard Normal\")) +\n  scale_color_manual(name = \"Distribution\", \n                     values = c(\"df=1\" = \"red\", \"df=2\" = \"red\", \"df=3\" = \"red\", \n                                \"df=5\" = \"green\", \"df=10\" = \"green\", \"df=30\" = \"green\", \n                                \"Standard Normal\" = \"blue\")) +\n  labs(title = \"Density of the t-distribution compared to the standard normal distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nF-distribution\nLet \\(U\\) and \\(V\\) be two independent RVs following \\(\\chi^2\\) distributions with \\(\\nu_1\\) and \\(\\nu_2\\) degrees of freedom, respectively. Then the distribution of the random variable \\(F=\\frac{U / \\nu_1}{V / \\nu_2}\\) is known as \\(F\\)-distribution.\n\nExample\nIf \\(S_1^2\\) and \\(S_2^2\\) are the variances of independent RVs of size \\(n_1\\) and \\(n_2\\) taken from normal populations with variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) respectively, then \\[\nF=\\frac{S_1^2 / \\sigma_1^2}{S_2^2 / \\sigma_2^2}=\\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2}\n\\] follows an \\(F\\)-distribution with \\(\\nu_1=n_1-1\\) and \\(\\nu_2=n_2-1\\) degrees of freedom.\n\n\nCode to plot examples of F-distribution\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a sequence of values\nx &lt;- seq(0, 5, length.out = 1000)\n\n# Calculate the density for different degrees of freedom\ndf1_values &lt;- c(1, 2, 5, 10, 100)\ndf2_values &lt;- c(1, 1, 2, 1, 100)\ndistributions &lt;- lapply(1:length(df1_values), function(i) df(x, df1 = df1_values[i], df2 = df2_values[i]))\n\n# Create a data frame to store the data\ndf_data &lt;- data.frame(x = x)\nfor (i in seq_along(df1_values)) {\n  df_data[paste0(\"df1\", df1_values[i], \"_df2\", df2_values[i])] &lt;- distributions[[i]]\n}\n\n# Plot the densities\nggplot(df_data, aes(x = x)) +\n  geom_line(aes(y = df11_df21, color = \"d1=1, d2=1\")) +\n  geom_line(aes(y = df12_df21, color = \"d1=2, d2=1\")) +\n  geom_line(aes(y = df15_df22, color = \"d1=5, d2=2\")) +\n  geom_line(aes(y = df110_df21, color = \"d1=10, d2=1\")) +\n  geom_line(aes(y = df1100_df2100, color = \"d1=100, d2=100\")) +\n  scale_color_manual(name = \"\", \n                     values = c(\"d1=1, d2=1\" = \"red\", \"d1=2, d2=1\" = \"black\", \n                                \"d1=5, d2=2\" = \"blue\", \"d1=10, d2=1\" = \"green\", \n                                \"d1=100, d2=100\" = \"grey\")) +\n  labs(title = \"F-distribution with different degrees of freedom\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nApplication\nAnalysis of variance (ANOVA)"
  },
  {
    "objectID": "aph101.html#point-estimation",
    "href": "aph101.html#point-estimation",
    "title": "APH101-Biostatistics And R",
    "section": "Point Estimation",
    "text": "Point Estimation\nA point estimate of a population characteristic is a single number that is based on sample data and represents a plausible value of the characteristic."
  },
  {
    "objectID": "aph101.html#introduction",
    "href": "aph101.html#introduction",
    "title": "APH101-Biostatistics And R",
    "section": "Introduction",
    "text": "Introduction\n\nIf we were to construct a 95% confidence interval for some population characteristics (population proportion p or population mean \\(\\mu\\)), we would be using a method that is successful 95% of the time.\nThis is also about the question relevant to “How to choose a sample size”"
  },
  {
    "objectID": "aph101.html#definition-1",
    "href": "aph101.html#definition-1",
    "title": "APH101-Biostatistics And R",
    "section": "Definition",
    "text": "Definition\nA \\(100(1-\\alpha) \\%\\) confidence interval is an interval of the form \\(\\hat{\\theta}_L&lt;\\theta&lt;\\hat{\\theta}_U\\), where \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_U\\) are respectively values of \\(\\widehat{\\Theta}_L\\) and \\(\\widehat{\\Theta}_U\\) obtained for a particular sample, based on \\[\nP\\left(\\widehat{\\Theta}_L&lt;\\theta&lt;\\widehat{\\Theta}_U\\right)=1-\\alpha \\quad ; \\quad 0&lt;\\alpha&lt;1\n\\] in the estimation of population parameter \\(\\theta\\)."
  },
  {
    "objectID": "aph101.html#interpretation",
    "href": "aph101.html#interpretation",
    "title": "APH101-Biostatistics And R",
    "section": "Interpretation",
    "text": "Interpretation\nFor confidence level of 95%, t for any normal distribution: About 95% of the values are within 1.96 standard deviations of the mean. (Recall the concept of Z-scores)\nThat is, if this method was used to generate an interval estimate over and over again with different samples, in the long run 95% of the resulting intervals would include the actual value of the characteristic being estimated.\n\nThe confidence level 95% refers to the method used to construct the interval rather than to any particular interval, such as the one we obtained.\n\n\neg\n\nProportion\n\ncheck to make sure that the three necessary conditions are met:\n\n\\(n\\hat p \\ge 10, n(1-\\hat p) \\ge 10\\)\n\\(\\frac {\\hat p -p }{\\sigma/\\sqrt n} = 1.96\\))\n\nsample size question\n\nUsing sample proportion with 95% confidence interval, and we want ME no more than 5%, we may be interested in solving n from the following equation:\n0.05=1.96 \\(\\sqrt {\\frac {\\hat p(1-\\hat p) }{ n}}\\)\n\n\nCI on Mean\nHere, \\(\\bar x\\) is the sample mean from a simple random sample.\n\\(\\mu\\) is the population mean which we are interested in estimating.\n\nCI on \\(\\mu\\) with \\(\\sigma\\) known n \\(\\ge 30\\) or the population is normal — Use z-statistics\nCI: \\(\\bar x \\pm z_{\\alpha/2} \\frac {\\sigma}{\\sqrt n}\\), for example, 95% CI is \\(\\bar x \\pm 1.96 \\frac {\\sigma}{\\sqrt n}\\)\n\n\nOne-side Confidence Bound on \\(\\mu\\) with \\(\\sigma\\) known n \\(\\ge 30\\) or the population is normal — Use z-statistics\nUpper one-side bound: \\(\\mu &lt;\\bar x + z_{\\alpha} \\frac {\\sigma}{\\sqrt n}\\)\nLower one-side bound: \\(\\mu &gt;\\bar x - z_{\\alpha} \\frac {\\sigma}{\\sqrt n}\\)\nFor example, 95% Confidence bound on \\(\\mu\\) is \\(\\bar x \\pm 1.645 \\frac {\\sigma}{\\sqrt n}\\)\n\n\nCI on \\(\\mu\\) with \\(\\sigma\\) unknown and the population is normal — Use t-statistics (use s as the estimate for σ (t-statistics with df = n-1))\nCI: \\(\\bar x \\pm t_{\\alpha/2} \\frac {s}{\\sqrt n}\\), for example, 95% CI is \\(\\bar x \\pm t_{0.025} \\frac {s}{\\sqrt n}\\) and df = n-1\nRemark: The distribution of t is more spread out than the standard normal distribution but when n \\(\\ge 30\\), t and z are very close to each other.\n\n\nCI for \\(\\mu_1 - \\mu_2\\), both \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are known\nCI of \\(\\mu_1-\\mu_2\\): \\((\\bar x_1 - \\bar x_2) \\pm z_{\\alpha/2} \\sqrt {\\frac {\\sigma_1^2}{n_1} + \\frac {\\sigma_2^2}{n_2}}\\)\n\n\nCI for \\(\\mu_1 - \\mu_2\\), both \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown but assumed equal\nCI of \\(\\mu_1-\\mu_2\\): \\((\\bar x_1 - \\bar x_2) \\pm t_{\\alpha/2} s_p \\sqrt {\\frac {1}{n_1} + \\frac {1}{n_2}}\\) with df = \\(n_1+n_2-2\\) where \\(s_p = \\sqrt {\\frac {(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\n\n\nCI for paired observations\nPrevious, we have two independent samples, now we have two dependent samples. We can use the difference between the two samples to construct a confidence interval.\nCI of \\(\\mu_d\\): \\(\\bar d \\pm t_{\\alpha/2} \\frac {s_d}{\\sqrt n}\\) with df = n-1 where \\(s_d\\) is the sample standard deviation of the differences \\(d_i = x_{1i} - x_{2i}\\) and \\(\\bar d\\) is the sample mean of the differences.\n\n\nEstimating \\(\\sigma\\)\na \\(100(1-\\alpha)\\%\\) CI for \\(\\sigma^2\\) is \\(\\left(\\frac{(n-1) S^2}{\\chi_{\\alpha / 2, n-1}^2}, \\frac{(n-1) S^2}{\\chi_{1-\\alpha / 2, n-1}^2}\\right)\\)\nwhere \\(S^2\\) is the sample variance and \\(\\chi_{\\alpha / 2, n-1}^2\\) and \\(\\chi_{1-\\alpha / 2, n-1}^2\\) are the critical values of the chi-square distribution with \\(n-1\\) degrees of freedom.\n\n\nEstimating \\(\\sigma_1^2/ \\sigma_2^2\\)\nA \\(100(1-\\alpha)\\%\\) CI for \\(\\frac{\\sigma_1^2}{\\sigma_2^2}\\) using F-statistics with \\(f_{1-\\alpha/2}(n_1-1,n_2-1)=1/f_{\\alpha/2}(n_1-1,n_2-1)\\) is \\[\n\\frac{s_1^2}{s_2^2} \\frac{1}{f_{\\alpha / 2}\\left(n_1-1, n_2-1\\right)}&lt;\\frac{\\sigma_1^2}{\\sigma_2^2}&lt;\\frac{s_1^2}{s_2^2} f_{\\alpha / 2}\\left(n_2-1, n_1-1\\right)\n\\] where \\(f_{\\alpha / 2}\\left(v_1, v_2\\right)\\) is an \\(F\\)-value with \\(v_1\\) and \\(v_2\\) degrees of freedom, leaving an area of \\(\\alpha / 2\\) to the right, and \\(f_{\\alpha / 2}\\left(v_2, v_1\\right)\\) is a similar \\(F\\)-value with \\(v_2\\) and \\(v_1\\) degrees of freedom."
  },
  {
    "objectID": "aph101.html#example-of-one-sample-t-test",
    "href": "aph101.html#example-of-one-sample-t-test",
    "title": "APH101-Biostatistics And R",
    "section": "Example of one-sample t-test",
    "text": "Example of one-sample t-test\nA marine biologist is studying a species of fish known to have an average length of 20 cm in ocean populations. A new population in a freshwater lake is being analyzed to determine if the environmental differences have altered the fish’s average length. The biologist measures the lengths of 10 randomly selected fish, yielding the following data:\n22, 23, 21, 24, 22, 20, 25, 19, 23, 22\nAssuming the data satisfy the assumption of normality, please address the following using a significance level of 0.1:\n\na\n\nnull hypothesis: The mean length of fish is 20 cm (\\(H_0: \\mu = 20\\)).\nalternative hypothesis: The mean length of fish is not 20 cm (\\(H_1: \\mu \\ne 20\\)).\n\n\n\nb\nSince the data is supposed to be normally distributed, the sampling distribution of the sample mean follows t-distribution. The t-test statistic is calculated as follows:\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nThe t-test statistic is calculated as follows:\n\ndata2_4 &lt;- c(22, 23, 21, 24, 22, 20, 25, 19, 23, 22)\n\nx_bar &lt;- mean(data2_4)\n\ns &lt;- sd(data2_4)\n\nt &lt;- (x_bar - 20) / (s / sqrt(length(data2_4)))\n\nt\n\n[1] 3.705882\n\n\nThe t-test statistic is 3.705882…\n\n\nc\nUsing pt() function, the p-value is calculated as follows:\n\np_value &lt;- 2 * pt(-t, df = 9)\np_value\n\n[1] 0.004875954\n\n\nThe p-value is approximately 0.1. Since the p-value is less than 0.1, we reject the null hypothesis.\nTherefore, there is sufficient evidence to conclude that the population mean is not equal to 20 which means the environmental differences have altered the fish’s average length.\n\n\nd\nUsing qt() function to find the critical value for a two-tailed test with 90% confidence level:\n\nt_critical &lt;- qt(0.95, df = 9)\nt_critical\n\n[1] 1.833113\n\n\nThe critical value for a two-tailed test with 90% confidence level is about 1.833113.\nSince the t-test statistic 3.705882 is greater than the critical value 1.833113, which is in the critical region. Therefore, we reject the null hypothesis.\nAlso, we could use confidence interval to verify the result. The 90% confidence interval for the population mean is calculated as follows:\n\nci_4 &lt;- c(x_bar - t_critical * s / sqrt(10), x_bar + t_critical * s / sqrt(10))\nci_4\n\n[1] 21.06124 23.13876\n\n\nSo the 90% confidence interval for the population mean is about (21.1, 23.2).\nThe confidence interval does not contain the hypothesized value 20. Therefore, we reject the null hypothesis.\nTherefore, there is sufficient evidence to conclude that the population mean is not equal to 20 which means the environmental differences have altered the fish’s average length."
  },
  {
    "objectID": "aph101.html#example-of-unpaired-t-test",
    "href": "aph101.html#example-of-unpaired-t-test",
    "title": "APH101-Biostatistics And R",
    "section": "Example of unpaired t-test",
    "text": "Example of unpaired t-test\nSuppose \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are unknown but assumed equal. We want to test the null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) against the alternative hypothesis \\(H_1: \\mu_1 \\ne \\mu_2\\). The test statistic is given by\n\\[\nt = \\frac{\\bar x_1 - \\bar x_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\nwhere \\(s_p\\) is the pooled sample standard deviation, given by\n\\[\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\]"
  },
  {
    "objectID": "aph101.html#example-of-one-sample-variance-test",
    "href": "aph101.html#example-of-one-sample-variance-test",
    "title": "APH101-Biostatistics And R",
    "section": "Example of one-sample Variance Test",
    "text": "Example of one-sample Variance Test\n\\(\\chi^2\\)-test for variance. Suppose \\(X_1, \\cdots, X_n\\) are i.i.d. normal random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We want to test the null hypothesis \\(H_0: \\sigma^2 = \\sigma_0^2\\) against the alternative hypothesis \\(H_1: \\sigma^2 \\ne \\sigma_0^2\\). The test statistic is given by\n\\[\n\\chi^2 = \\frac{(n - 1)s^2}{\\sigma^2}\n\\]"
  },
  {
    "objectID": "aph101.html#example-of-two-sample-variance-test",
    "href": "aph101.html#example-of-two-sample-variance-test",
    "title": "APH101-Biostatistics And R",
    "section": "Example of two-sample Variance Test",
    "text": "Example of two-sample Variance Test\n\n\n\n\n\n\n\n\n\n\\(H_0\\)\nTest Statistic\n\\(H_1\\)\nRejection Region\n\n\n\n\n\\(\\sigma_1^2=\\sigma_2^2\\)\n\\(f=\\frac{s_1^2}{s_2^2}\\)\n\\(\\sigma_1^2&lt;\\sigma_2^2\\)\n\\(f&lt;f_\\alpha\\left(\\nu_1, \\nu_2\\right)\\)\n\n\n\n\n\\(\\sigma_1^2&gt;\\sigma_2^2\\)\n\\(f&gt;f_{1-\\alpha}\\left(\\nu_1, \\nu_2\\right)\\)\n\n\n\n\\(\\sigma_1^2 \\neq \\sigma_2^2\\)\n\\(f&lt;f_{\\alpha / 2}\\left(\\nu_1, \\nu_2\\right)\\) or \\(f&gt;f_{1-\\alpha / 2}\\left(\\nu_1, \\nu_2\\right)\\)\n\n\n\n\\(\\nu_1=n_1-1\\) and \\(\\nu_2=n_2-1\\) are two degree of freedom.\n\n\n\n\n\n\n\\[\nF=\\frac{\\frac{\\left(n_1-1\\right) S_1^2}{\\sigma_1^2} /\\left(n_1-1\\right)}{\\frac{\\left(n_2-1\\right) S_2^2}{\\sigma_2^2} /\\left(n_2-1\\right)}=\\frac{\\sigma_2^2 S_1^2}{\\sigma_1^2 S_2^2}\n\\]\nIf \\(\\sigma_1^2=\\sigma_2^2\\), we have \\[\nF=\\frac{S_1^2}{S_2^2} \\sim F_{n_1-1, n_2-1}\n\\]"
  },
  {
    "objectID": "aph101.html#anova-analysis-of-variance",
    "href": "aph101.html#anova-analysis-of-variance",
    "title": "APH101-Biostatistics And R",
    "section": "ANOVA– Analysis of Variance",
    "text": "ANOVA– Analysis of Variance\n\none-way ANOVA\n\nwe need to test the null hypothesis that the group population means are all the same against the alternative that at least one group population mean differs from the others. That is,\n\\(H_0: \\mu_1=\\mu_2=\\cdots=\\mu_k\\) against \\(H_1: \\text { at least one } \\mu_i \\text { differs from the others.}\\)\nANOVA Table\n\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSum Sq\nMean Sq\nF value\np value\n\n\n\n\nFactor\nm-1\n11.84 (SS between)\n2.9587 (MSB)\n8.074 (MSB/MSW)\n\\(5.38 \\mathrm{e}-05\\) (p-value)\n\n\nError\nn-m\n16.49 (SS Within)\n0.3664 (MSW)\n\n\n\n\nTotal\nn-1\n28.33 (SS Total)\n\n\n\n\n\n\nSource means “the source of the variation in the data.” the possible sources for a one-factor study are Factor, Residuals, and Total.\nFactor means “the variability due to the factor of interest.” In the drug example, the factor was the different drug. In the learning example on the previous page, the factor was the method of learning. Sometimes the row heading is labeled as Between.\nError (or Residuals) means “the variability within the groups” or “unexplained random error.” Sometimes the row heading is labeled as Within.\nTotal means “the total variation in the data from the grand mean”.\nDF means “the degrees of freedom in the source.”\nSum Sq means “the sum of squares due to the source.”\nMean Sq means “the mean sum of squares due to the source.”\nF value means “the F-statistic.”\nP value means “the P-value.”\nSS(Total)=SS(Between)+SS(Within), where\nSS(Between) is the sum of squares between the group means and the grand mean. As the name suggests, it quantifies the variability between the groups of interest.\nSS(Within) is the sum of squares between the data and the group means. It quantifies the variability within the groups of interest.\nSS(Total) is the sum of squares between the n data points and the grand mean. As the name suggests, it quantifies the total variability in the observed data.\n\ntwo-way ANOVA\n\nWe can extend the idea of a one-way ANOVA, which tests the effects of one factor on a response variable, to a two-way ANOVA which tests the effects of two factors and their interaction on a response variable.\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSum Sq\nMSW\nF\n\n\n\n\nCells\n\\(a b-1\\)\n\\(\\sum_{i=1}^a \\sum_{j=1}^b n\\left(\\bar{X}_{i j}-\\bar{X}_{\\ldots . .}\\right)^2\\)\n\n\n\n\nA\na-1\nbn \\(\\sum_{i=1}^a\\left(\\bar{X}_{i . .}-\\bar{X}_{\\ldots .}\\right)^2\\)\nSS(A)\nMS(Error\n\n\nB\nb-1\nan \\(\\sum_{j=1}^b\\left(\\bar{X}_{. j .}-\\bar{X}_{\\ldots .}\\right)^2\\)\nSS(B)\nMS(B)\n\n\n\\(\\mathrm{A} \\times \\mathrm{B}\\)\n\\((a-1)(b-1)\\)\nSS(Cells)-SS(A)-SS(B)\nSS(AB)\nMS(Error\n\n\n\n\n\nDF(A \\(\\times\\) B)\nMS(Error\n\n\nError\n\\(a b(n-1)\\)\n\\(\\sum_{i=1}^a \\sum_{j=1}^b \\sum_{l=1}^n\\left(X_{i j l}-\\bar{X}_{i j} .\\right)^2\\)\nSS(Error)\n\n\n\nTotal\n\\(a b n-1\\)\n\\(\\sum_{i=1}^a \\sum_{j=1}^b n\\left(X_{i j l}-\\bar{X}_{\\ldots .}\\right)^2\\)\n\n\n\n\n\n\n\\(F=\\frac{\\mathrm{MS}(\\mathrm{A})}{\\mathrm{MS}(\\text { Error })}\\), for \\(H_0\\) : no effect of factor A on response variable,\n\\(F=\\frac{\\mathrm{MS}(\\mathrm{B})}{\\mathrm{MS}(\\text { Error) }}\\), for \\(H_0\\) : no effect of factor B on response variable,\n\\(F=\\frac{\\mathrm{MS}(\\mathrm{A} \\times \\mathrm{B})}{\\mathrm{MS}(\\text { Error })}\\), for \\(H_0\\) : no effect of interaction on response variable.\n\nWe reject any \\(H_0\\) if \\(F \\geq F_{\\text {critical }}\\); otherwise, we do not reject \\(H_0\\).\n\nExample of two-way ANOVA\nTwo-way ANOVA. In this question, we will use the built-in R data set ToothGrowth to perform two-way ANOVA test. ToothGrowth includes information from a study on the effects of vitamin C on tooth growth in Guinea pigs. The response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC). Assuming the data satisfy the assumptions of normality and equal variance, please address the following using a significance level of 0.05\n\na\n\nThe effects of vitamin C on tooth growth in guinea pigs:\nnull hypothesis: \\(H_0\\): mean tooth growth for all doses of vitamin C are equal\nalternative hypothesis: \\(H_1\\): at least one of the means of all doses of vitamin C is different from the others\nThe effects of delivery method on tooth growth in guinea pigs:\nnull hypothesis: \\(H_0\\): mean tooth growth for the delivery method of orange juice and ascorbic acid are equal.\nalternative hypothesis \\(H_1\\): mean tooth growth for the delivery method of orange juice and ascorbic acid are different.\nThe interaction effects of the dose of vitamin C and delivery method on tooth growth in guinea pigs:\nnull hypothesis: \\(H_0:\\) there is no interaction between the dose of vitamin C and delivery method on tooth growth in guinea pigs, meaning that the relationship between vitamin C and tooth growth is the same for both delivery methods (similarly, the relationship between delivery method and tooth growth is the same for all doses of vitamin C).\nalternative hypothesis: \\(H_1\\): there is an interaction between the dose vitamin C and delivery method on tooth growth in guinea pigs, meaning that the relationship between vitamin C and tooth growth is different for both delivery methods (similarly, the relationship between delivery method and tooth growth depends on the dose of vitamin C).\n\n\n\nb\nWe can plot the relationship one by one using two plots\n\nlibrary(ggplot2)\ndata(ToothGrowth)\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n# potential effects of vitamin C on tooth growth.\n\nggplot(ToothGrowth, aes(x = factor(dose), y = len)) +\n  geom_boxplot() +\n  labs(x = \"Dose (mg/day)\", y = \"Tooth Growth (len)\", title = \"Tooth Growth by Dose of vitamin C\")\n\n\n\n\n\n\n\n# potential effects of delivery method on tooth growth.\n\nggplot(ToothGrowth, aes(x = supp, y = len)) +\n  geom_boxplot() +\n  labs(x = \"Delivery Method\", y = \"Tooth Growth (len)\", title = \"Tooth Growth by Delivery Method\") \n\n\n\n\n\n\n\n\nor just one:\n\nlibrary(ggplot2)\n\n# potential effects of vitamin C and delivery method. \n\n# OJ represents orange juice and VC represents ascorbic acid.\n\nggplot(ToothGrowth, aes(x = factor(dose), y = len, fill = supp)) +\n  geom_boxplot() +\n  labs(x = \"Dose (mg/day)\", y = \"Tooth Growth (len)\", title = \"Tooth Growth by Dose and Delivery Method\") \n\n\n\n\n\n\n\n\n\n\nc\n\n# Perform two-way ANOVA\n\nanova_result &lt;- aov(len ~ supp * dose, data = ToothGrowth)\n\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsupp         1  205.4   205.4  12.317 0.000894 ***\ndose         1 2224.3  2224.3 133.415  &lt; 2e-16 ***\nsupp:dose    1   88.9    88.9   5.333 0.024631 *  \nResiduals   56  933.6    16.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince all p-values are less than 0.05, we reject all null hypotheses. Therefore, there is sufficient evidence to conclude that the dose of vitamin C, delivery method, and their interaction have significant effects on tooth growth in guinea pigs.\n\n\nd\n\nlibrary(car)\n\nLoading required package: carData\n\nqqPlot(anova_result$residuals, main = \"QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n[1] 22 50"
  },
  {
    "objectID": "aph101.html#non-parametric-tests",
    "href": "aph101.html#non-parametric-tests",
    "title": "APH101-Biostatistics And R",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\n\nApplication of testing the goodness of fit\nTesting whether there is a “good fit” between the observed data and the assumed probability model amounts to testing:\n\nConstruction of test statistics with an example of 2 categories\nPopulation is 60% female and 40% male. Then, if a sample of 100 students yields 53 females and 47 males, can we conclude that the sample is (random and) representative of the population? That is, how “good” do the data “fit” the assumed probability model of 60% female and 40% male?\nHere, let \\(Y_1\\) denote the number of females selected, \\(Y_1 \\sim B(n,p_1)\\) and let \\(Y_2\\) denote males selected, \\(Y_2 = (n-Y_1)\\sim B(n,p_2)=B(n,1-p_1)\\).\nfor samples satisfying the general rule of thumb (the expected number of successes must be at least 5 and the expected number of failures must be at least 5), we can use the normal approximation to the binomial distribution. The test statistic is given by\n\\[\nZ = \\frac{Y_1 - np_1}{\\sqrt{np_1(1-p_1)}}\\sim N(0,1)\n\\]\nwhich is at least approximately normally distributed.\nand \\[\nZ^2 =Q_1= \\frac{(Y_1 - np_1)^2}{np_1(1-p_1)}\\sim \\chi^2(1)\n\\]\nwhich is an approximate chi-square distribution with one degree of freedom.\nNow we can multiply \\(Q_1\\) by 1 = \\((1-p_1)+p_1\\) to get\n\\[\nQ_1 = \\frac{(Y_1 - np_1)^2(1-p_1)}{np_1(1-p_1)} + \\frac{(Y_1 - np_1)^2p_1}{np_1(1-p_1)}\\sim \\chi^2(1)\n\\]\nSince \\(Y_1=n-Y_2\\) and \\(p_1=1-p_2\\), after simplifying, we have\n\\[\nQ_1 = \\frac{(Y_1-np_1)^2}{np_1} + \\frac{(-(Y_2-np_2))^2}{np_2}\\sim \\chi^2(1)\n\\]\nwhich is \\(Q_1=\\sum_{i=1}^2 \\frac{\\left(Y_i-n p_i\\right)^2}{n p_i}=\\sum_{i=1}^2 \\frac{(\\text { Observed }- \\text { Expected })^2}{\\text { Expected }}\\sim \\chi^2(1)\\)\nHence, it is observed that if the observed counts are very different from the expected counts, then the test statistic will be large. So we reject the null hypothesis if \\(Q_1\\) is large and how large is large is determined by the critical value of the chi-square distribution with one degree of freedom.\nThe statistics \\(Q_1\\) is called the chi-square goodness of fit statistic.\nGoing back to the example,\n\n\\(H_0\\):\\(p_F = 0.6\\)\n\\(H_1\\):\\(p_F \\ne 0.6\\)\n\nwe can calculate the test statistic using a significant level of \\(\\alpha = 0.05\\) (\\(\\chi^2_{0.05,1}=3.84\\))as follows:\n\\[\nQ_1 = \\frac{(53-60)^2}{60} + \\frac{(47-40)^2}{40} = 2.04\n\\]\nSince \\(Q_1=2.04&lt;3.84\\), we do not reject the null hypothesis. Therefore, we conclude that the sample is (random and) representative of the population.\nThis can be extended to k categories\n\n\nConstruction of test statistics with an example of k categories\nFor categories more than 2, i.e.\n\n\n\n\n\n\n\n\n\n\n\nCategories\n1\n2\n\\(\\cdots\\)\n\\(k-1\\)\n\\(k\\)\n\n\n\n\nObserved\n\\(Y_1\\)\n\\(Y_2\\)\n\\(\\cdots\\)\n\\(Y_{k-1}\\)\n\\(n-Y_1-Y_2-\\cdots-Y_{k-1}\\)\n\n\nExpected\n\\(n p_1\\)\n\\(n p_2\\)\n\\(\\cdots\\)\n\\(n p_{k-1}\\)\n\\(n p_k\\)\n\n\n\nKarl Pearson showed that the chi-square statistic \\(Q_{k-1}\\) defined as: \\[\nQ_{k-1}=\\sum_{i=1}^k \\frac{\\left(Y_i-n p_i\\right)^2}{n p_i}\n\\] follows approximately a chi-square random variable with \\(k-1\\) degrees of freedom. Let’s try it out on an example.\n\nExample:\n\n\n\n\nCategories\nBrown\nYellow\nOrange\nGreen\nCoffee\nTotal\n\n\n\n\nObserved \\(y_i\\)\n224\n119\n130\n48\n59\n580\n\n\nAssumed \\(H_0\\left(p_i\\right)\\)\n0.4\n0.2\n0.2\n0.1\n0.1\n1.0\n\n\nExpected \\(n p_i\\)\n232\n116\n116\n58\n58\n580\n\n\n\n\\(Q_4=\\frac{(224-232)^2}{232}+\\frac{(119-116)^2}{116}+\\frac{(130-116)^2}{116}+\\frac{(48-58)^2}{58}+\\frac{(59-58)^2}{58}=3.784\\)\nBecause there are \\(k=5\\) categories, we have to compare our chisquare statistic \\(Q_4\\) to a chi-square distribution with \\(k-1=5-1=4\\) degrees of freedom: \\[\nQ_4=3.784&lt;\\chi_{4,0.05}^2=9.488\n\\] we fail to reject the null hypothesis.\n\n\n\nApplication of testing for homogeneity\nThis is to look at a method for testing whether two or more multinomial distributions are equal.\n\nExample:\n\nTest the hypothesis that the acceptances of males and females are ditributed equally among the four schools,\n\n\n\n(Acceptances)\nBus\nEng\nL Arts\nSci\n(FIXED) Total\n\n\n\n\nMale\n240 (20%)\n480 (40%)\n120 (10%)\n360 (30%)\n1200\n\n\nFemale\n240 (30%)\n80 (10%)\n320 (40%)\n160 (20%)\n800\n\n\nTotal\n480 (24%)\n560 (28%)\n440 (22%)\n520 (26%)\n2000\n\n\n\nHere,\n\\(H_0: p_{M B}=p_{F B}, p_{M E}=p_{F E}, p_{M L}=p_{F L}\\), and \\(p_{M S}=p_{F S}\\)\n\\(H_1: p_{M B} \\neq p_{F B}\\) or \\(p_{M E} \\neq p_{F E}\\) or \\(p_{M L} \\neq p_{F L}\\), or \\(p_{M S} \\neq p_{F S}\\)\nwhere:\n\n\\(p_{M j}\\) is the proportion of males accepted into school \\(j=B, E, L, S\\).\n\\(p_{F j}\\) is the proportion of females accepted into school \\(j=B, E, L, S\\).\n\nIn conducting such a hypothesis test, we’re comparing the proportions of two multinomial distributions.\n\n\n\n\n\n\n\n\n\n\n\n#(Acc)\nBus ( \\(j=1\\) )\nEng ( \\(j=2\\) )\nL Arts ( \\(j=3\\) )\nSci \\((j=4)\\)\n(FIXED) Total\n\n\n\n\n\\(\\mathrm{M}(i=1)\\)\n\\(y_{11}\\left(\\hat{p}_{11}\\right)\\)\n\\(y_{12}\\left(\\hat{p}_{12}\\right)\\)\n\\(y_{13}\\left(\\hat{p}_{13}\\right)\\)\n\\(y_{14}\\left(\\hat{p}_{14}\\right)\\)\n\\(n_1=\\sum_{j=1}^k y_{1 j}\\)\n\n\nF \\((i=2)\\)\n\\(y_{21}\\left(\\hat{p}_{21}\\right)\\)\n\\(y_{22}\\left(\\hat{p}_{22}\\right)\\)\n\\(y_{23}\\left(\\hat{p}_{23}\\right)\\)\n\\(y_{24}\\left(\\hat{p}_{24}\\right)\\)\n\\(n_2=\\sum_{j=1}^k y_{2 j}\\)\n\n\nTotal\n\\(y_{11}+y_{21}\\left(\\hat{p}_1\\right)\\)\n\\(y_{12}+y_{22}\\left(\\hat{p}_2\\right)\\)\n\\(y_{13}+y_{23}\\left(\\hat{p}_3\\right)\\)\n\\(y_{14}+y_{24}\\left(\\hat{p}_4\\right)\\)\n\\(n_1+n_2\\)\n\n\n\nThe chi-square test statistic for testing the equality of two multinomial distributions: \\[\nQ=\\sum_{i=1}^2 \\sum_{j=1}^k \\frac{\\left(y_{i j}-n_i \\hat{p}_j\\right)^2}{n_i \\hat{p}_j}\n\\] follows an approximate chi-square distribution with \\(k-1\\) degrees of freedom. Reject the null hypothesis of equal proportions if \\(Q\\) is large (since if male and female distributed nearly equally, the expected number of each should be \\(n_i\\hat p_j\\)): \\[\nQ \\geq \\chi_{\\alpha, k-1}^2\n\\]\n(omit the derive of the above Q)\nGenerally,\n\\[\nQ=\\sum_{i=1}^h \\sum_{j=1}^k \\frac{\\left(y_{i j}-n_i \\hat{p}_j\\right)^2}{n_i \\hat{p}_j}\\sim \\chi^2_{(h-1)(k-1)}\n\\]\n\nFurther example\nThe head of a surgery department at a university medical center was concerned that surgical residents in training applied unnecessary blood transfusions at a different rate than the more experienced attending physicians. Therefore, he ordered a study of the 49 Attending Physicians and 71 Residents in Training with privileges at the hospital. For each of the 120 surgeons, the number of blood transfusions prescribed unnecessarily in a one-year period was recorded. Based on the number recorded, a surgeon was identified as either prescribing unnecessary blood transfusions Frequently, Occasionally, Rarely, or Never. Here’s a summary table (or “contingency table”) of the resulting data:\n\n\n\nPhysician\nFrequent\nOccasionally\nRarely\nNever\nTotal\n\n\n\n\nAttending\n6.942\n12.658\n22.05\n7.35\n49\n\n\nResident\n10.058\n18.342\n31.95\n10.65\n71\n\n\nTotal\n17\n31\n54\n18\n120\n\n\n\nHere,\n\\(H_0: p_{R F}=p_{A F}, p_{R O}=p_{A O}, p_{R R}=p_{A R}\\), and \\(p_{R N}=p_{A N}\\)\n\\(H_1: p_{R F} \\neq p_{A F}\\) or \\(p_{R O} \\neq p_{A O}\\) or \\(p_{R R} \\neq p_{A R}\\), or \\(p_{R N} \\neq p_{A N}\\)\nWe should also calculate the expected counts under the null hypothesis. The expected counts are calculated as follows:\n\n\n\nPhysician\nFrequent\nOccasionally\nRarely\nNever\nTotal\n\n\n\n\nAttending\n6.942\n12.658\n22.05\n7.35\n49\n\n\nResident\n10.058\n18.342\n31.95\n10.65\n71\n\n\nTotal\n17\n31\n54\n18\n120\n\n\n\nwhere, for example, \\(6.942=\\frac{17}{120} \\times 49\\) and \\(10.058=\\frac{17}{120} \\times 71\\). Now that we have the observed and expected counts, calculating the chisquare statistic is a straightforward exercise: \\[\nQ=\\frac{(2-6.942)^2}{6.942}+\\cdots+\\frac{(5-10.65)^2}{10.65}=31.88\n\\]\nThe chi-square test tells us to reject the null hypothesis, at the 0.05 level, if \\(Q\\) is greater than a chi-square random variable with 3 degrees of freedom, that is, if \\(Q=31.88&gt;7.815\\), we reject the null hypothesis.\n\n\n\nApplication of testing for independence\nThis is to look at whether two or more categorical variables are independent.\n(previously,the the sampling scheme involves: Taking two random (and therefore independent) samples with n1 and n2 fixed in advance and observing into which of the k categories the first random samples fall, and observing into which of the k categories the second random samples fall. )\nlets consider a different example to illustrate an alternative sampling scheme. Suppose 395 people are randomly selected, and are “cross-classified” into one of eight cells, depending into which age category they fall and whether or not they support legalizing marijuana:\n(the sampling scheme involves: Taking one random sample of size n, with n fixed in advance, and then “cross-classifying” each subject into one and only one of the mutually exclusive and exhaustive \\(A_i\\cap B_j\\) cells.)\n\n\n\n\n\n\n\n\n\n\n\n\nMarijuana Support\n\nVariable B (Age)\n\n\n\n\n\n\n\n\nVariable A\nOBSERVED\n\\((18-24) B_1\\)\n(25-34) \\(B_1 2\\)\n(35-49) \\(B_3\\)\n\\((50-64) B_4\\)\nTotal\n\n\n\n(YES) \\(A_1\\)\n60\n54\n46\n41\n201\n\n\n\n(NO) \\(A_2\\)\n40\n44\n53\n57\n194\n\n\n\nTotal\n100\n98\n99\n98\n\\(n=395\\)\n\n\n\nHere,\nH0 : Variable A is independent of variable B, that is \\(P(A_i\\cap B_j)=PA_i \\times B_j\\) for all i and j\nH1: Variable A is not independent of variable B.\nGenerally,\nSuppose we have \\(k\\) (column) levels of Variable B indexed by the letter \\(j\\), and \\(h\\) (row) levels of Variable \\(A\\) indexed by the letter \\(i\\). Then, we can summarize the data and probability model in tabular format, as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable B\n\n\n\n\n\n\n\n\n\n\nVariable A\n\\(B_1(j=1)\\)\n\\(B_2(j=2)\\)\n\\(B_3(j=3)\\)\n\\(B_4(j=4)\\)\nTotal\n\n\n\n\\(A_1(i=1)\\)\n\\(Y_{11}\\left(p_{11}\\right)\\)\n\\(Y_{12}\\left(p_{12}\\right)\\)\n\\(Y_{13}\\left(p_{13}\\right)\\)\n\\(Y_{14}\\left(p_{14}\\right)\\)\n\\(\\left(p_{1 .}\\right)\\)\n\n\n\n\\(A_2(i=2)\\)\n\\(Y_{21}\\left(p_{21}\\right)\\)\n\\(Y_{22}\\left(p_{22}\\right)\\)\n\\(Y_{23}\\left(p_{23}\\right)\\)\n\\(Y_{24}\\left(p_{24}\\right)\\)\n\\(\\left(p_{2 .}\\right)\\)\n\n\n\nTotal\n\\(\\left(p_{.1}\\right)\\)\n\\(\\left(p_{.2}\\right)\\)\n\\(\\left(p_{.3}\\right)\\)\n\\(\\left(p_{.4}\\right)\\)\n\\(n\\)\n\n\n\n\nwhere \\(p_{i j}=Y_{i j} / n, p_i .=\\sum_{j=1}^k p_{i j}\\), and \\(p_{. j}=\\sum_{i=1}^h p_{i j}\\)\n\\(Q=\\sum_{j=1}^k \\sum_{i=1}^h \\frac{\\left(y_{i j}-\\frac{y_i \\cdot y_{\\cdot j}}{n}\\right)^2}{\\frac{y_i \\cdot y_{\\cdot j}}{n}}\\sim \\chi^2_{(h-1)(k-1)}\\)\n\nAre chi-square statistic for homogeneity and the chi-square statistic for independence equivalent?\nAlthough their chi-square statistics are equivalent, the two tests are not equivalent since their sampling experiment designs are different.\nHere’s the table of expected counts:\n\n\n\nBicycle Riding Interest\n\nVariable B (Age)\n\n\n\n\n\n\n\n\nVariable A\nEXPECTED\n18-24\n25-34\n35-49\n50-64\nTotal\n\n\n\nYES\n50.886\n49.868\n50.377\n49.868\n201\n\n\n\nNO\n49.114\n48.132\n48.623\n48.132\n194\n\n\n\nTotal\n100\n98\n99\n98\n395\n\n\n\n\\[\nQ=\\frac{(60-50.886)^2}{50.886}+\\cdots+\\frac{(57-48.132)^2}{48.132}=8.006\n\\]\nThe chi-square test tells us to reject the null hypothesis, at the 0.05 level, since \\(Q\\) is greater than a chi-square random variable with 3 degrees of freedom, that is, \\(Q=8.006&gt;7.815\\).\n\n\nSummary\nParametric tests make assumptions that aspects of the data follow some sort of theoretical probability distribution. Non-parametric tests or distribution free methods do not, and are used when the distributional assumptions for a parametric test are not met. While this is an advantage, it often comes at a cost of power (in the sense they are less likely to be able to detect a difference when a true difference exists).\nMost non-parametric tests are just hypothesis tests; there is no estimation of a confidence interval.\nMost non-parametric methods are based on ranking the values of a variable in ascending order and then calculating a test statistic based on the sums of these ranks.\nNon-parametric tests include:\n\nTwo-sample independent t-test - Wilcoxon rank-sum test or Mann-Whitney U test\nPaired t-test - Wilcoxon signed-rank test\nOne-way ANOVA - Kruskal-Wallace Test\nNormality tests - Shapiro-Wilk test and Kolmogorov-Smirnov test"
  },
  {
    "objectID": "aph101.html#simple-linear-regression",
    "href": "aph101.html#simple-linear-regression",
    "title": "APH101-Biostatistics And R",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nBrief introduction\nSome people think simple methods is bad and like complicated methods , but actually simple is very good–SLR works very well in lots of situations.\nSLR is used to answer:\nis there a relationship between..\nHow strong the relationship is\nwhich variable contribute to this relationship\nHow accurate could we predict the response variable\nIs the relationship linear?\nIs there a synergy among independent variables?\nThis is a model with two random variables, X and Y, where we are trying to predict Y from X. Here are the model’s assumptions:\n\nThe distribution of X is arbitrary, possibly is even non-random;\nIf X=x, then \\(Y = \\beta_0+\\beta_1x+\\epsilon\\) for some constants \\(\\beta_0, \\beta_1\\) and some random noise variable \\(\\epsilon\\)\n\\(\\epsilon\\) has mean 0, a constant variance \\(\\sigma^2\\), and is uncorrelated with X and uncorrelated across observations Cov\\((\\epsilon_i, \\epsilon_j)=0\\) for \\(i \\ne j\\)\n\nUsing Least Squares, we can estimate \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\), which are unbiased estimates of \\(\\beta_0\\) and \\(\\beta_1\\).\n\nGaussian-Noise Simple Linear Regression Model\n\nNow we further assume that the distribution of \\(\\epsilon\\) is normal, i.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\), independent of X.\nThey tell us, exactly, the probability distribution for Y given X, and so will let us get exact distributions for predictions and for other inferential statistics.\n\n\nMaximum Likelihood Estimation (MLE)\n\nIntroduction to MLE\nLikelihood is a fundamental concept in statistics that measures how well a particular set of parameters (e.g., the mean of a distribution) explains observed data. Think of it as a “score” that tells you which parameter values make your data most plausible.\nCompared to probability, which answers: “What’s the chance of seeing this data if we assume specific parameters?” , likelihood answers: “Given this data, how plausible are these parameters?”\nIf the parameters are \\(b_0, b_1, s^2\\) (reserving the Greek letters for their true values), then \\(Y \\mid X=x \\sim N\\left(b_0+b_1 x, s^2\\right)\\), and \\(Y_i\\) and \\(Y_j\\) are independent given \\(X_i\\) and \\(X_j\\), so the overall likelihood is \\[\n\\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi s^2}} e^{-\\frac{\\left(y_i-\\left(b_0+b_i x_i\\right)\\right)^2}{2 s^2}}\n\\]\nAs usual, we work with the log-likelihood, which gives us the same information but replaces products with sums: \\[\nL\\left(b_0, b_1, s^2\\right)=-\\frac{n}{2} \\ln \\left(2 \\pi s^2\\right)-\\frac{1}{2 s^2} \\sum_{i=1}^n\\left(y_i-\\left(b_0+b_1 x_i\\right)\\right)^2\n\\]\nmaximize it:\n\\(\\begin{aligned} \\frac{\\partial L}{\\partial b_0} & =-\\frac{1}{2 s^2} \\sum_{i=1}^n 2\\left(y_i-\\left(b_0+b_1 x_i\\right)\\right)(-1) \\\\ \\frac{\\partial L}{\\partial b_1} & =-\\frac{1}{2 s^2} \\sum_{i=1}^n 2\\left(y_i-\\left(b_0+b_1 x_i\\right)\\right)\\left(-x_i\\right)\\end{aligned}\\)\n\n\nSame result of MLE as least squares in linear regression\nNotice that when we set these derivatives to zero, all the multiplicative constants - in particular, the prefactor of \\(1 / 2 s^2\\) - go away. We are left with \\[\n\\begin{aligned}\n\\sum_{i=1}^n\\left(y_i-\\left(\\hat{\\beta_0}+\\hat{\\beta_1} x_i\\right)\\right) & =0 \\\\\n\\sum_{i=1}^n\\left(y_i-\\left(\\hat{\\beta_0}+\\hat{\\beta_0} x_i\\right)\\right) x_i & =0\n\\end{aligned}\n\\]\nThese are, up to a factor of \\(1 / n\\), exactly the equations we got from the method of least squares. That means that the least squares solution is the maximum likelihood estimate under the Gaussian noise model.\nMaximum likelihood estimates of the regression curve coincide with least-squares estimates when the noise around the curve is additive, Gaussian, of constant variance, and both independent of \\(X\\) and of other noise terms. If any of those assumptions fail, maximum likelihood and least squares estimates can diverge.\n\n\n\nHypothesis testing for estimates with unknown \\(\\sigma^2\\)\n\nResidual sum of squares (RSS) and t-statistics construction in linear regression\nIt can be shown that (the proof is beyond the scope of this course) \\[\n\\frac{R S S}{\\sigma^2}=\\frac{(n-2) \\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi_{n-2}^2\n\\]\nThis allows us to construct a \\(t\\)-value \\[\nt=\\frac{\\hat{\\beta}-\\beta}{s_{\\hat{\\beta}}} \\sim t_{n-2}\n\\]\nUnder the normality assumption of the error terms, the estimator of the slope coefficient will itself be normally distributed with mean \\(\\beta_i\\) and variance \\(\\operatorname{Var}\\left[\\beta_i\\right]\\) For \\(\\hat{\\beta_1}\\), its mean is \\(\\beta_1\\) and its variance is \\(\\sigma^2 / \\sum\\left(x_i-\\bar{x}\\right)^2\\). When \\(\\sigma^2\\) is known, we know \\[\n\\frac{\\hat{\\beta}_1-\\beta_1}{\\frac{\\sigma}{\\sqrt{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}}}\n\\] follows standard normal distribution. However, in practice, \\(\\sigma^2\\) is often unknown. We then divide this standard normal distributed term by \\[\n\\sqrt{\\frac{(n-2) \\hat{\\sigma}^2}{(n-2) \\sigma^2}}=\\frac{\\hat{\\sigma}}{\\sigma}\n\\]\nTherefore, when we write \\[\ns_{\\hat{\\beta_1}}=\\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}}\n\\] we construct a \\(t\\)-statistic for \\(\\hat{\\beta_1}\\) with degrees of freedom \\(n-2\\). This then allows us to construct a \\(100(1-\\alpha) \\%\\) confidence interval for \\(\\beta_1\\) : \\[\n\\hat{\\beta_1} \\pm t_{n-2, \\alpha / 2} \\times \\boldsymbol{s}_{\\hat{\\beta_1}}\n\\]\nWe can also do similar calculation to get the \\(t\\)-statistic and confidence interval for \\(\\beta_0\\).\n\n\nHyphothesis Testing\n\\[\nt =\\hat {\\beta}-\\beta / s_{\\hat{\\beta_1}} \\sim t_{n-2}\n\\] \\(H_0: \\beta_i =0\\)\n\\(H_1: \\beta_i \\ne 0\\)\n\n\nCodes of linear regression with CI\n\nlmodel &lt;- lm(Petal.Length ~ Petal.Width, data=iris)\nsummary(lmodel)\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33542 -0.30347 -0.02955  0.25776  1.39453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08356    0.07297   14.85   &lt;2e-16 ***\nPetal.Width  2.22994    0.05140   43.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4782 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\nconfint(lmodel)\n\n                2.5 %   97.5 %\n(Intercept) 0.9393664 1.227750\nPetal.Width 2.1283752 2.331506\n\nconf_interval &lt;- predict(lmodel, data=iris, interval='confidence', level=0.95)\nplot(iris$Petal.Width, iris$Petal.Length,\n     xlab='Petal.Width', ylab='Petal.Length',\n     main='Simple Linear Regression')\nabline(lmodel, col='lightblue')\nmatlines(iris$Petal.Width, conf_interval[,2:3], col='blue', lty=2)\n\n\n\n\n\n\n\n# Using ggplot2\nlibrary(ggplot2)\nggplot(iris, aes(x=Petal.Width, y=Petal.Length)) +\n  geom_point() +\n  geom_smooth(method=stats::lm, se=T, level=0.95)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression and ANOVA\nfev_dat &lt;- read.table('fev_dat.txt', header=T)\nfev_dat_subset &lt;- fev_dat[fev_dat$age &gt;= 6 & fev_dat$age &lt;= 10,]\nggplot(fev_dat_subset, aes(x=age, y=FEV)) +\n  geom_point() +\n  geom_smooth(method=stats::lm, se=T, level=0.95)\nsummary(aov(FEV ~ age, data=fev_dat_subset))\nsummary(lm(FEV ~ age, data=fev_dat_subset))\nanova(lm(FEV ~ age, data=fev_dat_subset))\n\n\n\n\\(R^2\\)–the fraction of variability explained by the regression\n\\[\nR^2 =1-\\frac{SSR}{SSTO}\n\\]"
  },
  {
    "objectID": "aph101.html#multiple-linear-regression-mlr",
    "href": "aph101.html#multiple-linear-regression-mlr",
    "title": "APH101-Biostatistics And R",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\n\\(y=X\\beta+\\epsilon\\), where y is a n × 1 row vector, X is a n × (k + 1) matrix, and β is a (k + 1) × 1 column vector for all n observations.\n\nA potential problem in practice –multicollinearity\nWhen multicollinearity exists, any of the following pitfalls can be exacerbated:\n\nThe estimated regression coefficient of any one variable depends on which other predictors are included in the model\nThe precision of the estimated regression coefficients decreases as more predictors are added to the model\nThe marginal contribution of any one predictor variable in reducing the error sum of squares depends on which other predictors are already in the model\nTypothesis tests for βj = 0 may yield different conclusions depending on which predictors are in the model\n\n\nPerfect multicollinearity\nPerfect multicollinearity refers to a situation where the predictive variables have an exact linear relationship. When there is perfect collinearity, the design matrix X has less than full rank, and therefore the moment matrix X′X cannot be inverted. In this situation, the parameter estimates of the regression are not well-defined, as the system of equations has infinitely many solutions.\n\n\nImperfect multicollinearity\nImperfect multicollinearity refers to a situation where the predictive variables have a nearly exact linear relationship.\n\\(R^2=r^2\\) where r is the Pearson correlation coefficient.\n\n\n\nAdjusted R-squared\nAdjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a model. It provides a more accurate measure of the model’s explanatory power, penalizing for the addition of irrelevant predictors. This helps in comparing models with different numbers of predictors."
  },
  {
    "objectID": "aph101.html#odds",
    "href": "aph101.html#odds",
    "title": "APH101-Biostatistics And R",
    "section": "Odds",
    "text": "Odds\nOdds are another way of quantifying the probability of an event, commonly used in gambling (and logistic regression).\nFor some event \\(E\\), \\[\n\\operatorname{odds}(E)=\\frac{P(E)}{P\\left(E^c\\right)}=\\frac{P(E)}{1-P(E)}\n\\]\nThe odds ratio (OR) is the ratio of the odds of an event occurring in one group to the odds of it occuring in another group. If the event in each of the groups are \\(p_1\\) (first group) and \\(p_2\\) (second group), then the odds ratio is: \\[\n\\mathrm{OR}=\\frac{p_1 /\\left(1-p_1\\right)}{p_2 /\\left(1-p_2\\right)}\n\\]"
  },
  {
    "objectID": "aph101.html#random-component",
    "href": "aph101.html#random-component",
    "title": "APH101-Biostatistics And R",
    "section": "Random Component",
    "text": "Random Component\nThe random component specifies a distribution for the outcome variable (conditional on \\(X\\) ). In the case of linear regression, we assume that \\(Y \\mid X \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\), for some mean \\(\\mu\\) and variance \\(\\sigma^2\\). In the case of logistic regression, we assume that \\(Y \\mid X \\sim \\operatorname{Bern}(p)\\) for some probability \\(p\\).\nIn a generalized model, we are allowed to assume that \\(Y \\mid X\\) has a probability density function or probability mass function of the form \\[\nf(y ; \\theta, \\phi)=\\exp \\left(\\frac{y \\theta-b(\\theta)}{a(\\phi)}+c(y, \\phi)\\right)\n\\]\nHere \\(\\theta, \\phi\\) are parameters, and \\(a, b, c\\) are functions. Any density of the above form is called an exponential family density. The parameter \\(\\theta\\) is called the natural parameter, and the parameter \\(\\phi\\) the dispersion parameter."
  },
  {
    "objectID": "aph101.html#exponential-family",
    "href": "aph101.html#exponential-family",
    "title": "APH101-Biostatistics And R",
    "section": "Exponential Family",
    "text": "Exponential Family\nExponential families include many of the most common distributions. For example: - Exponential \\[\nf(y ; \\lambda)=\\lambda e^{-\\lambda y}=\\exp (-y \\lambda+\\ln \\lambda)\n\\] where \\(\\theta=-\\lambda, \\phi=1, b(\\theta)=\\ln \\lambda, a(\\phi)=1\\), and \\(c(y, \\phi)=0\\) - Poisson \\[\nf(y ; \\lambda)=\\frac{e^{-\\lambda} \\lambda^y}{y!}=\\exp (y \\ln \\lambda-\\lambda-\\ln (y!))\n\\] where \\(\\theta=\\ln \\lambda, \\phi=1, b(\\theta)=e^\\theta=\\lambda, a(\\phi)=1\\), and \\(c(y, \\phi)=-\\lambda-\\ln (y!)\\)"
  },
  {
    "objectID": "aph101.html#systematic-component-and-link-component",
    "href": "aph101.html#systematic-component-and-link-component",
    "title": "APH101-Biostatistics And R",
    "section": "Systematic Component and Link Component",
    "text": "Systematic Component and Link Component\nThe systematic component relates a parameter \\(\\eta\\) to the predictors \\(X\\). In a GLM, this is always done via \\[\n\\eta=X \\beta=\\beta_0+\\beta_1 X_1+\\cdots+\\beta_k X_k\n\\]\nWe will denote the expectation of the distribution in random component as \\(\\mu\\), i.e., \\(\\mathbb{E}[Y \\mid X]=\\mu\\). It will be our goal to estimate \\(\\mu\\). Finally, the link component connects the random and systematic components, via a link function \\(g\\). In particular, this link function provides a connection between \\(\\mu\\) and \\(\\eta\\), as in \\[\ng(\\mu)=\\eta \\quad \\text { or } \\quad \\mu=g^{-1}(\\eta)\n\\]"
  },
  {
    "objectID": "aph101.html#example-1",
    "href": "aph101.html#example-1",
    "title": "APH101-Biostatistics And R",
    "section": "Example",
    "text": "Example\n\nGaussian-noise Linear Regression\n\nRandom Component: \\(Y \\mid X \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\) and \\(\\mathbb{E}[Y \\mid X]=\\mu\\)\nSystematic Component: \\(\\eta=X \\beta\\)\nLink Component: \\(g(\\mu)=\\mu\\), so that \\(\\mu=\\eta=X \\beta\\)\n\n\n\nBernoulli\nSuppose that \\(Y \\in\\{0,1\\}\\), and we model the distribution of \\(Y \\mid X\\) as Bernoulli with success probability \\(p\\). Then the probability mass function (not a density, since \\(Y\\) is discrete) is \\[\nf(y)=p^y(1-p)^{1-y}\n\\]\nWe can rewrite to fit the exponential family form as \\[\n\\begin{aligned}\nf(y) & =\\exp (y \\log p+(1-y) \\log (1-p)) \\\\\n& =\\exp (y \\log (p /(1-p))+\\log (1-p))\n\\end{aligned}\n\\]\n\\[\nf(y ; \\theta, \\phi)=\\exp \\left(\\frac{y \\theta-b(\\theta)}{a(\\phi)}+c(y, \\phi)\\right)\n\\]\nHere we would identify \\(\\theta=\\log (p /(1-p))\\) as the natural parameter. Note that the mean here is \\(\\mu=p\\), and using the inverse of the above relationship, we can directly write the mean \\(p\\) as a function of \\(\\theta\\), as in \\(p=e^\\theta /\\left(1+e^\\theta\\right)\\). Hence \\(b(\\theta)=\\log (1-p)=-\\log \\left(1+e^\\theta\\right)\\). There is no dispersion parameter, so we can set \\(a(\\phi)=1\\). Also, \\(c(y, \\phi)=0\\)."
  },
  {
    "objectID": "aph101.html#some-definations",
    "href": "aph101.html#some-definations",
    "title": "APH101-Biostatistics And R",
    "section": "Some definations",
    "text": "Some definations\nHazard ratios; ratios of hazard functions between different groups (e.g., exposed vs. unexposed) while adjusting for confounders.\ncensoring - which occurs when the survival time is only partially known\n\nFixed type I censoring occurs when a study is designed to end after C years of follow-up. In this case, everyone who does not have an event observed during the course of the study is censored at C years.\nIn random type I censoring, the study is designed to end after C years, but censored subjects do not all have the same censoring time. This is the main type of right-censoring we will be concerned with.\nIn type II censoring, a study ends when there is a pre-specified number of events."
  },
  {
    "objectID": "aph101.html#kaplan-meier-estimate",
    "href": "aph101.html#kaplan-meier-estimate",
    "title": "APH101-Biostatistics And R",
    "section": "Kaplan-Meier estimate",
    "text": "Kaplan-Meier estimate\n\nlibrary(ISLR2)\nnames(BrainCancer)\n\n[1] \"sex\"       \"diagnosis\" \"loc\"       \"ki\"        \"gtv\"       \"stereo\"   \n[7] \"status\"    \"time\"     \n\nattach(BrainCancer)\ntable(status)\n\nstatus\n 0  1 \n53 35 \n\nlibrary(survival)\nfit.surv &lt;- survfit(Surv(time, status) ~ 1)\nplot(fit.surv, xlab = \"Months\",\n    ylab = \"Estimated Probability of Survival\")"
  },
  {
    "objectID": "aph101.html#cox-proportional-harzards-model",
    "href": "aph101.html#cox-proportional-harzards-model",
    "title": "APH101-Biostatistics And R",
    "section": "Cox-proportional harzards model",
    "text": "Cox-proportional harzards model\nS(t) = P(T&gt;t)=1-F(t)\n\\(h(t)=\\lim _{\\Delta t \\rightarrow 0} \\frac{P(t&lt;T \\leq t+\\Delta t \\mid T&gt;t)}{\\Delta t}\\)\n\nfit.all &lt;- coxph(\nSurv(time, status) ~ sex + diagnosis + loc + ki + gtv +\n   stereo)\nfit.all\n\nCall:\ncoxph(formula = Surv(time, status) ~ sex + diagnosis + loc + \n    ki + gtv + stereo)\n\n                       coef exp(coef) se(coef)      z        p\nsexMale             0.18375   1.20171  0.36036  0.510  0.61012\ndiagnosisLG glioma  0.91502   2.49683  0.63816  1.434  0.15161\ndiagnosisHG glioma  2.15457   8.62414  0.45052  4.782 1.73e-06\ndiagnosisOther      0.88570   2.42467  0.65787  1.346  0.17821\nlocSupratentorial   0.44119   1.55456  0.70367  0.627  0.53066\nki                 -0.05496   0.94653  0.01831 -3.001  0.00269\ngtv                 0.03429   1.03489  0.02233  1.536  0.12466\nstereoSRT           0.17778   1.19456  0.60158  0.296  0.76760\n\nLikelihood ratio test=41.37  on 8 df, p=1.776e-06\nn= 87, number of events= 35 \n   (1 observation deleted due to missingness)\n\nmodaldata &lt;- data.frame(\n     diagnosis = levels(diagnosis),\n     sex = rep(\"Female\", 4),\n     loc = rep(\"Supratentorial\", 4),\n     ki = rep(mean(ki), 4),\n     gtv = rep(mean(gtv), 4),\n     stereo = rep(\"SRT\", 4)\n     )\nsurvplots &lt;- survfit(fit.all, newdata = modaldata)\nplot(survplots, xlab = \"Months\",\n    ylab = \"Survival Probability\", col = 2:5)\nlegend(\"bottomleft\", levels(diagnosis), col = 2:5, lty = 1)"
  },
  {
    "objectID": "aph101.html#basic-calculation",
    "href": "aph101.html#basic-calculation",
    "title": "APH101-Biostatistics And R",
    "section": "basic calculation",
    "text": "basic calculation\n\nexp(1)  # exp() is the exponential function\n\n[1] 2.718282\n\nlog(5)  # unless you specify the base, R will assume base e\n\n[1] 1.609438\n\nlog(5, base=10)  # base 10\n\n[1] 0.69897"
  },
  {
    "objectID": "aph101.html#vectors",
    "href": "aph101.html#vectors",
    "title": "APH101-Biostatistics And R",
    "section": "Vectors",
    "text": "Vectors\n\nseq(from=1, to=4, by=1)\n\n[1] 1 2 3 4\n\nseq(1, 4)  # 'by' has a default of 1\n\n[1] 1 2 3 4\n\n1:4  # a shortcut for seq(1,4)\n\n[1] 1 2 3 4\n\nseq(1, 5, by=.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nseq(1, 5, length.out=11) \n\n [1] 1.0 1.4 1.8 2.2 2.6 3.0 3.4 3.8 4.2 4.6 5.0\n\n\n\nx &lt;- 1:4\nx * x\n\n[1]  1  4  9 16\n\nx %*% x    # scalar (\"inner\") product (1 x 1 matrix)\n\n     [,1]\n[1,]   30"
  },
  {
    "objectID": "aph101.html#vector-algebra",
    "href": "aph101.html#vector-algebra",
    "title": "APH101-Biostatistics And R",
    "section": "Vector Algebra",
    "text": "Vector Algebra\nAll algebra done with vectors will be done element-wise by default. For matrix and vector multiplication as usually defined by mathematicians, use %*% instead of *. So two vectors added together result in their individual elements being summed.\n\nx &lt;- 1:4\ny &lt;- 5:8\nx + y\n\n[1]  6  8 10 12\n\nx * y\n\n[1]  5 12 21 32"
  },
  {
    "objectID": "aph101.html#matrices",
    "href": "aph101.html#matrices",
    "title": "APH101-Biostatistics And R",
    "section": "Matrices",
    "text": "Matrices\n\na &lt;- c(1, 2, 3)\nb &lt;- c(4, 5, 6)\nrbind(a,b)  # Row Bind: a,b are rows in resultant matrix    \n\n  [,1] [,2] [,3]\na    1    2    3\nb    4    5    6"
  },
  {
    "objectID": "aph101.html#factors",
    "href": "aph101.html#factors",
    "title": "APH101-Biostatistics And R",
    "section": "Factors",
    "text": "Factors\n\niris$Species &lt;- factor(iris$Species,\n                       levels = c('versicolor','setosa','virginica'),\n                       labels = c('Versicolor','Setosa','Virginica'))\nboxplot(Sepal.Length ~ Species, data=iris)"
  },
  {
    "objectID": "aph101.html#list",
    "href": "aph101.html#list",
    "title": "APH101-Biostatistics And R",
    "section": "List",
    "text": "List\n\nmylist &lt;- list(letters=c(\"A\", \"b\", \"c\"), \n        numbers=1:3, matrix(1:25, ncol=5))\nhead(mylist)\n\n$letters\n[1] \"A\" \"b\" \"c\"\n\n$numbers\n[1] 1 2 3\n\n[[3]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    6   11   16   21\n[2,]    2    7   12   17   22\n[3,]    3    8   13   18   23\n[4,]    4    9   14   19   24\n[5,]    5   10   15   20   25"
  },
  {
    "objectID": "aph101.html#ggplot-2",
    "href": "aph101.html#ggplot-2",
    "title": "APH101-Biostatistics And R",
    "section": "ggplot 2",
    "text": "ggplot 2\n\nlibrary(ggplot2)\ndata(iris)  # load the iris dataset that comes with R\nstr(iris)   # what columns do we have to play with...\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +\n  geom_point(aes(color=Species)) +\n  labs(title=\"Iris Sepal Length vs Width\",\n       x=\"Sepal Length (cm)\",\n       y=\"Sepal Width (cm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\nboxplot(iris$Sepal.Length ~ iris$Species)\n\n\n\n\n\n\n\nhist(iris$Sepal.Length)\n\n\n\n\n\n\n\n# plot(iris$Sepal.Length, iris$Petal.Length)\nplot(Petal.Length ~ Sepal.Length, data=iris)\nabline(lm(Petal.Length ~ Sepal.Length, data=iris), col=\"red\")"
  },
  {
    "objectID": "aph101.html#data-manipulation",
    "href": "aph101.html#data-manipulation",
    "title": "APH101-Biostatistics And R",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# summary\n\ndata(iris)\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n# apply\n\niris &lt;- iris[,-5]  # Exclude the Species column\napply(iris, MARGIN=2, FUN=mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n# lappy and sapply\nx &lt;- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))\nlapply(x, quantile, probs = 1:3/4)\n\n$a\n 25%  50%  75% \n3.25 5.50 7.75 \n\n$beta\n      25%       50%       75% \n0.2516074 1.0000000 5.0536690 \n\n$logic\n25% 50% 75% \n0.0 0.5 1.0 \n\nsapply(x, quantile, probs = 1:3/4)\n\n       a      beta logic\n25% 3.25 0.2516074   0.0\n50% 5.50 1.0000000   0.5\n75% 7.75 5.0536690   1.0\n\n# select\n\nstarwars %&gt;% select(hair_color, skin_color, eye_color)\n\n# A tibble: 87 × 3\n   hair_color    skin_color  eye_color\n   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 blond         fair        blue     \n 2 &lt;NA&gt;          gold        yellow   \n 3 &lt;NA&gt;          white, blue red      \n 4 none          white       yellow   \n 5 brown         light       brown    \n 6 brown, grey   light       blue     \n 7 brown         light       blue     \n 8 &lt;NA&gt;          white, red  red      \n 9 black         light       brown    \n10 auburn, white fair        blue-gray\n# ℹ 77 more rows\n\n# filter\nstarwars %&gt;% filter(species == \"Droid\" & mass &lt; 100)\n\n# A tibble: 3 × 14\n  name  height  mass hair_color skin_color  eye_color birth_year sex   gender   \n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n1 C-3PO    167    75 &lt;NA&gt;       gold        yellow           112 none  masculine\n2 R2-D2     96    32 &lt;NA&gt;       white, blue red               33 none  masculine\n3 R5-D4     97    32 &lt;NA&gt;       white, red  red               NA none  masculine\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# arrange\nstarwars %&gt;% arrange(desc(height))\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Yarael …    264    NA none       white      yellow          NA   male  mascu…\n 2 Tarfful     234   136 brown      brown      blue            NA   male  mascu…\n 3 Lama Su     229    88 none       grey       black           NA   male  mascu…\n 4 Chewbac…    228   112 brown      unknown    blue           200   male  mascu…\n 5 Roos Ta…    224    82 none       grey       orange          NA   male  mascu…\n 6 Grievous    216   159 none       brown, wh… green, y…       NA   male  mascu…\n 7 Taun We     213    NA none       grey       black           NA   fema… femin…\n 8 Rugor N…    206    NA none       green      orange          NA   male  mascu…\n 9 Tion Me…    206    80 none       grey       black           NA   male  mascu…\n10 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n# mutate\n\nstarwars %&gt;% \n  mutate(bmi = mass / ((height / 100) ^ 2)) %&gt;% \n  select(name, bmi) %&gt;% head()\n\n# A tibble: 6 × 2\n  name             bmi\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Luke Skywalker  26.0\n2 C-3PO           26.9\n3 R2-D2           34.7\n4 Darth Vader     33.3\n5 Leia Organa     21.8\n6 Owen Lars       37.9\n\n# summarise\n\nstarwars %&gt;% \n  group_by(species) %&gt;% \n  summarise(mean_height = mean(height, na.rm = TRUE)) %&gt;% \n  arrange(desc(mean_height))\n\n# A tibble: 38 × 2\n   species  mean_height\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Quermian        264 \n 2 Wookiee         231 \n 3 Kaminoan        221 \n 4 Kaleesh         216 \n 5 Gungan          209.\n 6 Pau'an          206 \n 7 Besalisk        198 \n 8 Cerean          198 \n 9 Chagrian        196 \n10 Nautolan        196 \n# ℹ 28 more rows"
  },
  {
    "objectID": "aph101.html#evaluation-metrics-for-classification",
    "href": "aph101.html#evaluation-metrics-for-classification",
    "title": "APH101-Biostatistics And R",
    "section": "Evaluation Metrics for Classification",
    "text": "Evaluation Metrics for Classification\n\n#Load the dataset \nlibrary(readr)\ndata = read_csv('survey_lung_cancer.csv', show_col_types = FALSE)\ndata$LUNG_CANCER &lt;- ifelse(data$LUNG_CANCER==\"YES\", 1, 0)\nsummary(data)\n\n    GENDER               AGE           SMOKING      YELLOW_FINGERS\n Length:309         Min.   :21.00   Min.   :1.000   Min.   :1.00  \n Class :character   1st Qu.:57.00   1st Qu.:1.000   1st Qu.:1.00  \n Mode  :character   Median :62.00   Median :2.000   Median :2.00  \n                    Mean   :62.67   Mean   :1.563   Mean   :1.57  \n                    3rd Qu.:69.00   3rd Qu.:2.000   3rd Qu.:2.00  \n                    Max.   :87.00   Max.   :2.000   Max.   :2.00  \n    ANXIETY      PEER_PRESSURE   CHRONIC DISEASE    FATIGUE     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.498   Mean   :1.502   Mean   :1.505   Mean   :1.673  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n    ALLERGY         WHEEZING     ALCOHOL CONSUMING    COUGHING    \n Min.   :1.000   Min.   :1.000   Min.   :1.000     Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000     1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000     Median :2.000  \n Mean   :1.557   Mean   :1.557   Mean   :1.557     Mean   :1.579  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000     3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000     Max.   :2.000  \n SHORTNESS OF BREATH SWALLOWING DIFFICULTY   CHEST PAIN     LUNG_CANCER    \n Min.   :1.000       Min.   :1.000         Min.   :1.000   Min.   :0.0000  \n 1st Qu.:1.000       1st Qu.:1.000         1st Qu.:1.000   1st Qu.:1.0000  \n Median :2.000       Median :1.000         Median :2.000   Median :1.0000  \n Mean   :1.641       Mean   :1.469         Mean   :1.557   Mean   :0.8738  \n 3rd Qu.:2.000       3rd Qu.:2.000         3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :2.000       Max.   :2.000         Max.   :2.000   Max.   :1.0000  \n\n\n\nlibrary(ggplot2)\nggplot(data, aes(x = factor(SMOKING), fill = factor(LUNG_CANCER))) +\ngeom_bar(position = \"fill\") +\nscale_fill_manual(values = c(\"0\" = \"lightblue\", \"1\" = \"salmon\")) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n### Data SAMPLING ####\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nset.seed(101)\nsplit = createDataPartition(data$LUNG_CANCER, p = 0.80, list = FALSE)\ntrain_data = data[split,]\ntest_data = data[-split,]\nnrow(train_data)\n\n[1] 248\n\nnrow(test_data)\n\n[1] 61\n\nnrow(train_data)/nrow(data)\n\n[1] 0.802589\n\n\n\n#error metrics -- Confusion Matrix\nerr_metric=function(CM)\n{\n  TN =CM[1,1]\n  TP =CM[2,2]\n  FP =CM[1,2]\n  FN =CM[2,1]\n  precision =(TP)/(TP+FP)\n  recall_score =(TP)/(TP+FN)\n  f1_score=2*((precision*recall_score)/(precision+recall_score))\n  accuracy_model  =(TP+TN)/(TP+TN+FP+FN)\n  False_positive_rate =(FP)/(FP+TN)\n  False_negative_rate =(FN)/(FN+TP)\n  print(paste(\"Precision value of the model: \",round(precision,2)))\n  print(paste(\"Accuracy of the model: \",round(accuracy_model,2)))\n  print(paste(\"Recall value of the model: \",round(recall_score,2)))\n  print(paste(\"False Positive rate of the model: \",round(False_positive_rate,2)))\n  print(paste(\"False Negative rate of the model: \",round(False_negative_rate,2)))\n  print(paste(\"F1 score of the model: \",round(f1_score,2)))\n}\n\n\n# Logistic regression\nlogit_m =glm(formula = LUNG_CANCER ~ ., data = train_data, family = 'binomial')\nsummary(logit_m)\n\n\nCall:\nglm(formula = LUNG_CANCER ~ ., family = \"binomial\", data = train_data)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -32.13449    6.57393  -4.888 1.02e-06 ***\nGENDERM                  -0.84716    0.82793  -1.023 0.306203    \nAGE                       0.01537    0.03490   0.440 0.659756    \nSMOKING                   1.10495    0.82246   1.343 0.179119    \nYELLOW_FINGERS            1.10971    0.82005   1.353 0.175982    \nANXIETY                   2.08645    1.08610   1.921 0.054725 .  \nPEER_PRESSURE             1.94159    0.74009   2.623 0.008704 ** \n`CHRONIC DISEASE`         3.91378    1.12190   3.489 0.000486 ***\nFATIGUE                   2.62906    0.89032   2.953 0.003148 ** \nALLERGY                   1.42702    0.83764   1.704 0.088454 .  \nWHEEZING                  1.07933    0.90761   1.189 0.234357    \n`ALCOHOL CONSUMING`       2.41116    0.98495   2.448 0.014365 *  \nCOUGHING                  3.14783    1.22386   2.572 0.010110 *  \n`SHORTNESS OF BREATH`    -0.22681    0.84245  -0.269 0.787757    \n`SWALLOWING DIFFICULTY`   2.24613    1.24347   1.806 0.070865 .  \n`CHEST PAIN`              0.89356    0.73930   1.209 0.226791    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 198.230  on 247  degrees of freedom\nResidual deviance:  77.353  on 232  degrees of freedom\nAIC: 109.35\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n# Logistic regression\nlogit_m2 =glm(formula = LUNG_CANCER ~ ANXIETY+PEER_PRESSURE+`CHRONIC DISEASE`+FATIGUE+ALLERGY+`ALCOHOL CONSUMING`+COUGHING+`SWALLOWING DIFFICULTY`, data = train_data, family = 'binomial')\nsummary(logit_m2)\n\n\nCall:\nglm(formula = LUNG_CANCER ~ ANXIETY + PEER_PRESSURE + `CHRONIC DISEASE` + \n    FATIGUE + ALLERGY + `ALCOHOL CONSUMING` + COUGHING + `SWALLOWING DIFFICULTY`, \n    family = \"binomial\", data = train_data)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -27.3830     5.5206  -4.960 7.05e-07 ***\nANXIETY                   2.5514     0.8659   2.947 0.003213 ** \nPEER_PRESSURE             2.1822     0.7245   3.012 0.002593 ** \n`CHRONIC DISEASE`         3.5120     0.9696   3.622 0.000292 ***\nFATIGUE                   2.4939     0.6979   3.573 0.000353 ***\nALLERGY                   2.0104     0.7428   2.707 0.006796 ** \n`ALCOHOL CONSUMING`       2.5084     0.8653   2.899 0.003744 ** \nCOUGHING                  3.2220     0.9540   3.377 0.000732 ***\n`SWALLOWING DIFFICULTY`   2.5273     1.0096   2.503 0.012309 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 198.230  on 247  degrees of freedom\nResidual deviance:  83.501  on 239  degrees of freedom\nAIC: 101.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nlibrary(dplyr)\nlogit_P_prob = predict(logit_m, newdata = select(test_data, -LUNG_CANCER), type = 'response')\nlogit_P_prob[1:3]\n\n        1         2         3 \n0.9912600 0.9997708 0.9980692 \n\nlogit_P &lt;- ifelse(logit_P_prob &gt; 0.5, 1, 0) # Probability check\nlogit_P[1:3]\n\n1 2 3 \n1 1 1 \n\n\n\nCM = table(test_data$LUNG_CANCER, logit_P)\nprint(CM)\n\n   logit_P\n     0  1\n  0  3  2\n  1  1 55\n\n\n\nerr_metric(CM)\n\n[1] \"Precision value of the model:  0.96\"\n[1] \"Accuracy of the model:  0.95\"\n[1] \"Recall value of the model:  0.98\"\n[1] \"False Positive rate of the model:  0.4\"\n[1] \"False Negative rate of the model:  0.02\"\n[1] \"F1 score of the model:  0.97\"\n\n\n\n#ROC-curve using pROC library\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nroc_score=roc(test_data$LUNG_CANCER, logit_P_prob) #AUC score\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_score, main = \"ROC curve -- Logistic Regression\")"
  },
  {
    "objectID": "mth106.html#part-2",
    "href": "mth106.html#part-2",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "??5.1 part 2",
    "text": "??5.1 part 2\nplot of 3 cases damped motion\np31 可以震荡在0附近吗 pass x at most 1 time？\nundamped is similar to damped"
  },
  {
    "objectID": "mth106.html#有点分不清wranskian和",
    "href": "mth106.html#有点分不清wranskian和",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "？？？有点分不清wranskian和",
    "text": "？？？有点分不清wranskian和\nwronskian 是判断函数之间是否线性无关的一个方法 用求导\n而wronskian for linear system是判断线性系统的解是否线性无关的一个方法 不用求导"
  },
  {
    "objectID": "mth106.html#regular-strum-liouville-problem",
    "href": "mth106.html#regular-strum-liouville-problem",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "11.4 regular strum liouville problem",
    "text": "11.4 regular strum liouville problem\n这个3用背锅吗"
  },
  {
    "objectID": "mth106.html#proof",
    "href": "mth106.html#proof",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "Proof",
    "text": "Proof"
  },
  {
    "objectID": "mth107.html#matrix-basic-knowledge",
    "href": "mth107.html#matrix-basic-knowledge",
    "title": "MTH107—NOTEs",
    "section": "Matrix basic knowledge",
    "text": "Matrix basic knowledge\n\nAB \\(\\neq\\) BA – it is not a number, but a linear transformation (a function). AB is the composition of two functions, while BA is another composition of two functions. They are not the same in general."
  },
  {
    "objectID": "mth106.html#那个phase-portrait那个lambda符号一样的-趋于正无穷的时候-应该是偏向power更大的那里吧",
    "href": "mth106.html#那个phase-portrait那个lambda符号一样的-趋于正无穷的时候-应该是偏向power更大的那里吧",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "那个phase portrait那个lambda符号一样的 趋于正无穷的时候 应该是偏向power更大的那里吧",
    "text": "那个phase portrait那个lambda符号一样的 趋于正无穷的时候 应该是偏向power更大的那里吧"
  },
  {
    "objectID": "mth106.html#椭圆的phase-portrait-p32-notes",
    "href": "mth106.html#椭圆的phase-portrait-p32-notes",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "椭圆的phase portrait p32 notes",
    "text": "椭圆的phase portrait p32 notes"
  },
  {
    "objectID": "mth106.html#p6-c是任意数就行不写c哈",
    "href": "mth106.html#p6-c是任意数就行不写c哈",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "8.3 p6 c是任意数就行不写c哈",
    "text": "8.3 p6 c是任意数就行不写c哈"
  },
  {
    "objectID": "mth106.html#那个作业为啥对应了两个eigenfunctions",
    "href": "mth106.html#那个作业为啥对应了两个eigenfunctions",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "？？那个作业为啥对应了两个eigenfunctions",
    "text": "？？那个作业为啥对应了两个eigenfunctions"
  },
  {
    "objectID": "mth106.html#算inverse-matrix",
    "href": "mth106.html#算inverse-matrix",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "算inverse matrix",
    "text": "算inverse matrix"
  },
  {
    "objectID": "mth106.html#p6那个怎么就变成定积分了nie-为什么是等价的-8.3-page-38-包括要复习前面的和算出来直接带入的区别是什么",
    "href": "mth106.html#p6那个怎么就变成定积分了nie-为什么是等价的-8.3-page-38-包括要复习前面的和算出来直接带入的区别是什么",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "8.4 p6那个怎么就变成定积分了nie 为什么是等价的 8.3 page 38 –包括要复习前面的！！！和算出来直接带入的区别是什么",
    "text": "8.4 p6那个怎么就变成定积分了nie 为什么是等价的 8.3 page 38 –包括要复习前面的！！！和算出来直接带入的区别是什么"
  },
  {
    "objectID": "mth106.html#没看weight哪些概念",
    "href": "mth106.html#没看weight哪些概念",
    "title": "ODEs and PDEs with Dimension nalysis —MTH106 + Seminars hold by Prof. Andrew Lin",
    "section": "11.1没看weight哪些概念",
    "text": "11.1没看weight哪些概念"
  }
]