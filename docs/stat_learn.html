<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Foundation Math Learning - Statistical_Learning (Mechine_Learning)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Foundation Math Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://yuuuulu.github.io/"> 
<span class="menu-text">yuuuulu</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Homepage</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./stat_learn.html" aria-current="page"> 
<span class="menu-text">Statistical_Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Mathematical Analysis 1.html"> 
<span class="menu-text">Analysis 1</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Calculus.html"> 
<span class="menu-text">Calculus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./mth107.html"> 
<span class="menu-text">Advanced Linear Algebra</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./mth1113.html"> 
<span class="menu-text">Prob, and Stat.</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./APH203.html"> 
<span class="menu-text">Time Series</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Simple_ODEs.html"> 
<span class="menu-text">ODEs and PDEs with codes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./mth106.html"> 
<span class="menu-text">ODE, PDE, and Fourier Series</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#referece-books" id="toc-referece-books" class="nav-link active" data-scroll-target="#referece-books">Referece books</a></li>
  <li><a href="#statistics-history" id="toc-statistics-history" class="nav-link" data-scroll-target="#statistics-history">statistics history</a>
  <ul class="collapse">
  <li><a href="#oldclassical-bayesian-estimation-moment-estimation" id="toc-oldclassical-bayesian-estimation-moment-estimation" class="nav-link" data-scroll-target="#oldclassical-bayesian-estimation-moment-estimation">old/classical: Bayesian estimation; moment estimation</a></li>
  <li><a href="#moment-estimation" id="toc-moment-estimation" class="nav-link" data-scroll-target="#moment-estimation">Moment estimation</a></li>
  <li><a href="#bayesian-estimation" id="toc-bayesian-estimation" class="nav-link" data-scroll-target="#bayesian-estimation">Bayesian estimation</a></li>
  <li><a href="#fisher-mle" id="toc-fisher-mle" class="nav-link" data-scroll-target="#fisher-mle">Fisher: MLE</a></li>
  </ul></li>
  <li><a href="#modern-statistics" id="toc-modern-statistics" class="nav-link" data-scroll-target="#modern-statistics">Modern Statistics</a>
  <ul class="collapse">
  <li><a href="#empirical-kullback-leibler-divergence-loss-and-mle" id="toc-empirical-kullback-leibler-divergence-loss-and-mle" class="nav-link" data-scroll-target="#empirical-kullback-leibler-divergence-loss-and-mle">Empirical Kullback-Leibler Divergence Loss and MLE</a>
  <ul class="collapse">
  <li><a href="#kl-divergence-property" id="toc-kl-divergence-property" class="nav-link" data-scroll-target="#kl-divergence-property">KL-divergence Property</a></li>
  </ul></li>
  <li><a href="#sufficient-statistics" id="toc-sufficient-statistics" class="nav-link" data-scroll-target="#sufficient-statistics">Sufficient statistics</a></li>
  <li><a href="#rao-blackwell-theorem" id="toc-rao-blackwell-theorem" class="nav-link" data-scroll-target="#rao-blackwell-theorem">Rao-Blackwell Theorem</a></li>
  </ul></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a>
  <ul class="collapse">
  <li><a href="#ols" id="toc-ols" class="nav-link" data-scroll-target="#ols">OLS</a>
  <ul class="collapse">
  <li><a href="#eg-simplified-ceres-orbit-problem" id="toc-eg-simplified-ceres-orbit-problem" class="nav-link" data-scroll-target="#eg-simplified-ceres-orbit-problem">Eg Simplified Ceres Orbit Problem</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization">regularization</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">loss function</a></li>
  <li><a href="#mle" id="toc-mle" class="nav-link" data-scroll-target="#mle">MLE</a></li>
  </ul></li>
  <li><a href="#ps-parameter-statistics" id="toc-ps-parameter-statistics" class="nav-link" data-scroll-target="#ps-parameter-statistics">ps: parameter statistics</a>
  <ul class="collapse">
  <li><a href="#skew" id="toc-skew" class="nav-link" data-scroll-target="#skew">skew</a></li>
  <li><a href="#wls" id="toc-wls" class="nav-link" data-scroll-target="#wls">WLS</a></li>
  <li><a href="#concave-and-hessian-matrices" id="toc-concave-and-hessian-matrices" class="nav-link" data-scroll-target="#concave-and-hessian-matrices">Concave and Hessian Matrices</a>
  <ul class="collapse">
  <li><a href="#semi--defined-matrices" id="toc-semi--defined-matrices" class="nav-link" data-scroll-target="#semi--defined-matrices">(Semi-) Defined Matrices</a></li>
  <li><a href="#xtax-is-a-elliptic" id="toc-xtax-is-a-elliptic" class="nav-link" data-scroll-target="#xtax-is-a-elliptic"><span class="math inline">\(X^TAX\)</span> is a elliptic</a></li>
  </ul></li>
  <li><a href="#see-3-blue-1-brown-video" id="toc-see-3-blue-1-brown-video" class="nav-link" data-scroll-target="#see-3-blue-1-brown-video">See 3 blue 1 brown video</a></li>
  <li><a href="#mahalanobis-distance-and-euclidean-distance" id="toc-mahalanobis-distance-and-euclidean-distance" class="nav-link" data-scroll-target="#mahalanobis-distance-and-euclidean-distance">Mahalanobis distance and Euclidean distance</a></li>
  <li><a href="#categorical-variables-in-linear-logistic-regression-reference-variable" id="toc-categorical-variables-in-linear-logistic-regression-reference-variable" class="nav-link" data-scroll-target="#categorical-variables-in-linear-logistic-regression-reference-variable">Categorical variables in linear / logistic regression –reference variable</a></li>
  </ul></li>
  <li><a href="#glm" id="toc-glm" class="nav-link" data-scroll-target="#glm">glm</a>
  <ul class="collapse">
  <li><a href="#special-case-logistic-regression" id="toc-special-case-logistic-regression" class="nav-link" data-scroll-target="#special-case-logistic-regression">special case : logistic regression</a></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic regression</a></li>
  <li><a href="#variable-transformation-formulathe-first-conversation-with-dr.-zhu" id="toc-variable-transformation-formulathe-first-conversation-with-dr.-zhu" class="nav-link" data-scroll-target="#variable-transformation-formulathe-first-conversation-with-dr.-zhu">variable transformation formula–the first conversation with Dr.&nbsp;Zhu</a>
  <ul class="collapse">
  <li><a href="#variable-transformation-formula" id="toc-variable-transformation-formula" class="nav-link" data-scroll-target="#variable-transformation-formula">Variable Transformation Formula</a></li>
  <li><a href="#proof-process" id="toc-proof-process" class="nav-link" data-scroll-target="#proof-process">Proof Process</a></li>
  </ul></li>
  <li><a href="#sampling-methods" id="toc-sampling-methods" class="nav-link" data-scroll-target="#sampling-methods">Sampling Methods</a>
  <ul class="collapse">
  <li><a href="#bootstrap" id="toc-bootstrap" class="nav-link" data-scroll-target="#bootstrap">Bootstrap</a>
  <ul class="collapse">
  <li><a href="#example-of-calculating-p-value-of-ks-statstics-goodness-of-fit-with-the-problem-of-censoring-data-distribution-estimate" id="toc-example-of-calculating-p-value-of-ks-statstics-goodness-of-fit-with-the-problem-of-censoring-data-distribution-estimate" class="nav-link" data-scroll-target="#example-of-calculating-p-value-of-ks-statstics-goodness-of-fit-with-the-problem-of-censoring-data-distribution-estimate">Example of calculating p-value of KS-statstics (Goodness of fit) with the problem of censoring data distribution estimate</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#some-perspectives-to-penalized-regression-mode" id="toc-some-perspectives-to-penalized-regression-mode" class="nav-link" data-scroll-target="#some-perspectives-to-penalized-regression-mode">Some Perspectives to Penalized Regression Mode</a>
  <ul class="collapse">
  <li><a href="#ols-mle-solution-to-lr-with-issues" id="toc-ols-mle-solution-to-lr-with-issues" class="nav-link" data-scroll-target="#ols-mle-solution-to-lr-with-issues">OLS / MLE Solution to LR with issues</a></li>
  <li><a href="#why-does-the-terms-of-the-regularization-look-like-this" id="toc-why-does-the-terms-of-the-regularization-look-like-this" class="nav-link" data-scroll-target="#why-does-the-terms-of-the-regularization-look-like-this">Why does the terms of the regularization look like this?</a>
  <ul class="collapse">
  <li><a href="#choose-prior-distribution-pbeta" id="toc-choose-prior-distribution-pbeta" class="nav-link" data-scroll-target="#choose-prior-distribution-pbeta">Choose Prior Distribution <span class="math inline">\(P(\beta)\)</span></a></li>
  <li><a href="#choose-prior-distribution-pbeta-1" id="toc-choose-prior-distribution-pbeta-1" class="nav-link" data-scroll-target="#choose-prior-distribution-pbeta-1">Choose Prior Distribution <span class="math inline">\(P(\beta)\)</span></a></li>
  </ul></li>
  <li><a href="#further-check-lambda-and-corresponding-term-in-the-posterior-likelihoodhow-the-penalized-terms-are-derived-from-the-bayesian-perspective" id="toc-further-check-lambda-and-corresponding-term-in-the-posterior-likelihoodhow-the-penalized-terms-are-derived-from-the-bayesian-perspective" class="nav-link" data-scroll-target="#further-check-lambda-and-corresponding-term-in-the-posterior-likelihoodhow-the-penalized-terms-are-derived-from-the-bayesian-perspective">Further check <span class="math inline">\(\lambda\)</span> and corresponding term in the posterior likelihood–How the penalized terms are derived from the Bayesian perspective</a>
  <ul class="collapse">
  <li><a href="#gaussian" id="toc-gaussian" class="nav-link" data-scroll-target="#gaussian">Gaussian:</a></li>
  <li><a href="#laplace" id="toc-laplace" class="nav-link" data-scroll-target="#laplace">Laplace:</a></li>
  </ul></li>
  <li><a href="#how-to-choose-the-lambda" id="toc-how-to-choose-the-lambda" class="nav-link" data-scroll-target="#how-to-choose-the-lambda">How to choose the <span class="math inline">\(\lambda\)</span>?</a></li>
  <li><a href="#lagrange-multipliers-perspective-of-penalized-models" id="toc-lagrange-multipliers-perspective-of-penalized-models" class="nav-link" data-scroll-target="#lagrange-multipliers-perspective-of-penalized-models">Lagrange Multipliers Perspective of Penalized Models</a></li>
  <li><a href="#geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective" id="toc-geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective" class="nav-link" data-scroll-target="#geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective">Geometric Interpretation of Penalized Models in Lagrange Multipliers Perspective</a></li>
  <li><a href="#geometric-interpretation-of-lasso-in-lagrange-multipliers-perspective-and-bayesian-perspective" id="toc-geometric-interpretation-of-lasso-in-lagrange-multipliers-perspective-and-bayesian-perspective" class="nav-link" data-scroll-target="#geometric-interpretation-of-lasso-in-lagrange-multipliers-perspective-and-bayesian-perspective">Geometric Interpretation of LASSO in Lagrange Multipliers Perspective and Bayesian Perspective</a></li>
  <li><a href="#story" id="toc-story" class="nav-link" data-scroll-target="#story">Story</a></li>
  <li><a href="#storyrob-tibshiranis-motivation" id="toc-storyrob-tibshiranis-motivation" class="nav-link" data-scroll-target="#storyrob-tibshiranis-motivation">Story—Rob Tibshirani’s motivation</a></li>
  <li><a href="#understanding-of-lambda" id="toc-understanding-of-lambda" class="nav-link" data-scroll-target="#understanding-of-lambda">Understanding of <span class="math inline">\(\lambda\)</span></a></li>
  <li><a href="#matrix-form-of-penalized-regression-models" id="toc-matrix-form-of-penalized-regression-models" class="nav-link" data-scroll-target="#matrix-form-of-penalized-regression-models">Matrix Form of Penalized Regression Models</a>
  <ul class="collapse">
  <li><a href="#MatrixFormRidge" id="toc-MatrixFormRidge" class="nav-link" data-scroll-target="#MatrixFormRidge">Ridge Regression</a></li>
  </ul></li>
  <li><a href="#matrix-form-of-penalized-regression-models-1" id="toc-matrix-form-of-penalized-regression-models-1" class="nav-link" data-scroll-target="#matrix-form-of-penalized-regression-models-1">Matrix Form of Penalized Regression Models</a>
  <ul class="collapse">
  <li><a href="#lasso-regression" id="toc-lasso-regression" class="nav-link" data-scroll-target="#lasso-regression">Lasso Regression</a></li>
  </ul></li>
  <li><a href="#differences-between-lasso-and-ridge" id="toc-differences-between-lasso-and-ridge" class="nav-link" data-scroll-target="#differences-between-lasso-and-ridge">Differences between Lasso and Ridge</a></li>
  <li><a href="#r-code-example-of-penalized-linear-regression-models" id="toc-r-code-example-of-penalized-linear-regression-models" class="nav-link" data-scroll-target="#r-code-example-of-penalized-linear-regression-models">R code Example of Penalized Linear Regression Models</a></li>
  <li><a href="#remark" id="toc-remark" class="nav-link" data-scroll-target="#remark">Remark</a>
  <ul class="collapse">
  <li><a href="#r-code-of-penalized-cox-regression---lasso-using-glmnet-package" id="toc-r-code-of-penalized-cox-regression---lasso-using-glmnet-package" class="nav-link" data-scroll-target="#r-code-of-penalized-cox-regression---lasso-using-glmnet-package">R Code of Penalized Cox Regression - Lasso using <code>glmnet</code> package</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a>
  <ul class="collapse">
  <li><a href="#the-greatest-truths-are-the-simplest." id="toc-the-greatest-truths-are-the-simplest." class="nav-link" data-scroll-target="#the-greatest-truths-are-the-simplest.">The greatest truths are the simplest.</a></li>
  <li><a href="#everything-is-connected-to-each-other.-good-theory-interpretes-this-connection-interestingly." id="toc-everything-is-connected-to-each-other.-good-theory-interpretes-this-connection-interestingly." class="nav-link" data-scroll-target="#everything-is-connected-to-each-other.-good-theory-interpretes-this-connection-interestingly.">Everything is connected to each other. Good theory interpretes this connection interestingly.</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a>
  <ul class="collapse">
  <li><a href="#proof-of-the-chebyshevs-inequality" id="toc-proof-of-the-chebyshevs-inequality" class="nav-link" data-scroll-target="#proof-of-the-chebyshevs-inequality">proof of the Chebyshev’s Inequality</a></li>
  <li><a href="#law-of-large-numbers-lln" id="toc-law-of-large-numbers-lln" class="nav-link" data-scroll-target="#law-of-large-numbers-lln">Law of Large Numbers (LLN)</a></li>
  <li><a href="#central-limit-theorem-clt" id="toc-central-limit-theorem-clt" class="nav-link" data-scroll-target="#central-limit-theorem-clt">Central Limit Theorem (CLT)</a></li>
  </ul></li>
  <li><a href="#confidence-interval" id="toc-confidence-interval" class="nav-link" data-scroll-target="#confidence-interval">Confidence Interval</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#what-is-the-good-confidence-interval" id="toc-what-is-the-good-confidence-interval" class="nav-link" data-scroll-target="#what-is-the-good-confidence-interval">what is the good confidence interval?</a></li>
  <li><a href="#pivotconstruction-of-confidence-interval" id="toc-pivotconstruction-of-confidence-interval" class="nav-link" data-scroll-target="#pivotconstruction-of-confidence-interval">Pivot—Construction of Confidence Interval</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Statistical_Learning (Mechine_Learning)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="referece-books" class="level1">
<h1>Referece books</h1>
<ul>
<li><p>Efron, B., &amp; Hastie, T. (2021). Computer age statistical inference, student edition: algorithms, evidence, and data science (Vol. 6). Cambridge University Press.</p></li>
<li><p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). An introduction to statistical learning.</p></li>
<li><p>Casella, G., &amp; Berger, R. (2024). Statistical inference. Chapman and Hall/CRC.</p></li>
<li><p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning.</p></li>
</ul>
</section>
<section id="statistics-history" class="level1">
<h1>statistics history</h1>
<p>population <span class="math inline">\(f(x;\theta)\)</span> and <span class="math inline">\(\theta\)</span> is unknown</p>
<section id="oldclassical-bayesian-estimation-moment-estimation" class="level2">
<h2 class="anchored" data-anchor-id="oldclassical-bayesian-estimation-moment-estimation">old/classical: Bayesian estimation; moment estimation</h2>
</section>
<section id="moment-estimation" class="level2">
<h2 class="anchored" data-anchor-id="moment-estimation">Moment estimation</h2>
<ul>
<li>For the moment estimation, we do not know the density so we could not calculate the expectation directly. Instead, we use LLN to approximate the expectation with sample mean. We then have</li>
</ul>
<p><span class="math display">\[
m_1(\theta) = E[X] \approx \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>then <span class="math display">\[
\hat \theta = m_1^{-1}(\frac{1}{n} \sum_{i=1}^n X_i)
\]</span></p>
<p>For the multiple parameter case, we could use multiple moments to estimate the parameters. (k equations to solve k unknown parameters). Here we do not know the precise distribution, so we could not calculate the moments directly. Instead, we use LLN to approximate the moments with sample moments.</p>
<p>Suppose <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are iid samples from the distribution with parameters <span class="math inline">\(f(x;\theta_1, \theta_2, \ldots, \theta_k)\)</span> then by LLM.</p>
<p>We then have approximatedmoment equation:</p>
<p><span class="math display">\[
E[X^j] = m_j(\theta_1, \theta_2, \ldots, \theta_k) \approx \frac{1}{n} \sum_{i=1}^n X_i^j, j=1,2,\ldots,k
\]</span></p>
<p>then we could solve the equations to get the estimators.</p>
<p>The solution <span class="math inline">\(\hat \theta_1, \hat \theta_2, \ldots, \hat \theta_k\)</span> are the moment estimators of the parameters.</p>
</section>
<section id="bayesian-estimation" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-estimation">Bayesian estimation</h2>
<p><span class="math inline">\(f(x|\theta)\)</span> is regarded as conditional density and the unknown parameter <span class="math inline">\(\theta\)</span> is the condition. <span class="math inline">\(\theta\)</span> is a random variable with prior distribution <span class="math inline">\(\pi(\theta)\)</span>.</p>
<p><span class="math inline">\((X, \theta)\)</span> has joint distribution <span class="math inline">\(f(x,\theta)=f(x|\theta)\pi(\theta)\)</span></p>
<p>Then we get the marginal distribution of <span class="math inline">\(X\)</span>:</p>
<p><span class="math inline">\(f_X(x) = \int_{-\infty}^{+\infty} f(x|\theta)\pi(\theta)d\theta=\int_{-\infty}^{+\infty} f(x;\theta)\pi(\theta)d\theta\)</span></p>
<p>the Bayesian formula is</p>
<p><span class="math display">\[ \frac {P(A|C_i)P(C_i)}{\sum_j P(A|C_j)P(C_j)} =P(C_i|A)\]</span></p>
<p><span class="math display">\[
f(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{f_X(x)} = \frac{f(x;\theta)\pi(\theta)}{\int_{-\infty}^{+\infty} f(x;\theta)\pi(\theta)d\theta}
\]</span></p>
<p>Then above we have used Bayes theorem to get the posterior distribution of <span class="math inline">\(\theta\)</span> given <span class="math inline">\(X=x\)</span></p>
<p>This is the updation of your belief or learning.</p>
<p>Bayesian statistics is unclear and ambiguous, but leaving us with no choice but to incorporate the most fundamental knowledge we have （不清不楚，不得不用）. We then update our understanding through experimental data. If it cannot be accomplished in one attempt, we iterate multiple times—eventually, we will gain a solid understanding of the parameters. The learning process is one of repeated iteration; there is no need to expect it to be completed in a single attempt.</p>
<p>When we do not know every knowledge about the model, we could use the prior of uniform distribution to represent our ignorance. That is, everyone is equally likely.</p>
<p>for <span class="math inline">\(f(x;\theta)\)</span>, we could calculate the expectation ; mode; CI</p>
<p>modern statistics</p>
<p><span class="math inline">\(L(\theta) = E[R(\theta,X)]\)</span></p>
<p><span class="math inline">\(\hat \theta = argmin_\theta L(\theta)\)</span></p>
<p>where <span class="math inline">\(R(\theta,X)\)</span> is the loss function</p>
</section>
<section id="fisher-mle" class="level2">
<h2 class="anchored" data-anchor-id="fisher-mle">Fisher: MLE</h2>
<p><strong>Fisher ambitious ： 不想依赖于贝叶斯，选prior的时候不能包含个人的任何认知，那就选客观的uniform分布</strong> (Fisher ambitious: Not wanting to rely on Bayesian, when choosing prior, no personal cognition can be included. Then choose the objective uniform distribution)</p>
<p><span class="math inline">\(\pi (\theta|x) {d\theta}= d\theta\)</span> uniform</p>
<p><span class="math inline">\(f(\theta|x_1, x_2, \ldots, x_n) = \frac{f(x_1, x_2, \ldots, x_n;\theta)\pi(\theta)}{\int f(x_1, x_2, \ldots, x_n;\theta)\pi(\theta)d\theta}\)</span> = <span class="math inline">\(L(\theta)\)</span></p>
<p><span class="math display">\[
= \frac{f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta) }{\int f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta)  d\theta}=L(\theta)
\]</span></p>
<p>Mode:</p>
<p>Find maximum of <span class="math inline">\(f(\theta|x_1, x_2, \ldots, x_n)\)</span> i.e.&nbsp;L(<span class="math inline">\(\theta\)</span>)</p>
<p><span class="math inline">\(\hat \theta = argmax_\theta L(\theta)=argmax_\theta \frac{f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta)}{f_X(x_1,x_2,..x_n)}\)</span></p>
<p><span class="math inline">\(= argmax_\theta f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta)=argmax_\theta L(\theta)\)</span> this is the likelihood function</p>
<p>When Fisher wanted to publish his idea to the Royal Society, he had been harshly commented through the discussion of the Royal Society. This is a good tradition but nowadays’ conference does ntot keep this. This means now they just have a presentation but the comments will not appear in the published paper.</p>
<p>then we could use log likelihood to simplify the calculation of the derivative to get the maximum likelihood estimator.</p>
<p>modern: Loss function</p>
</section>
</section>
<section id="modern-statistics" class="level1">
<h1>Modern Statistics</h1>
<p>Loss function is a “scanning apparatus”. It will take special value at the true parameter such as 0 or minimum point.</p>
<section id="empirical-kullback-leibler-divergence-loss-and-mle" class="level2">
<h2 class="anchored" data-anchor-id="empirical-kullback-leibler-divergence-loss-and-mle">Empirical Kullback-Leibler Divergence Loss and MLE</h2>
<p>(Empirical loss means that since we do not know the true distribution, we could use the sample to approximate the loss function. )</p>
<p>Consider the empirical loss function：</p>
<p><span class="math display">\[l(\theta) = \mathbb{E}_{\Theta} \left[ \frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)} \right]\]</span></p>
<p><span class="math display">\[= \int \frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)} f(x;\theta) dx\]</span></p>
<p><span class="math display">\[= \int \left[ \log \frac{f(x;\theta_0)}{f(x;\theta)} \right] f(x;\theta_0) dx\]</span></p>
<p><span class="math display">\[= \mathbb{E}_{\Theta_0} \left[ \log \frac{f(x;\theta_0)}{f(x;\theta)} \right]\]</span></p>
<p><span class="math display">\[\approx \frac{1}{n} \sum_{i=1}^{n} \log \frac{f(x_i;\theta_0)}{f(x_i;\theta)}\]</span></p>
<p><span class="math display">\[= \frac{1}{n} \sum_{i=1}^{n} \log f(x_i;\theta_0) - \frac{1}{n} \sum_{i=1}^{n} \log f(x_i;\theta)\]</span></p>
<p><span class="math display">\[\text{Argmin}(\hat{\theta}) = \text{Argmin}\left[ -\frac{1}{n} \sum_{i=1}^{n} \log f(x_i;\theta) \right]\]</span></p>
<p><span class="math display">\[= \text{Argmax}\left[ \sum_{i=1}^{n} \log f(x_i;\theta) \right]\]</span></p>
<p><span class="math display">\[= \text{Argmax}\left[ \log \prod_{i=1}^{n} f(x_i;\theta) \right]\]</span></p>
<section id="kl-divergence-property" class="level3">
<h3 class="anchored" data-anchor-id="kl-divergence-property">KL-divergence Property</h3>
<p>Let <span class="math inline">\(E_{\theta}\)</span> be the expectation of <span class="math inline">\(f_{\theta}(x)\)</span>.</p>
<p>the true model <span class="math inline">\(f(x;\theta_0)\)</span>，and the proposed model is <span class="math inline">\(f(x,\theta)\)</span>。</p>
<p>KL (entropy) divergence:</p>
<p><span class="math display">\[\mathbb{E}_{\theta} \left[ \frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)} \right]\]</span></p>
<p>Let <span class="math inline">\(\varphi(x) = x \log x\)</span>，then <span class="math inline">\(\varphi'(x) = \log x + 1\)</span>，<span class="math inline">\(\varphi''(x) = \frac{1}{x} &gt; 0\)</span>，</p>
<p>So <span class="math inline">\(\varphi(x)\)</span> is a convex function over <span class="math inline">\((0,+\infty)\)</span>.。</p>
<p>By Jensen inequality：</p>
<p><span class="math display">\[\varphi\left( \mathbb{E}_{\theta} \left[ \frac{f(x;\theta_0)}{f(x;\theta)} \right] \right) \leq \mathbb{E}_{\theta} \left[ \varphi\left( \frac{f(x;\theta_0)}{f(x;\theta)} \right) \right]\]</span></p>
<p><span class="math display">\[\mathbb{E}_{\theta} \left[ \frac{f(x;\theta_0)}{f(x;\theta)} \right] = \int_{-\infty}^{\infty} \frac{f(x;\theta_0)}{f(x;\theta)} f(x;\theta) dx\]</span></p>
<p><span class="math display">\[= \int_{-\infty}^{\infty} f(x;\theta_0) dx = 1\]</span></p>
<p>Thus</p>
<p><span class="math display">\[\varphi(1) = 1 \times \log 1 = 0 \leq \mathbb{E}_{\theta} \left[ \frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)} \right]\]</span></p>
<p>Loss function</p>
<p><span class="math display">\[l(\theta) = \mathbb{E}_{\theta} \left[ \frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)} \right] \geq 0\]</span></p>
<p>and <span class="math inline">\(l(\theta_0) = 0\)</span>，so <span class="math inline">\(\theta_0\)</span> is <span class="math inline">\(l(\theta)\)</span>’s minimum point</p>
<p>Empirical estimators are:</p>
<p><span class="math display">\[l(\theta) \approx \frac{1}{n} \sum_{i=1}^n \frac{f(x_i;\theta_0)}{f(x_i;\theta)} \log \frac{f(x_i;\theta_0)}{f(x_i;\theta)}\]</span></p>
<p><strong>Conclusion: Maximum likelihood estimation is equivalent to minimizing the KL divergence, which provides the theoretical basis of information theory for MLE. </strong></p>
<p>DeepSeek loss function—Feed the data - gradient descent—LLM</p>
</section>
</section>
<section id="sufficient-statistics" class="level2">
<h2 class="anchored" data-anchor-id="sufficient-statistics">Sufficient statistics</h2>
<p><span class="math display">\[
\hat \theta =T(X_1, X_2, \ldots, X_n)
\]</span> where</p>
<p><span class="math display">\[
T : R^n \to R
\]</span> The sufficient statistics is a compression of the data that retains all the information needed to estimate the parameter <span class="math inline">\(\theta\)</span>; It is the filtering of the information that thorwing away the noise; It is the distillation of the data that keeps the essence. Therefore, the sufficiency means no information is lost and we combine the information into 2 numbers.</p>
<ul>
<li><p>Two sufficient statistics have invertible transformation between them. Therefore there are many sufficient statistics.</p></li>
<li><p>sufficient statistics is not unbiased</p></li>
<li></li>
</ul>
</section>
<section id="rao-blackwell-theorem" class="level2">
<h2 class="anchored" data-anchor-id="rao-blackwell-theorem">Rao-Blackwell Theorem</h2>
<ul>
<li><p>Motivation: not only do we need the sufficiency of the statistics, but also we want the estimator to be unbiased and with minimum variance.</p></li>
<li><p>the proof includes the idea of changing the double expectation into the iterated expectation with one of it to be the conditional expectation(just like changing the double integral into the iterated integral). This proof demonstrates the charisma of the conditional expectation which improves the efficiency.</p></li>
</ul>
</section>
</section>
<section id="linear-regression" class="level1">
<h1>Linear Regression</h1>
<section id="ols" class="level2">
<h2 class="anchored" data-anchor-id="ols">OLS</h2>
<p><span class="math inline">\((y-Xw)^T \Sigma ^{-1 }(y-Xw)\)</span></p>
<p>homo assumption: <span class="math inline">\(\Sigma = \sigma^2 I\)</span> (a circle) but this can also be generalized to <span class="math inline">\(\Sigma = \sigma^2 D\)</span> where <span class="math inline">\(D\)</span> is a diagonal matrix (a ellipse).</p>
<p><span class="math inline">\(X(X^TX)^{-1}X^T\)</span> could be seen as a projection matrix, so if <span class="math inline">\(w=(X^TX)^{-1}X^Ty\)</span>, then <span class="math inline">\(Xw\)</span> is the projection of <span class="math inline">\(y\)</span> onto the column space of <span class="math inline">\(X\)</span>. From a similar perspective, <span class="math inline">\(y-Xw\)</span> is the residual vector, which is orthogonal to the column space of <span class="math inline">\(X\)</span>.</p>
<p>If singular, use psuedo-inverse: <span class="math inline">\(w = X^+y\)</span> that could be linked to SVD, where <span class="math inline">\(X^+\)</span> is the Moore-Penrose pseudo-inverse of <span class="math inline">\(X\)</span>, where SVD is <span class="math inline">\(X = U \Sigma V^T\)</span>, then <span class="math inline">\(X^+ = V \Sigma^{-1} U^T\)</span>. –this case has many solutions.</p>
<p>Remark: often we have n &gt;&gt; d+1, so the matrix <span class="math inline">\(X^TX\)</span> is invertible, but if <span class="math inline">\(n &lt; d+1\)</span>, then <span class="math inline">\(X^TX\)</span> is singular, and we need to use the pseudo-inverse. (<span class="math inline">\(X^TX\)</span> is invertible if and only if the columns of <span class="math inline">\(X\)</span> are linearly independent, which is often the case in practice when n &gt;&gt; d+1 , but not always.)</p>
<section id="eg-simplified-ceres-orbit-problem" class="level3">
<h3 class="anchored" data-anchor-id="eg-simplified-ceres-orbit-problem">Eg Simplified Ceres Orbit Problem</h3>
<p>SEE PPT</p>
<p>Gauss’s secret Weapon —- observe 3 points</p>
<p>200 years ago: agrues about the relationship of the linear regression and Normal — think of MLE</p>
</section>
</section>
<section id="regularization" class="level2">
<h2 class="anchored" data-anchor-id="regularization">regularization</h2>
<p>It is used to limited the situation that n &lt; d+1</p>
</section>
<section id="loss-function" class="level2">
<h2 class="anchored" data-anchor-id="loss-function">loss function</h2>
<p>Why using square : —- firstly, the geometric meaning — projection</p>
</section>
<section id="mle" class="level2">
<h2 class="anchored" data-anchor-id="mle">MLE</h2>
<p>Likelihood function</p>
</section>
</section>
<section id="ps-parameter-statistics" class="level1">
<h1>ps: parameter statistics</h1>
<p>Suppose the data satisfies the normal distribution — in that case, we could use the mean = <span class="math inline">\(\mu = \frac{1}{n} \sum_{i=1}^n x_i\)</span></p>
<section id="skew" class="level3">
<h3 class="anchored" data-anchor-id="skew">skew</h3>
<p>mean &gt; median &gt; mode — right skew</p>
<p>skew should use median or mode to represent some characteristic</p>
<p>Gaussion did not publish until 9 years later, he has really understood the relationship between linear regression and normal distribution,</p>
</section>
<section id="wls" class="level2">
<h2 class="anchored" data-anchor-id="wls">WLS</h2>
<p>Standard OLS assumes Var(εi ) = σ2 (constant variance)</p>
<p>Real-world data often violate this assumption, leading to heteroscedasticity (non-constant variance of errors), say Var(εi ) = σ2 Di or = <span class="math inline">\(\sigma_i^2\)</span>, where Di is a diagonal matrix with varying variances for each observation.</p>
<p>OLS estimates remain unbiased but not efficient (higher variance than necessary) when the errors are heteroscedastic (i.e., the variance of the errors is not constant across observations).</p>
<p>( error satisfies the normal distribution” fix x and we hace the points follow normal distribution)</p>
<p>Consequences of ignoring the heteroscedasticity:</p>
<p><span class="math inline">\((y-Xw)^T \Sigma ^{-1 }(y-Xw)\)</span></p>
<p><span class="math inline">\(\Sigma ^{-1 }\)</span> is the weight matrix which is diagonal for uncorrelated errors and could be a full matrix for correlated errors.</p>
<p>Remark: standard OLS assumes <span class="math inline">\(\Sigma = \sigma^2 I\)</span>, which is a special case of the weighted least squares where all weights are equal.</p>
<p>Problem: can not know D, So we use the weighted least squares to estimate D (iteration)</p>
<p>Now, the weighted residual sum of squares is:</p>
<p><span class="math display">\[
(y-Xw)^T \Sigma^{-1} (y-Xw)
\]</span></p>
<p>Let $ D = ^{-1} $, then the weighted residual sum of squares is:</p>
<p><span class="math display">\[
y^TDy - 2w^TX^TDy + w^TX^TDXw
\]</span></p>
<p><span class="math inline">\(\hat w_{OLS} = (X^TDX)^{-1}X^TDy\)</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>mod_wls <span class="op">=</span> sm.WLS(y, X, weights<span class="op">=</span><span class="dv">1</span><span class="op">/</span>var_est)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>res_wls <span class="op">=</span> mod_wls.fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="concave-and-hessian-matrices" class="level2">
<h2 class="anchored" data-anchor-id="concave-and-hessian-matrices">Concave and Hessian Matrices</h2>
<section id="semi--defined-matrices" class="level3">
<h3 class="anchored" data-anchor-id="semi--defined-matrices">(Semi-) Defined Matrices</h3>
<section id="for-the-non-eigenvector-x-the-role-of-matrix-a-can-be-regarded-as-projecting-x-onto-the-eigenvector-basis-and-then-stretching-it-in-each-basis-direction." class="level4">
<h4 class="anchored" data-anchor-id="for-the-non-eigenvector-x-the-role-of-matrix-a-can-be-regarded-as-projecting-x-onto-the-eigenvector-basis-and-then-stretching-it-in-each-basis-direction.">For the non-eigenvector x, the role of matrix A can be regarded as projecting x onto the eigenvector basis and then stretching it in each basis direction.</h4>
<p>Since the eigenvalues are non-negative, this stretching will not reverse the direction of the vector in any direction. From the perspective that the space of A is A linear combination of its eigenvector and generalized eigenvector, explain why the role of matrix A can be regarded as projecting x onto the basis of the eigenvectors:</p>
<p>Under the basis of the eigenvectors, when we apply A to a vector x, we can think of it as first projecting x onto the subspace spanned by the eigenvectors (write x as the linear combination of eigenvectors) and then stretching it according to the eigenvalues associated with those eigenvectors (Ax=<span class="math inline">\(\lambda\)</span>x).</p>
<ul>
<li>Remark: We do not need to consider the generalized eigenvectors here. This is because whether it is a positive definite, semi-positive definite or negative definite matrix, their eigenvectors can support the entire space without the need for generalized eigenvectors. This is because these matrices are all symmetric matrices, and symmetric matrices can always be diagonalized, and their eigenvectors can form a set of bases in space, i.e.</li>
</ul>
<p>Since <span class="math display">\[
A = A ^T
\]</span> and <span class="math display">\[
Av_1 = \lambda_1 v_1
\]</span> and <span class="math display">\[
Av_2 = \lambda_2 v_2
\]</span></p>
<p>then we have</p>
<p><span class="math display">\[
v_2^TAv_1=\lambda_1 v_2^Tv_1
\]</span> Since <span class="math inline">\(A=A^T\)</span>,</p>
<p><span class="math display">\[
v_2^TAv_1 = (Av_2)^Tv_1 = \lambda_2 v_2^Tv_1
\]</span></p>
<p>So</p>
<p><span class="math display">\[
\lambda_1 v_2^Tv_1 = \lambda_2 v_2^Tv_1
\]</span></p>
<p>So</p>
<p>if <span class="math inline">\(\lambda_1 \neq \lambda_2\)</span>, then <span class="math inline">\(v_2^Tv_1 = 0\)</span>, which means <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are orthogonal and therefore can form a basis of the space.</p>
</section>
<section id="use-the-understanding-of-left-multiplication-to-explain-why-the-role-of-matrix-a-can-be-regarded-as-projecting-x-onto-the-basis-of-the-eigenvectors" class="level4">
<h4 class="anchored" data-anchor-id="use-the-understanding-of-left-multiplication-to-explain-why-the-role-of-matrix-a-can-be-regarded-as-projecting-x-onto-the-basis-of-the-eigenvectors">??? USe the understanding of “Left multiplication” to explain why the role of matrix A can be regarded as projecting x onto the basis of the eigenvectors:</h4>
</section>
<section id="if-x-has-full-rank-then-xtx-is-positive-definite." class="level4">
<h4 class="anchored" data-anchor-id="if-x-has-full-rank-then-xtx-is-positive-definite.">if X has full rank, then <span class="math inline">\(X^TX\)</span> is positive definite.</h4>
<ul>
<li>Singular Value Decomposition (SVD):</li>
</ul>
<p>Every matrix can be decomposed into the product of three matrices: <span class="math inline">\(X = U \Sigma V^T\)</span>, where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal matrices and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with non-negative entries (the singular values). The columns of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are the left and right singular vectors, respectively.</p>
<p>We now have:</p>
<p><span class="math display">\[
X^TX= V \Sigma^T U^T U \Sigma V^T = V \Sigma^2 V^T
\]</span></p>
<p>This means that the eigenvalues of <span class="math inline">\(X^TX\)</span> are the squares of the singular values of <span class="math inline">\(X\)</span>, which are non-negative. Therefore, <span class="math inline">\(X^TX\)</span> is positive semi-definite.</p>
<p>When X has full rank, all singular values are positive, and thus <span class="math inline">\(X^TX\)</span> is positive definite.</p>
</section>
</section>
<section id="xtax-is-a-elliptic" class="level3">
<h3 class="anchored" data-anchor-id="xtax-is-a-elliptic"><span class="math inline">\(X^TAX\)</span> is a elliptic</h3>
<p><span class="math inline">\(A=Q\Lambda Q^T\)</span> is the eigendecomposition of <span class="math inline">\(A\)</span>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix whose columns are the eigenvectors of <span class="math inline">\(A\)</span>, and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix whose diagonal entries are the eigenvalues of <span class="math inline">\(A\)</span>.</p>
<p>Then we have:</p>
<p><span class="math display">\[
X^TAX = X^TQ\Lambda Q^TX = (Q^TX)^T\Lambda (Q^TX) = Y^T\Lambda Y=1
\]</span></p>
<p>where <span class="math inline">\(Y = Q^TX\)</span>.</p>
<p>this is saying <span class="math inline">\(\lambda_1 y_1^2 + \lambda_2 y_2^2 + ... + \lambda_n y_n^2 = 1\)</span>, where <span class="math inline">\(lambda_i\)</span> are the eigenvalues of <span class="math inline">\(A\)</span> and <span class="math inline">\(y_i\)</span> are the components of the vector <span class="math inline">\(Y\)</span>.</p>
<p>If <span class="math inline">\(\lambda_i &gt; 0\)</span> for all <span class="math inline">\(i\)</span>, then this is an equation of an ellipsoid in the space spanned by the eigenvectors of <span class="math inline">\(A\)</span>. A special case is when all <span class="math inline">\(\lambda_i\)</span> are equal, which corresponds to a sphere or a circle in 2D.</p>
</section>
</section>
<section id="see-3-blue-1-brown-video" class="level2">
<h2 class="anchored" data-anchor-id="see-3-blue-1-brown-video">See 3 blue 1 brown video</h2>
</section>
<section id="mahalanobis-distance-and-euclidean-distance" class="level2">
<h2 class="anchored" data-anchor-id="mahalanobis-distance-and-euclidean-distance">Mahalanobis distance and Euclidean distance</h2>
<p>The differences between the Euclidean distance and Mahalanobis is the linear transformation</p>
</section>
<section id="categorical-variables-in-linear-logistic-regression-reference-variable" class="level2">
<h2 class="anchored" data-anchor-id="categorical-variables-in-linear-logistic-regression-reference-variable">Categorical variables in linear / logistic regression –reference variable</h2>
</section>
</section>
<section id="glm" class="level1">
<h1>glm</h1>
<ul>
<li><p>random component: <span class="math inline">\(p (y|\theta, \phi) = exp ( (y \theta - b(\theta)) / a(\phi) + c(y, \phi) )\)</span></p></li>
<li><p>systematic component: <span class="math inline">\(Xw\)</span></p></li>
<li><p>link function:</p></li>
</ul>
<section id="special-case-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="special-case-logistic-regression">special case : logistic regression</h2>
<p>软分类 给概率而非分到哪一类</p>
<p>Supervised learning: 既定事实—-0，1</p>
<p>unsupervised learning： 无既定事实</p>
<p>机器学习： 感知机 （单个神经元）</p>
<p>非线形： 线性套函数</p>
<p>why logit</p>
<p>from 1 to 0 ;</p>
<p>KL distance — whethere the 2 distributions have significant difference<br>
</p>
<p>xiangnong 香浓shang</p>
<p>no analytic solution – Newton-Raphson method (IRLS) / gradient Ascent – only suitable for concave function</p>
<p><span class="math inline">\(w^{(t+1)} = w^{(t)} + \lambda ..\)</span>, where the lambda is called the rate of learning – too small might be slow, too large might be in a cycle which could not be converge</p>
<p>training data and test data</p>
<p>2,8 分 — cross validation</p>
<ul>
<li>Extension of logistic regression : Softmax regression (multinomial logistic regression) – multiple classes</li>
</ul>
<p>Micro jordan</p>
<p>Cross-entropy loss</p>
<p>neural networks use it</p>
<p>batch effect</p>
<p>可解释性神经网络： 给理由为什么lambda</p>
<p>alphago： 不懂围棋的人用足够厉害的算法就能赢</p>
<p>x <span class="math inline">\(-&gt;^f\)</span> x — score to decide the goodness of f</p>
<p>Fisher’s discriminant analysis (FDA) – linear discriminant analysis (LDA) – linear regression</p>
<p>Contribution:</p>
<p>Perceptron algorithm</p>
<p>LDA has the analytic solution</p>
<p>激活函数</p>
<p>logistic</p>
<p>RELU</p>
<p>Perceptron</p>
</section>
</section>
<section id="logistic-regression" class="level1">
<h1>Logistic regression</h1>
<p>Surf poster– gradient accent</p>
</section>
<section id="variable-transformation-formulathe-first-conversation-with-dr.-zhu" class="level1">
<h1>variable transformation formula–the first conversation with Dr.&nbsp;Zhu</h1>
<ul>
<li>idea: CDF’s derivative is PDF and ..</li>
</ul>
<p>Assume we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, where <span class="math inline">\(Y = g(X)\)</span> is a monotonic and differentiable function of <span class="math inline">\(X\)</span>. Our goal is to find the probability density function <span class="math inline">\(f_Y(y)\)</span> of <span class="math inline">\(Y\)</span>, given the probability density function <span class="math inline">\(f_X(x)\)</span> of <span class="math inline">\(X\)</span>.</p>
<section id="variable-transformation-formula" class="level3">
<h3 class="anchored" data-anchor-id="variable-transformation-formula">Variable Transformation Formula</h3>
<p>The variable transformation formula is stated as follows: <span class="math display">\[f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|\]</span></p>
</section>
<section id="proof-process" class="level3">
<h3 class="anchored" data-anchor-id="proof-process">Proof Process</h3>
<ol type="1">
<li><p><strong>Transformation of the Cumulative Distribution Function (CDF):</strong></p>
<p>First, consider the cumulative distribution function <span class="math inline">\(F_Y(y)\)</span> of <span class="math inline">\(Y\)</span>, which is defined as the probability that <span class="math inline">\(Y\)</span> is less than or equal to <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[F_Y(y) = P(Y \leq y)\]</span> Since <span class="math inline">\(Y = g(X)\)</span>, we can express this probability in terms of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[F_Y(y) = P(g(X) \leq y)\]</span></p></li>
<li><p><strong>Using the Inverse Function Transformation:</strong></p>
<p>If <span class="math inline">\(g(x)\)</span> is strictly increasing (or strictly decreasing), then <span class="math inline">\(g(X) \leq y\)</span> is equivalent to <span class="math inline">\(X \leq g^{-1}(y)\)</span> (or <span class="math inline">\(X \geq g^{-1}(y)\)</span>). Therefore:</p>
<p><span class="math display">\[F_Y(y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))\]</span></p>
<p>where <span class="math inline">\(F_X(x)\)</span> is the cumulative distribution function of <span class="math inline">\(X\)</span>.</p></li>
<li><p><strong>Deriving the Probability Density Function (PDF):</strong></p>
<p>To find the probability density function <span class="math inline">\(f_Y(y)\)</span> of <span class="math inline">\(Y\)</span>, we need to differentiate <span class="math inline">\(F_Y(y)\)</span> with respect to <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} F_X(g^{-1}(y))\]</span></p>
<p>Using the chain rule, we get:</p>
<p><span class="math display">\[f_Y(y) = f_X(g^{-1}(y)) \frac{d}{dy} g^{-1}(y)\]</span></p></li>
<li><p><strong>Considering the Absolute Value of the Derivative:</strong></p>
<p>Since the probability density function must be non-negative, we take the absolute value of the derivative: <span class="math display">\[f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|\]</span></p></li>
</ol>
<p>This completes the proof of the variable transformation formula. This formula is very useful when dealing with transformations of random variables, especially in statistical analysis and probability modeling. With this formula, we can derive the probability density function of one related random variable from the known probability density function of another.</p>
</section>
</section>
<section id="sampling-methods" class="level1">
<h1>Sampling Methods</h1>
<section id="bootstrap" class="level2">
<h2 class="anchored" data-anchor-id="bootstrap">Bootstrap</h2>
<section id="example-of-calculating-p-value-of-ks-statstics-goodness-of-fit-with-the-problem-of-censoring-data-distribution-estimate" class="level3">
<h3 class="anchored" data-anchor-id="example-of-calculating-p-value-of-ks-statstics-goodness-of-fit-with-the-problem-of-censoring-data-distribution-estimate">Example of calculating p-value of KS-statstics (Goodness of fit) with the problem of censoring data distribution estimate</h3>
<p>Research Question: No Difference Between Empirical (KM-method) and Theoretical Distributions(estimated) (Does My Interval-Censored Data Come from the Estimated (theoretical) Distribution?)</p>
<p><strong>Null Hypothesis</strong>: The data indeed come from the estimated distribution.</p>
<p><strong>Steps of the Parametric Bootstrap Method</strong>:</p>
<ol type="1">
<li><p><strong>Simulate a World Where the Null Hypothesis Holds</strong>:</p>
<ul>
<li><p>Generate a large number (B = 1000) of new samples from the estimated distribution.</p></li>
<li><p>Apply the exact same interval-censoring structure to each new sample (using the original left_times and right_times).</p></li>
</ul>
<p><strong>Assume the observed time points does not change</strong></p></li>
<li><p><strong>Calculate Bootstrap KS Values</strong>:</p>
<ul>
<li><p>For each bootstrap sample, compute the KS-statistic.</p></li>
<li><p>Obtain 1000 bootstrap KS values (ks_bootstrap).</p></li>
</ul></li>
<li><p><strong>Calculate the p-value</strong>:</p>
<ul>
<li>p = (number of ks_bootstrap <span class="math inline">\(≥\)</span> ks_stat) / B (a larger ks-stat indicates a greater difference between the data and the distribution).</li>
</ul></li>
</ol>
<p><strong>Interpretation</strong>: - A large p-value: If H₀ is true, the probability of observing a KS statistic as large as that from the original data is high. Therefore, we do not have sufficient evidence to reject H₀ and conclude that the data likely come from this estimated distribution. This is because a large p-value implies that it is quite probable to generate a sample with a difference from the theoretical distribution at least as large as that observed in the current data. In other words, the observed difference is not unusual and falls within the normal range of random variation. Hence, we cannot reject the hypothesis that the data come from the estimated distribution.</p>
</section>
</section>
</section>
<section id="some-perspectives-to-penalized-regression-mode" class="level1">
<h1>Some Perspectives to Penalized Regression Mode</h1>
<section id="ols-mle-solution-to-lr-with-issues" class="level2">
<h2 class="anchored" data-anchor-id="ols-mle-solution-to-lr-with-issues">OLS / MLE Solution to LR with issues</h2>
<p>We have learned the equivalence of Ordinary Least Squares (OLS) and Maximum Likelihood Estimation (MLE) in the context of linear regression assuming the error term follows a Gaussian distribution but with limitations as below:</p>
<ul>
<li>Issues:</li>
</ul>
<p>Even if the OLS or MLE solution is optimal in terms of minimizing the error, it may lead to overfitting, especially when the number of features is large relative to the number of observations. This can result in high variance and poor generalization to new data.</p>
<p><strong>How to handle the issues?</strong></p>
<span class="math display">\[\begin{align*}
\text{LASSO:} &amp; \quad Argmin_{\beta} \sum_{i=1}^{N} \left( y_i - \sum_{j}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j}^{p} |\beta_j|, \quad \text{where } \lambda &gt; 0 \\
\text{Ridge:} &amp; \quad Argmin_{\beta} \sum_{i=1}^{N} \left( y_i - \sum_{j}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j}^{p} \beta_j^2, \quad \text{where } \lambda &gt; 0
\end{align*}\]</span>
<ul>
<li><p>OLS perspective thinking: This shrinkage method not only achieves the objective of making the error to be smallest like OLS, but also prevents overfitting by penalizing large coefficients.</p></li>
<li><p>MLE perspective thinking: Compared with MLE, this method could be regarded as a Bayesian estimation that can avoid overfitting by introducing the prior distribution <span class="math inline">\(P(\beta)\)</span>, especially when the data volume is small, the prior information can play a regularization role.</p></li>
</ul>
</section>
<section id="why-does-the-terms-of-the-regularization-look-like-this" class="level2">
<h2 class="anchored" data-anchor-id="why-does-the-terms-of-the-regularization-look-like-this">Why does the terms of the regularization look like this?</h2>
<p>Just like what we have learned about the relationship between the OLS and MLE and get the idea of why the error term is the square shape, now we are going to use the perspective of <strong>Bayesian</strong> to understand the penalized terms</p>
<section id="choose-prior-distribution-pbeta" class="level3">
<h3 class="anchored" data-anchor-id="choose-prior-distribution-pbeta">Choose Prior Distribution <span class="math inline">\(P(\beta)\)</span></h3>
<p>Idea:</p>
<p>We insist that <span class="math inline">\(\beta\)</span> should be more likely to be small also with small variances (very near to 0). This means we should pick a distribution that has a peak around zero and decays quickly as we move away from zero.</p>
<ul>
<li><strong>Prior Distribution</strong>: In the context of penalized regression, we choose a prior distribution for the coefficients <span class="math inline">\(\beta\)</span> that reflects our beliefs about their values. Common choices include:
<ul>
<li><strong>Laplace Prior</strong>: Assumes coefficients follow a Laplace distribution, leading to <span class="math inline">\(L^1\)</span> regularization (Lasso Regression).</li>
<li><strong>Gaussian Prior</strong>: Assumes coefficients are normally distributed around zero, which leads to <span class="math inline">\(L^2\)</span> regularization (Ridge Regression).</li>
</ul></li>
</ul>
<p>This also means it would take extreame evidence with the data that we see in order to accept very large and very high variances beta because of the prior.</p>
</section>
<section id="choose-prior-distribution-pbeta-1" class="level3">
<h3 class="anchored" data-anchor-id="choose-prior-distribution-pbeta-1">Choose Prior Distribution <span class="math inline">\(P(\beta)\)</span></h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3590925628.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="321"></p>
</figure>
</div>
<section id="maximize-the-posterior-probability-to-get-the-point-estimation-of-beta-ridge" class="level4">
<h4 class="anchored" data-anchor-id="maximize-the-posterior-probability-to-get-the-point-estimation-of-beta-ridge">Maximize the posterior probability to get the point estimation of <span class="math inline">\(\beta\)</span> – Ridge</h4>
<p><span class="math display">\[
P(\beta|y) \propto P(y|\beta) \cdot P(\beta)
\]</span></p>
<p><span class="math display">\[
P(y|\beta) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{-\frac{(y - \beta^T x)^2}{2\sigma^2}\right\}
\]</span></p>
<p>And for the prior part, we assume <span class="math inline">\(beta\)</span> follows a Gaussian distribution, $(0, ’^2) $,</p>
<p><span class="math display">\[
P(\beta) = \frac{1}{\sqrt{2\pi}\sigma'} \exp\left\{-\frac{\beta^2}{2\sigma'^2}\right\}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
\hat{\beta} = \arg\max_\beta \log P(\beta|Y)
\]</span></p>
<p><span class="math display">\[
= \arg\max_{\beta} \log P(Y|\beta)P(\beta)
\]</span></p>
<p><span class="math display">\[
= \arg\max_{\beta} \log \prod_{i=1}^N P(y_i|\beta)P(\beta)
\]</span></p>
<p><span class="math display">\[
= \arg\max_{\beta}  \sum_{i=1}^N \log[P(y_i|\beta)P(\beta)]
\]</span></p>
<p><span class="math display">\[
= \arg\max_{\beta}  \sum_{i=1}^N \left[ \log\left(\frac{1}{\sqrt{2\pi}\sigma } \exp\left\{-\frac{(y - \beta^T x)^2}{2\sigma^2}\right\}\right) + \log(\frac{1}{\sqrt{2\pi}\sigma'})- \frac{\beta^2}{2\sigma'^2} \right]
\]</span></p>
<p><span class="math display">\[
= \arg\max_{\beta} \sum_{i=1}^N \left[ \log\left(\frac{1}{2\pi \sigma' \sigma}\right) - \frac{(y_i - \beta^T x_i)^2}{2\sigma^2} - \frac{\beta^2}{2\sigma'^2} \right]
\]</span></p>
<p><span class="math display">\[
= \arg\min_\beta \sum_{i=1}^N \left[ \frac{(y_i - \beta^T x_i)^2}{2\sigma^2} + \frac{\beta^2}{2\sigma'^2} \right]
\]</span></p>
<p><span class="math display">\[
= \arg\min_\beta (Y-X\beta)^T (\frac{1}{2\sigma^2}I_N) (Y-X\beta) + \frac{1}{\sigma'^2} \beta^2
\]</span></p>
<p>((Weighted least squares with the weight <span class="math inline">\(\frac{1}{2\sigma^2}I_N\)</span> and the penalty term <span class="math inline">\(\frac{1}{\sigma'^2} \beta^2\)</span></p>
<p>If we let <span class="math inline">\(\lambda = \frac{1}{\sigma'^2}\)</span>, then the loss function is <span class="math inline">\(L(\beta) = \sum_{i=1}^N (\beta^T x_i - y_i)^2 + \lambda \beta^T \beta\)</span>, which is equivalent to the minimization of the regularized least squares in <span class="math inline">\(L^2\)</span>.</p>
<ul>
<li>Remark of the Ridge Regression</li>
</ul>
<p>Ridge regression solution is also the posterior mean, this is because the likelihood of the data given the parameters is Gaussian, and the prior is also Gaussian, which results in a posterior that is also Gaussian by the property of the conjugate prior of the Gaussian distribution, i.e.</p>
<p><strong>NEED to ADD!</strong></p>
</section>
<section id="maximize-the-posterior-probability-to-get-the-point-estimation-of-beta-lasso" class="level4">
<h4 class="anchored" data-anchor-id="maximize-the-posterior-probability-to-get-the-point-estimation-of-beta-lasso">Maximize the posterior probability to get the point estimation of <span class="math inline">\(\beta\)</span> – Lasso</h4>
<p>Now the prior part is assumed to be a Laplace distribution, <span class="math inline">\(\beta \sim \text{Laplace}(0, b), i.e., P(\beta) = \frac{1}{2b} \exp\left(-\frac{|\beta|}{b}\right)\)</span>.</p>
<p><span class="math display">\[
\arg\max_{\beta}  \sum_{i=1}^N \left[ \log\left(\frac{1}{\sqrt{2\pi}\sigma_{\beta} } \exp\left\{-\frac{(y - \beta^T x)^2}{2\sigma^2}\right\}\right) - \log 2b - \frac{|\beta|}{b} \right]
\]</span></p>
<p><span class="math display">\[
= \arg\max_{\beta} \sum_{i=1}^N \left[ \log\left(\frac{1}{\sqrt{2\pi}\sigma_{\beta} 2b}\right) - \frac{(y_i - \beta^T x_i)^2}{2\sigma^2} - \frac{|\beta|}{b} \right]
\]</span></p>
<p><span class="math display">\[
= \arg\min_\beta \sum_{i=1}^N \left[ \frac{(y_i - \beta^T x_i)^2}{2\sigma^2} + \frac{|\beta|}{b} \right]
\]</span></p>
<p><span class="math display">\[
= \arg\min_\beta (Y-X\beta)^T (\frac{1}{2\sigma^2}I_N) (Y-X\beta) + \frac{1}{b} |\beta|
\]</span></p>
<p>If we let <span class="math inline">\(\lambda = \frac{1}{b}\)</span>, then the loss function is <span class="math inline">\(L(\beta) = \sum_{i=1}^N (\beta^T x_i - y_i)^2 + \lambda |\beta|\)</span>, which is equivalent to the minimization of the regularized least squares in <span class="math inline">\(L^1\)</span>.</p>
</section>
</section>
</section>
<section id="further-check-lambda-and-corresponding-term-in-the-posterior-likelihoodhow-the-penalized-terms-are-derived-from-the-bayesian-perspective" class="level2">
<h2 class="anchored" data-anchor-id="further-check-lambda-and-corresponding-term-in-the-posterior-likelihoodhow-the-penalized-terms-are-derived-from-the-bayesian-perspective">Further check <span class="math inline">\(\lambda\)</span> and corresponding term in the posterior likelihood–How the penalized terms are derived from the Bayesian perspective</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3590925628.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="162"></p>
</figure>
</div>
<section id="gaussian" class="level3">
<h3 class="anchored" data-anchor-id="gaussian">Gaussian:</h3>
<p><span class="math inline">\(\lambda = \frac{1}{\sigma'^2}\)</span> i.e.&nbsp;Larger <span class="math inline">\(\lambda\)</span> with less <span class="math inline">\(\sigma'^2\)</span> means more regularization corresponding to smaller variance of the prior distribution (normal)</p>
</section>
<section id="laplace" class="level3">
<h3 class="anchored" data-anchor-id="laplace">Laplace:</h3>
<p><span class="math inline">\(\lambda = \frac{1}{b}\)</span> i.e.&nbsp;Larger <span class="math inline">\(\lambda\)</span> with less <span class="math inline">\(b\)</span> means more regularization corresponding to smaller variance of the prior distribution (laplace)</p>
</section>
</section>
<section id="how-to-choose-the-lambda" class="level2">
<h2 class="anchored" data-anchor-id="how-to-choose-the-lambda">How to choose the <span class="math inline">\(\lambda\)</span>?</h2>
<ul>
<li><strong>Cross-Validation</strong>: A common method to choose the regularization parameter <span class="math inline">\(\lambda\)</span> is through cross-validation. This involves splitting the data into training and validation sets, fitting the model with different values of <span class="math inline">\(\lambda\)</span>, and selecting the one that minimizes the prediction error on the validation set.</li>
</ul>
</section>
<section id="lagrange-multipliers-perspective-of-penalized-models" class="level2">
<h2 class="anchored" data-anchor-id="lagrange-multipliers-perspective-of-penalized-models">Lagrange Multipliers Perspective of Penalized Models</h2>
<p>Lagrange multipliers provide a way to incorporate constraints into optimization problems. In the context of penalized regression, we can view the regularization term as a constraint on the size of the coefficients.</p>
<p>Choose Lasoo as an example, the Lagrange multipliers perspective could be expressed as:</p>
<p><span class="math display">\[
\left( \hat{\alpha}, \hat{\beta} \right) = \arg \min \left\{ \sum_{i=1}^{N} \left( y_{i} - \alpha - \sum_{j} \beta_{j} x_{ij} \right)^{2} \right\} \quad \text{subject to} \quad \sum_{j} \left| \beta_{j} \right| \leq t.
\]</span></p>
<p>i.e.</p>
<p><span class="math display">\[
g(\beta)= \sum_{j} \left| \beta_{j} \right| - t \leq 0
\]</span></p>
<p><span class="math display">\[
L(\alpha,\beta, \lambda) = \sum_{i=1}^{N} \left( y_{i} - \alpha - \sum_{j} \beta_{j} x_{ij} \right)^{2} + \lambda \left( \sum_{j} \left| \beta_{j} \right| - t \right)
\]</span></p>
<p><span class="math display">\[
\partial L / \partial \alpha = 0
\]</span></p>
<p><span class="math display">\[
\hat{\alpha} = \bar{y} - \sum_{j} \hat{\beta}_{j} \bar{x}_{j}
\]</span></p>
<p><span class="math display">\[
\partial L / \partial \beta_{j} = 0
\]</span></p>
<p><span class="math display">\[
-2 \sum_{i=1}^{N} x_{ij} \left( y_{i} - \hat{\alpha} - \sum_{k} \hat{\beta}_{k} x_{ik} \right) + \lambda \cdot sign(\hat{\beta}_{j}) = 0
\]</span></p>
<p>This may use the iterative method to solve the equation.</p>
<p>Generally, the formula could be expressed as:</p>
<p><span class="math display">\[
\min_{\beta} \left\{ \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_p^p \right\}
\]</span></p>
<p>where <span class="math inline">\(\lVert \beta \rVert_p^p\)</span> is the <span class="math inline">\(L^p\)</span> norm of the coefficients, which serves as a penalty term.</p>
<p>The gradient of the objective function of Ridge with respect to <span class="math inline">\(\beta\)</span> is given by:</p>
<p><span class="math display">\[
\nabla J(\beta) = -2X^T(y - X\beta) + \lambda \nabla \lVert \beta \rVert_p^p
\]</span></p>
<p>(see matrix form (<a href="#MatrixFormRidge">Go to Matrix Form</a>) below for more details)</p>
<p>This should be set to zero to find the optimal <span class="math inline">\(\beta\)</span>, which means the gradient of the loss function (the first term) is equal to the gradient of the penalty term (the second term) scaled by <span class="math inline">\(\lambda\)</span>.</p>
</section>
<section id="geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective" class="level2">
<h2 class="anchored" data-anchor-id="geometric-interpretation-of-penalized-models-in-lagrange-multipliers-perspective">Geometric Interpretation of Penalized Models in Lagrange Multipliers Perspective</h2>
<p><img src="images/clipboard-4201197806.png" class="img-fluid"></p>
</section>
<section id="geometric-interpretation-of-lasso-in-lagrange-multipliers-perspective-and-bayesian-perspective" class="level2">
<h2 class="anchored" data-anchor-id="geometric-interpretation-of-lasso-in-lagrange-multipliers-perspective-and-bayesian-perspective">Geometric Interpretation of LASSO in Lagrange Multipliers Perspective and Bayesian Perspective</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3731736673.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="404"></p>
</figure>
</div>
</section>
<section id="story" class="level2">
<h2 class="anchored" data-anchor-id="story">Story</h2>
<ul>
<li>Humorous story</li>
</ul>
<p>Ryan Tibshirani interviews Rob Tibshirani about his work on the Lasso. Ryan said that when he was young he was into draw a diamond and circles and he happened to draw a circle that just touched the corner of a diamond and that led to a big idea Rob did not credited him for. “I did not cite you,” Rob answered humorously.</p>
<ul>
<li>Idea</li>
</ul>
<p>Yes, this is actually an interesting geometric meaning of LASSO.</p>
<ul>
<li>To me(a biostat student) —a big meaning!!</li>
</ul>
<p>Rob Tibshirani previously went back to his hometown of Toronto when he was a professor in preventative medicine and biostatistics and he was working on LASSO here.</p>
</section>
<section id="storyrob-tibshiranis-motivation" class="level2">
<h2 class="anchored" data-anchor-id="storyrob-tibshiranis-motivation">Story—Rob Tibshirani’s motivation</h2>
<ul>
<li><p>Paper about non-negative garrote by Leo Breiman</p>
<p>Leo started with least squares estimates and then multiply them by non-negative constants that were bounderd by some bounded T, which had the effect of producing a sparse solution because being non-negative with a bound of 0 forces sparsity.</p></li>
<li><p>why choose “Lasso” name</p>
<p>More gentle than garrote and more softer term than garrote</p></li>
<li><p>Many similar things that time</p>
<p>You do not need to do research on unique things, but promising things.</p></li>
<li><p>From LM to GLM to Cox, this generalization is cool</p></li>
</ul>
</section>
<section id="understanding-of-lambda" class="level2">
<h2 class="anchored" data-anchor-id="understanding-of-lambda">Understanding of <span class="math inline">\(\lambda\)</span></h2>
<p>Lagrange Multipliers help explain how <span class="math inline">\(\lambda\)</span> balances fitting the data well:</p>
<p>Bigger <span class="math inline">\(\lambda\)</span> makes the ellipse (representing the constraint region in the parameter space in WLS, if it satisfies the homoscedasticity, it would be a circle) smaller. In this case, the gradient of <span class="math inline">\(\beta\)</span> becomes steeper with a larger <span class="math inline">\(\lambda\)</span>, which means that the optimization algorithm will grow the penalty of <span class="math inline">\(\beta\)</span> to stay within the shrinking feasible region. This helps reduce variance and prevent overfitting, but overly large <span class="math inline">\(\lambda\)</span> can cause underfitting.</p>
</section>
<section id="matrix-form-of-penalized-regression-models" class="level2">
<h2 class="anchored" data-anchor-id="matrix-form-of-penalized-regression-models">Matrix Form of Penalized Regression Models</h2>
<section id="MatrixFormRidge" class="level3">
<h3 class="anchored" data-anchor-id="MatrixFormRidge">Ridge Regression</h3>
<p><strong>Objective Function</strong></p>
<p><span class="math display">\[
\min_{\beta} \left\{ \lVert y - X\beta \rVert_2^2 + \lambda \lVert \beta \rVert_2^2 \right\}
\]</span></p>
<p><strong>Matrix Form Derivation:</strong></p>
<ul>
<li>Expand the loss term: <span class="math display">\[
  \lVert y - X\beta \rVert_2^2 = (y - X\beta)^T (y - X\beta) = y^T y - 2y^T X\beta + \beta^T X^T X\beta.
  \]</span></li>
<li>Add the <span class="math inline">\(L^2\)</span> penalty: <span class="math display">\[
  J(\beta) = y^T y - 2y^T X\beta + \beta^T X^T X\beta + \lambda \beta^T \beta.
  \]</span></li>
<li>Compute the gradient: <span class="math display">\[
  \frac{\partial J}{\partial \beta} = \nabla J(\beta) = -2X^T(y - X\beta) + \lambda \nabla \lVert \beta \rVert_p^p=-2X^T y + 2X^T X\beta + 2\lambda \beta.
  \]</span></li>
<li>Set gradient to zero: <span class="math display">\[
  -X^T y + X^T X\beta + \lambda \beta = 0 \implies (X^T X + \lambda I)\beta = X^T y.
  \]</span></li>
<li>Solve for <span class="math inline">\(\beta\)</span>: <span class="math display">\[
  \hat{\beta}_{\text{ridge}} = (X^T X + \lambda I)^{-1} X^T y
  \]</span></li>
</ul>
<p><strong>Why Regularization Helps:</strong></p>
<p>The term <span class="math inline">\(\lambda I\)</span> ensures <span class="math inline">\(X^T X + \lambda I\)</span> is always invertible (since <span class="math inline">\(\lambda &gt; 0\)</span> adds positive values to the diagonal, guaranteeing full rank). This avoids singularity issues in <span class="math inline">\(X^T X\)</span> when features are collinear.</p>
</section>
</section>
<section id="matrix-form-of-penalized-regression-models-1" class="level2">
<h2 class="anchored" data-anchor-id="matrix-form-of-penalized-regression-models-1">Matrix Form of Penalized Regression Models</h2>
<section id="lasso-regression" class="level3">
<h3 class="anchored" data-anchor-id="lasso-regression">Lasso Regression</h3>
<p>Formula:</p>
<p><span class="math display">\[
\hat{\beta}_{\text{lasso}} = \arg\min_{\beta} \left\{ \|y - X\beta\|_2^2 + \lambda \|\beta\|_1 \right\}
\]</span></p>
<p>Want</p>
<p><span class="math display">\[
\min_{\beta} \left\{ \|y - X\beta\|_2^2 + \lambda \sum_{j=1}^p |\beta_j| \right\}
\]</span></p>
<p>Matrix Form Derivation:</p>
<p>Lasso lacks a closed-form solution due to the non-differentiable L1 norm. Instead, we use the subgradient optimality condition:</p>
<ul>
<li>Subgradient equation: <span class="math display">\[
  -2X^T(y - X\beta) + \lambda \cdot \text{sign}(\beta) = 0,
  \]</span> where <span class="math inline">\(\text{sign}(\beta)\)</span> is defined component-wise:</li>
</ul>
<p><strong>Key Insight</strong></p>
<ul>
<li>For <span class="math inline">\(\beta_j \neq 0\)</span>, the solution balances data fit and shrinkage.</li>
<li>For <span class="math inline">\(\beta_j = 0\)</span>, the condition requires: <span class="math display">\[
  \left| 2 \mathbf{x}_j^T (y - X\beta) \right| \leq \lambda.
  \]</span> This induces sparsity (exact zeros in <span class="math inline">\(\beta\)</span>), which Ridge cannot achieve.</li>
</ul>
</section>
</section>
<section id="differences-between-lasso-and-ridge" class="level2">
<h2 class="anchored" data-anchor-id="differences-between-lasso-and-ridge">Differences between Lasso and Ridge</h2>
<ul>
<li>Lasso could realize variable selection by shrinking some coefficients to exactly zero, while Ridge regression shrinks all coefficients but does not set any to zero.</li>
</ul>
</section>
<section id="r-code-example-of-penalized-linear-regression-models" class="level2">
<h2 class="anchored" data-anchor-id="r-code-example-of-penalized-linear-regression-models">R code Example of Penalized Linear Regression Models</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install and load necessary packages</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("glmnet")</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("ISLR2")</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: Matrix</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded glmnet 4.1-8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Hitters dataset</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Hitters)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove missing values</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the predictor matrix and response variable</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># exclude the intercept term</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., Hitters)[, <span class="sc">-</span><span class="dv">1</span>] </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Hitters<span class="sc">$</span>Salary</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and test sets</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x), <span class="fu">nrow</span>(x) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> (<span class="sc">-</span>train)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> y[test]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the Lasso model on the training data</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># LASSO-LR</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>lasso.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                    <span class="at">lambda =</span> grid, <span class="at">family =</span> <span class="st">'gaussian'</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the Lasso model to visualize coefficient paths</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):
collapsing to unique 'x' values</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="stat_learn_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cross-validation to find the optimal lambda</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="stat_learn_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test set using the optimal lambda</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>lasso.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso.mod, <span class="at">s =</span> bestlam, <span class="at">newx =</span> x[test, ])</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>test.mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((lasso.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Test MSE with Lasso and optimal lambda:"</span>, test.mse))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Test MSE with Lasso and optimal lambda: 143673.618543046"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the Lasso model on the full dataset using the optimal lambda</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>lasso.full <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>,<span class="at">lambda =</span> bestlam)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>lasso.coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso.full)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Lasso coefficients on the full dataset:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Lasso coefficients on the full dataset:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(lasso.coef)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>20 x 1 sparse Matrix of class "dgCMatrix"
                       s0
(Intercept)   -3.42073206
AtBat          .         
Hits           2.02965136
HmRun          .         
Runs           .         
RBI            .         
Walks          2.24850782
Years          .         
CAtBat         .         
CHits          .         
CHmRun         0.04994886
CRuns          0.22212444
CRBI           0.40183027
CWalks         .         
LeagueN       20.83775664
DivisionW   -116.39019204
PutOuts        0.23768309
Assists        .         
Errors        -0.93567863
NewLeagueN     .         </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>result_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(lasso.coef[lasso.coef <span class="sc">!=</span> <span class="dv">0</span>], <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in matrix(lasso.coef[lasso.coef != 0], ncol = 3, byrow = TRUE): data
length [10] is not a sub-multiple or multiple of the number of rows [4]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(result_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]         [,2]      [,3]
[1,] -3.42073206    2.0296514 2.2485078
[2,]  0.04994886    0.2221244 0.4018303
[3,] 20.83775664 -116.3901920 0.2376831
[4,] -0.93567863   -3.4207321 2.0296514</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 提取非零系数（排除截距）</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>non_zero_coef <span class="ot">&lt;-</span> lasso.coef[lasso.coef[,<span class="dv">1</span>] <span class="sc">!=</span> <span class="dv">0</span>, ][<span class="sc">-</span><span class="dv">1</span>]  <span class="co"># 去除截距项</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>coef_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">names</span>(non_zero_coef),</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">Coefficient =</span> <span class="fu">as.numeric</span>(non_zero_coef)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 按系数绝对值排序</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>coef_df <span class="ot">&lt;-</span> coef_df[<span class="fu">order</span>(<span class="fu">abs</span>(coef_df<span class="sc">$</span>Coefficient), <span class="at">decreasing =</span> <span class="cn">TRUE</span>), ]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘制条形图</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(coef_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Variable, Coefficient), </span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>                   <span class="at">y =</span> Coefficient, </span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>                   <span class="at">fill =</span> <span class="fu">ifelse</span>(Coefficient <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">"Positive"</span>, <span class="st">"Negative"</span>))) <span class="sc">+</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"Positive"</span> <span class="ot">=</span> <span class="st">"dodgerblue"</span>, <span class="st">"Negative"</span> <span class="ot">=</span> <span class="st">"firebrick"</span>)) <span class="sc">+</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Lasso Regression Coefficients"</span>,</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="fu">paste</span>(<span class="st">"Optimal lambda ="</span>, <span class="fu">round</span>(bestlam, <span class="dv">4</span>)),</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Predictor Variables"</span>,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Coefficient Value"</span>,</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill =</span> <span class="st">"Effect Direction"</span>) <span class="sc">+</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span>  <span class="co"># 水平条形图更易阅读变量名</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>,</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>, <span class="at">size =</span> <span class="dv">14</span>),</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.text.y =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="stat_learn_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="remark" class="level2">
<h2 class="anchored" data-anchor-id="remark">Remark</h2>
<p>Actually the penalized model could be applied in many regression models not only for linear regression, but also for logistic regression, Poisson regression, Cox regression, etc.</p>
<p>The penalized terms could be added to the loss function of these models in a similar way.</p>
<section id="r-code-of-penalized-cox-regression---lasso-using-glmnet-package" class="level3">
<h3 class="anchored" data-anchor-id="r-code-of-penalized-cox-regression---lasso-using-glmnet-package">R Code of Penalized Cox Regression - Lasso using <code>glmnet</code> package</h3>
<p>Want to minimize <span class="math display">\[
-\log \left( \prod_{i:\delta_i=1} \frac{\exp\left(\sum_{j=1}^p x_{ij}\beta_j\right)}{\sum_{i':y_{i'}\geq y_i}\exp\left(\sum_{j=1}^p x_{i'j}\beta_j\right)} \right) + \lambda P(\beta),
\]</span> where <span class="math inline">\(\delta_i\)</span> is the indicator function for censoring and <span class="math inline">\(P(\beta) = \sum_{j=1}^p \beta_j^2\)</span> corresponds to a ridge penalty, or <span class="math inline">\(P(\beta) = \sum_{j=1}^p |\beta_j|\)</span> corresponds to a lasso penalty.</p>
<p><strong>R Code example for LASSO in Cox Regression applied on selecting important features of pesticide poisoning</strong></p>
<p><a href="https://yutong04.github.io/">Click here to see LASSO code provided by SURF2024 instructed by Dr.He</a></p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>We have seen many perspectives to understand the penalized regression models, including:</p>
<ul>
<li><p>Bayes</p></li>
<li><p>Lagrange multipliers</p></li>
<li><p>matrix form</p></li>
</ul>
<section id="the-greatest-truths-are-the-simplest." class="level3">
<h3 class="anchored" data-anchor-id="the-greatest-truths-are-the-simplest.">The greatest truths are the simplest.</h3>
<p>Penalized regression models balance the trade-off between fitting the data well and keeping the model simple.</p>
</section>
<section id="everything-is-connected-to-each-other.-good-theory-interpretes-this-connection-interestingly." class="level3">
<h3 class="anchored" data-anchor-id="everything-is-connected-to-each-other.-good-theory-interpretes-this-connection-interestingly.">Everything is connected to each other. Good theory interpretes this connection interestingly.</h3>
<p><strong>Linear Regression</strong>: <strong>OLS</strong> and <strong>MLE</strong></p>
<p><strong>Penalized regression models</strong>: <strong>Lagrange multiplier</strong> (restriction in the geometric meaning and also algebra interpretation) and <strong>Bayesian perspective</strong> (prior: history knowledge restriction)</p>
</section>
</section>
</section>
<section id="background" class="level1">
<h1>Background</h1>
<section id="proof-of-the-chebyshevs-inequality" class="level2">
<h2 class="anchored" data-anchor-id="proof-of-the-chebyshevs-inequality">proof of the Chebyshev’s Inequality</h2>
<ul>
<li>Proof of the Markov’s Inequality</li>
</ul>
<p>If <span class="math inline">\(X\)</span> is a non-negative random variable and <span class="math inline">\(a&gt;0\)</span>, then <span class="math display">\[P(X\ge a) \le \frac{E(X)}{a}\]</span> <span class="math display">\[
E(X) = \int_{0}^{\infty} x f(x) \, dx
\]</span></p>
<p><span class="math display">\[
E(X) = \int_{0}^{a} x f(x) \, dx + \int_{a}^{\infty} x f(x) \, dx
\]</span> <span class="math display">\[
\int_{a}^{\infty} x f(x) \, dx \geq \int_{a}^{\infty} a f(x) \, dx = a \int_{a}^{\infty} f(x) \, dx
\]</span></p>
<p><span class="math display">\[
E(X) \geq a \cdot P(X \geq a)
\]</span></p>
<p><span class="math display">\[
P(X \geq a) \leq \frac{E(X)}{a}
\]</span></p>
<ul>
<li>Proof of the Chebyshev’s Inequality</li>
</ul>
<p><span class="math display">\[
P((X - \mu)^2 \geq k^2\sigma^2) \leq \frac{E[(X-\mu)^2]}{k^2\sigma^2}=\frac{\sigma^2}{k^2\sigma^2} = \frac{1}{k^2}
\]</span></p>
<p><span class="math display">\[
P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}
\]</span></p>
</section>
<section id="law-of-large-numbers-lln" class="level2">
<h2 class="anchored" data-anchor-id="law-of-large-numbers-lln">Law of Large Numbers (LLN)</h2>
<ul>
<li>Weak Law of Large Numbers</li>
</ul>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be a sequence of i.i.d. random variables with finite mean <span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>. Then for any <span class="math inline">\(\epsilon &gt; 0\)</span>,</p>
<p><span class="math display">\[
P\left(\left|\frac{1}{n}\sum_{i=1}^{n} X_i - \mu\right| \geq \epsilon\right) \to 0 \text{ as } n \to \infty
\]</span></p>
<ul>
<li>Strong Law of Large Numbers</li>
</ul>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be a sequence of i.i.d. random variables with finite mean <span class="math inline">\(\mu\)</span>. Then,</p>
<p><span class="math display">\[
P\left(\lim_{n \to \infty} \frac{1}{n}\sum_{i=1}^{n} X_i = \mu\right) = 1
\]</span></p>
<ul>
<li>Proof of the Weak Law of Large Numbers</li>
</ul>
<p>By Chebyshev’s inequality, for any <span class="math inline">\(\epsilon &gt; 0\)</span>,</p>
<p><span class="math display">\[
P\left(\left|\frac{1}{n}\sum_{i=1}^{n} X_i - \mu\right| \geq \epsilon\right) \leq \frac{\text{Var}\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right)}{\epsilon^2} = \frac{\frac{1}{n^2}\cdot n\sigma^2}{\epsilon^2}=\frac{\sigma^2/n}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2}
\]</span></p>
<p>As <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\frac{\sigma^2}{n\epsilon^2} \to 0\)</span>. Therefore, <span class="math display">\[
P\left(\left|\frac{1}{n}\sum_{i=1}^{n} X_i - \mu\right| \geq \epsilon\right) \to 0 \text{ as } n \to \infty
\]</span></p>
<ul>
<li>Proof of the Strong Law of Large Numbers (using Borel-Cantelli Lemma)</li>
</ul>
<p>By Chebyshev’s inequality, for any <span class="math inline">\(\epsilon &gt; 0\)</span>,</p>
<p><span class="math display">\[
P\left(\left|\frac{1}{n}\sum_{i=1}^{n} X_i - \mu\right| \geq \epsilon\right) \leq \frac{\sigma^2}{n\epsilon^2}
\]</span></p>
<p>Let <span class="math inline">\(A_n = \left\{\left|\frac{1}{n}\sum_{i=1}^{n} X_i - \mu\right| \geq \epsilon\right\}\)</span>. Then,</p>
<p><span class="math display">\[
\sum_{n=1}^{\infty} P(A_n) \leq \sum_{n=1}^{\infty} \frac{\sigma^2}{n\epsilon^2} = \frac{\sigma^2}{\epsilon^2} \sum_{n=1}^{\infty} \frac{1}{n}
\]</span></p>
<p>The series <span class="math inline">\(\sum_{n=1}^{\infty} \frac{1}{n}\)</span> diverges, so we cannot directly apply the Borel-Cantelli lemma here. However, we can use a modified approach. Consider the events <span class="math inline">\(B_k = \left\{\left|\frac{1}{2^k}\sum_{i=1}^{2^k} X_i - \mu\right| \geq \epsilon\right\}\)</span>. Then,</p>
<p><span class="math display">\[
\sum_{k=1}^{\infty} P(B_k) \leq \sum_{k=1}^{\infty} \frac{\sigma^2}{2^k\epsilon^2} = \frac{\sigma^2}{\epsilon^2} \sum_{k=1}^{\infty} \frac{1}{2^k} = \frac{\sigma^2}{\epsilon^2}
\]</span></p>
<p>Since <span class="math inline">\(\sum_{k=1}^{\infty} P(B_k)\)</span> converges, by the Borel-Cantelli lemma(如果一系列独立事件的概率之和是有限的，那么这些事件无限次发生的概率为零), we have</p>
<p><span class="math display">\[
P(B_k \text{ i.o.}) = 0
\]</span></p>
<p>(i.o means infinitely often)</p>
<p>(p.s.: if 一系列独立事件的概率之和是无限的，并且这些事件是独立的，那么这些事件无限次发生的概率为一)</p>
<p>This implies that <span class="math display">\[
P\left(\lim_{k \to \infty} \frac{1}{2^k}\sum_{i=1}^{2^k} X_i = \mu\right) = 1
\]</span></p>
<p>Now, for any <span class="math inline">\(n\)</span>, there exists a <span class="math inline">\(k\)</span> such that <span class="math inline">\(2^k \leq n &lt; 2^{k+1}\)</span>. We can write <span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n} X_i = \frac{1}{n}\left(\sum_{i=1}^{2^k} X_i + \sum_{i=2^k+1}^{n} X_i\right)
\]</span></p>
<p>As <span class="math inline">\(k \to \infty\)</span>, the second term <span class="math inline">\(\frac{1}{n}\sum_{i=2^k+1}^{n} X_i\)</span> becomes negligible, and we have <span class="math display">\[
\lim_{n \to \infty} \frac{1}{n}\sum_{i=1}^{n} X_i = \mu
\]</span></p>
<p>with probability 1. Therefore,</p>
<p><span class="math display">\[
P\left(\lim_{n \to \infty} \frac{1}{n}\sum_{i=1}^{n} X_i = \mu\right) = 1
\]</span></p>
</section>
<section id="central-limit-theorem-clt" class="level2">
<h2 class="anchored" data-anchor-id="central-limit-theorem-clt">Central Limit Theorem (CLT)</h2>
</section>
</section>
<section id="confidence-interval" class="level1">
<h1>Confidence Interval</h1>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Statistical inference is a solution with likelihood that such solution is successful.</p>
<ul>
<li><p>Since we want the probability of the estimation and the probability of a point estimation is always 0, we need to consider the interval estimation.</p></li>
<li><p>guess of <span class="math inline">\(\theta_0\)</span> needs us to know the domain of <span class="math inline">\(\theta_0\)</span> [L,U].</p>
<p>Let the sample to be <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> and the domain is a random interval <span class="math inline">\([L(X_1, X_2, \ldots, X_n), U(X_1, X_2, \ldots, X_n)]\)</span> with L &lt; U. The success is <span class="math inline">\(P(L \leq \theta_0 \leq U)=1-\alpha\)</span>.</p></li>
<li><p>metaphor using the fisherman catching fish</p>
<p>The fisherman wants to catch a fish with length <span class="math inline">\(\theta_0\)</span>. He uses a bamboo bucket with size [L,U] to randomly catch the fish. If the fish is in the net, then he is successful. The size of the net is random because it depends on the sample data.</p>
<p>The hook is the point estimation of <span class="math inline">\(\theta_0\)</span>.</p></li>
</ul>
</section>
<section id="what-is-the-good-confidence-interval" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-good-confidence-interval">what is the good confidence interval?</h2>
<p>Given the confidence level <span class="math inline">\(1-\alpha\)</span>, we want to find the interval [L,U] as small as possible such that <span class="math inline">\(P(L \leq \theta_0 \leq U)=1-\alpha\)</span> to ensure the accuracy of the estimation.</p>
</section>
<section id="pivotconstruction-of-confidence-interval" class="level2">
<h2 class="anchored" data-anchor-id="pivotconstruction-of-confidence-interval">Pivot—Construction of Confidence Interval</h2>
<ul>
<li><p>pivot : <span class="math inline">\(T(x_1, x_2, \ldots, x_n; \theta_0) \in f(t)\)</span>. The <span class="math inline">\(\theta_0\)</span> is apparent in the pivot</p></li>
<li><p>statistics : <span class="math inline">\(T(x_1, x_2, \ldots, x_n) \in f(t; \theta_0)\)</span>. The <span class="math inline">\(theta_0\)</span> is hidden in the statistic so we must excavate it to get the real meaning of it.</p></li>
</ul>
<p>A pivot is a function of the sample data and the parameter of interest that has a probability distribution that does not depend on the parameter. Pivots are useful for constructing confidence intervals because they allow us to make probabilistic statements about the parameter without knowing its true value.</p>
<p>Getting L and U such that the confidence interval is as small as possible is a problem of minimization optimization with extra constraints.</p>
<p>Lagrange Multiplier Method could be used to solve the optimization problem:</p>
<p>Use the standard normal distribution as an example:</p>
<p>Want to minimize U-L subject to the constraint that <span class="math inline">\(P(L \leq \mu \leq U) = 1 - \alpha\)</span>.</p>
<p><span class="math display">\[
\phi (a,b,\lambda) = b-a + \lambda \left[ P(a \leq Z \leq b) - (1 - \alpha) \right]
\]</span></p>
<p>i.e.</p>
<p><span class="math display">\[
\phi (a,b,\lambda) = b-a + \lambda (\int _{a}^{b} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy - (1 - \alpha))
\]</span></p>
<p>Taking partial derivatives and setting them to zero:</p>
<p><span class="math display">\[
\frac{\partial \phi}{\partial a} = -1 + \lambda \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{a^2}{2}} = 0
\]</span></p>
<p><span class="math display">\[
\frac{\partial \phi}{\partial b} = 1 + \lambda \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{b^2}{2}} = 0
\]</span></p>
<p><span class="math display">\[
\frac{\partial \phi}{\partial \lambda} = \int _{a}^{b} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy - (1 - \alpha) = 0
\]</span></p>
<p>By adding the first two equations, we get:</p>
<p><span class="math display">\[
\lambda \cdot \frac{1}{\sqrt{2\pi}} \left( e^{-\frac{a^2}{2}} - e^{-\frac{b^2}{2}} \right) = 0
\]</span></p>
<p>since <span class="math inline">\(\lambda \neq 0\)</span>, we have <span class="math inline">\(e^{-\frac{a^2}{2}} = e^{-\frac{b^2}{2}}\)</span>, which implies <span class="math inline">\(a^2 = b^2\)</span> or <span class="math inline">\(a = -b\)</span> (since <span class="math inline">\(a &lt; b\)</span>).</p>
<p>Therefore, for the standard normal distribution, the best confidence interval that minimizes the width <span class="math inline">\(U - L\)</span> for a given confidence level <span class="math inline">\(1 - \alpha\)</span> is symmetric around zero, i.e., <span class="math inline">\(L = -z_{\alpha/2}\)</span> and <span class="math inline">\(U = z_{\alpha/2}\)</span>, where <span class="math inline">\(z_{\alpha/2}\)</span> is the critical value from the standard normal distribution corresponding to the upper <span class="math inline">\(\alpha/2\)</span> percentile.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>