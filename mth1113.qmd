---
title: "MTH113 Intro to probability and statistics+APH003--exploring world through data"
editor: visual
---

-   Motivation

The history of the development of statistics has helped me gain a deeper understanding of this fascinating and practical field of knowledge. Each advancement represents a leap from specific facts to broader, generalized conclusions. What truly captivates me is the remarkable alignment between natural phenomena and statistical principles; there always seems to be a coincidence where reality and theory intersect.

Initially, I only grasped the surface of this knowledge. Then, as I delved deeper, I began to understand the underlying principles through observable phenomena. Eventually, almost miraculously, I realized how incredibly useful these theories are and how they coincide with natural occurrences. This process of moving closer and closer to generalization has profoundly illustrated to me the truth in the saying: "Mathematics is the art of giving the same name to different things."

# 十堂极简概率课 中信出版 diaconis

# bivariate data

# linear regression

![](images/clipboard-1774849890.png)

## background

-   Regression to the mean (Galton's thinking)

![](images/clipboard-3333660003.png)

![](images/clipboard-1881050695.png)

## it is not stable to predict the data outside our data sample'

## Residual plost should have no pattern

across the whole range, it could not be showing a certain trend or a specific shape.

positive and negative points sperate averagely .

![](images/clipboard-2419630434.png)

![](images/clipboard-3592387892.png)

```{r}
# 数据
x <- c(50, 55, 50, 79, 44, 37, 70, 45, 49)  # Rock surface area
y <- c(152, 48, 22, 35, 38, 171, 13, 185, 25)  # Algae colony density

# (a) 计算最小二乘回归方程
model <- lm(y ~ x)
summary(model)

# 获取回归系数
intercept <- coef(model)[1]
slope <- coef(model)[2]
cat("最小二乘回归方程: y =", intercept, "+", slope, "* x\n")

# (b) 计算 R^2 值并解释
r_squared <- summary(model)$r.squared
cat("R^2 值:", r_squared, "\n")
cat("解释: R^2 表示了", round(r_squared * 100, 2), "% 的 y 的变异可以通过 x 来解释。\n")

# (c) 计算残差标准误差 s_e
se <- summary(model)$sigma
cat("残差标准误差 s_e:", se, "\n")
cat("解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\n")

# (d) 判断线性关系的方向和强度
correlation <- cor(x, y)
cat("相关系数 r:", correlation, "\n")

if (correlation > 0) {
  direction <- "正相关"
} else {
  direction <- "负相关"
}

if (abs(correlation) > 0.7) {
  strength <- "强相关"
} else if (abs(correlation) > 0.3) {
  strength <- "中等相关"
} else {
  strength <- "弱相关"
}

cat("线性关系:", direction, "且为", strength, "\n")

```

```{r}
# 数据
quality_rating <- c(111, 113, 93, 130, 170, 87, 83, 117, 135, 109)
satisfaction_rating <- c(832, 845, 794, 854, 836, 842, 877, 745, 797, 795)

# 计算相关系数
correlation_coefficient <- cor(quality_rating, satisfaction_rating)
print(paste("相关系数 r:", correlation_coefficient))

# 绘制散点图
plot(quality_rating, satisfaction_rating,
     main = "Scatterplot of Quality Rating vs. Satisfaction Rating",
     xlab = "Quality Rating",
     ylab = "Satisfaction Rating",
     pch = 19, col = "blue")
abline(lm(satisfaction_rating ~ quality_rating), col = "red")  # 添加回归线

```

# skew

mean is much sensitive than the median so the outlier drags the mean to the skew...... ![](images/clipboard-2849357671.png)

![](images/clipboard-2111278063.png)

![](images/clipboard-2144359450.png)

![](images/clipboard-1854557292.png)

As long as we know the mean and standard deviation of the data, we can determine how far a certain proportion of the data falls, without knowing the specific distribution shape of the data.

# outlier (eg of normal distribution)

a mild outlier if it lies more than 1.5(iqr) away from the nearest quartile (the nearest end of the box);

an extreme outlier if it lies more than 3(iqr) away from the nearest quartile.

(These definitions and distances are based on the hypothetical Normal distribution (bell shaped, symmetric, normal tails). When there is a reason to suspect that the distribution is skewed, the bounds should be changed.)

# modified boxplot

A modified boxplot is a box plot where the whiskers only extend to the largest (or smallest) observation that is not an outlier and the outliers are plotted using a full circle (mild) or empty circle (extreme).

If there are no outliers, then the the whiskers end at the maximum (or minimum)

```{r}
# 数据
ratios <- c(0.553, 0.570, 0.576, 0.601, 0.606, 0.606, 0.609, 0.611, 
            0.615, 0.628, 0.654, 0.662, 0.668, 0.670, 0.672, 0.690, 
            0.693, 0.749, 0.844, 0.933)

# 计算四分位数、IQR、温和和极端异常值的界限
Q1 <- quantile(ratios, 0.25)
Q3 <- quantile(ratios, 0.75)
median_val <- median(ratios)
iqr <- Q3 - Q1
mild_outlier_limit <- 1.5 * iqr
extreme_outlier_limit <- 3 * iqr

lower_mild <- Q1 - mild_outlier_limit
lower_extreme <- Q1 - extreme_outlier_limit
upper_mild <- Q3 + mild_outlier_limit
upper_extreme <- Q3 + extreme_outlier_limit

# 标记温和和极端异常值
mild_outliers <- ratios[ratios > upper_mild & ratios <= upper_extreme | ratios < lower_mild & ratios >= lower_extreme]
extreme_outliers <- ratios[ratios > upper_extreme | ratios < lower_extreme]

# 绘制箱线图，并标记温和和极端异常值
boxplot(ratios, main = "Modified Boxplot of Width-to-Length Ratios", ylim = c(0.3, 1))
points(which(ratios %in% mild_outliers), mild_outliers, pch = 16, col = "blue")    # 实心圆表示温和异常值
points(which(ratios %in% extreme_outliers), extreme_outliers, pch = 1, col = "red") # 空心圆表示极端异常值

# 输出四分位数、中位数和IQR结果

#确定第一四分位数（Q1）：这是数据从小到大排列后，位于下四分之一位置的值，表示前25%的数据范围的最大值。
#确定第三四分位数（Q3）：这是数据从小到大排列后，位于上四分之一位置的值，表示后75%的数据范围的最小值。

cat("Q1:", Q1, "\n")
cat("Median:", median_val, "\n")
cat("Q3:", Q3, "\n")
cat("IQR:", iqr, "\n")
cat("Lower mild outlier limit:", lower_mild, "\n")
cat("Lower extreme outlier limit:", lower_extreme, "\n")
cat("Upper mild outlier limit:", upper_mild, "\n")
cat("Upper extreme outlier limit:", upper_extreme, "\n")

```

```{r}
# 加载ggplot2包
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# 数据
ratios <- c(0.553, 0.570, 0.576, 0.601, 0.606, 0.606, 0.609, 0.611, 
            0.615, 0.628, 0.654, 0.662, 0.668, 0.670, 0.672, 0.690, 
            0.693, 0.749, 0.844, 0.933)

# 计算四分位数、IQR、温和和极端异常值的界限
Q1 <- quantile(ratios, 0.25)
Q3 <- quantile(ratios, 0.75)
median_val <- median(ratios)
iqr <- Q3 - Q1
mild_outlier_limit <- 1.5 * iqr
extreme_outlier_limit <- 3 * iqr

lower_mild <- Q1 - mild_outlier_limit
lower_extreme <- Q1 - extreme_outlier_limit
upper_mild <- Q3 + mild_outlier_limit
upper_extreme <- Q3 + extreme_outlier_limit

# 标记温和和极端异常值
mild_outliers <- ratios[ratios > upper_mild & ratios <= upper_extreme | ratios < lower_mild & ratios >= lower_extreme]
extreme_outliers <- ratios[ratios > upper_extreme | ratios < lower_extreme]

# 创建数据框
data <- data.frame(ratios = ratios)
data$outlier_type <- ifelse(data$ratios %in% mild_outliers, "Mild Outlier",
                            ifelse(data$ratios %in% extreme_outliers, "Extreme Outlier", "Normal"))

# 使用ggplot2绘制
ggplot(data, aes(x = "", y = ratios)) +
  geom_boxplot(outlier.shape = NA, fill = "lightblue") +  # 不显示默认的异常值
  geom_point(data = subset(data, outlier_type == "Mild Outlier"), aes(y = ratios), color = "blue", size = 3, shape = 16) + # 温和异常值，实心圆
  geom_point(data = subset(data, outlier_type == "Extreme Outlier"), aes(y = ratios), color = "red", size = 3, shape = 1) + # 极端异常值，空心圆
  labs(title = "Modified Boxplot of Width-to-Length Ratios", y = "Width-to-Length Ratios") +
  theme_minimal() +
  theme(axis.title.x = element_blank()) + # 移除x轴标签
  coord_cartesian(ylim = c(0.3, 1))      # 设置y轴范围

```

# tutorial

dotplot没有纵轴in r(vs scallar plot)

price of a textbook is discrete

zip code is categorical

dotplot and scallar plot

do not manipulate data---experimental

table 2.1

11

huge data for 顺序的 shuffle is ok

but ..

the likelihood is totally different

# The role of statistics and the Data Analysis Process

## Intro

stat is a large field in math involving the collection, organization, analysis,interpretation, and presentation of data(a collection of observations on one or more variables(A characteristic whose value may change from one observation to another))

Statistics is the scientific discipline that provides methods to help us make sense of data.

It is important to be able to:

1 Extract information from tables, charts, and graphs.

2 Follow numerical arguments.

3 Understand the basics of how data should be gathered, summarized, and analysed to draw statistical conclusions.

The Data Analysis Process

1 Understanding the nature of the research problem or goals.

2 Deciding what to measure and how.

3 Collecting data.

4 Data summarization and preliminary analysis.

5 Formal Data Analysis (Statistical Methods).

6 Interpretation of the results.

## populations and samples

population: The entire collection of individuals or objects about which information is desired

sample: A sample is a subset of the population, selected for study.

then select the sample

then we could summarize it using 2 branches of stat.--- Decriptive stat.(methods for organizing and summarizing data.) or inferential stat.(generalizing from a sample(incomplete information) to the population from which the sample was selected and assessing the reliability of such generalizations.So we run the risk(An important aspect of statistics and making statistics inferences involves quantifying the chance of making an incorrect conclusions.))

### descriptive stat

### inferential stat

sample

## Types of data

### uni data set and bivariate and multivariate

### categorical and numerical(discrete and continuous) with plot using excel (data analysis) or rstudio plot (ggplot2)

for categorial data we could use a bar chart which is a graph of a frequency distribution for categorical data.

for a small numerical data we could use dotplot

-   discrete

```{r}

library(ggplot2)

# creat data：Wechat number
discrete_data <- data.frame(value = c(30, 15, 20,30,60))

# plot
ggplot(discrete_data, aes(x = value)) +
  geom_dotplot(binwidth = 1, dotsize = 1) +
  ggtitle("Dot Plot of Discrete Data (Number of Wechats)") +
  theme_minimal()


```

-   continuous

```{r}

all_athletes <- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, 
                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)
basketball <- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, 
                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)

# 设置画布的高度，以便将两个图绘制在同一页面上
plot.new()
plot.window(xlim = c(0, 100), ylim = c(0.5, 2.5))

# 绘制 Basketball 数据的 dotplot
stripchart(basketball, method = "stack", at = 2, pch = 16, col = "orange", 
           add = TRUE, offset = 0.5, cex = 1.2)

# 绘制 All Athletes 数据的 dotplot
stripchart(all_athletes, method = "stack", at = 1, pch = 16, col = "orange", 
           add = TRUE, offset = 0.5, cex = 1.2)

# 添加 X 轴
axis(1, at = seq(10, 100, by = 10), labels = seq(10, 100, by = 10))

# 添加标签
text(-5, 2, "Basketball", xpd = TRUE, adj = 1)
text(-5, 1, "All Athletes", xpd = TRUE, adj = 1)

# 添加横线
abline(h = 1.5, col = "black", lwd = 2)

# 添加 X 轴标签
title(xlab = "Graduation rates (%)")


```

```{r}
# creat data--time spent in minutes
continuous_data <- data.frame(value = c(6, 5.25, 3.62,1,2,3.1,3.2,4,5,6,7,4,10))

# dotplot
ggplot(continuous_data, aes(x = value)) +
  geom_dotplot(binwidth = 0.1, dotsize = 1) +
  ggtitle("Dot Plot of Continuous Data (Time Spent in Minutes)") +
  theme_minimal()

```

```{r}

library(ggplot2)

# 毕业率数据
school <- 33:68
all_athletes <- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, 
                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)
basketball <- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, 
                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)

# 创建数据框
data <- data.frame(school, all_athletes, basketball)

# 画图
ggplot() +
  geom_dotplot(data = data, aes(x = all_athletes, y = "All Athletes"), binaxis = 'x', stackdir = 'up', dotsize = 0.5) +
  geom_dotplot(data = data, aes(x = basketball, y = "Basketball"), binaxis = 'x', stackdir = 'up', dotsize = 0.5, color = "red") +
  xlab("Graduation rates (%)") +
  ylab("") +
  theme_minimal() +
  ggtitle("Dotplot of Graduation Rates for All Athletes and Basketball Players")


```

![histogram excel plot](images/clipboard-3026116502.png)

## collect data sensibly

# chapter 2

## Two types of studies: Observational studies and Experiments.

### Observational

A study in which the investigator observes characteristics of a sample selected from one or more existing populations. The goal is to draw conclusions about the corresponding population or about differences between two or more populations.

In an observational study, it is impossible to draw clear cause-and-effect conclusions

### Experiments

A study in which the investigator observes how a response variable behaves when one or more explanatory variables, also called factors, are manipulated.

A well-designed experiment can result in data that provide evidence for a cause-and-effect relationship.

![](images/clipboard-858121668.png)

-   Experimental conditions: Any particular combination of values for the explanatory variables, which are also called treatments.

### comparison

-   Both observational studies and experiments can be used to compare groups, but in an experiment the researcher controls who is in which group, whereas this is not the case in an observational study.

-   In an observational study, it is impossible to draw clear cause-andeffect conclusions

### confounding vars

A variable that is related to both how the experimental groups were formed and the response variable of interest.

-   Two methods for data collection: Sampling and Experimentation.

-   distinguish between selection bias, measurement or response bias, and non-response bias.

-   select a simple random sample from a given population.

-   distinguish between simple random sampling, stratified random sampling, cluster sampling, systematic sampling, and convenience sampling

## variable

### response variable--y

The response variable is the focus of a question in a study or experiment.

### explanotory variable--x

An explanatory variable is one that explains for changes in the response variable.

### experiments and obeservational study

### bias

selection bias：When the way the sample is selected systematically excludes some part of the population of interest.

measurement or response bias

eg: survey question/scale(The scale or a machine used for measurements is not calibrated properly)

Non-response Bias:When responses are not obtained from all individuals selected for inclusion in the sample.

non-response bias can distort results if those who respond differ in important ways from those who do not respond (e.g. laziness a confounding variable).

### random sampling

def: A sample that is selected from a population in a way that ensures that every different possible sample of size n has the same chance of being selected.

the same chance to be selected

counter eg:

Consider 100 students in a classroom, 60 females and 40 males. If we randomly sample 6 females, and 4 males, then each female has a 6/60 = 0.1 chance of being selected. Same for males, 4/40=0.1. However, not every group of 10 students is equally likely to be selected. This is not simple random sampling

The random selection process allows us to be confident that the sample adequately reflects the population, even when the sample consists of only a small fraction of the population.

eg.Voting Sample Size in a country

### stratified and cluster

-   stratified random sampling:In stratified random sampling, separate simple random samples are independently selected from each subgroup. Each subgroup is called a strata.

In general, it is much easier to produce relatively accurate estimates of characteristics of a homogeneous group than of a heterogeneous group.

stratified: according to certain characteristic

eg.Even with a small sample, it is possible to obtain an accurate estimate of the average grade point average (GPA) of students graduating with high honours from a university (Similar high grades, homogenous, thus only sample a few students). On the other hand, producing a reasonably accurate estimate of the average GPA of all seniors at the university, a much more diverse group of GPAs, is a more difficult task. **Not only does this ensure that students at each GPA level are represented, it also allows for a more accurate estimate of the overall average GPA.**

-   cluster reflect general characteristic about the whole entire population

cluster: randomly groups

Cluster sampling involves dividing the population of interest into non-overlapping subgroups, called clusters. Clusters are then selected at random, and then all individuals in the selected clusters are included in the sample.

### systematic sampling

A value k is specified (e.g. k = 50 or k = 200). Then one of the first k individuals is selected at random, after which every k-th individual in the sequence is included in the sample. A sample selected in this way is called a 1 in k systematic sample.

In the case of large samples, it can ensure that the sample is evenly distributed in the population.

# Random Variable

## Definition: Random Variable (R.V.)

A numerical variable whose value depends on the outcome of a chance experiment. A random variable associates a numerical value with each outcome of a chance experiment.(Think of it as a rule that translates each result of a chance event into a number.)

A real-valued random variable X is a function

Continuous random variable X: S--\> R is continuous if its set of possible values includes an entire interval on the number line.

# sample space of 1 variable

X : S → $\mathbb R$, where S is the sample space of a chance experiment.

Discrete random variable: A random variable is discrete if its set of possible value is a collection of isolated points along the number line.(counting)

Continuous random variable: A random variable is continuous if its set of possible values includes an entire interval on the number line.(measurement), which could not be count.

### eg

Examples: Coin Tossing (Discrete Random Variable) If we flip a coin 5 times, let X be the number of heads we get. Possible values of X {0,1,2,3,4,5} (where 0 means no heads, and 5 means all heads). Here, X turns each outcome of multiple coin tosses into a count of heads. Departure Time (Continuous Random Variable) Imagine tracking when people leave a subway station between 10 PM and 11 PM. Let Y represent the time (in hours) someone leaves, so Y can be any number from 10 to 11. Here, Y assigns each departure time to a point in the range \[10,11\].

### explaination

In short:

Random variables convert random events into numbers. Discrete random variables take specific values (like counting heads). Continuous random variables take any value in a range (like time).

-   Multiple variables

## Probability Mass Function and Cumulative Distribution Function for Discrete Random Variables

Probability Mass Function (PMF): $p_x (x) := P(X = x) ,\forall x$

Cumulative Distribution Function (CDF): $F_x (x) := P(X \leq x) ,\forall x$

```{r}
# 定义每个事件的概率和对应的 X 值
outcomes <- c("GGGG", "EGGG", "GEGG", "GGEG", "GGGE", 
              "EEGG", "EGEG", "EGGE", "GEEG", "GEGE", 
              "GGEE", "GEEE", "EEEG", "EEGE", "EEEE")
probabilities <- c(0.1296, 0.0864, 0.0864, 0.0864, 0.0864, 
                   0.0576, 0.0576, 0.0576, 0.0576, 0.0576, 
                   0.0384, 0.0384, 0.0384, 0.0384, 0.0256)
X_values <- c(0, 1, 1, 1, 1, 
              2, 2, 2, 2, 2,
              3, 3, 3, 3, 4)

# 计算每个 X 值的 PMF 通过分组和求和
pmf <- tapply(probabilities, X_values, sum)

# 定义可能的 X 值
X_values_unique <- sort(unique(X_values))

# 计算 CDF
cdf <- cumsum(pmf)

# 确保 CDF 在 x > 4 时为 1
cdf <- c(cdf, 1)

# 更新 X 值以包括 x > 4 的情况
X_values_unique <- c(X_values_unique, ">4")

# 创建数据框显示 PMF 和 CDF
table <- data.frame(
  X = X_values_unique,
  `PMF P(X=x)` = c(pmf, 1-0.1296-0.3456-0.2880-0.1536-0.0256),  # PMF 没有对应的 x > 4 值，填 NA
  `CDF F(X<=x)` = cdf
)

# 打印表格
print(table)

```

-   Note that the domain of a cdf is (−∞, ∞)

$\mathrm{pmf} \Longrightarrow \mathrm{cdf}$ $$
F_x(x)=\sum_{y \leq x} p_x(y)
$$ cdf $\Longrightarrow$ pmf Suppose $X$ takes ordered values $x_1, x_2, x_3, \cdots$, then $$
\begin{aligned}
p_X\left(x_i\right) & =P\left(X=x_i\right)=P\left(x_{i-1}<X \leq x_i\right) \\
& =P\left(X \leq x_i\right)-P\left(X \leq x_{i-1}\right) \\
& =F\left(x_i\right)-F\left(x_{i-1}\right)
\end{aligned}
$$

-   remark: The probability of a discrete distribution varies depending on the inclusion and exclusion of the boundary values.

## Expectation and Variance for Discrete Random Variables

$$
\frac{0 \cdot f_0+1 \cdot f_1+2 \cdot f_2+\cdots+n \cdot f_n}{N}=\frac{1}{N} \sum_{i=0}^n i \cdot f_i
$$

Note that in $\frac{1}{N} \sum_{i=0}^n i \cdot f_i$, $$
\lim _{N \rightarrow \infty} \frac{f_i}{N}=P(X=i)
$$

So the average number will be $$
\sum_{i=0}^n i \cdot P(X=i)
$$

-   Definition: Expectation Given a discrete random variable $X$, the expectation of $X$ is $$
    E[X]=\sum_x x \cdot p_X(x)
    $$

-   Properties of Expectation

-   If $c$ is a constant, then $E[c]=c$.

-   If $X \geq 0$ then $E[X] \geq 0$.

-   If $a \leq X \leq b$ then $a \leq E[X] \leq b$.

-   Proof of 3: First show $E[X] \geq a$, then show $E[X] \leq b$, $$
    \begin{aligned}
    E[X] & =\sum_x x p_X(x) \geq \sum_x a p_X(x), \\
    & =a \sum_x p_x(x)=a .
    \end{aligned}
    $$

Similarly, $E[X] \leq b$.

-   Suppose $X$ is a discrete random variable and $Y=g(X)$, then $$
    \begin{aligned}
    E[Y] & =\sum_y y p_Y(y)=\sum_y y P(Y=y) \\
    & =\sum_y y \sum_{\{x: g(x)=y\}} P(X=x) \\
    & =\sum_y \sum_{\{x: g(x)=y\}} y P(X=x) \\
    & =\sum_y \sum_{\{x: g(x)=y\}} g(x) P(X=x) \\
    & =\sum_x g(x) P(X=x)
    \end{aligned}
    $$

-   First moment of $X$ (mean): $$
    E[X]=\sum_x x p_X(x) .
    $$

-   Second moment of $X$ : $$
    E\left[X^2\right]=\sum_x x^2 p_x(x) .
    $$

-   In general, $E[g(X)] \neq g(E[X])$. For example, let $g(x)=x^2$, and consider $X$ such that $$
    p_X(x)= \begin{cases}0.5, & \text { for } x=-1 \\ 0.5, & \text { for } x=1\end{cases}
    $$

Then clearly $E\left[X^2\right]=1 \neq 0=(E[X])^2$.

-   There are exceptions (e.g. when g is linear)!

-   Linearity of Expectation

E\[aX + b\] = aE\[X\] + b

proof:

Suppose $g(x)=a x+b$. Then $$
\begin{aligned}
E[g(X)] & =\sum_x g(x) p_X(x), \\
& =\sum_x(a x+b) p_x(x), \\
& =\sum_x a x p_x(x)+\sum_x b p_x(x), \\
& =a \sum_x x p_x(x)+b \sum_x p_x(x), \\
& =a E[X]+b=g(E[X]),
\end{aligned}
$$ this implies $$
E[a X+b]=a E[X]+b
$$

-   Remark: Apart from this case, always assume $E[g(X)] \neq g(E[X])$.

# sample space of multivariables

G:gender; F: year---corresponding to a certain student:S

(G, H) : S → $\mathbb R^2$, S={1,2,3,4}

# joint distribution of X and Y

The joint probability mass function (i.e., joint pmf) of X and Y for discrete random variables is defined as $p_{X,Y} (x, y) := P(X = x \text{ and } Y = y)$

P\[(X,Y) ∈ A\] =$\sum_{(x,y)\in A}p_{X,Y}(x,y)$,where A belongs to a subset of the $\mathbb R^2$ where X and Y taking values.

# Marginal Probability Mass Function

Let $X$ and $Y$ have the joint probability mass function $p_{X, Y}(x, y)$ with space $\mathcal{S}$. The probability mass function of $X$ (or $Y$ ) alone, is called the marginal probability mass function of $X$ (or $Y$ ) and defined by: $$
\begin{aligned}
& p_X(x)=P(X=x)=\sum_y p_{X, Y}(x, y) \\
& p_Y(y)=P(Y=y)=\sum_x p_{X, Y}(x, y)
\end{aligned}
$$

For the marginal of X, we sum over all values of y.

For the marginal of Y, we sum over all values of x.

eg.--helping understand both sample space, event and Marginal probability mass function

![](images/clipboard-206208503.png)

# Functions of multiple random variables

it has the same philosophy as one random variable things, such as expectation:

$$E[g(X,Y)]=\sum_x \sum_y g(x,y)p_{X,Y}(x,y)$$, also the linearity of expectation

eg. Z = X + 2Y.

![](images/clipboard-369824916.png)

# independence

-   P($A\cap B$)=P(A)P(B) or P(A\|B)=P(A)

-   Two discrete random variables X and Y are independent if P(X = x and Y = y) = P(X = x)P(Y = y):

$p_{X,Y}(x,y)=p_X(x)p_Y(y), \forall x,y$

$p_{X|Y}(x|y)=p_X(x), \forall x,y$

it is easy to detect the dependent as long as we find one example, eg: $P_{X|Y}(1|1)$ is not equal to $P(X=1)$

## independence, expectations(mean) and variance

Independence and Expectations If $X$ and $Y$ are independent, then $$
E[X Y]=E[X] E[Y] .
$$

Proof: We use $E[g(X, Y)]$ where $g(x, y)=x y$. $$
\begin{aligned}
E[X Y] & =\sum_x \sum_y x y p_{X, Y}(x, y) \\
& =\sum_x \sum_y x y p_X(x) p_Y(y), \quad(\text { by inde } \\
& =\left(\sum_x x p_X(x)\right)\left(\sum_y y p_Y(y)\right)=E[X] E[Y] .
\end{aligned}
$$ (by independence)

Similarly, if $X$ and $Y$ are independent, then $$
E[g(X) h(Y)]=E[g(X)] E[h(Y)]
$$

-   Independence and Variances

It is always true that $$
\operatorname{Var}(a X)=a^2 \operatorname{Var}(X), \quad \text { and } \quad \operatorname{Var}(X+a)=\operatorname{Var}(X)
$$

In general, when we have a sum of random variables $X$ and $Y$ $$
\operatorname{Var}(X+Y) \neq \operatorname{Var}(X)+\operatorname{Var}(Y) .
$$

It is only true if $X$ and $Y$ are independent

-   Sum of Variance for independent R.V. If two random variables $X$ and $Y$ are independent then $$
    \operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)
    $$

-   Sum of Variance for independent R.V.

If two random variables $X$ and $Y$ are independent then $$
\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y) .
$$

Proof: Independence implies $E[X Y]=E[X] E[Y]$. Thus $$
\begin{aligned}
& \operatorname{Var}(X+Y)=E\left[(X+Y-(E[X]+E[Y]))^2\right], \\
& =E\left[(X-E[X])^2+2(X-E[X])(Y-E[Y])+(Y-E[Y])^2\right], \\
& =\operatorname{Var}(X)+\operatorname{Var}(Y)+2 E[(X-E[X])(Y-E[Y])]
\end{aligned}
$$

As $X$ is indep to $Y$, then $X-\mu_X$ is indep to $Y-\mu_Y$ so $$
E[(X-E[X])(Y-E[Y])]=E[X-E[X]] E[Y-E[Y]]=0
$$

Example: Assume independence, $\operatorname{Var}(3 X-5 Y)=$ $$
\operatorname{Var}(3 X)+\operatorname{Var}(-5 Y)=9 \operatorname{Var}(X)+25 \operatorname{Var}(Y)
$$

-   Example. Independence, mean and variance

Let $Y$ be the random variable denoting the total number of heads by tossing a coin $n$ times. Find the mean and variance of $Y$.

Let $$
Y_i= \begin{cases}1, & \text { if the } i^{\text {th }} \text { toss gets a head } \\ 0, & \text { otherwise. }\end{cases}
$$

Then $Y=Y_1+Y_2+\cdots+Y_n$ where $Y_1, Y_2, \cdots, Y_n$ are independent. For any $i=1,2, \cdots, n$, we have $$
\begin{array}{ccc}
y & 0 & 1 \\
P_{Y_i}(y) & 1 / 2 & 1 / 2
\end{array}
$$

As $E\left[Y_i\right]=\frac{1}{2}$ and $\operatorname{Var}\left(Y_i\right)=\frac{1}{4}$ for $i=1,2, \cdots, n$ $$
\begin{gathered}
E[Y]=E\left[Y_1\right]+E\left[Y_2\right]+\cdots+E\left[Y_n\right]=\frac{n}{2} \\
\operatorname{Var}(Y)=\operatorname{Var}\left(Y_1\right)+\operatorname{Var}\left(Y_2\right)+\cdots+\operatorname{Var}\left(Y_n\right)=\frac{n}{4}
\end{gathered}
$$ (for not independent cases we have the same result as E but not Var because Var is not linear)

# Continuous Random Variables

We have to consider intervals instead of one when referring to continuous random variables or the probability will equals to 0, which has no meaning.

eg. $P(a\leq Y\leq b)=P(a<Y<b)$ if Y is a continuous variable.

## Definition. Probability Density Function (pdf)

For a continuous random variable X, the Probability Density Function (PDF) of X is f(x) where P(X = x) = 0 for all x and for any a ≤ b

## discrete and continuous

![](images/clipboard-1910128728.png)

Density function's value could be greater than 1 because the integral of it in a very tiny interval could be very small, which represents the probability.

# Covariance

![](images/clipboard-4061434485.png)

## wait for reviewing.....

![](images/clipboard-2808960987.png)
