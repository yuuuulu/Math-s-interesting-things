---
title: "MTH113 Intro to probability and statistics+APH003--exploring world through data"
editor: visual
---

-   Motivation

The history of the development of statistics has helped me gain a deeper understanding of this fascinating and practical field of knowledge. Each advancement represents a leap from specific facts to broader, generalized conclusions. What truly captivates me is the remarkable alignment between natural phenomena and statistical principles; there always seems to be a coincidence where reality and theory intersect.

Initially, I only grasped the surface of this knowledge. Then, as I delved deeper, I began to understand the underlying principles through observable phenomena. Eventually, almost miraculously, I realized how incredibly useful these theories are and how they coincide with natural occurrences. This process of moving closer and closer to generalization has profoundly illustrated to me the truth in the saying: "Mathematics is the art of giving the same name to different things."

Theorems come, theorems go. Only examples are lying forever. (Practice to use statistics to interpret real world examples)

# 把公式推导单独放一栏（后）

# error?

-   Independent = Disjoint? NO!

    Independent vs Disjoint: If P(E) \> 0 and P(F) \> 0, then E and F can NOT be both independent and disjoint.

    Subsets are dependent. If E ⊂ F and neither P(E) = 0 nor P(F) = 1, then E and F are dependent.

    Complements are dependent. If neither P(E) = 0 nor P(E) = 1, then E and E C are dependent.

# My reading during this course：

## 十堂极简概率课 中信出版 diaconis

## 心理统计

## The lady tasting tea

## 概率论与数理统计（第三版） 峁诗松等老师编著

## Probability....

# bivariate data

# linear regression

![](images/clipboard-1774849890.png)

![](images/clipboard-807464638.png)

## background

-   Regression to the mean (Galton's thinking)

![](images/clipboard-3333660003.png)

![](images/clipboard-1881050695.png)

## it is not stable to predict the data outside our data sample'

## Residual plot should have no pattern

across the whole range, it could not be showing a certain trend or a specific shape.

positive and negative points sperate averagely .

![](images/clipboard-2419630434.png)

![](images/clipboard-3592387892.png)

```{r}
# 数据
x <- c(50, 55, 50, 79, 44, 37, 70, 45, 49)  # Rock surface area
y <- c(152, 48, 22, 35, 38, 171, 13, 185, 25)  # Algae colony density

# (a) 计算最小二乘回归方程
model <- lm(y ~ x)
summary(model)

# 获取回归系数
intercept <- coef(model)[1]
slope <- coef(model)[2]
cat("最小二乘回归方程: y =", intercept, "+", slope, "* x\n")

# (b) 计算 R^2 值并解释
r_squared <- summary(model)$r.squared
cat("R^2 值:", r_squared, "\n")
cat("解释: R^2 表示了", round(r_squared * 100, 2), "% 的 y 的变异可以通过 x 来解释。\n")

# (c) 计算残差标准误差 s_e
se <- summary(model)$sigma
cat("残差标准误差 s_e:", se, "\n")
cat("解释: s_e 表示了回归模型的平均预测误差，越小表明预测的精确度越高。\n")

# (d) 判断线性关系的方向和强度
correlation <- cor(x, y)
cat("相关系数 r:", correlation, "\n")

if (correlation > 0) {
  direction <- "正相关"
} else {
  direction <- "负相关"
}

if (abs(correlation) > 0.7) {
  strength <- "强相关"
} else if (abs(correlation) > 0.3) {
  strength <- "中等相关"
} else {
  strength <- "弱相关"
}

cat("线性关系:", direction, "且为", strength, "\n")

```

```{r}
# 数据
quality_rating <- c(111, 113, 93, 130, 170, 87, 83, 117, 135, 109)
satisfaction_rating <- c(832, 845, 794, 854, 836, 842, 877, 745, 797, 795)

# 计算相关系数
correlation_coefficient <- cor(quality_rating, satisfaction_rating)
print(paste("相关系数 r:", correlation_coefficient))

# 绘制散点图
plot(quality_rating, satisfaction_rating,
     main = "Scatterplot of Quality Rating vs. Satisfaction Rating",
     xlab = "Quality Rating",
     ylab = "Satisfaction Rating",
     pch = 19, col = "blue")
abline(lm(satisfaction_rating ~ quality_rating), col = "red")  # 添加回归线

```

# skew

Data skewed to the left(negatively skewed) have a longer left tail, and the mean and media are to the left of the mode.

Data skewed to the right(positively skewed) have a longer right tail, and the mean and media are to the right of the mode.

mean is much sensitive than the median to the extreame value(not a resistant measure of center) so the outlier drags the mean to the skew...... ![](images/clipboard-2849357671.png)

Compare $Q_2-Q_1$ and $Q_3-Q_2$ to decide left/right skew or symmetry.

ps: the **trimmed mean** is more resistant.

![](images/clipboard-2111278063.png)

![](images/clipboard-2144359450.png)

![](images/clipboard-1854557292.png)

As long as we know the mean and standard deviation of the data, we can determine how far a certain proportion of the data falls, without knowing the specific distribution shape of the data.

# outlier (eg of normal distribution)

a mild outlier if it lies more than 1.5(iqr) away from the nearest quartile (the nearest end of the box);

an extreme outlier if it lies more than 3(iqr) away from the nearest quartile.

(These definitions and distances are based on the hypothetical Normal distribution (bell shaped, symmetric, normal tails). When there is a reason to suspect that the distribution is skewed, the bounds should be changed.)

# quartile

$Q_1$(First quartile): at least 25% of the sorted values are less than or equal to $Q_1$ and at least 75% of the values are greater than or equal to $Q_1$

$Q_3$(Third quartile): ..

# modified boxplot

A modified boxplot is a box plot where the whiskers only extend to the largest (or smallest) observation that is not an outlier and the outliers are plotted using a full circle (mild) or empty circle (extreme).

If there are no outliers, then the the whiskers end at the maximum (or minimum)

```{r}
# 数据
ratios <- c(0.553, 0.570, 0.576, 0.601, 0.606, 0.606, 0.609, 0.611, 
            0.615, 0.628, 0.654, 0.662, 0.668, 0.670, 0.672, 0.690, 
            0.693, 0.749, 0.844, 0.933)

# 计算四分位数、IQR、温和和极端异常值的界限
Q1 <- quantile(ratios, 0.25)
Q3 <- quantile(ratios, 0.75)
median_val <- median(ratios)
iqr <- Q3 - Q1
mild_outlier_limit <- 1.5 * iqr
extreme_outlier_limit <- 3 * iqr

lower_mild <- Q1 - mild_outlier_limit
lower_extreme <- Q1 - extreme_outlier_limit
upper_mild <- Q3 + mild_outlier_limit
upper_extreme <- Q3 + extreme_outlier_limit

# 标记温和和极端异常值
mild_outliers <- ratios[ratios > upper_mild & ratios <= upper_extreme | ratios < lower_mild & ratios >= lower_extreme]
extreme_outliers <- ratios[ratios > upper_extreme | ratios < lower_extreme]

# 绘制箱线图，并标记温和和极端异常值
boxplot(ratios, main = "Modified Boxplot of Width-to-Length Ratios", ylim = c(0.3, 1))
points(which(ratios %in% mild_outliers), mild_outliers, pch = 16, col = "blue")    # 实心圆表示温和异常值
points(which(ratios %in% extreme_outliers), extreme_outliers, pch = 1, col = "red") # 空心圆表示极端异常值

# 输出四分位数、中位数和IQR结果

#确定第一四分位数（Q1）：这是数据从小到大排列后，位于下四分之一位置的值，表示前25%的数据范围的最大值。
#确定第三四分位数（Q3）：这是数据从小到大排列后，位于上四分之一位置的值，表示后75%的数据范围的最小值。

cat("Q1:", Q1, "\n")
cat("Median:", median_val, "\n")
cat("Q3:", Q3, "\n")
cat("IQR:", iqr, "\n")
cat("Lower mild outlier limit:", lower_mild, "\n")
cat("Lower extreme outlier limit:", lower_extreme, "\n")
cat("Upper mild outlier limit:", upper_mild, "\n")
cat("Upper extreme outlier limit:", upper_extreme, "\n")

```

```{r}
# 加载ggplot2包
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# 数据
ratios <- c(0.553, 0.570, 0.576, 0.601, 0.606, 0.606, 0.609, 0.611, 
            0.615, 0.628, 0.654, 0.662, 0.668, 0.670, 0.672, 0.690, 
            0.693, 0.749, 0.844, 0.933)

# 计算四分位数、IQR、温和和极端异常值的界限
Q1 <- quantile(ratios, 0.25)
Q3 <- quantile(ratios, 0.75)
median_val <- median(ratios)
iqr <- Q3 - Q1
mild_outlier_limit <- 1.5 * iqr
extreme_outlier_limit <- 3 * iqr

lower_mild <- Q1 - mild_outlier_limit
lower_extreme <- Q1 - extreme_outlier_limit
upper_mild <- Q3 + mild_outlier_limit
upper_extreme <- Q3 + extreme_outlier_limit

# 标记温和和极端异常值
mild_outliers <- ratios[ratios > upper_mild & ratios <= upper_extreme | ratios < lower_mild & ratios >= lower_extreme]
extreme_outliers <- ratios[ratios > upper_extreme | ratios < lower_extreme]

# 创建数据框
data <- data.frame(ratios = ratios)
data$outlier_type <- ifelse(data$ratios %in% mild_outliers, "Mild Outlier",
                            ifelse(data$ratios %in% extreme_outliers, "Extreme Outlier", "Normal"))

# 使用ggplot2绘制
ggplot(data, aes(x = "", y = ratios)) +
  geom_boxplot(outlier.shape = NA, fill = "lightblue") +  # 不显示默认的异常值
  geom_point(data = subset(data, outlier_type == "Mild Outlier"), aes(y = ratios), color = "blue", size = 3, shape = 16) + # 温和异常值，实心圆
  geom_point(data = subset(data, outlier_type == "Extreme Outlier"), aes(y = ratios), color = "red", size = 3, shape = 1) + # 极端异常值，空心圆
  labs(title = "Modified Boxplot of Width-to-Length Ratios", y = "Width-to-Length Ratios") +
  theme_minimal() +
  theme(axis.title.x = element_blank()) + # 移除x轴标签
  coord_cartesian(ylim = c(0.3, 1))      # 设置y轴范围

```

# Measures of relative standing

## z Scores

eg. height(among women or men (come from different populations)instead of just comparing the height itself)

# tutorial

dotplot没有纵轴in r(vs scallar plot)

price of a textbook is discrete

zip code is categorical

dotplot and scallar plot

do not manipulate data---experimental

table 2.1

11

huge data for 顺序的 shuffle is ok

but ..

the likelihood is totally different

# The role of statistics and the Data Analysis Process

## Intro

stat is a large field in math involving the collection, organization, analysis,interpretation, and presentation of data(a collection of observations on one or more variables(A characteristic whose value may change from one observation to another))

Statistics is the scientific discipline that provides methods to help us make sense of data.

It is important to be able to:

1 Extract information from tables, charts, and graphs.

2 Follow numerical arguments.

3 Understand the basics of how data should be gathered, summarized, and analysed to draw statistical conclusions.

The Data Analysis Process

1 Understanding the nature of the research problem or goals.

2 Deciding what to measure and how.

3 Collecting data.

4 Data summarization and preliminary analysis.

5 Formal Data Analysis (Statistical Methods).

6 Interpretation of the results.

## populations and samples

population: The entire collection of individuals or objects about which information is desired

sample: A sample is a subset of the population, selected for study.

then select the sample

then we could summarize it using 2 branches of stat.--- Decriptive stat.(methods for organizing and summarizing data.) or inferential stat.(generalizing from a sample(incomplete information) to the population from which the sample was selected and assessing the reliability of such generalizations.So we run the risk(An important aspect of statistics and making statistics inferences involves quantifying the chance of making an incorrect conclusions.))

### descriptive stat

### inferential stat

sample

## Types of data

### uni data set and bivariate and multivariate

### categorical and numerical(discrete and continuous) with plot using excel (data analysis) or rstudio plot (ggplot2)

for categorial data we could use a bar chart which is a graph of a frequency distribution for categorical data.

for a small numerical data we could use dotplot

-   discrete

```{r}

library(ggplot2)

# creat data：Wechat number
discrete_data <- data.frame(value = c(30, 15, 20,30,60))

# plot
ggplot(discrete_data, aes(x = value)) +
  geom_dotplot(binwidth = 1, dotsize = 1) +
  ggtitle("Dot Plot of Discrete Data (Number of Wechats)") +
  theme_minimal()


```

-   continuous

```{r}

all_athletes <- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, 
                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)
basketball <- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, 
                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)

# 设置画布的高度，以便将两个图绘制在同一页面上
plot.new()
plot.window(xlim = c(0, 100), ylim = c(0.5, 2.5))

# 绘制 Basketball 数据的 dotplot
stripchart(basketball, method = "stack", at = 2, pch = 16, col = "orange", 
           add = TRUE, offset = 0.5, cex = 1.2)

# 绘制 All Athletes 数据的 dotplot
stripchart(all_athletes, method = "stack", at = 1, pch = 16, col = "orange", 
           add = TRUE, offset = 0.5, cex = 1.2)

# 添加 X 轴
axis(1, at = seq(10, 100, by = 10), labels = seq(10, 100, by = 10))

# 添加标签
text(-5, 2, "Basketball", xpd = TRUE, adj = 1)
text(-5, 1, "All Athletes", xpd = TRUE, adj = 1)

# 添加横线
abline(h = 1.5, col = "black", lwd = 2)

# 添加 X 轴标签
title(xlab = "Graduation rates (%)")


```

```{r}
# creat data--time spent in minutes
continuous_data <- data.frame(value = c(6, 5.25, 3.62,1,2,3.1,3.2,4,5,6,7,4,10))

# dotplot
ggplot(continuous_data, aes(x = value)) +
  geom_dotplot(binwidth = 0.1, dotsize = 1) +
  ggtitle("Dot Plot of Continuous Data (Time Spent in Minutes)") +
  theme_minimal()

```

```{r}

library(ggplot2)

# 毕业率数据
school <- 33:68
all_athletes <- c(79, 79, 86, 85, 95, 78, 89, 84, 81, 85, 89, 89, 85, 85, 81, 80, 98, 84, 
                  80, 82, 81, 70, 85, 87, 83, 86, 92, 85, 93, 94, 76, 69, 82, 80, 94, 98)
basketball <- c(55, 36, 83, 20, 100, 62, 100, 100, 90, 91, 93, 89, 90, 80, 46, 75, 100, 71, 
                50, 62, 82, 50, 100, 83, 90, 64, 91, 67, 83, 100, 83, 100, 83, 63, 91, 95)

# 创建数据框
data <- data.frame(school, all_athletes, basketball)

# 画图
ggplot() +
  geom_dotplot(data = data, aes(x = all_athletes, y = "All Athletes"), binaxis = 'x', stackdir = 'up', dotsize = 0.5) +
  geom_dotplot(data = data, aes(x = basketball, y = "Basketball"), binaxis = 'x', stackdir = 'up', dotsize = 0.5, color = "red") +
  xlab("Graduation rates (%)") +
  ylab("") +
  theme_minimal() +
  ggtitle("Dotplot of Graduation Rates for All Athletes and Basketball Players")


```

![histogram excel plot](images/clipboard-3026116502.png)

## collect data sensibly

# chapter 2

## Two types of studies: Observational studies and Experiments.

### Observational

A study in which the investigator observes characteristics of a sample selected from one or more existing populations. The goal is to draw conclusions about the corresponding population or about differences between two or more populations.

In an observational study, it is impossible to draw clear cause-and-effect conclusions

### Experiments

A study in which the investigator observes how a response variable behaves when one or more explanatory variables, also called factors, are manipulated.

A well-designed experiment can result in data that provide evidence for a cause-and-effect relationship.

![](images/clipboard-858121668.png)

-   Experimental conditions: Any particular combination of values for the explanatory variables, which are also called treatments.

### comparison

-   Both observational studies and experiments can be used to compare groups, but in an experiment the researcher controls who is in which group, whereas this is not the case in an observational study.

-   In an observational study, it is impossible to draw clear cause-andeffect conclusions

### confounding vars

A variable that is related to both how the experimental groups were formed and the response variable of interest.

-   Two methods for data collection: Sampling and Experimentation.

-   distinguish between selection bias, measurement or response bias, and non-response bias.

-   select a simple random sample from a given population.

-   distinguish between simple random sampling, stratified random sampling, cluster sampling, systematic sampling, and convenience sampling

## variable

### response variable--y

The response variable is the focus of a question in a study or experiment.

### explanotory variable--x

An explanatory variable is one that explains for changes in the response variable.

### experiments and obeservational study

### bias

selection bias：When the way the sample is selected systematically excludes some part of the population of interest.

measurement or response bias

eg: survey question/scale(The scale or a machine used for measurements is not calibrated properly)

Non-response Bias:When responses are not obtained from all individuals selected for inclusion in the sample.

non-response bias can distort results if those who respond differ in important ways from those who do not respond (e.g. laziness a confounding variable).

### random sampling

def: A sample that is selected from a population in a way that ensures that every different possible sample of size n has the same chance of being selected.

the same chance to be selected

counter eg:

Consider 100 students in a classroom, 60 females and 40 males. If we randomly sample 6 females, and 4 males, then each female has a 6/60 = 0.1 chance of being selected. Same for males, 4/40=0.1. However, not every group of 10 students is equally likely to be selected. This is not simple random sampling

The random selection process allows us to be confident that the sample adequately reflects the population, even when the sample consists of only a small fraction of the population.

eg.Voting Sample Size in a country

### stratified and cluster

-   stratified random sampling:In stratified random sampling, separate simple random samples are independently selected from each subgroup. Each subgroup is called a strata.

In general, it is much easier to produce relatively accurate estimates of characteristics of a homogeneous group than of a heterogeneous group.

stratified: according to certain characteristic

eg.Even with a small sample, it is possible to obtain an accurate estimate of the average grade point average (GPA) of students graduating with high honours from a university (Similar high grades, homogenous, thus only sample a few students). On the other hand, producing a reasonably accurate estimate of the average GPA of all seniors at the university, a much more diverse group of GPAs, is a more difficult task. **Not only does this ensure that students at each GPA level are represented, it also allows for a more accurate estimate of the overall average GPA.**

-   cluster reflect general characteristic about the whole entire population

cluster: randomly groups

Cluster sampling involves dividing the population of interest into non-overlapping subgroups, called clusters. Clusters are then selected at random, and then all individuals in the selected clusters are included in the sample.

### systematic sampling

A value k is specified (e.g. k = 50 or k = 200). Then one of the first k individuals is selected at random, after which every k-th individual in the sequence is included in the sample. A sample selected in this way is called a 1 in k systematic sample.

In the case of large samples, it can ensure that the sample is evenly distributed in the population.

# Random Variable

## Random Variable (R.V.)

A numerical variable whose value depends on the outcome of a chance experiment. A random variable associates a numerical value with each outcome of a chance experiment. (Think of it as a rule that translates each result of a chance event into a number.)

In shorta: random variables convert random events into numbers.

A real-valued random variable X is a function: X : S → $\mathbb R$, where S is the sample space of a chance experiment.

Continuous random variable X: S--\> R is continuous if its set of possible values includes an entire interval on the number line(measurement), which could not be count.

Discrete random variable: X: S--\> R if its set of possible value is a collection of isolated points along the number line.(counting)

### eg

Examples: Coin Tossing (Discrete Random Variable) If we flip a coin 5 times, let X be the number of heads we get. Possible values of X {0,1,2,3,4,5} (where 0 means no heads, and 5 means all heads). Here, X turns each outcome of multiple coin tosses into a count of heads.

Departure Time (Continuous Random Variable) Imagine tracking when people leave a subway station between 10 PM and 11 PM. Let Y represent the time (in hours) someone leaves, so Y can be any number from 10 to 11. Here, Y assigns each departure time to a point in the range \[10,11\].

## sample space of multivariables

G:gender; F: year---corresponding to a certain student:S

(G, H) : S → $\mathbb R^2$, S={1,2,3,4}

...

## Probability Mass Function and Cumulative Distribution Function for Discrete Random Variables

Probability Mass Function (PMF): $p_x (x) := P(X = x) ,\forall x$

Cumulative Distribution Function (CDF): $F_x (x) := P(X \leq x) ,\forall x$

```{r}
# 定义每个事件的概率和对应的 X 值
outcomes <- c("GGGG", "EGGG", "GEGG", "GGEG", "GGGE", 
              "EEGG", "EGEG", "EGGE", "GEEG", "GEGE", 
              "GGEE", "GEEE", "EEEG", "EEGE", "EEEE")
probabilities <- c(0.1296, 0.0864, 0.0864, 0.0864, 0.0864, 
                   0.0576, 0.0576, 0.0576, 0.0576, 0.0576, 
                   0.0384, 0.0384, 0.0384, 0.0384, 0.0256)
X_values <- c(0, 1, 1, 1, 1, 
              2, 2, 2, 2, 2,
              3, 3, 3, 3, 4)

# 计算每个 X 值的 PMF 通过分组和求和
pmf <- tapply(probabilities, X_values, sum)

# 定义可能的 X 值
X_values_unique <- sort(unique(X_values))

# 计算 CDF
cdf <- cumsum(pmf)

# 确保 CDF 在 x > 4 时为 1
cdf <- c(cdf, 1)

# 更新 X 值以包括 x > 4 的情况
X_values_unique <- c(X_values_unique, ">4")

# 创建数据框显示 PMF 和 CDF
table <- data.frame(
  X = X_values_unique,
  `PMF P(X=x)` = c(pmf, 1-0.1296-0.3456-0.2880-0.1536-0.0256),  # PMF 没有对应的 x > 4 值，填 NA
  `CDF F(X<=x)` = cdf
)

# 打印表格
print(table)

```

-   Note that the domain of a cdf is (−∞, ∞)

$\mathrm{pmf} \Longrightarrow \mathrm{cdf}$ $$
F_x(x)=\sum_{y \leq x} p_x(y)
$$ cdf $\Longrightarrow$ pmf Suppose $X$ takes ordered values $x_1, x_2, x_3, \cdots$, then $$
\begin{aligned}
p_X\left(x_i\right) & =P\left(X=x_i\right)=P\left(x_{i-1}<X \leq x_i\right) \\
& =P\left(X \leq x_i\right)-P\left(X \leq x_{i-1}\right) \\
& =F\left(x_i\right)-F\left(x_{i-1}\right)
\end{aligned}
$$

-   remark: The probability of a discrete distribution varies depending on the inclusion and exclusion of the boundary values.

## Expectation and Variance for Discrete Random Variables

$$
\frac{0 \cdot f_0+1 \cdot f_1+2 \cdot f_2+\cdots+n \cdot f_n}{N}=\frac{1}{N} \sum_{i=0}^n i \cdot f_i
$$

Note that in $\frac{1}{N} \sum_{i=0}^n i \cdot f_i$, $$
\lim _{N \rightarrow \infty} \frac{f_i}{N}=P(X=i)
$$

So the average number will be $$
\sum_{i=0}^n i \cdot P(X=i)
$$

-   Definition: Expectation Given a discrete random variable $X$, the expectation of $X$ is $$
    E[X]=\sum_x x \cdot p_X(x)
    $$

-   Properties of Expectation

-   If $c$ is a constant, then $E[c]=c$.

-   If $X \geq 0$ then $E[X] \geq 0$.

-   If $a \leq X \leq b$ then $a \leq E[X] \leq b$.

-   Proof of 3: First show $E[X] \geq a$, then show $E[X] \leq b$, $$
    \begin{aligned}
    E[X] & =\sum_x x p_X(x) \geq \sum_x a p_X(x), \\
    & =a \sum_x p_x(x)=a .
    \end{aligned}
    $$

Similarly, $E[X] \leq b$.

-   Suppose $X$ is a discrete random variable and $Y=g(X)$, then $$
    \begin{aligned}
    E[Y] & =\sum_y y p_Y(y)=\sum_y y P(Y=y) \\
    & =\sum_y y \sum_{\{x: g(x)=y\}} P(X=x) \\
    & =\sum_y \sum_{\{x: g(x)=y\}} y P(X=x) \\
    & =\sum_y \sum_{\{x: g(x)=y\}} g(x) P(X=x) \\
    & =\sum_x g(x) P(X=x)
    \end{aligned}
    $$

-   First moment of $X$ (mean): $$
    E[X]=\sum_x x p_X(x) .
    $$

-   Second moment of $X$ : $$
    E\left[X^2\right]=\sum_x x^2 p_x(x) .
    $$

-   In general, $E[g(X)] \neq g(E[X])$. For example, let $g(x)=x^2$, and consider $X$ such that $$
    p_X(x)= \begin{cases}0.5, & \text { for } x=-1 \\ 0.5, & \text { for } x=1\end{cases}
    $$

Then clearly $E\left[X^2\right]=1 \neq 0=(E[X])^2$.

-   There are exceptions (e.g. when g is linear)!

-   Linearity of Expectation

E\[aX + b\] = aE\[X\] + b

proof:

Suppose $g(x)=a x+b$. Then $$
\begin{aligned}
E[g(X)] & =\sum_x g(x) p_X(x), \\
& =\sum_x(a x+b) p_x(x), \\
& =\sum_x a x p_x(x)+\sum_x b p_x(x), \\
& =a \sum_x x p_x(x)+b \sum_x p_x(x), \\
& =a E[X]+b=g(E[X]),
\end{aligned}
$$ this implies $$
E[a X+b]=a E[X]+b
$$

-   Remark: Apart from this case, always assume $E[g(X)] \neq g(E[X])$.

# joint distribution of X and Y

The joint probability mass function (i.e., joint pmf) of X and Y for discrete random variables is defined as $p_{X,Y} (x, y) := P(X = x \text{ and } Y = y)$

P\[(X,Y) ∈ A\] =$\sum_{(x,y)\in A}p_{X,Y}(x,y)$,where A belongs to a subset of the $\mathbb R^2$ where X and Y taking values.

# Marginal Probability Mass Function

Let $X$ and $Y$ have the joint probability mass function $p_{X, Y}(x, y)$ with space $\mathcal{S}$. The probability mass function of $X$ (or $Y$ ) alone, is called the marginal probability mass function of $X$ (or $Y$ ) and defined by: $$
\begin{aligned}
& p_X(x)=P(X=x)=\sum_y p_{X, Y}(x, y) \\
& p_Y(y)=P(Y=y)=\sum_x p_{X, Y}(x, y)
\end{aligned}
$$

For the marginal of X, we sum over all values of y.

For the marginal of Y, we sum over all values of x.

eg.--helping understand both sample space, event and Marginal probability mass function

![](images/clipboard-206208503.png)

# Functions of multiple random variables

it has the same philosophy as one random variable things, such as expectation:

$$E[g(X,Y)]=\sum_x \sum_y g(x,y)p_{X,Y}(x,y)$$, also the linearity of expectation

eg. Z = X + 2Y.

![](images/clipboard-369824916.png)

# independence

-   P($A\cap B$)=P(A)P(B) or P(A\|B)=P(A)

-   Two discrete random variables X and Y are independent if P(X = x and Y = y) = P(X = x)P(Y = y):

$p_{X,Y}(x,y)=p_X(x)p_Y(y), \forall x,y$

$p_{X|Y}(x|y)=p_X(x), \forall x,y$

it is easy to detect the dependent as long as we find one example, eg: $P_{X|Y}(1|1)$ is not equal to $P(X=1)$

## independence, expectations(mean) and variance

Independence and Expectations If $X$ and $Y$ are independent, then $$
E[X Y]=E[X] E[Y] .
$$

Proof: We use $E[g(X, Y)]$ where $g(x, y)=x y$. $$
\begin{aligned}
E[X Y] & =\sum_x \sum_y x y p_{X, Y}(x, y) \\
& =\sum_x \sum_y x y p_X(x) p_Y(y), \quad(\text { by inde } \\
& =\left(\sum_x x p_X(x)\right)\left(\sum_y y p_Y(y)\right)=E[X] E[Y] .
\end{aligned}
$$ (by independence)

Similarly, if $X$ and $Y$ are independent, then $$
E[g(X) h(Y)]=E[g(X)] E[h(Y)]
$$

-   Independence and Variances

It is always true that $$
\operatorname{Var}(a X)=a^2 \operatorname{Var}(X), \quad \text { and } \quad \operatorname{Var}(X+a)=\operatorname{Var}(X)
$$

In general, when we have a sum of random variables $X$ and $Y$ $$
\operatorname{Var}(X+Y) \neq \operatorname{Var}(X)+\operatorname{Var}(Y) .
$$

It is only true if $X$ and $Y$ are independent

-   Sum of Variance for independent R.V. If two random variables $X$ and $Y$ are independent then $$
    \operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)
    $$

-   Sum of Variance for independent R.V.

If two random variables $X$ and $Y$ are independent then $$
\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y) .
$$

Proof: Independence implies $E[X Y]=E[X] E[Y]$. Thus $$
\begin{aligned}
& \operatorname{Var}(X+Y)=E\left[(X+Y-(E[X]+E[Y]))^2\right], \\
& =E\left[(X-E[X])^2+2(X-E[X])(Y-E[Y])+(Y-E[Y])^2\right], \\
& =\operatorname{Var}(X)+\operatorname{Var}(Y)+2 E[(X-E[X])(Y-E[Y])]
\end{aligned}
$$

As $X$ is indep to $Y$, then $X-\mu_X$ is indep to $Y-\mu_Y$ so $$
E[(X-E[X])(Y-E[Y])]=E[X-E[X]] E[Y-E[Y]]=0
$$

Example: Assume independence, $\operatorname{Var}(3 X-5 Y)=$ $$
\operatorname{Var}(3 X)+\operatorname{Var}(-5 Y)=9 \operatorname{Var}(X)+25 \operatorname{Var}(Y)
$$

-   Example. Independence, mean and variance

Let $Y$ be the random variable denoting the total number of heads by tossing a coin $n$ times. Find the mean and variance of $Y$.

Let $$
Y_i= \begin{cases}1, & \text { if the } i^{\text {th }} \text { toss gets a head } \\ 0, & \text { otherwise. }\end{cases}
$$

Then $Y=Y_1+Y_2+\cdots+Y_n$ where $Y_1, Y_2, \cdots, Y_n$ are independent. For any $i=1,2, \cdots, n$, we have $$
\begin{array}{ccc}
y & 0 & 1 \\
P_{Y_i}(y) & 1 / 2 & 1 / 2
\end{array}
$$

As $E\left[Y_i\right]=\frac{1}{2}$ and $\operatorname{Var}\left(Y_i\right)=\frac{1}{4}$ for $i=1,2, \cdots, n$ $$
\begin{gathered}
E[Y]=E\left[Y_1\right]+E\left[Y_2\right]+\cdots+E\left[Y_n\right]=\frac{n}{2} \\
\operatorname{Var}(Y)=\operatorname{Var}\left(Y_1\right)+\operatorname{Var}\left(Y_2\right)+\cdots+\operatorname{Var}\left(Y_n\right)=\frac{n}{4}
\end{gathered}
$$ (for not independent cases we have the same result as E but not Var because Var is not linear)

# Continuous Random Variables

We have to consider intervals instead of one when referring to continuous random variables or the probability will equals to 0, which has no meaning.

eg. $P(a\leq Y\leq b)=P(a<Y<b)$ if Y is a continuous variable.

## Definition. Probability Density Function (pdf)

For a continuous random variable X, the Probability Density Function (PDF) of X is f(x) where P(X = x) = 0 for all x and for any a ≤ b

## discrete and continuous

![](images/clipboard-1910128728.png)

Density function's value could be greater than 1 because the integral of it in a very tiny interval could be very small, which represents the probability(which could not be greater than 1 and since the interval could be very tiny this condition is satisfied!).(This reminder notices that the value of pdf is not the probability since the integration is the probability which is totally 1)

# Covariance

![](images/clipboard-4061434485.png)

## wait for reviewing.....

![](images/clipboard-2808960987.png)

# Descrete distributions

A probability distribution is a graph, table of formula that gives the probability for each value of the random variable.

## Bernoulli is composed by binomial

## Hypergeometric distribution could be approximated by Binomial when samples are large

## Poisson distribution as the limit of Binomial distribution when the number of trials is large and the probabiliy of success of each trial is inverse-proportional to the number of trials

-   The Poisson distribution is a discrete probability distribution that applies to occurrences of some event over a specified interval. The random variable x is the number of occurrences of the event in an interval.The probability of the event occurring x times over an interval is given by $$P(x)=\frac{u^x\cdot e^{-\mu}}{x!}$$ where the random variable x is the number of occurences of an event over some interval and the ovvurrences must be random and independent of each other.

中心极限定理：当泊松分布的参数 λ 较大时，泊松分布的形状会接近正态分布。这是因为中心极限定理指出，大量独立随机变量的和趋向于正态分布，而泊松分布可以看作是大量伯努利试验成功次数的分布，当试验次数足够多时，其和可以用正态分布来近似

### ???The occurrences must be uniformly distributed over the interval being used

The mean is $\mu$

The standard deviation is $\sigma= \sqrt \mu$

-   eg of Poisson distribution: (describing the behavior of rare events(with small probabilities). radioactive decay, arrivals of people in a line, eagles nesting in a region, patients arriving at an emergency room(the local hospital experiences a mean of 2.3 patients arriving at the emergency room during 10-11 P.M. on Fri. is known, we can find the probability that for a randomly selected Fri. between 10-11 P.M., exactly four patients arrive), Internet users logging onto a Web site )

-   Comparison between Binomial:

Binomal distribution is affected by yhe smaple size n and the probability p, whereas the Poisson distribution is affected only by mean $\mu$

A binomial distribution has a limit of possible values but a Poisson distribution has a possible values x without upper bound.

Suppose the $X_n,n\geq1$ is a sequence of random variables such that $X_n$\~Bin($n,p_n$), where $p_n$\~$\lambda/n$ as $n->\infty$ $lim_{n->\infty}(np_0)=\lambda$, **intuitively we can observe that** $\lambda$ is the mean of

given the well-known limit $\lim _{n \rightarrow \infty}\left(1-\frac{\lambda}{n}\right)^n=e^{-\lambda}$, and $$
\begin{aligned}
& \frac{n}{n} \frac{n-1}{n} \ldots \frac{n-k+1}{n}=\prod_{i=0}^{k-1}\left(1-\frac{i}{n}\right) \\
& \lim _{n \rightarrow \infty} \prod_{i=0}^{k-1}\left(1-\frac{i}{n}\right)=1 \\
& P\left(X_n=k\right)=\binom{n}{k} p_n^k\left(1-p_n\right)^{n-k} \\
&= \frac{n \cdots(n-k+1)}{k!} p_n^k\left(1-p_n\right)^n\left(1-p_n\right)^{-k} \\
&= \frac{1}{k!}(\underbrace{n p_n}_{\rightarrow \lambda})^k \underbrace{\frac{n}{n} \frac{n-1}{n} \cdots \frac{n-k+1}{n}}_{\rightarrow 1} \underbrace{\left(1-p_n\right)^n}_{\rightarrow \mathrm{e}^{-\lambda}} \underbrace{\left(1-p_n\right)^{-k}}_{\rightarrow 1} \\
& \rightarrow \frac{\lambda^k}{k!} \mathrm{e}^{-\lambda} \underbrace{}_{\text {as } n \rightarrow \infty .}
\end{aligned}
$$

Check that $p_x(k), k=0,1,2, \cdots$ defines a probability mass function: given the Taylor Series of $e^\lambda$ around $\lambda=0$ is given by $f(x)=$ $\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} x^n$ and $e^\lambda=\sum_{n=0}^{\infty} \frac{\lambda^n}{n!}$. Hence, $$
\begin{aligned}
\sum_{k=0}^{\infty} p_X(k) & =\sum_{k=0}^{\infty} \frac{\lambda^k}{k!} \mathrm{e}^{-\lambda} \\
& =\mathrm{e}^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} \\
& =\mathrm{e}^{-\lambda} \cdot \mathrm{e}^\lambda \\
& =1 .
\end{aligned}
$$ \## Geometric distribution (and Geometric series--powerful!fantastic series)

-   The probability of the first happening

-   well-defined $\begin{aligned} \sum_{k=1}^{\infty} p_X(k) & =\sum_{k=1}^{\infty}(1-p)^{k-1} p \\ & =p \sum_{k=0}^{\infty}(1-p)^k \\ & =p \cdot \frac{1}{1-(1-p)} \\ & =1 .\end{aligned}$

-   Tail probability of the Geometric distribution

Let $X \sim \operatorname{Geom}(p)$ then $$
\begin{aligned}
P(X>n) & =P(X=n+1)+P(X=n+2)+P(X=n+3)+\cdots \\
& =(1-p)^n p+(1-p)^{n+1} p+(1-p)^{n+2} p+\cdots \\
& =(1-p)^n p\left(1+(1-p)+(1-p)^2+\cdots\right) \\
& =(1-p)^n p \frac{1}{1-(1-p)} \\
& =(1-p)^n
\end{aligned}
$$ for any $n=0,1,2, \ldots$

-   Memoryless property of Geometric distribution

Suppose that $X \sim \operatorname{Geom}(p)$ and $n \in\{1,2,3, \cdots\}$. Then $$
P(X-n=k \mid X>n)=P(X=k) \quad, k=1,2,3, \cdots
$$

That is, the distribution of $X-n$ under the probability function $P(\cdot \mid X>n)$ is the same as the distribution of $X$

the memoryless property is saying that given the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success (independent)

-   Expectation

Suppose that $X \sim \operatorname{Geom}(p)$. Then $$
\begin{aligned}
E(X)=\sum_{k=1}^{\infty} k P(X=k) & =\sum_{k=1}^{\infty} k(1-p)^{k-1} p \\
& =p \sum_{k=1}^{\infty}\left[-\frac{\mathrm{d}}{\mathrm{~d} p}(1-p)^k\right] \\
& =-p \frac{\mathrm{~d}}{\mathrm{~d} p}\left[\sum_{k=0}^{\infty}(1-p)^k\right] \\
& =-p \frac{\mathrm{~d}}{\mathrm{~d} p}\left[\frac{1}{1-(1-p)}\right] \\
& =-p \frac{\mathrm{~d}}{\mathrm{~d} p}\left[\frac{1}{p}\right] \\
& =\frac{1}{p}
\end{aligned}
$$ — variance

Likewise, $$
\begin{aligned}
E(X(X-1)) & =\sum_{k=1}^{\infty} k(k-1) P(X=k) \\
& =\sum_{k=2}^{\infty} k(k-1)(1-p)^{k-1} p \\
& =p(1-p) \sum_{k=2}^{\infty} k(k-1)(1-p)^{k-2} \\
& =p(1-p) \frac{\mathrm{d}^2}{\mathrm{~d} p^2}\left[\sum_{k=0}^{\infty}(1-p)^k\right] \\
& =p(1-p) \frac{\mathrm{d}^2}{\mathrm{~d} p^2} \frac{1}{p} \\
& =p(1-p) \cdot \frac{2}{p^3} \\
& =\frac{2(1-p)}{p^2} .
\end{aligned}
$$ $$\begin{aligned} \operatorname{Var}(X) & =E(X(X-1))+E(X)-(E(X))^2 \\ & =\frac{2(1-p)}{p^2}+\frac{1}{p}-\frac{1}{p^2} \\ & =\frac{1-p}{p^2}\end{aligned}$$

## Geometric distribution

### The irrelevance of past events to the probability of future independent events

Given the first n trials are unsuccessful, the number of trials until success after the first n trials has the same distribution as the unconditional number of trials until success.

X\~Geom(p) and n\$\in \${1,2,3,....}

P(X-n=k\|X\>n)=P(X=k), k=1,2,3,...

Tail probability for the probability calculation of more/higher than….

## Addition

Independent random variable with the same distribution allows the addition law

# Normal distribution

## interpretation:

-   Standard normally distributed sth. z=1.58(corresponding to 0.9429):

    the probability of randomly selecting sth. with a value less than 1.58(unit) is equal to the area(probability) of 0.9429.

    (Or: 94.29% of sth, will have a value below 1.58(unit))

## Sampling distributions and Estimators

We are beginning to embark a jourfa d dney that allows us to learn about populations by obtaining data from samples since it is rare that we know all values in an entire population.

**Sampling distribution of a statistic** is the probability distribution of a sample statistics (such as mean/proportion which tend to target the population mean/proportion), with all samples having the same sample size. This concept is important to understand. The behavior of a statistic can be known by understanding its distribution( (The random variable in this case is the value of that sample statistics)). Under certain condition, the distribution of sampling mean/proportion approximates a normal distribution.

**Though statistics does not depend on unknown parameters, the distribution of it depend on unknown parameters.(eg. Normal distribution of sample means depends on population mean(an unknow parameter) and standard deviation)**

(ps: the advantage of sampling with replacement:

when selecting a relatively small sample from a large population, it makes no significant difference whether we sample with or without replacement.

Sampling with replacement results in independent events that are unaffected by previous outcomes, and independent events are easier to analyze and they result in simpler formulas.)

**For a fixed sample size, the mean of all possible sample means is equal to the mean of population though sample means vary(sampling variability)**

## Unbiased estimators and biased estimators

Statistics that target population parameters: Mean, variance, proportion

Statistics that do target population parameters: Median, Range, Standard Deviation

(the bias is relatively small then sampling standard deviation in large samples so s is ofent used to estimate $\sigma$)

# The Central Limit Thm

It is the foundation for estimation population parameters and hypothesis testing

(Recall: A random variable is a variable that has a single numerical value(eg. x=1,x=2), determined by chance, for each outcome of a procedure随机变量是一种变量，对于一个过程的每个结果都有一个随机确定的单一数值(可以理解为，现象作用其上

A probability distribution is a graph, table or formula that gives the probability for each value of a random variable

The sampling distribution of the mean is the probability fistribution of sample means, with all sample having the same sample size n)

As the sample size increases, the corresponding sample means tend to vary less. The central limit thm tells us that if the sample size is large engough, the distribution of sample means can be approximated by normal distribution

Conclusion of CLT: (it is so important since it allows us to use the basic normal distribution methods in a wide variety of different circumstances)

The distribution of sample means will, as the sample size increases, approach a normal distribution

The mean of all sample means is the population mean $\mu.$. $$\mu_{\bar x}=\mu$$

The standard deviation of all sample means is $\sigma/ \sqrt n$ (i,e, the normal distribution from conclusion"The distribution of sample means will, as the sample size increases, approach a normal distribution" has standard deviation $\sigma/ \sqrt n$ ) $$\sigma_{\bar x}=\frac{\sigma}{\sqrt n}$$ $\sigma_{\bar x}$ is often called the standard error of the mean.

30

Notice: Be careful to look at if it is from a normally ditributed population or a mean for some sample/group.

If given the population mean is a and a sample(c subjects) mean is b which has so small probbility under the sampling distribution with this populatio mean, if the mean is really a, then there is an extremely small probability of getting a sample mean of b or lower when c subjects are randomly selected. we can interpret it in such 2 ways:

1)  population mean is true and their sample represents a chance event that is extreamely rare

2)  population mean is not true and sample is typical.

Because the probability is too low, it seems more reasonable to conclude that the population mean is lower than a---hypothesis testing's thinking

### Optional: Correction for a finite population

When sampling without replacement and the sample size n is greater than 5% of the finite populaiton size B(i.e. n\>0.05N), ...

## Using the Normal distribution as an approximation to the binomial distribution

requirement: np, n(1-p) $\geq$ 5

Be careful: adjust x for continuity by + or - 0.5(eg. at least 99, choose 98.5)

## to hypothesis testing

a question (eg):

In a test of a gender-selection technique assume that 100 couples using a particular treatment give birth to 52 girls (and 48 boys). If the technique has no effect, then the probability of a girl is approximately 0.5. If the probability of a girl is 0.5, find the probability that among 100 newborn babies, exactly 52 are girls. Based on the result, is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?

it is a binomial distribution with np=nq=100\*0.5=50$\geq 5$

so we use the normal distribution with mean of 50 and $\sigma = \sqrt {npq} = 5$as an approximation to the binomial distribution

to answer"is there strong evidence supporting a claim that the gender-selection technique increases the likelihood that a baby is a girl?", we need to calculate more than 52(x successes among n trials is an unusually high number of successes if P($x\geq a$) is very small)

原因是因为如果只看52这个数字肯定概率很小，因为任何单个数字发生的可能性概率都很小

So if the answer of P($x\geq 52$) is small we could conclude that the gender selection is useful

(总结：如果是0。5概率来看的话52以上本来就不是难事，as indicated by the such large probability of P($x\geq 52$) 所以我们没有充分证据拒绝“not effective”这个假设前提)

此处还没有引入假设检验所以都是用using probability to determine when results are unusual这个思想来思考问题的：

-   Unusually low： x successes among n trails is an unually low number of successes if P(x or fewer) is very small

![](images/clipboard-804415.png)

Interpretation for another example of gender selection(using only unusual results to explain): Because the probability of more than 13 girls, which is 0.001 is so low, we conclude that it is unusual to get 13 girls among 14 babies (using binomial to calculate it). This suggests that the technique of gender selection appears to be effective since it is highly unlikely that the result of 13 girls among 14 births happened by chance.

## Normal distribution

If a variable is the superposition result of a large number of small independent random factors, then the variable must obey the normal distribution of variables

e.g. measure error

### Assessing Normality

In general, quantile plots can be used to assess any probability distribution.

For a normal quantile plot(or normal probability plot), it is a graph of points(x,y) where each x vaue is from the original set of sample data and each y value is the cooresponding z score that is a quantile value expected from the standard normal distribution

#### Procedures

-   Histogram(not helpful for small data set)

-   outliers: reject normality if there is more than 1 outlier present(not helpful for small data set)

-   normal quantile plot:

sort data from lowest to highest

## inferences from 2 samples introduces the differences between two populaton means using matched pairs but correlaition and regression analyze the association between the 2 variables and if such an association exists we wnat to describe it with an equation that can be used for predictions

paired sampled data(or called bivariate data)

-   a correlation exists between two variables when one of them is related to the other in some way.

-   the linear correlation coefficient r measures the strength of the linear association between the paired x- and y-quantitative values in a sample. Its value is computed by using the formula(Pearson(1857-1937) product moment correlation coefficient)

......otherwise there is not sufficient evidence to support the conclusion of a significant linear equation

!!!: interpreting r: explained variation: the value of $r^2$ is the proportion of the variation in y that is explained by the linear association between x and y.(and the other percentage is explained by factors other thanx such as characteristics not included in the study)

-   correlation does not imply causality,just the association

-   average suppress individual variation and may inflate the correlation coefficient(the linear correlation coefficient became higher when reginal averages were used)

# point estimation and confident interval
