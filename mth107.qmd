---
title: "MTH107---NOTEs"
format: html
editor: visual
---

# Chapter 0: motivation/reviews

## Linearty is everywhere--Linearty is the easiest one

matrix-vector multiplication

differentiation/integration

ODE(mth106)

Recurrence Relations and Statistical Models

## comparison

### Real R space

A matrix A in this space is a real matrix, which maps vectors in $\mathbb{R^n}$ to another vector in $\mathbb{R^n}$ through multiplication: $$
A: \mathbb{R^n} \to \mathbb{R^n}
$$ $$
A \cdot v = w \in \mathbb{R^n}
$$

**real matrix**

**concrete**

### Abstract vector space over R or C

linear transformation: $$
T: V \to W
$$ **abstract and more general vector space over** $\mathbb{R^n}$ or $\mathbb{C^n}$

## Notations

-   $$
    a \in A
    $$ eg. $$
    3 \in \mathbb{Z}
    $$

-   $\emptyset$

-   Two sets A and B are equal if and only if they contain the same elements: $$
    A = B
    $$ if and only if $$
    \forall x \, (x \in A \iff x \in B)
    $$ \$\$ A \subseteq B

-   $$ intersection: $$ A \cap B

-   $$ union: $$ A \cup B \$\$

## Maps

**maps(functions)**

$$
f: A \to B, \quad a \mapsto f(a)
$$ composition: Given two functions $f: B \to C$ and $g: A \to B$, the composition of f and g is denoted as: $$
(f \circ g)(x) = f(g(x)), \quad \text{for all} \, x \in A
$$

$$
a \mapsto c(a \mapsto b \mapsto c)
$$

### injective--one to one

if $a_1 \neq a_2$ then $f(a_1) \neq f(a_2)$

if $f(a_1) = f(a_2)$, then $a_1=a_2$

### surjective--onto

for $f: A \to B$

$$
\forall b \in B, \, \exists a \in A \, \text{such that} \, f(a) = b
$$

### bijetive--both inj and suj

for $f: A \to B$ $$
\forall b \in B, \, \exists! a \in A \, \text{such that} \, f(a) = b
$$ （use $\exists!$expresses only exist one）

## subsets,inclusions, restrictions

### subsets

$A \subseteq B$

### empty set is a subset of every set B.

$$
\emptyset \to B
$$

### inclusion

if A $\subseteq$ B, we have a map: $$
\iota: A \to B
$$

$$
\iota(a) = a
$$, called the inclusion of A (into B)

### restriction

That is:

if A $\subseteq$ B and $f:B \to C$, we have a map for all a $\in$ A: $$
f|_A: A \to C
$$:

$$
f|_A(a) = f(a)
$$ called the restriction of f to A

$$
f|_A = f \circ \iota_A
$$ because $$
f \circ \iota_A(a)=f(\iota_A(a))=f(a)=f|_A(a)
$$

the reason to define it:

-   domains differ

For example, consider a map $g|_B: B \to C$, where the function takes each element $x$ and maps it to $x + 2$. Let the sets be as follows:

-   $A = \mathbb{N} \subseteq B = \mathbb{R}$

-   $C = \mathbb{R}$ is the codomain of the function.

The map $g$ is defined as: $$ g: B \to C, \quad g(x) = x + 2 \quad \text{for all } x \in B $$

Now, consider the restriction of $g$ to $A$, denoted $g|_A: A \to C$, where: $$ g|_A: A \to C, \quad g|_A(x) = x + 2 \quad \text{for all } x \in A $$

Although both $g: \mathbb{R} \to \mathbb{R}$ and $g|_A: \mathbb{N} \to \mathbb{R}$ follow the same rule $x \mapsto x + 2$, they cannot be considered the same map because their domains differ.

(here, A $\subseteq$ B)

-   focus on the specific area within the whole area

### set of all maps from A to B(f)

if a and b are sets we define $B^A={f:A--->B (map)}$ the set of all maps from A to B

$$
B^A = \{ f: A \to B \}
$$ $B^\emptyset = \{ \emptyset \to B \}$ has only 1 element even if B=$\emptyset$

-   The set $B^{\emptyset}$ contains only the zero function. proof: suppose f,g $\in$ $B^\emptyset = \{f: \emptyset \to B \}$, imagine f != g, we can derive $\exists x \in \emptyset$ s.t. f(x) != g(x), which is absurd, so f=g

suppose the only one element is sth.:

sth. should follow the quality of the set, which is:

sth. + sth. = sth.

$\lambda$sth. =sth.

so sth. = 0

#### cardinality

\|\| \#

\|A\|: number of elements in A(set)

if A and B are finite sets #A=n #B=m, then $|B^A| = m^n$

## relevant to dear regression analysis

![MLR](images/clipboard-3617718739.png)

**matrix** 

### 3.1 Vector and Matrix Multiplication in Regression

In linear regression, we often work with matrix and vector representations. Given a matrix $X$ (which represents the design matrix with each row corresponding to one observation and each column to a predictor), and a vector $Y$ (representing the observed responses), the regression model can be written as:

$$
Y = X \beta + \epsilon
$$

Where: - $Y$: The $n \times 1$ response vector, - $X$: The $n \times p$ design matrix, - $\beta$: The $p \times 1$ vector of unknown coefficients, - $\epsilon$: The $n \times 1$ vector of errors or residuals.

The ordinary least squares (OLS) estimator for $\beta$, denoted $\hat{\beta}$, is obtained by minimizing the residual sum of squares:

$$
\hat{\beta} = (X^T X)^{-1} X^T Y
$$

### 3.2 Orthogonality of Vectors

Consider two vectors $X_1$ and $X_2$. If $X_1^T X_2 = 0$, we say the vectors are orthogonal, meaning they are uncorrelated in terms of inner product space. In regression, orthogonality implies no collinearity between predictors.

Orthogonality plays an important role in simplifying matrix operations in regression. For example, if $X_1$ is orthogonal to $X_2$, the matrix $X^T X$ will have off-diagonal elements equal to zero, simplifying the computation of the inverse.

### 3.3 Properties of Matrix $X^T X$

If the matrix $X^T X$ is invertible, the OLS estimator exists and is unique. One important property of the matrix $X^T X$ is that it is symmetric and positive semi-definite. If $X$ has full column rank (meaning that the columns of $X$ are linearly independent), then $X^T X$ is positive definite, and its inverse exists.

#### Inverse of $X^T X$:

If $X^T X$ is invertible, then:

$$
(X^T X)^{-1} X^T X = I_p
$$

Where $I_p$ is the identity matrix of size $p \times p$.

### 3.4 Projection Matrix in OLS

The projection matrix (or hat matrix) $P$ is defined as:

$$
P = X (X^T X)^{-1} X^T
$$

This matrix $P$ projects the observed data vector $Y$ onto the column space of $X$, giving the fitted values $\hat{Y}$:

$$
\hat{Y} = P Y
$$

The residuals $\hat{\epsilon}$, which are the differences between the observed and fitted values, are given by:

$$
\hat{\epsilon} = Y - \hat{Y} = (I_n - P) Y
$$

Where $I_n$ is the $n \times n$ identity matrix. The projection matrix $P$ has several key properties: - $P^2 = P$ (idempotent), - $P^T = P$ (symmetric).

### 3.5 Residual Sum of Squares (RSS)

The residual sum of squares (RSS) is given by:

$$
\text{RSS} = \sum_{i=1}^{n} \hat{\epsilon}_i^2 = \hat{\epsilon}^T \hat{\epsilon} = (Y - X \hat{\beta})^T (Y - X \hat{\beta})
$$

Substituting $\hat{\beta} = (X^T X)^{-1} X^T Y$, we get:

$$
\text{RSS} = Y^T (I_n - P) Y
$$

### 3.6 Variance-Covariance Matrix of $\hat{\beta}$

The variance-covariance matrix of the OLS estimator $\hat{\beta}$ is:

$$
\text{Var}(\hat{\beta}) = \sigma^2 (X^T X)^{-1}
$$

Where $\sigma^2$ is the variance of the error terms. This matrix provides information about the precision of the estimated coefficients.

------------------------------------------------------------------------

### 4. Properties of Orthogonal Projections

Consider a vector $y$ in the vector space spanned by the columns of $X$. The projection of $y$ onto this space can be written as:

$$
\hat{y} = P y
$$

If $P$ is the projection matrix, then the following properties hold: - $P^T = P$ (symmetric), - $P^2 = P$ (idempotent), - $(I - P)$ is also a projection matrix (projecting onto the orthogonal complement).

Thus, we can decompose any vector $Y$ as:

$$
Y = \hat{Y} + (Y - \hat{Y}) = P Y + (I - P) Y
$$

Where $\hat{Y}$ is the projection onto the space spanned by $X$, and $(Y - \hat{Y})$ is the projection onto the orthogonal complement.

### Vector and Matrix Definitions

-   A vector $\perp$ to a matrix $X$ (i.e., orthogonal to the column space of the matrix $X$) implies that:

$$
X^T v = 0
$$

This condition means that the vector $v$ is orthogonal to every column of the matrix $X$. In regression, this condition often arises when discussing residuals, which are orthogonal to the fitted values (i.e., the column space of $X$).

### Properties of Orthogonality

1.  If $X^T v = 0$, then $v \perp X$ (i.e., $v$ is orthogonal to the column space of $X$).
2.  If $X$ has linearly independent columns, then $X^T X$ is invertible.
3.  If $X$ does not have full column rank, then $X^T X$ is not invertible. In this case, we cannot directly compute $(X^T X)^{-1}$.

### Full Column Rank and Invertibility

For $X$ to have full column rank, the number of columns $p$ must be less than or equal to the number of observations $n$:

列空间的正交补空间只有零向量 he orthogonal complement of a column space has only zero vectors

If $X$ has full column rank, then $X^T X$ is invertible. This property is crucial for solving the normal equations in ordinary least squares (OLS) regression.

### Symmetric and Invertible Properties

Given that $X^T X$ is symmetric, the following property holds:

$$
((X^T X)^{-1})^T = (X^T X)^{-1}
$$

This is a key result in linear regression, allowing us to solve for the OLS estimator of $\beta$:

$$
\hat{\beta} = (X^T X)^{-1} X^T Y
$$

This property ensures that the normal equations have a unique solution when $X^T X$ is invertible.

------------------------------------------------------------------------

### 总结：

1.  **正交性**：矢量与矩阵的正交性在回归中经常用于描述残差与拟合值之间的关系。
2.  **矩阵的性质**：矩阵 $X^T X$ 的可逆性取决于 $X$ 的列满秩（线性无关性）。当 $X$ 的列满秩时，$X^T X$ 是可逆的，从而确保了线性回归模型的唯一解。
3.  **矩阵运算**：通过矩阵运算 $(X^T X)^{-1}$，可以得到 OLS 回归系数的估计值 $\hat{\beta}$。

# Chapter 1: vector spaces

## finite space

For finite spaces ,only R\^n and C\^n.

**(For infinite spaces, there are many, e.g. a continuous set: \[0,1\] to ...)**

## R\^n notification

$$
\mathbb{R}^n = \{ (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} \text{ for } i = 1, 2, \dots, n \}
$$ real numbers

### define 2 oprations on the set R\^n

addition: $$
(x_1, x_2, \dots, x_n) + (y_1, y_2, \dots, y_n) = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n)
$$

-   **Left-hand side**: $(x_1, x_2, \dots, x_n), (y_1, y_2, \dots, y_n) \in \mathbb{R}^n$

-   **Right-hand side**: The result of $x_i + y_i$ is in $\mathbb{R}$.

scalar multiplication: for lambda $\in$ R,

$$
\lambda \cdot (x_1, x_2, \dots, x_n) = (\lambda x_1, \lambda x_2, \dots, \lambda x_n)
$$

-   **Left-hand side**: $\lambda \in \mathbb{R}, (x_1, x_2, \dots, x_n) \in \mathbb{R}^n$.

-   **Right-hand side**: The result $\lambda x_i \in \mathbb{R}$, for each i.

these 2 oprations generalize the standard operations on R2 and R3

### geometric realization

矢量三角形--addition

直线上--scalar multiplication

mention: for n greater than or equal to 4 we cannot visualize vectors in the real world but e can still use them to solove real world problems

if a and b are finite sets

### to simplify notations we sometimes use a single letter for vectors:

x= $\vec x$ =(x_1,x_2,....)

R\^n as a vector space

so we have(rn,+, mutiplication notation). the operations satisfies some useful properties that turn rn into a real vector space

## relation between rn and + .

$(R^n,+,\cdot)$

it means that + and $\cdot$ satisfy the following axioms:

1)  $\forall x,y \in rn$: x+y=y+x. commutativity

2)  $\forall x,y,z \in rn$: (x+y)+z=x+(y+z). associativity

3)  x + 0 = 0 + x neutral element for addition

4)  $x + (-x) = (-x) + x = 0$

------inverse for addition -x=y

5)  $1 \cdot x = x$ Identity Element for Scalar Multiplication

6)  $(\lambda \mu) \cdot x = \lambda \cdot (\mu \cdot x)$ compatibilily of multiplication

7)  $\lambda \cdot (x + y) = \lambda \cdot x + \lambda \cdot y$ Distributivity of Scalar Multiplication Over Vector Addition

8)  $(\lambda + \mu) \cdot x = \lambda \cdot x + \mu \cdot x$ Distributivity of Scalar Addition

### ?

ask again, sorry: since negetive number set satisfies 2 oprations but not 8 axioms I want to ensure **if a finite set satisfies the 2 operations it is not satisfy all 1-8 axioms instead of F\^n**

is that because of the defination of F-vector space?(abstract vector space-defination) \## Cn complex Vector Space

$i^2 = -1$

Define $\mathbb {C}^n$ as the set of all ordered n-tuples of complex numbers: $$
\mathbb{C}^n = \{ (z_1, z_2, \dots, z_n) \mid z_i \in \mathbb{C}, \, i = 1, 2, \dots, n \}
$$

### Operations on C\^n

We define **addition** and **scalar multiplication** on $\mathbb{C}^n$ in the same way as we did on $\mathbb{R}^n$, but using complex numbers:

#### Addition:

For two vectors $(z_1, z_2, \dots, z_n)$ and $(w_1, w_2, \dots, w_n) \in \mathbb{C}^n$:

$$
(z_1, z_2, \dots, z_n) + (w_1, w_2, \dots, w_n) = (z_1 + w_1, z_2 + w_2, \dots, z_n + w_n)
$$

#### Scalar Multiplication:

For a scalar $\lambda \in \mathbb{C}$ and a vector $(z_1, z_2, \dots, z_n) \in \mathbb{C}^n$:

$$
\lambda \cdot (z_1, z_2, \dots, z_n) = (\lambda z_1, \lambda z_2, \dots, \lambda z_n)
$$

Thus, $(\mathbb{C}^n, +, \cdot)$ is a complex vector space because it satisfies the vector space axioms 1-8, where we replaced $\mathbb{R}^n$ with $\mathbb{C}^n$. Many of the results from MTH107 will hold regardless of whether we are using $\mathbb{R}$ or $\mathbb{C}$, so we will often use $\mathbb{F}$ to represent either $\mathbb{R}$ or $\mathbb{C}$.

For example, $\mathbb{F}^n$ is an $\mathbb{F}$-vector space, where $\mathbb{F}$ could be either $\mathbb{R}$ or $\mathbb{C}$.

### Generalization (Not on the Exam):

#### Finite Field Example:

$\mathbb{F}_2 = \{0, 1\}$: the finite field with two elements.

Many of our results hold in a more general setting, where $\mathbb{F}$ is a **field**—a set in which we can perform addition, multiplication, subtraction, and division (except division by zero).

Examples of fields include:

-   $\mathbb{R}$: the real numbers
-   $\mathbb{C}$: the complex numbers
-   $\mathbb{Q}$: the rational numbers
-   $\mathbb{F}_2 = \{0, 1\}$: the finite field with two elements

## Abstract Vector Space:

We can generalize this idea by replacing $\mathbb{F}^n$ with some abstract space $V$, define addition $+$ and scalar multiplication $\cdot$, and check if they satisfy the eight vector space axioms.

### Definitions:

if V and W are sets, V$\times$ W = {(v,w)\|v$\in$ V, w$\in$ W}

For a set $V$, **addition** on $V$ is a map:

#### ？

difference？ 4-d based on definition

$$
V \times V \to V
$$

It maps an element set to their addition. For example, if $V = \mathbb{R}^2$:

$$
(v, w) \mapsto v + w
$$

Example: $(1,2) + (3,4) = (4,6)$, which is also in $V$.

A **scalar multiplication** is a map:

$$
F \times V \to V
$$

For example, $(\lambda, v) \mapsto \lambda v$.

**An** $F$-vector space is a set $V$ with an addition $+$ and scalar multiplication $\cdot$ by elements of $F$, such that $(V, +, \cdot)$ satisfies the vector space axioms 1-8, where $\mathbb{R}$ is replaced by $F$, and $\mathbb{R}^n$ is replaced by $V$.

### Remarks and Examples:

-   **Vectors**: Vectors are elements of $V$, denoted as $v \in V$.
-   **Field** $F$: The choice of $F$ matters! For example, we will see later that $\mathbb{C}^n$ is a complex vector space of dimension $n$, but is also a real vector space of dimension $2n$.

## ？prove？

above field F

107's learning need of C

2n proof???

### Examples:

For any field $F$,

1.  **Trivial Vector Space**: the set $V = \{0\}$ is a trivial $F$-vector space with addition $0 + 0 = 0$ and scalar multiplication $\lambda \cdot 0 = 0$.

2.  **Finite-Dimensional Vector Space**: $F^n$ is an $F$-vector space, and $F^0 = \{0\}$.

3.  **Infinite-Dimensional Vector Space**: Let $F^\infty$ be the space of infinite sequences, where:

$$
F^\infty = \{ (x_1, x_2, \dots) \mid x_i \in F, \, i = 1, 2, \dots \}
$$

Addition and scalar multiplication are defined **component-wise**:

$$
(x_1, x_2, \dots) + (y_1, y_2, \dots) = (x_1 + y_1, x_2 + y_2, \dots)
$$

4.  **Function Space**: Let $S$ be a set, then:

$$
F^S = \{ f: S \to F \, \text{(maps from S to F)} \}
$$

Addition and scalar multiplication are defined **pointwise**. For functions $f, g \in F^S$ and $\lambda \in F$:

$$
(f + g)(x) = f(x) + g(x) \quad \forall x \in S
$$

$$
(\lambda \cdot f)(x) = \lambda \cdot f(x) \quad \forall x \in S
$$

$F^S$ is an F-vector space

## ?

is there any quick way to think this kind of question instead of proving 8 axioms one by one?

#### proof of FS

$F^S$ is a vector space

Proof: we need to check the 8 axioms from the defination

(core: use the element here to x and then see the equality of the 2 function on the each side of the equality using the quality of $\mathbb F^n$)

1)  f + g = g+f?

these 2 functions ((f+g)(x) and (g+f)(x)) are equal if and only if they have values agrees on every $s \in S$

$$
(f+g)(x)=f(x)+g(x)
$$ which is $\in F$ so it is equal to g(x)+f(x)(addition axiom)=(g+f)(x)

2)(f+g)+h = f+(g+h)

\[(f+g)+h\](x)=(f+g)(x)+h(x)=\[f(x)+g(x)\]+h(x), which are $\in F$, so

=f(x)+\[g(x)+h(x)\](axiom 3)=f(x)+(g+h)(x)=\[f+(g+h)\](x)

3)  $\exists 0: 0+f=f$ for any $f \in F^S$

yes, define the zero function $0_F:$ to be the constant function $0_F$, i.e. $0_F(x)=0_F$

4)  for $f\in F^S$, can we find an inverse?

-f is defined by (-f)(x)=-f(x)

check: (f+(-f))(x)=f(x)+(-f(x))=$0_F$

so f+(-f)=$0_{F^S}$

5)  1f=f $（1\cdot f)(x)=1\cdot f(x)(\in F)=f(x)$

6)  (ab)f=a(bf)

pass

7)a(f+g)=af+ag

a(f+g)(x)=a(f(x)+g(x))=af(x)+ag(x)=(af+ag)(x)

8)(a+b)f=af+bf

(a+b)f(x)=af(x)+bf(x)

(af+bf)(x)=(af)(x)+(bf)(x)=af(x)+bf(x)

#### Special Cases:

1.  **Empty Set**: If $S = \emptyset$, then:

$$
F^{\emptyset} = \{ 0 \}
$$ the proof has been proved above on "set of all maps from A to B(f)"

2.  **Finite Set**: If $S = \{1, 2, \dots, n\}$, then:

$$
F^{\{1, 2, \dots, n\}} = F^n
$$

##### proof of special 2:

Let $F^n$ represent the n-dimensional vector space over a field $F$, and $F^{\{1, \dots, n\}}$ represent the set of functions from the set $\{1, \dots, n\}$ to $F$, i.e., it assigns a scalar from $F$ to each index in $\{1, \dots, n\}$.

We will prove that there exists a linear map $T: F^n \to F^{\{1, \dots, n\}}$ such that:

1.  $T$ is a bijection (i.e., both injective and surjective).
2.  $T$ preserves addition: $$ 
    T(f + g) = T(f) + T(g) 
    $$ for all $f, g \in F^n$.
3.  $T$ preserves scalar multiplication: $$ 
    T(a f) = a T(f) 
    $$ for all $f \in F^n$ and $a \in F$.

A map that satisfies these two conditions is called a linear map, and we will learn about it later in class. A linear map that is a bijection is called an isomorphism.

**Define the Map** $T$

Define the map $T: F^n \to F^{\{1, \dots, n\}}$ as follows:

For each vector $(a_1, a_2, \dots, a_n) \in F^n$, define the corresponding function $T((a_1, a_2, \dots, a_n)) = f \in F^{\{1, \dots, n\}}$ by: $$
f(i) = a_i \quad \text{for each} \, i = 1, 2, \dots, n.
$$ Thus, $T((a_1, a_2, \dots, a_n)) = (f(1), f(2), \dots, f(n)) = (a_1, a_2, \dots, a_n)$.

-   **Injectivity**: Suppose $T((a_1, a_2, \dots, a_n)) = T((b_1, b_2, \dots, b_n))$. This implies that for each $i$, $a_i = b_i$. Therefore, $(a_1, a_2, \dots, a_n) = (b_1, b_2, \dots, b_n)$, so $T$ is injective.

-   **Surjectivity**: Given any function $f \in F^{\{1, \dots, n\}}$, we can find a vector $(a_1, a_2, \dots, a_n) \in F^n$ such that $f(i) = a_i$ for each $i = 1, 2, \dots, n$. Therefore, $T$ is surjective.

Since $T$ is both injective and surjective, it is a bijection.

**Prove that** $T$ Preserves Addition

Let $(a_1, a_2, \dots, a_n), (b_1, b_2, \dots, b_n) \in F^n$. Then:

$$
T((a_1, a_2, \dots, a_n) + (b_1, b_2, \dots, b_n)) = T((a_1 + b_1, a_2 + b_2, \dots, a_n + b_n)).
$$

$$
f(i) = a_i + b_i, \quad i = 1, 2, \dots, n.
$$ (the definition of $T$) the other hand site: $$
T((a_1, a_2, \dots, a_n)) + T((b_1, b_2, \dots, b_n)) = (a_1, a_2, \dots, a_n) + (b_1, b_2, \dots, b_n),
$$ which results f, also.

**Prove that** $T$ Preserves Scalar Multiplication

Let $(a_1, a_2, \dots, a_n) \in F^n$ and $c \in F$. Then: $$
T(c \cdot (a_1, a_2, \dots, a_n)) = T((c a_1, c a_2, \dots, c a_n)).
$$

$$
f(i) = c a_i, \quad i = 1, 2, \dots, n.
$$ (the definition of $T$)

the other hand site: $$
c \cdot T((a_1, a_2, \dots, a_n)) = c \cdot (a_1, a_2, \dots, a_n),
$$ which also results in $f$

Since T is an isomorphism (bijection + linear), then $T^{-1}$ is automatically linear. That is we automatically have $T^{-1}(x+y)= T^{-1}(x)+ T^{-1}(y)$, and the same for scalar multiplication. So they are the same in vector space, too.

We have shown that $T$ is a bijection and preserves both addition and scalar multiplication. Therefore, $T$ is a linear isomorphism, and $F^n$ and $F^{\{1, \dots, n\}}$ are isomorphic as vector spaces.

(**Last remark**: If $T$ is an isomorphism (i.e., bijective and linear), then $T^{-1}$ is automatically linear. That is, we automatically have $T^{-1}(x + y) = T^{-1}(x) + T^{-1}(y)$, and the same holds for scalar multiplication. Therefore, there is no need to check these properties for $T^{-1}$.)

(a more detailed one proving bijetive: $F^{\{1, 2, 3, \dots, n\}}$ is Equivalent to $F^n$

To prove that $F^{\{1, 2, 3, \dots, n\}}$ is equivalent to $F^n$, we show there is a bijection between the two sets, meaning each element in $F^{\{1, 2, 3, \dots, n\}}$ corresponds to a unique element in $F^n$, and vice versa.

By definition, $F^{\{1, 2, 3, \dots, n\}}$ is the set of all functions from $\{1, 2, 3, \dots, n\}$ to $F$. Each function $f$ can be written as:

$$
f = (f(1), f(2), \dots, f(n)).
$$

$F^n$ is the set of ordered $n$-tuples $(a_1, a_2, \dots, a_n)$, where each $a_i \in F$.

Mapping from $F^{\{1, 2, 3, \dots, n\}}$ to $F^n$---

For any function $f \in F^{\{1, 2, 3, \dots, n\}}$, we define the corresponding tuple in $F^n$ as:

$$
T(f) = (f(1), f(2), \dots, f(n)).
$$

Mapping from $F^n$ to $F^{\{1, 2, 3, \dots, n\}}$----

For any tuple $(a_1, a_2, \dots, a_n) \in F^n$, define the corresponding function in $F^{\{1, 2, 3, \dots, n\}}$ as:

$$
T^{-1}(a_1, a_2, \dots, a_n)(i) = a_i, \quad \text{for each } i \in \{1, 2, \dots, n\}.
$$

Let $f \in F^{\{1, 2, 3, \dots, n\}}$--

$$
T(f) = (f(1), f(2), \dots, f(n)).
$$

then, define

$$
T^{-1}(f(1), f(2), \dots, f(n))(i) = f(i), \quad \text{for each } i \in \{1, 2, \dots, n\}.
$$

Thus,

$$
T^{-1}(T(f)) = f.
$$

Let $(a_1, a_2, \dots, a_n) \in F^n$. Applying $T^{-1}$, we get the function $f$ such that $f(i) = a_i$ for each $i$. And then,

$$
T(T^{-1}(a_1, a_2, \dots, a_n)) = (a_1, a_2, \dots, a_n).
$$

Thus,

$$
T \circ T^{-1} = \text{id}_{F^n}.
$$ This equation means that for any element $(a_1, a_2, \dots, a_n) \in F^n$, applying $T^{-1}$ to obtain a function $f$, and then applying $T$ to $f$, returns the original tuple:

$$
T^{-1} \circ T = \text{id}_{F^{\{1,2,3,...n\}}}.
$$

This equation means that for any function $f \in F^{\{1, 2, 3, \dots, n\}}$, applying $T$ to obtain a tuple $(f(1), f(2), \dots, f(n))$, and then applying $T^{-1}$ to that tuple, returns the original function:

Since $T$ and $T^{-1}$ are inverses of each other, we have established a bijection between $F^{\{1, 2, 3, \dots, n\}}$ and $F^n$. Therefore, $F^{\{1, 2, 3, \dots, n\}}$ and $F^n$ are equivalent as sets and vector spaces.)

## ？

二维映射到三维的linear transformation's geometric meaning

## General properties of vector space

Let V be a vector space over F

We will prove some properties of V using only the defination (axioms 1-8)

### Proposition

1)  The zero(additive identity) is unique. That is: $\exists ! 0\in V s.t. 0+V=V, \forall v\in V$

proof: Suppose we have 2 zero elements: 0 and 0'

0=0'+0=0+0'=0'

2)  $\forall v\in V$ there exists a unique additive inverse

Supppose w and w' are 2 inverses for v, w=0+w=(v+w')+w=v+(w'+w)=v+(w+w')=(v+w)+w'=0+w'=w'

3)  $\forall v\in V$, $O_F\cdot V=O_V$

### ?

why $0_F\cdot V=0_V+w$ F---\>V????

why we could think of let w be the inverse of $0_F\cdot V$? I always just could recite I mean remember the process instead of write it down smoothly

(see 0_f as 0 is also okay)

proof: $0_F$$\cdot$V= $(0_F+0_F)$$\cdot$V=$0_Fv+0_Fv$

let w be the inverse of $0_F\cdot v$(use proposition 2)

then $0_V$=$0_F\cdot v+w$=$(0_Fv+0_Fv)+w=0_Fv+(0_Fv+w)=0_Fv$

4)  $\forall x\in F: x\cdot O_V=O_F$

proof: $x\cdot 0_V=x\cdot (0_V+0_V)=x\cdot 0_V+x\cdot 0_V$

so $0_V=x\cdot 0_V$

#### ?

difference between o_Fv=0_v and xo_v=0_f(3 and 4 proposition)

### Property

$\forall v\in V,(-1)\cdot V=-V$

proof: V+(-1)V=(1+(-1))$\cdot$V=0$\cdot$V=0

so (-1)V is an addictive inverse of V

so we finish proof(by the addictive inverse)

#### reminder of computing inverse of a matrix

method1:

det(A)

每一个位置的det构成的矩阵：B

cofactor matrix:$(-1)^{n+m}$

C=B$\times$ cofactor matrix

$A^{-1}=1/det(A)$ times $C^T$

method2: work for the tansformation of a matrix

# Tutorial

## problems

-   summarize1: give opposite example and draw pictures are okay.

eg.![a problem](images/clipboard-3068495766.png)

eg.

![](images/clipboard-1407097711.png)

## understand again

![understand vector space](images/clipboard-1253681054.png)
