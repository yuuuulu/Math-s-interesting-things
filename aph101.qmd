---
title: "APH101-Biostatistics And R"
format: html
editor: visual
toc: true
---

# I am deeply grateful to Dr. Daiyun Huang for his instruction in the "Biostatistics and R" course. This course has marked the beginning of my formal journey into the world of statistics. Dr. Huang's comprehensive lectures, accompanied by detailed PDFs, and his well-structured labs, complete with code and solutions, have been incredibly valuable. They have not only enhanced my intuitive grasp of key statistical concepts, such as hypothesis testing, but also significantly improved my proficiency in R programming.

# Also, thanks to the course of "Introduction to statistical learning" by Trevor Hastie and Robert Tibshirani. Examples in their online video lectures are abundant and easy to understand for beginners. Their book is also very helpful.


# Review and Introduce Probability

## Simple Random Sampling

We can get information about the population by taking a sample from it. The sample should be representative of the population.

A simple random sample of size n is a sample selected from a population in such a way that every possible sample of size n has the same chance of being selected and without replacement.

## Unbiased estimator

An estimator is a statistic used to estimate a population parameter. An estimator is unbiased if the expected value of the estimator equals the population parameter.

$E(\hat \theta) = \theta$

where $\hat \theta$ is the estimator and $\theta$ is the population parameter.

eg.

$$
\mathbb{E}[\hat{p}]=\mathbb{E}\left[\frac{X_1+X_2+\cdots+X_n}{n}\right]=\frac{1}{n}\left(\mathbb{E}\left[X_1\right]+\cdots+\mathbb{E}\left[X_n\right]\right)=p
$$

## Variance

$$
\operatorname{Var}[X]=\mathbb{E}\left[X^2\right]-(\mathbb{E}[X])^2
$$

$$
\begin{aligned}
\mathbb{E}\left[\hat{p}^2\right] & =\mathbb{E}\left[\left(\frac{X_1+X_2+\cdots+X_n}{n}\right)^2\right] \\
& =\frac{1}{n^2} \mathbb{E}\left[X_1^2+\cdots+X_n^2+2\left(X_1 X_2+X_1 X_3+\cdots+X_{n-1} X_n\right)\right] \\
& =\frac{1}{n^2}\left(n \mathbb{E}\left[X_1^2\right]+2\binom{n}{2} \mathbb{E}\left[X_1 X_2\right]\right) \\
& =\frac{1}{n} \mathbb{E}\left[X_1^2\right]+\frac{n-1}{n} \mathbb{E}\left[X_1 X_2\right]
\end{aligned}
$$

$$
\mathbb{E}\left[\hat{p}^2\right]=\frac{1}{n} \mathbb{E}\left[X_1^2\right]+\frac{n-1}{n} \mathbb{E}\left[X_1 X_2\right]
$$

Since $X_1$ is 0 or $1, X_1=X_1^2$. Then $\mathbb{E}\left[X_1^2\right]=\mathbb{E}\left[X_1\right]=p$.

Notice: $X_1$ and $X_2$ are not independent.

$$
\mathbb{E}\left[X_1 X_2\right]=\mathbb{P}\left[X_1=1, X_2=1\right]=\mathbb{P}\left[X_1=1\right] \mathbb{P}\left[X_2=1 \mid X_1=1\right]
$$

$$
\mathbb{P}\left[X_1=1\right]=p, \quad \mathbb{P}\left[X_2=1 \mid X_1=1\right]=\frac{N p-1}{N-1}
$$

$$
\begin{aligned}
\operatorname{Var}[\hat{p}] & =\mathbb{E}\left[\hat{p}^2\right]-(\mathbb{E}[\hat{p}])^2 \\
& =\frac{1}{n} p+\frac{n-1}{n} p\left(\frac{N p-1}{N-1}\right)-p^2 \\
& =\left(\frac{1}{n}-\frac{n-1}{n} \frac{1}{N-1}\right) p+\left(\frac{n-1}{n} \frac{N}{N-1}-1\right) p^2 \\
& =\frac{N-n}{n(N-1)} p+\frac{n-N}{n(N-1)} p^2 \\
& =\frac{p(1-p)}{n} \frac{N-n}{N-1}=\frac{p(1-p)}{n}\left(1-\frac{n-1}{N-1}\right)
\end{aligned}
$$ When N is much bigger than n, it is $\frac{p(1-p)}{n}$, which is like we sample n things with replacement (independently).

## Sampling distribution

The sampling distribution of a statistic is the probability distribution of that statistic based on a random sample.

### Sample mean of i.i.d. normals

Sample mean follows a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$.

## Simulation

Below codes explains the central limit theorem (CLT) and the sampling distribution of the sample proportion.

```{r}
library(ggplot2)

set.seed(111)

population_size <- 12141897

p <- 0.54

num_simulations <- 50

sample_size <- 500

rep()
p_hat_values <- replicate(num_simulations, {
  # Simulate sampling from the population
  sample <- sample(c(rep(1,population_size * p), rep(0,population_size*(1-p))),sample_size, replace =FALSE) # replicate(num_simulations, {...})：这个函数会重复执行大括号中的代码num_simulations次，并将每次的结果存储在一个向量中； rep创建一个包含population_size * p个1和population_size * (1-p)个0的向量，模拟总体。
mean(sample) #calculate the p_hat for each sample
})


  
histogram <- ggplot(data.frame(p = p_hat_values), aes(x=p))+
  geom_histogram(binwidth = 0.01, fill ="blue", color = "black")+
  labs(title =" ",
       x = "p_hat",
       y= "Frequency")+
  theme_minimal()

print(histogram)

```

## Moment generating functions

Moment generating function (MGF) and characteristic function are powerful functions that describe the underlying features of a random variable.

### Definition

$M_x(t) = \mathbb E (e^{tx})$

### Theorems about MGF

1.  If $X$ and $Y$ are random variables with the same MGF, which is finite on $\left[-t_0, t_0\right]$ for some $t_0>0$, then $X$ and $Y$ have the same distribution.

**MGFs can be used as a tool to determine if two random variables have the identical CDF.**

2.  Let $X_1, \cdots, X_n$ be independent random variables, with MGFs $M_{X_1}, \cdots, M_{X_n}$. Then the MGF of their sum is given by $$
    M_{X_1+\cdots+X_n}(t)=M_{X_1}(t) \cdots M_{X_n}(t)
    $$

#### Example of Gamma and Exponential MGF

A gamma distribution with shape $r=1$ is an exponential distribution. If $X \sim \operatorname{Gamma}\left(r_X, \lambda\right)$ and $Y \sim \operatorname{Gamma}\left(r_Y, \lambda\right)$ are independent, then we have $X+Y \sim \operatorname{Gamma}\left(r_X+r_Y, \lambda\right)$. As a special case, if $X_1, X_2, \cdots, X_n$ are i.i.d. with $\operatorname{Exp}(\lambda)$ distribution, then $X_1+X_2+\cdots+X_n$ has $\operatorname{Gamma}(n, \lambda)$ distribution.

Suppose $X \sim \operatorname{Exp}(\lambda)$, for $\lambda>0$. Then $$
M_X(t)=\mathbb{E}\left[e^{t X}\right]=\int_0^{\infty} e^{t x} \lambda e^{-\lambda x} d x=\lambda \int_0^{\infty} e^{(t-\lambda) x} d x
$$

Similar to Gamma MGF, the integral of Exponential MGF converges only for $t<\lambda$. For $t<\lambda$, we can integrate: $$
M_X(t)=\lambda\left[\frac{e^{(t-\lambda) x}}{t-\lambda}\right]_0^{\infty}=\frac{\lambda}{\lambda-t}
$$

Since the Gamma MGF is $M_X(t)=\frac{\lambda^r}{(\lambda-t)^r}$ for any $t<\lambda$ For shape $r=1$, Exponential MGF = Gamma MGF.

## Distribution Transformation

### Universality of the Uniform---From uniform you can get everything

Let u\~unif(0,1), F be a CDF(assume F is strictly increased, continuous). Then there comes a theorem: $$
x = F^{-1}(u)
$$ Then $$
X \sim F
$$ (Proof: $$
P(X\leq x)=P(F^{-1}(u)\leq x)=P(F(F^{-1}(u))\leq F(x))=P(u\leq F(x))=F(x)
$$)

You can convert from the random uniforms to whatever you want to simulate. One example is the simulation of Logistic distribution: F(X)=$e^x$/($1+e^x$)

u\~unif(0,1), $$
X = F^{-1}(u)=log(u/1-u)
$$ and X\~F

Another example is that we could try to use uniform to simulate normal distribution:

The Box-Muller transform generates pairs of independent standard normally distributed (zero mean, unit variance) random numbers, given a source of uniformly distributed random numbers.

Given two independent random variables $U_1$ and $U_2$ that are uniformly distributed on the interval (0, 1), we can generate two independent standard normal random variables $Z_0$ and $Z_1$ using the following formulas:

$$
Z_0 = \sqrt{-2 \ln U_1} \cos(2 \pi U_2)
$$

$$
Z_1 = \sqrt{-2 \ln U_1} \sin(2 \pi U_2)
$$

Inversely, the example could be: let $Z_0$ and $Z_1$ be standard normal random variables with the following values: $Z_0$ = 0.5 and $Z_1$ = -1.0.

1.  **Compute CDF values**:

    For standard normal distribution, the CDF $$ \Phi(z) $$ is given by:

    $$
    \Phi(z) = \frac{1}{2} \left[ 1 + \text{erf} \left( \frac{z}{\sqrt{2}} \right) \right]
    $$

    (ps:$$
    \text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} \, dt
    $$)

    -   For $$ Z_0 = 0.5 $$:

        $$
        \Phi(0.5) = \frac{1}{2} \left[ 1 + \text{erf} \left( \frac{0.5}{\sqrt{2}} \right) \right]
        $$

    -   For $$ Z_1 = -1.0 $$:

        $$
        \Phi(-1.0) = \frac{1}{2} \left[ 1 + \text{erf} \left( \frac{-1.0}{\sqrt{2}} \right) \right]
        $$

2.  **Compute Uniform values**:

    Since the CDF values $$ \Phi(Z_0) $$ and $$ \Phi(Z_1) $$ are in the range \[0, 1\], we can directly use them as uniform random variables $U_0$ and $U_1$.

    -   $$U_0 = \Phi(0.5)$$
    -   $$U_1 = \Phi(-1.0)$$

3.  **Result**:

    -   Using the error function values:
        -   $$ \Phi(0.5) \approx 0.6915 $$
        -   $$ \Phi(-1.0) \approx 0.1587 $$

    Thus, the corresponding uniform distribution values are:

    -   $$ U_0 \approx 0.6915 $$
    -   $$ U_1 \approx 0.1587 $$

Another example is that we can create a function that has good quality as F in the theorem, for example, $$
u=F(x)=1-e^{-x}          (x>0)
$$ then we can simulate X\~F:X=-ln(1-u)\~F

Also, if $$
X \sim F
$$

Then, $$
F(x) \sim unif(0,1)
$$ e.g. let X\~F, if$$
F(x_0)=1/3$$ then $$P(F(X)\leq 1/3)=P(X\leq x_0)=F(x_0)=1/3 $$ which follows the uniform distribution(0,1) because 1/3 generates 1/3. (Uniform distribution: Probability(CDF) is proportional to length)

### Normal Distribution

#### From Standard Normal Distribution to Normal Distribution

$f(z)=ce^{-z^2/2}$ is a function with good qualities such as symmetric........

To generate normalization constant c using CDF=1, instead of using impossible integral methods to compute it we should try to find the area under it. In that case, we transfer the integral shape to the double integral's multiplication then we get c: $$
\int_{-\infty}^{\infty}\exp(-z^2/2) dz=\int_{-\infty}^{\infty}\exp(-z^2/2) dz=\int_{-\infty}^{\infty}\exp(-x^2/2)dx\int_{-\infty}^{\infty}\exp(-y^2/2)dy
$$ $$
=\int_{0}^{\infty}\exp(-r^2/2)r dr\int_{0}^{2π} d\theta\ =\sqrt2\pi\
$$ So $$
c=1/\sqrt2\pi\
$$ .......

### From.....

111

#### Exponential distribution

the only one parameter is the rate parameter. The probability density function (pdf) of an exponential distribution with rate parameter (\lambda \> 0) is given by:

$$
f_X(x) = 
\begin{cases} 
\lambda e^{-\lambda x} & \text{for } x \geq 0, \\
0 & \text{for } x < 0.
\end{cases}
$$ The cumulative distribution function (cdf) of an exponential distribution with rate parameter (\lambda \> 0) is given by:

$$
F_X(x) = 
\begin{cases} 
1 - e^{-\lambda x} & \text{for } x \geq 0, \\
0 & \text{for } x < 0.
\end{cases}
$$ 2.Let Y\~\lambda x, then Y \~Expo(1)

proof: since $$
P(Y\leq y)=P(X\leq y/\lambda)=1-e^{-y}
$$ (just plot it into x) and we could check that E\[X\]=Var\[X\]=1, so X=Y/$\lambda$ has E\[x\]=1/$\lambda$, Var\[x\]=1/$\lambda^2$

e.g. Memoryless Property(This property implies that the remaining lifetime distribution does not depend on how much time has already elapsed.)(The exponential distribution is the only continuous distribution that has the memoryless property):$$ P(X\geq s+t|X\geq s)=P(X\geq t)$$ which is actually satisfied by the exponential and we could prove it(though it intuitively makes sence): Here $$
P(X\geq s)=1-P(X\leq s)=e^{-\lambda s}
$$ $$
P(X \geq s+t \mid X \geq s)=\frac{P(X \geq s+t \text { and } X \geq s)}{P(X \geq s)}
$$

Since $X \geq s+t$ implies $X \geq s$, we can simplify the numerator: $$
P(X \geq s+t \mid X \geq s)=\frac{P(X \geq s+t)}{P(X \geq s)}
$$

Now, substitute the survival function for the exponential distribution: $$
P(X \geq s+t \mid X \geq s)=\frac{e^{-\lambda(s+t)}}{e^{-\lambda s}}
$$

Simplify the expression: $$
P(X \geq s+t \mid X \geq s)=\frac{e^{-\lambda s} \cdot e^{-\lambda t}}{e^{-\lambda s}}=e^{-\lambda t}
$$

Notice that $e^{-\lambda t}=P(X \geq t)$ : $$
P(X \geq s+t \mid X \geq s)=P(X \geq t)
$$ usefulness of it: $$
X\sim Expo(\lambda),E(X|X>a)=a+E(X-a|X>a)=a+1/\lambda
$$

e.g.2The hazard rate (or failure rate) for an exponential distribution is constant over time. For an exponential random variable $X$ with rate parameter $\lambda$, the hazard rate is: $$
h(t)=\frac{f_X(t)}{1-F_X(t)}=\lambda .
$$

A constant hazard rate implies that the event is equally likely to occur at any point in time, which is a reasonable assumption for many processes, such as the lifetime of certain electronic components or the occurrence of certain types of random failures.

e.g.3 The exponential distribution is closely related to the Poisson process, which is a process that models the occurrence of events happening independently at a constant average rate. If the times between consecutive events in a Poisson process are independent and identically distributed, then these interarrival times follow an exponential distribution. This relationship makes the exponential distribution a natural choice in contexts where events occur randomly over time, such as phone calls arriving at a switchboard or buses arriving at a bus stop.

#### Ga

If r.v.X\~N(0,1), then $X^2$\~$Ga$(1/2,1/2)

Proof: If r.v. (X \sim N(0,1)), then (X\^2 \sim \text{Ga}\left(\frac{1}{2}, \frac{1}{2}\right))

Let (X) be a random variable such that (X \sim N(0,1)).

The probability density function (pdf) of (X) is: $$
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}, \quad -\infty < x < \infty.
$$

First, we find the pdf of (Y = X\^2).

The cumulative distribution function (CDF) of (Y) is given by: $$
F_Y(y) = P(Y \leq y) = P(X^2 \leq y).
$$

Since (X\^2 \geq 0), we only consider (y \geq 0): $$
F_Y(y) = P(-\sqrt{y} \leq X \leq \sqrt{y}).
$$

Using the CDF of the normal distribution, we have: $$
F_Y(y) = \int_{-\sqrt{y}}^{\sqrt{y}} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx.
$$

The pdf of (Y) is the derivative of the CDF: $$
f_Y(y) = \frac{d}{dy} F_Y(y).
$$

$$
F_Y(y) = 2 \int_{0}^{\sqrt{y}} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx.
$$

$$
F_Y(y) = 2 \int_{0}^{\sqrt{y}} \frac{1}{\sqrt{2\pi}} e^{-u} \frac{du}{\sqrt{2u}} = \frac{2}{\sqrt{2\pi}} \int_{0}^{\sqrt{y}} e^{-u} \frac{du}{\sqrt{2u}}.
$$

Thus, $$
f_Y(y) = \frac{d}{dy} \left( \frac{2}{\sqrt{2\pi}} \int_{0}^{\sqrt{y}} e^{-u} \frac{du}{\sqrt{2u}} \right).
$$

Differentiating with respect to y gives: $$
f_Y(y) = \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{y}{2}} \cdot y^{-\frac{1}{2}}.
$$

Simplifying, we get: $$
f_Y(y) = \frac{1}{\sqrt{2\pi}} y^{-\frac{1}{2}} e^{-\frac{y}{2}}.
$$

$$
Y = X^2 \sim \text{Ga}\left(\frac{1}{2}, \frac{1}{2}\right).
$$

#### Chi-square

Chi-square is a

1.Let ( $Z_1$, $Z_2$, \ldots, $Z_i$ ) are independent standard normal random variables(i.e. Z\~N(0,1)), then the random variable ( X ) defined by

$$
X = Z_1^2 + Z_2^2 + \cdots + Z_i^2
$$ ($Z_j$\~iid.N(0,1))

follows a chi-square distribution with ( i ) degrees of freedom. So chi-square of 1 is the same thing as Gamma of (1/2,1/2) so chi-square of n is Gamma(n/2,1/2)

2.If $$
Z_i = \frac{x_i - \mu}{\sigma}
$$ The sum of squared standardized deviations is:

$$
\sum_{i=1}^n Z_i^2 = \sum_{i=1}^n \left( \frac{x_i - \mu}{\sigma} \right)^2 = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
$$

Let $x_1, x_2, \cdots, x_n$ be samples of $N\left(\mu, \sigma^2\right)$ , $\mu$ is a known constant, find the distribution of statistics: $$
T=\sum_{i=1}^n\left(x_i-\mu\right)^2
$$

sol: $y_i=\left(x_i-\mu\right) / \sigma, i=1,2, \cdots, n$, 则 $y_1, y_2, \cdots, y_n$ are iid r.v. of $N(0,1)$,so$$
\frac{T}{\sigma^2}=\sum_{i=1}^n\left(\frac{x_i-\mu}{\sigma}\right)^2=\sum_{i=1}^n y_i^2 \sim \chi^2(n),
$$ (i.e.$$
\sum_{i=1}^n Z_i^2 {\sigma^2}\sim \chi^2(n)$$)

Besides, $T$'s PDF is $$
p(t)=\frac{1}{\left(2 \sigma^2\right)^{n / 2} \Gamma(n / 2)} \mathrm{e}^{-\frac{1}{2 \sigma^2} t^{\frac{n}{2}}-1}, \quad t>0,
$$

which is Gamma distribution $G a\left(\frac{n}{2}, \frac{1}{2 \sigma^2}\right) \cdot$

3.chi-square is uesful because of theorems below:let $x_1, x_2, \cdots, x_n$ are samples from $N\left(\mu, \sigma^2\right)$ , whose sample mean and sample variance are$$
\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i \text { and } s^2=\frac{1}{n-1} \sum_{i=1}^n\left(x_i-\bar{x}\right)^2,
$$

then we can get: (1) $\bar{x}$ and $s^2$ are independent; (2) $\bar{x} \sim N\left(\mu, \sigma^2 / n\right)$; (3) $\frac{(n-1) s^2}{\sigma^2} \sim \chi^2(n-1)$.

Proof: $$
p\left(x_1, x_2, \cdots, x_n\right)=\left(2 \pi \sigma^2\right)^{-n / 2} \mathrm{e}^{-\sum_{i=1}^n \frac{\left(x_i-\mu\right)^2}{2 \sigma^2}}=\left(2 \pi \sigma^2\right)^{-n / 2} \exp \left\{-\frac{\sum_{i=1}^n x_i^2-2 n \bar{x} \mu+n \mu^2}{2 \sigma^2}\right\}
$$

denote$\boldsymbol{X}=\left(x_1, x_2, \cdots, x_n\right)^{\mathrm{T}}$, then we create an $n$-dimension orthogonal $\boldsymbol{A}$ and every element in the first row is $1 / \sqrt{n}$, such as $$
A=\left(\begin{array}{ccccc}
\frac{1}{\sqrt{n}} & \frac{1}{\sqrt{n}} & \frac{1}{\sqrt{n}} & \cdots & \frac{1}{\sqrt{n}} \\
\frac{1}{\sqrt{2 \cdot 1}} & -\frac{1}{\sqrt{2 \cdot 1}} & 0 & \cdots & 0 \\
\frac{1}{\sqrt{3 \cdot 2}} & \frac{1}{\sqrt{3 \cdot 2}} & -\frac{2}{\sqrt{3 \cdot 2}} & \cdots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
\frac{1}{\sqrt{n(n-1)}} & \frac{1}{\sqrt{n(n-1)}} & \frac{1}{\sqrt{n(n-1)}} & \cdots & -\frac{n-1}{\sqrt{n(n-1)}}
\end{array}\right),
$$ 令 $\boldsymbol{Y}=\left(y_1, y_2, \cdots, y_n\right)^{\mathrm{T}}=\boldsymbol{A} \boldsymbol{X}$, $|Jacobi|=1$, and we can find thatt$$
\begin{gathered}
\bar{x}=\frac{1}{\sqrt{n}} y_1, \\
\sum_{i=1}^n y_i^2=\boldsymbol{Y}^{\mathrm{T}} \boldsymbol{Y}=\boldsymbol{X}^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{X}=\sum_{i=1}^n x_i^2,
\end{gathered}
$$

so$y_1, y_2, \cdots, y_n$ 's joint density function is $$
\begin{aligned}
p\left(y_1, y_2, \cdots, y_n\right) & =\left(2 \pi \sigma^2\right)^{-n / 2} \exp \left\{-\frac{\sum_{i=1}^n y_i^2-2 \sqrt{n} y_1 \mu+n \mu^2}{2 \sigma^2}\right\} \\
& =\left(2 \pi \sigma^2\right)^{-n / 2} \exp \left\{-\frac{\sum_{i=2}^n y_i^2+\left(y_1-\sqrt{n} \mu\right)^2}{2 \sigma^2}\right\}
\end{aligned}
$$

So, $\boldsymbol{Y}=\left(y_1, y_2, \cdots, y_n\right)^{\mathrm{T}}$ independently distributed as normal distribution and their variances are all equal to$\sigma^2$, but their means are not all the same because $y_2, \cdots, y_n$ 's means are $0, y_1$'s is $\sqrt{n} \mu$, which ends our proof of (2). $$
(n-1) s^2=\sum_{i=1}^n\left(x_i-\bar{x}\right)^2=\sum_{i=1}^n x_i^2-(\sqrt{n} \bar{x})^2=\sum_{i=1}^n y_i^2-y_1^2=\sum_{i=2}^n y_i^2,
$$

This proves conclusion (1). Since $y_2, \cdots, y_n$ are independently and identically distributed as $N\left(0, \sigma^2\right)$, we have: $$
\frac{(n-1) s^2}{\sigma^2}=\sum_{i=2}^n\left(\frac{y_i}{\sigma}\right)^2 \sim \chi^2(n-1) .
$$

Theorem is proved. (similar to the proof above this maybe easier to understand:$\begin{aligned} & i z\left(Y_1, Y_2, \cdots, Y_2\right)^{\top}=A\left(X_1, \cdots, x_n\right)^{\top} \\ & \text { then } \sum_{i=1}^n Y_i^2=\left(Y_1, \cdots, Y_n\right)\left(Y_1, \cdots, Y_n\right)^{\top} \\ & =\left[A\left(x_1, \cdots, x_n\right)^{\top}\right]^{\top}\left[A\left(x_1, \cdots, X_n\right)^{\top}\right] \\ & =\left(x_1, \cdots, x_n\right) A^{\top} A\left(x_1, \cdots, x_n\right)^{\top} \\ & =\left(x_1, \cdots, x_n\right) E\left(x_1, \cdots, x_n\right)^{\top}=\sum_{i=1}^n x_i^2 \\ & \end{aligned}$) $\begin{aligned} & \text { besides } Y_1=\frac{1}{\sqrt{n}} x_1+\cdots+\frac{1}{\sqrt{n}} x_n=\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \\ & \text { and } Y_1=\sqrt{n} \cdot \frac{1}{n} \sum_{i=1}^n X_i=\sqrt{n} \bar{X}, \text { then } \bar{x}=\frac{1}{\sqrt{n}} y_i \\ & B S^2=\frac{1}{n-1} \sum_{i=1}^n\left(x_i-\bar{x}\right)^2=\frac{1}{n-1}\left[\sum_{i=1}^n x_i^2-n \bar{x}^2\right] \\ & =\frac{1}{n-1}\left[\sum_{i=1}^n Y_i^2-Y_1^2\right]=\frac{1}{n-1} \sum_{i=2}^n Y_i^2 \\ & 2 \oplus L=(\sqrt{2 \pi} \sigma)^{-n} \exp \left[-\frac{1}{2 \sigma^2} \sum_{i=1}^n\left(x_i-\mu\right)^2\right] \text {. } \\ & =(\sqrt{2 \pi} \sigma)^{-n} \exp \left[-\frac{1}{2 \sigma^2}\left(\sum_{i=1}^n x_i^2-2 \mu n \bar{x}+\mu^2 n\right]\right. \\ & =(\sqrt{2 \pi} \sigma)^{-n} \exp \left[-\frac{1}{2 \sigma^2}\left(\sum_{1=1}^n y_i{ }^2-2 \mu n \frac{1}{\sqrt{n}} Y_1+n \mu^2\right]\right. \\ & \end{aligned}$ $$=(\sqrt2\pi\sigma)^{-1}exp[-1/2\sigma^2(Y_1-\sqrt nu)^2]×(\sqrt2\pi\sigma)^{-1}exp[-1/2\sigma^2{Y_2}^2]*...*(\sqrt2\pi\sigma)^{-1}exp[-1/2\sigma^2{Y_n}^2]
$$

So L is $Y_1$...$Y_n$'s joint density function and so they are independent. Besides, we have proved that its mean is $1/\sqrt n$$Y_1$ and $S^2$=$1/n-1 \Sigma{i=2}Y_i^2$, so the normal distribution's mean and variance are independent.

When the random variable $\chi^2 \sim \chi^2(n)$, for a given $\alpha$ (where $0<$ $\alpha<1$ ), the value $\chi_{1-\alpha}^2(n)$ satisfying the probability equation $P\left(\chi^2 \leqslant \chi_{1-\alpha}^2(n)\right)=1-$ $\alpha$ is called the $1-\alpha$ quantile of the $\chi^2$ distribution with $n$ degrees of freedom.

Suppose the random variables $X_1 \sim \chi^2(m)$ and $X_2 \sim \chi^2(n)$, and $X_1$ and $X_2$ are independent. Then the distribution of $F=\frac{X_1 / m}{X_2 / n}$ is called the $\mathrm{F}$ distribution with $m$ and $n$ degrees of freedom, denoted as $F \sim F(m, n)$. Here, $m$ is called the numerator degrees of freedom and $n$ the denominator degrees of freedom. We derive the density function of the $\mathrm{F}$ distribution in two steps. First, we derive the density function of $Z=\frac{X_1}{X_2}$. Let $p_1(x)$ and $p_2(x)$ be the density functions of $\chi^2(m)$ and $\chi^2(n)$ respectively. According to the formula for the distribution of the quotient of independent random variables, the density function of $Z$ is: $$
\begin{gathered}
p_Z(z)=\int_0^{\infty} x_2 p_1\left(z x_2\right) p_2\left(x_2\right) \mathrm{d} x_2 \\
=\frac{z^{\frac{m}{2}-1}}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right) 2^{\frac{m+n}{2}}} \int_0^{\infty} x_2^{\frac{n}{2}-1} e^{-\frac{x_2}{2}(1+z)} \mathrm{d} x_2 .
\end{gathered}
$$

Using the transformation $u=\frac{x_2}{2}(1+z)$, we get: $$
p_Z(z)=\frac{z^{\frac{m}{2}-1}(1+z)^{-\frac{m+n}{2}}}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right)} \int_0^{\infty} u^{\frac{n}{2}-1} e^{-u} \mathrm{~d} u
$$

The final integral is the gamma function $\Gamma\left(\frac{n}{2}\right)$, so: $$
p_Z(z)=\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right)} z^{\frac{m}{2}-1}(1+z)^{-\frac{m+n}{2}}, \quad z \geq 0 .
$$

Second, we derive the density function of $F=\frac{n}{m} Z$. Let the value of $F$ be $y$. For $y \geq 0$, we have: $$
\begin{aligned}
p_F(y) & =p_Z\left(\frac{m}{n} y\right) \cdot \frac{m}{n}=\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right)}\left(\frac{m}{n} y\right)^{\frac{m}{2}-1}\left(1+\frac{m}{n} y\right)^{-\frac{m+n}{2}} \cdot \frac{m}{n} \\
& =\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right)}\left(\frac{m}{n}\right)\left(\frac{m}{n} y\right)^{\downarrow \frac{2}{2}-1}\left(1+\frac{m}{n} y\right)^{-\frac{m+n}{2}}
\end{aligned}
$$

When the random variable $F \sim F(m, n)$, for a given $\alpha$ (where $0<\alpha<1$ ), the value $F_{1-\alpha}(m, n)$ satisfying the probability equation $P\left(F \leqslant F_{1-\alpha}(m, n)\right)=1-\alpha$ is called the $1-\alpha$ quantile of the $\mathrm{F}$ distribution with $m$ and $n$ degrees of freedom. By the construction of the $\mathrm{F}$ distribution, if $F \sim F(m, n)$, then $1 / F \sim F(n, m)$. Therefore, for a given $\alpha$ (where $0<\alpha<1$ ), $$
\alpha=P\left(\frac{1}{F} \leqslant F_\alpha(n, m)\right)=P\left(F \geqslant \frac{1}{F_\alpha(n, m)}\right) .
$$

Thus, $$
P\left(F \leqslant \frac{1}{F_\alpha(n, m)}\right)=1-\alpha
$$

This implies $$
F_\alpha(n, m)=\frac{1}{F_{1-\alpha}(m, n)} .
$$

Corollary Suppose $x_1, x_2, \cdots, x_m$ is a sample from $N\left(\mu_1, \sigma_1^2\right)$ and $y_1, y_2, \cdots, y_n$ is a sample from $N\left(\mu_2, \sigma_2^2\right)$, and these two samples are independent. Let: $$
s_x^2=\frac{1}{m-1} \sum_{i=1}^m\left(x_i-\bar{x}\right)^2, \quad s_y^2=\frac{1}{n-1} \sum_{i=1}^n\left(y_i-\bar{y}\right)^2,
$$ where $$
\bar{x}=\frac{1}{m} \sum_{i=1}^m x_i, \quad \bar{y}=\frac{1}{n} \sum_{i=1}^n y_i
$$ then $$
F=\frac{s_x^2 / \sigma_1^2}{s_y^2 / \sigma_2^2} \sim F(m-1, n-1) .
$$

In particular, if $\sigma_1^2=\sigma_2^2$, then $F=\frac{s_x}{s_y^2} \sim F(m-1, n-1)$. Proof: Since the two samples are independent, $s_x^2$ and $s_y^2$ are independent. According to a Theorem , we have $$
\frac{(m-1) s_x^2}{\sigma_1^2} \sim \chi^2(m-1), \quad \frac{(n-1) s_y^2}{\sigma_2^2} \sim \chi^2(n-1) .
$$

By the definition of the $\mathrm{F}$ distribution, $F \sim F(m-1, n-1)$. Corollary: Suppose $x_1, x_2, \cdots, x_n$ is a sample from a normal distribution $N\left(\mu, \sigma^2\right)$, and let $\bar{x}$ and $s^2$ denote the sample mean and sample variance of the sample, respectively. Then $$
t=\frac{\sqrt{n}(\bar{x}-\mu)}{s} \sim t(n-1) .
$$

Proof: From a Theorem we obtain $$
\frac{\bar{x}-\mu}{\sigma / \sqrt{n}} \sim N(0,1)
$$

Then, $$
\frac{\sqrt{n}(\bar{x}-\mu)}{s}=\frac{\frac{\bar{x}-\mu}{\sigma / \sqrt{n}}}{\sqrt{\frac{(n-1) s^2 / \sigma^2}{n-1}}}
$$

Since the numerator is a standard normal variable and the denominator's square root contains a $\chi^2$ variable with $n-1$ degrees of freedom divided by its degrees of freedom, and they are independent, by the definition of the $t$ distribution, $t \sim t(n-1)$. The proof is complete.

Corollary: In the notation of Corollary , assume $\sigma_1^2=\sigma_2^2=\sigma^2$, and let $$
s_w^2=\frac{(m-1) s_x^2+(n-1) s_y^2}{m+n-2}=\frac{\sum_{i=1}^m\left(x_i-\bar{x}\right)^2+\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}{m+n-2}
$$

Then $$
\frac{(\bar{x}-\bar{y})-\left(\mu_1-\mu_2\right)}{s_w \sqrt{\frac{1}{m}+\frac{1}{n}}} \sim t(m+n-2)
$$

Proof: Since $\bar{x} \sim N\left(\mu_1, \frac{\sigma^2}{m}\right), \bar{y} \sim N\left(\mu_2, \frac{\sigma^2}{n}\right)$, and $\bar{x}$ and $\bar{y}$ are independent, we have

$$
\bar{x}-\bar{y} \sim N\left(\mu_1-\mu_2,\left(\frac{1}{m}+\frac{1}{n}\right) \sigma^2\right) .
$$

Thus, $$
\frac{(\bar{x}-\bar{y})-\left(\mu_1-\mu_2\right)}{\sigma \sqrt{\frac{1}{m}+\frac{1}{n}}} \sim N(0,1) .
$$

By a Theorem , we know that $\frac{(m-1) s_x^2}{\sigma^2} \sim \chi^2(m-1)$ and $\frac{(n-1) s_y^2}{\sigma^2} \sim \chi^2(n-1)$, and they are independent. By additivity, we have $$
\frac{(m+n-2) s_w^2}{\sigma^2}=\frac{(m-1) s_x^2+(n-1) s_y^2}{\sigma^2} \sim \chi^2(m+n-2) .
$$

Since $\bar{x}-\bar{y}$ and $s_w^2$ are independent, by the definition of the $\mathrm{t}$ distribution, we get the desired result. $\square$

One interesting example shows the relationship of above distributions used charismatically to solve problems: r.v.: $X_1 ， X_2 ， X_3 ， X_4$ indpendently identically distribute(iid) as $N\left(0 . \sigma^2\right)$. $Z=\left(x_1^2+x_2^2\right) /\left(x_1^2+x_2^2+x_3^2+x_4^2\right)$ prove: $Z \sim U(0.1)$. $$
\begin{aligned}
& \text { Solution: } Let Y=\frac{X_3^2+X_4^2}{X_1^2+X_2^2}=\frac{\left[\left(\frac{X_3}{\sigma}\right)^2+\left(\frac{X_4}{\sigma}\right)^2\right] / 2}{\left[\left(\frac{X_1}{\sigma}\right)^2+\left(\frac{X_2}{\sigma}\right)^2\right] / 2} \sim F(2,2) . \\
& \text { i.e.  } f_Y(y)=\frac{1}{(1+y)^2},  y>0 \\
& \text { then } P(Z \leq z)=P\left(\frac{1}{1+Y} \leq z\right)=P\left(Y \geqslant \frac{1}{z}-1\right) \\
& =\int_{\frac{1}{z}-1}^{+\infty} \frac{1}{(1+y)^2} d y=z \quad \text { H } 0<z<1 . \\
& \therefore Z \sim U(0.1)
\end{aligned}
$$ (ps:\$

```{=tex}
\begin{aligned} & f(x)=\frac{\Gamma\left(\frac{n_1+n_2}{2}\right)}{\Gamma\left(\frac{n_2}{2}\right) \Gamma\left(\frac{n_1}{2}\right)}\left(\frac{n_1}{n_2}\right)\left(\frac{n_1}{n_2} x\right)^{\frac{n_1}{2}-1}\left(1+\frac{n_1}{n_2} x\right)^{\frac{-1}{2}\left(n_1+n_2\right)} . \\ & \text { of these } x>0 \text {. and } E(x)=\frac{n_2}{n_2-2} \text {, when } n_2>2 \text {. } \\ & \operatorname{Var}(X)=\frac{2 n_2^2\left(n_1+n_2-2\right)}{n_1\left(n_2-2\right)^2\left(n_2-4\right)} \text {when $n_2>4$. } \\ & \end{aligned}
```
\

## PDF functions's transformation(from books)

### Standard normal distribution

Pdf : $f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$\

### Chi-square distribution

### From standard normal distribution $f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$, we want to get Chi-square distribution $f(x; k) = \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2 - 1} e^{-x/2}$

Assuming that $Z \sim N(0, 1)$\
$W=Z^2$\
So we can deduce the pdf for the variable W $$
\begin{align}
f_W(w)&=Pr(Z^2=w)\\
& =f_Z(\sqrt{w}) \left| \frac{d(\sqrt{w})}{dw} \right| + f_Z(-\sqrt{w}) \left| \frac{d(-\sqrt{w})}{dw} \right|\\
&=2 \cdot \frac{1}{\sqrt{2\pi}} e^{-w / 2} \cdot \frac{1}{2\sqrt{w}} = \frac{1}{\sqrt{2\pi w}} e^{-w / 2}\\
\end{align}
$$

$\Gamma(z) = \int_0^\infty t^{z-1} e^{-t} \, dt$\
$\Gamma(1/2)$=$\sqrt(\pi)$,we can know that $W\sim \chi^2(1)$\

Now, we can introduce the variable Y.\
$Y=\sum_{i=1}^k Z_i^2$ ($Z_i$ is independent of each other)\
We want to get the mgf of Y $$
\begin{align}
M_Y(t)&=E[e^{tY}]\\
& =E[e^{tZ_1^2}e^{tZ_2^2}...e^{tZ_k^2}]\\
& =E[e^{tZ_1^2}]E[e^{tZ_2^2}]...E[e^{tZ_k^2}] \\
& = \prod_{i=1}^k M_{Z_i^2}(t)\\
&=(1-2t)^{-r1/2}(1-2t)^{-r2/2}...(1-2t)^{-rk/2}\\
& =(1-2t)^{-\sum_{i=1}^kri/2}\\
\end{align}
$$ Because the mgf of Chi-square function is $(1-2t)^{-r/2}$\
r is the degree of freedom of this chi-square function\
So $Y\sim \chi^2(r1+r2+...+rk)$\
r1,r2...rk represents the degree of freedom of every single sample\
Here, df=1 for every sample

### Student- t distribution

### From standard normal distribution $f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$, we want to get Student- t distribution $f(t) = \frac{\Gamma \left(\frac{\nu+1}{2}\right)}{\sqrt{\nu \pi} \, \Gamma \left(\frac{\nu}{2}\right)} \left(1 + \frac{t^2}{\nu}\right)^{-\frac{\nu+1}{2}}$, where $\Gamma$ is the gamma function and $\nu$ is the degrees of freedom.

Given two independent variables Z and V ($Z \sim N(0, 1)$ & $V\sim \chi^2(\nu)$), then we construct a new variable $T=\frac{Z}{\sqrt(V/\nu)}$.\
The joint pdf is $$
g(z,v)=\frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}\frac{1}{2^{\nu/2} \Gamma(\nu/2)} v^{\nu/2 - 1} e^{-v/2}
$$ Cdf of T is given by $$
\begin{align}
F(t) & =Pr(\frac{Z}{\sqrt(V/\nu)}\leq t)\\
& =Pr(Z\leq {\sqrt(V/\nu)}t)\\
& =\int_{0}^{\infty} \int_{-\infty}^{\sqrt(V/\nu)t} g(z,v) \, dz \, dv
\end{align}
$$ Simplify F(t) $$
F(t)=\frac{1}{\sqrt{\pi}\Gamma(\nu/2)}\int_{0}^{\infty}[\int_{-\infty}^{\sqrt(V/\nu)t} \frac{e^{-z^2/2}}{2^\frac{\nu+1}{2}}dz]v^{\frac{\nu}{2}-1}e^{-\frac{v}{2}}dv
$$

To get pdf, we will differentiate F(t) $$
\begin{align}
f(t) &=F'(t)=\frac{1}{\sqrt{\pi}\Gamma(\nu/2)}\int_{0}^{\infty} \frac{e^{-(v/2)(t^2/\nu)}}{2^{\frac{\nu+1}{2}}}\sqrt\frac{v}{\nu}v^{\nu/2-1}e^{-\frac{v}{2}}dv\\
&=\frac{1}{\sqrt{\pi\nu}\Gamma(\nu/2)}\int_{0}^{\infty}\frac{v^{(\nu+1)/2-1}}{2^{(\nu+1)/2}}e^{-(\nu/2)(1+t^2/\nu)}dv
\end{align}
$$ We need to make the change of variables: $y=(1+t^2/\nu)v$\
And we need to change dv: $\frac{dv}{dy}=\frac{1}{1+t^2/\nu}$

$$
\begin{align}
f(t)&=\frac{\Gamma[(\nu+1)/2]}{\sqrt{\pi\nu}\Gamma(\nu/2)}[\frac{1}{(1+t^2/\nu)^{(\nu+1)/2}}]\int_{0}^{\infty}\frac{y^{(\nu+1)/2-1}}{\Gamma[(\nu+1)/2]2^{(\nu+1)/2}}e^{-y/2}dy
\end{align}
$$ This part $\int_{0}^{\infty}\frac{y^{(\nu+1)/2-1}}{\Gamma[(\nu+1)/2]2^{(\nu+1)/2}}e^{-y/2}dy$ is equal to 1, because this part is the whole area under the chi-square distribution with $\nu+1$ degrees of freedom. So, the pdf for T can be written as follows $$
f(t)=\frac{\Gamma[(\nu+1)/2]}{\sqrt{\pi\nu}\Gamma(\nu/2)}\frac{1}{(1+t^2/\nu)^{(\nu+1)/2}}
$$

### F-distribution

### We will do some trnsformation on chi-square distribution to get F-distribution

Assuming that we have two independent random variables $$
X \sim \chi^2(n_1) \quad and\quad  Y \sim \chi^2(n_2)\
$$ Now,we will define a new variable F $$
F = \frac{(X / n_1)}{(Y / n_2)}
$$ This looks a little complex, so let's do some simplification. $$
U = \frac{X}{n_1} \quad \text{and} \quad V = \frac{Y}{n_2}\
$$

So, $F = \frac{U}{V}$.\
Because, X and Y are independent of each other. Obviously, U and V are also independent of each other.\

$$
f_{U,V}(u,v) = f_U(u) f_V(v)
$$ $$
f_U(u) = \frac{(n_1 u)^{n_1/2 - 1} e^{-n_1 u/2}}{2^{n_1/2} \Gamma(n_1/2)}
$$ $$
f_V(v) = \frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \Gamma(n_2/2)}
$$ To find the joint density function of F & V, we use Jacobian transformation $$
J =\left| \frac{\partial(U,V)}{\partial(F,V)} \right| = \left| \begin{matrix}
\frac{\partial U}{\partial F} & \frac{\partial U}{\partial V} \\
\frac{\partial V}{\partial F} & \frac{\partial V}{\partial V}
\end{matrix} \right| = \left| \begin{matrix}
V & F \\
0 & 1
\end{matrix} \right| = V
$$ $$
f_{F,V}(f,v) = f_{U,V}(u,v) \left| \frac{\partial(u,v)}{\partial(f,v)} \right|= f_{U,V}(u,v)v
$$ Then, we substitute$f_U(u)$ and $f_V(v)$ into $f_{F,V}(f,v)$ $$
f_{F,V}(f,v) = \frac{(n_1 u)^{n_1/2 - 1} e^{-n_1 u/2}}{2^{n_1/2} \Gamma(n_1/2)} \cdot \frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \Gamma(n_2/2)} \cdot v
$$ Use the condition u=fv $$
\begin{align}
f_{F,V}(f,v) &= \frac{(n_1 fv)^{n_1/2 - 1} e^{-n_1 fv/2}}{2^{n_1/2} \Gamma(n_1/2)} \cdot \frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \Gamma(n_2/2)} \cdot v\\
& =\frac{(n_1 f v)^{n_1/2 - 1} e^{-n_1 f v/2}}{2^{n_1/2} \Gamma(n_1/2)} \cdot \frac{(n_2 v)^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \Gamma(n_2/2)} \cdot v\\
& = \frac{n_1^{n_1/2-1} f^{n_1/2 - 1} v^{n_1/2 - 1} e^{-n_1 f v/2}}{2^{n_1/2} \Gamma(n_1/2)} \cdot \frac{n_2^{n_2/2-1} v^{n_2/2 - 1} e^{-n_2 v/2}}{2^{n_2/2} \Gamma(n_2/2)} \cdot v\\
& = \frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} v^{(n_1 + n_2)/2 - 1} e^{-(n_1 f + n_2) v/2}}{2^{(n_1 + n_2)/2} \Gamma(n_1/2) \Gamma(n_2/2)}
\end{align}
$$ We will integrate this density function with respect to V $$
\begin{align}
f_F(f) &= \int_0^\infty f_{F,V}(f,v) dv\\
&= \int_0^\infty \frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} v^{(n_1 + n_2)/2 - 1} e^{-(n_1 f + n_2) v/2}}{2^{(n_1 + n_2)/2} \Gamma(n_1/2) \Gamma(n_2/2)} dv\\
\end{align}
$$ Let's do some substitutions to make it look simpler $$
\begin{align}
c = \frac{n_1 f + n_2}{2}
\end{align}
$$ This is the definition of Gamma function $$
\int_0^\infty v^{(n_1 + n_2)/2 - 1} e^{-c v} dv = \frac{\Gamma((n_1 + n_2)/2)}{c^{(n_1 + n_2)/2}}
$$

Then $$
f_F(f) = \frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1}}{2^{(n_1 + n_2)/2} \Gamma(n_1/2) \Gamma(n_2/2)} \cdot \frac{\Gamma((n_1 + n_2)/2)}{\left(\frac{n_1 f + n_2}{2}\right)^{(n_1 + n_2)/2}}
$$ Let's rewrite it in an approximate F-distribution pdf form $$
f_F(f) = \frac{n_1^{n_1/2-1} n_2^{n_2/2-1} f^{n_1/2 - 1} \Gamma((n_1 + n_2)/2)}{2^{(n_1 + n_2)/2} \Gamma(n_1/2) \Gamma(n_2/2)} \cdot \left(\frac{2}{n_1 f + n_2}\right)^{(n_1 + n_2)/2}
$$

### beta and Gamma

When considering the product of two Gamma functions $$(\Gamma(a) \Gamma(b))$$, we can write it as two independent integrals:

$$
\Gamma(a) \Gamma(b) = \left( \int_0^\infty t^{a-1} e^{-t} \, dt \right) \left( \int_0^\infty s^{b-1} e^{-s} \, ds \right)
$$

To convert this product into a double integral, we can use a change of variables. Consider using polar coordinates in the two independent integrals, which allows us to use certain integration techniques to simplify them. First, we transform the integration region from Cartesian coordinates to polar coordinates:

$$
t = r \cos \theta, \quad s = r \sin \theta
$$

The Jacobian determinant is $$r$$, so the integral can be rewritten as:

$$
\Gamma(a) \Gamma(b) = \int_0^\infty \int_0^\infty t^{a-1} e^{-t} s^{b-1} e^{-s} \, dt \, ds
$$

Using the polar coordinate transformation, the integral becomes:

$$
= \int_0^\frac{\pi}{2} \int_0^\infty (r \cos \theta)^{a-1} e^{-r \cos \theta} (r \sin \theta)^{b-1} e^{-r \sin \theta} r \, dr \, d\theta
$$

Separating all $$r$$ and $$\theta$$ related terms:

$$
= \int_0^\frac{\pi}{2} (\cos \theta)^{a-1} (\sin \theta)^{b-1} \, d\theta \int_0^\infty r^{a+b-1} e^{-r(\cos \theta + \sin \theta)} \, dr
$$

First, compute the $$r$$ integral part:

$$
\int_0^\infty r^{a+b-1} e^{-r(\cos \theta + \sin \theta)} \, dr = \frac{\Gamma(a+b)}{(\cos \theta + \sin \theta)^{a+b}}
$$

Next, compute the $$\theta$$ integral part:

$$
\int_0^\frac{\pi}{2} (\cos \theta)^{a-1} (\sin \theta)^{b-1} \, d\theta = B(a, b)
$$

where $$B(a, b)$$ is the Beta function, defined as:

$$
B(a, b) = \int_0^1 t^{a-1} (1-t)^{b-1} \, dt
$$

Therefore, we get:

$$
\Gamma(a) \Gamma(b) = \frac{\Gamma(a+b)}{(\cos \theta + \sin \theta)^{a+b}} \cdot B(a, b)
$$

Since $$\cos \theta + \sin \theta = 1$$ (in polar coordinates), we have:

$$
\Gamma(a) \Gamma(b) = \Gamma(a+b) B(a, b)
$$

Thus, the double integral form of the Gamma function product directly comes from the polar transformation and the application of the Beta function. This is an important mathematical technique used to simplify complex integrals and functional relationships.

### Why is the integral result as shown?

The given integral,

$$
\int_0^\infty r^{a+b-1} e^{-r (\cos \theta + \sin \theta)} \, dr,
$$

can be evaluated using the definition of the Gamma function. The Gamma function $$\Gamma(z)$$ is defined as:

$$
\Gamma(z) = \int_0^\infty t^{z-1} e^{-t} \, dt.
$$

To see why the integral can be expressed in terms of the Gamma function, let's rewrite the integral in a form that matches the Gamma function's definition. The integral has the form:

$$
\int_0^\infty r^{a+b-1} e^{-r (\cos \theta + \sin \theta)} \, dr.
$$

Let's set $$t = r (\cos \theta + \sin \theta)$$, then $$r = \frac{t}{\cos \theta + \sin \theta}$$ and $$dr = \frac{dt}{\cos \theta + \sin \theta}$$. Substituting these into the integral gives:

$$
\int_0^\infty \left( \frac{t}{\cos \theta + \sin \theta} \right)^{a+b-1} e^{-t} \cdot \frac{dt}{\cos \theta + \sin \theta}.
$$

Simplifying inside the integral:

$$
\int_0^\infty \frac{t^{a+b-1}}{(\cos \theta + \sin \theta)^{a+b}} e^{-t} \, dt.
$$

Since $$(\cos \theta + \sin \theta)^{a+b}$$ is a constant with respect to $$t$$, it can be factored out of the integral:

$$
\frac{1}{(\cos \theta + \sin \theta)^{a+b}} \int_0^\infty t^{a+b-1} e^{-t} \, dt.
$$

The integral

$$
\int_0^\infty t^{a+b-1} e^{-t} \, dt
$$

is recognized as the Gamma function $$\Gamma(a+b)$$. Thus, we have:

$$
\frac{1}{(\cos \theta + \sin \theta)^{a+b}} \Gamma(a+b).
$$

Therefore,

$$
\int_0^\infty r^{a+b-1} e^{-r (\cos \theta + \sin \theta)} \, dr = \frac{\Gamma(a+b)}{(\cos \theta + \sin \theta)^{a+b}}.
$$

This demonstrates why the integral is evaluated as shown:

$$
\int_0^\infty r^{a+b-1} e^{-r (\cos \theta + \sin \theta)} \, dr = \frac{\Gamma(a+b)}{(\cos \theta + \sin \theta)^{a+b}}.
$$ (method 2 for integral calculation: x=uv,y=u(1-v) J=-u)

## Three Main Sampling distribution

### Chi-square distribution

Suppose $X_1, \cdots, X_n \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}(0,1)$.the distribution of the statistic $$
X_1^2+\cdots+X_n^2
$$ is called a chi-square distribution with $n$ degrees of freedom, denoted by $\chi^2(n)$.

Besides, random variable $X_i^2 \sim \operatorname{Gamma}\left(\frac{1}{2}, \frac{1}{2}\right)$ corresponds to the chi-squared distribution with 1 degree of freedom, denoted as $\chi_1^2$.

This is derived by the MGF:

Since $$
M_{X_1^2+\cdots+X_n^2}(t)=M_{X_1^2}(t) \times \cdots \times M_{X_n^2}(t)= \begin{cases}\infty & t \geq \frac{1}{2} \\ (1-2 t)^{-\frac{n}{2}} & t<\frac{1}{2}\end{cases}
$$

This is the MGF of the $\operatorname{Gamma}\left(\frac{n}{2}, \frac{1}{2}\right)$ distribution, so $X_1^2+\cdots+X_n^2 \sim \operatorname{Gamma}\left(\frac{n}{2}, \frac{1}{2}\right)$. This is called the chi-squared distribution with $\mathbf{n}$ degree of freedom, denoted $\chi_n^2$.

#### Properties


-   If $W_1, \ldots, W_n$ are independent $\chi^2$ random variables with, respectively, $v_1, \cdots, v_n$ degrees of freedom, then the random variable $W_1+\cdots+W_n$ follows a $\chi^2$-distribution with $v_1+\cdots+v_n$ degree of freedom.

-   The random variable $\frac{(\bar{X}-\mu)^2}{\sigma^2 / n}$ follows a $\chi^2$-distribution with 1 degree of freedom when $X$ follows a normal distribution with mean $\mu$ and variance $\sigma^2$.

#### Code to plot examples of Chi-square distribution

```{r}
library(ggplot2)

# Create a sequence of values
x <- seq(0, 20, length.out = 200)

# Calculate the density for different degrees of freedom
df4 <- dchisq(x, df = 4)
df8 <- dchisq(x, df = 8)
df12 <- dchisq(x, df = 12)

chi_data <- data.frame(x, df4, df8, df12)
ggplot(chi_data, aes(x)) +
  geom_line(aes(y = df4, color = "df=4")) +
  geom_line(aes(y = df8, color = "df=8")) +
  geom_line(aes(y = df12, color = "df=12")) +
  labs(title = "Chi-Square Distribution",
       x = "Value",
       y = "Density") +
  scale_color_manual(name = "Degrees of Freedom", values = c("df=4" = "blue","df=8" = "red", "df=12" = "green")) +
  theme_minimal()
```

#### Application

Chi-square distribution is primarily used in testing:

-   Goodness-of-fit

-   Independence in contingency tables

### Student's t-distribution

#### Construction

The statistic $T=\frac{\bar{X}-\mu}{S / \sqrt{n}}$ follows a $t$-distribution with $v=n-1$ degrees of freedom when $X_1, \cdots, X_n$ are i.i.d. normal RVs.

$$
\bar{X} \sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \quad \frac{\bar{X}-\mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1) \quad \frac{(n-1) s^2}{\sigma^2} \sim \chi_{n-1}^2
$$

If we know the population variance $\sigma^2$, we can easily do inference using the statistic $\frac{\bar{X}-\mu}{\sigma / \sqrt{n}}$. However, $\sigma^2$ is usually unknown in practice.

$$
\bar{X} \sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \quad \frac{\bar{X}-\mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1) \quad \frac{(n-1) s^2}{\sigma^2} \sim \chi_{n-1}^2
$$

We can construct the $t$-statistic using the sample variance $S^2$ : $$
T=\frac{\frac{\bar{X}-\mu}{\sigma / \sqrt{n}}}{\sqrt{\frac{(n-1) s^2}{\sigma^2} /(n-1)}}=\frac{\bar{X}-\mu}{S / \sqrt{n}}
$$

Notice the sample mean $\bar{X}$ and the sample variance $S^2$ are independent (the proof is beyond the scope of this course). So the $T$ is now a ratio of a standard normal variable and the square root of a $\chi^2 \mathrm{RV}$ divided by its degrees of freedom. This is the definition of a $t$-distribution with $n-1$ degrees of freedom.

#### Properties

The $t$-distribution is primarily used in contexts where the underlying population is assumed to be normally distributed, especially when the sample size is small. Used extensively in problems that deal with inference about population mean $\mu$ when population variance $\sigma^2$ is unknown; problems where one is trying to determine if means from two samples are significantly different when population variances $\sigma_1^2$ and $\sigma_2^2$ are unknown.

#### Code to plot examples of t-distribution

```{r}
# Load necessary libraries
library(ggplot2)

# Create a sequence of values
x <- seq(-4, 4, length.out = 1000)

# Calculate the density for different degrees of freedom
df_values <- c(1, 2, 3, 5, 10, 30)
distributions <- lapply(df_values, function(df) dt(x, df = df))

# Create a data frame to store the data
df_data <- data.frame(x = x)
for (i in seq_along(df_values)) {
  df_data[paste0("df", df_values[i])] <- distributions[[i]]
}

# Plot the densities
ggplot(df_data, aes(x = x)) +
  geom_line(aes(y = df1, color = "df=1")) +
  geom_line(aes(y = df2, color = "df=2")) +
  geom_line(aes(y = df3, color = "df=3")) +
  geom_line(aes(y = df5, color = "df=5")) +
  geom_line(aes(y = df10, color = "df=10")) +
  geom_line(aes(y = df30, color = "df=30")) +
  geom_line(aes(y = dnorm(x), color = "Standard Normal")) +
  scale_color_manual(name = "Distribution", 
                     values = c("df=1" = "red", "df=2" = "red", "df=3" = "red", 
                                "df=5" = "green", "df=10" = "green", "df=30" = "green", 
                                "Standard Normal" = "blue")) +
  labs(title = "Density of the t-distribution compared to the standard normal distribution",
       x = "x",
       y = "Density") +
  theme_minimal()
```

### F-distribution

Let $U$ and $V$ be two independent RVs following $\chi^2$ distributions with $\nu_1$ and $\nu_2$ degrees of freedom, respectively. Then the distribution of the random variable $F=\frac{U / \nu_1}{V / \nu_2}$ is known as $F$-distribution.

#### Example

If $S_1^2$ and $S_2^2$ are the variances of independent RVs of size $n_1$ and $n_2$ taken from normal populations with variances $\sigma_1^2$ and $\sigma_2^2$ respectively, then $$
F=\frac{S_1^2 / \sigma_1^2}{S_2^2 / \sigma_2^2}=\frac{\sigma_2^2 S_1^2}{\sigma_1^2 S_2^2}
$$ follows an $F$-distribution with $\nu_1=n_1-1$ and $\nu_2=n_2-1$ degrees of freedom.

#### Code to plot examples of F-distribution

```{r}
# Load necessary libraries
library(ggplot2)

# Create a sequence of values
x <- seq(0, 5, length.out = 1000)

# Calculate the density for different degrees of freedom
df1_values <- c(1, 2, 5, 10, 100)
df2_values <- c(1, 1, 2, 1, 100)
distributions <- lapply(1:length(df1_values), function(i) df(x, df1 = df1_values[i], df2 = df2_values[i]))

# Create a data frame to store the data
df_data <- data.frame(x = x)
for (i in seq_along(df1_values)) {
  df_data[paste0("df1", df1_values[i], "_df2", df2_values[i])] <- distributions[[i]]
}

# Plot the densities
ggplot(df_data, aes(x = x)) +
  geom_line(aes(y = df11_df21, color = "d1=1, d2=1")) +
  geom_line(aes(y = df12_df21, color = "d1=2, d2=1")) +
  geom_line(aes(y = df15_df22, color = "d1=5, d2=2")) +
  geom_line(aes(y = df110_df21, color = "d1=10, d2=1")) +
  geom_line(aes(y = df1100_df2100, color = "d1=100, d2=100")) +
  scale_color_manual(name = "", 
                     values = c("d1=1, d2=1" = "red", "d1=2, d2=1" = "black", 
                                "d1=5, d2=2" = "blue", "d1=10, d2=1" = "green", 
                                "d1=100, d2=100" = "grey")) +
  labs(title = "F-distribution with different degrees of freedom",
       x = "x",
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "top")
```

#### Application

Analysis of variance (ANOVA)

# Estimation of Population Characteristic

## Point Estimation

A point estimate of a population characteristic is a single number that is based on sample data and represents a plausible value of the characteristic.

# Interval Estimation---Confidence interval(CI)

An interval estimate of a parameter $\theta$ is an interval of the form $\hat{\theta}_L<\theta<\hat{\theta}_U$, where $\hat{\theta}_L$ and $\hat{\theta}_U$ depend on the value of $\hat{\theta}$ for a particular sample and also on the sampling distribution of $\hat{\Theta}$.

## Introduction

-   If we were to construct a 95% confidence interval for some population characteristics (population proportion p or population mean $\mu$), we would be using a method that is successful 95% of the time.

-   This is also about the question relevant to "How to choose a sample size"

## Definition

A $100(1-\alpha) \%$ confidence interval is an interval of the form $\hat{\theta}_L<\theta<\hat{\theta}_U$, where $\hat{\theta}_L$ and $\hat{\theta}_U$ are respectively values of $\widehat{\Theta}_L$ and $\widehat{\Theta}_U$ obtained for a particular sample, based on $$
P\left(\widehat{\Theta}_L<\theta<\widehat{\Theta}_U\right)=1-\alpha \quad ; \quad 0<\alpha<1
$$ in the estimation of population parameter $\theta$.

## Interpretation

For confidence level of 95% CI for any normal distribution: About 95% of the values are within 1.96 standard deviations of the mean. (Recall the concept of Z-scores)

That is, if $\text{Estimate}± (Z \times \sigma) $ was used to generate an interval estimate over and over again with different samples, in the long run 95% of the resulting intervals would include the actual value of the characteristic being estimated.

-   The confidence level 95% refers to the method used to construct the interval rather than to any particular interval, such as the one we obtained.

### eg

#### Proportion

-   check to make sure that the three necessary conditions are met:

$n\hat p \ge 10, n(1-\hat p) \ge 10$

$\frac {\hat p -p }{\sigma/\sqrt n} = 1.96$)

-   sample size question

Using sample proportion with 95% confidence interval, and we want ME no more than 5%, we may be interested in solving n from the following equation:

0.05=1.96 $\sqrt {\frac {\hat p(1-\hat p) }{ n}}$

#### CI on Mean

Here, $\bar x$ is the sample mean from a simple random sample.

$\mu$ is the population mean which we are interested in estimating.

##### CI on $\mu$ with $\sigma$ known n $\ge 30$ or the population is normal --- Use z-statistics

CI: $\bar x \pm z_{\alpha/2} \frac {\sigma}{\sqrt n}$, for example, 95% CI is $\bar x \pm 1.96 \frac {\sigma}{\sqrt n}$

##### One-side Confidence Bound on $\mu$ with $\sigma$ known n $\ge 30$ or the population is normal --- Use z-statistics

Upper one-side bound: $\mu <\bar x + z_{\alpha} \frac {\sigma}{\sqrt n}$

Lower one-side bound: $\mu >\bar x - z_{\alpha} \frac {\sigma}{\sqrt n}$

For example, 95% Confidence bound on $\mu$ is $\bar x \pm 1.645 \frac {\sigma}{\sqrt n}$

##### CI on $\mu$ with $\sigma$ unknown and the population is normal --- Use t-statistics (use s as the estimate for σ (t-statistics with df = n-1))

CI: $\bar x \pm t_{\alpha/2} \frac {s}{\sqrt n}$, for example, 95% CI is $\bar x \pm t_{0.025} \frac {s}{\sqrt n}$ and df = n-1

Remark: The distribution of t is more spread out than the standard normal distribution but when n $\ge 30$, t and z are very close to each other.

##### CI for $\mu_1 - \mu_2$, both $\sigma_1^2$ and $\sigma_2^2$ are known

CI of $\mu_1-\mu_2$: $(\bar x_1 - \bar x_2) \pm z_{\alpha/2} \sqrt {\frac {\sigma_1^2}{n_1} + \frac {\sigma_2^2}{n_2}}$

##### CI for $\mu_1 - \mu_2$, both $\sigma_1^2$ and $\sigma_2^2$ are unknown but assumed equal

CI of $\mu_1-\mu_2$: $(\bar x_1 - \bar x_2) \pm t_{\alpha/2} s_p \sqrt {\frac {1}{n_1} + \frac {1}{n_2}}$ with df = $n_1+n_2-2$ where $s_p = \sqrt {\frac {(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}$

##### CI for paired observations

Previous, we have two independent samples, now we have two dependent samples. We can use the difference between the two samples to construct a confidence interval.

CI of $\mu_d$: $\bar d \pm t_{\alpha/2} \frac {s_d}{\sqrt n}$ with df = n-1 where $s_d$ is the sample standard deviation of the differences $d_i = x_{1i} - x_{2i}$ and $\bar d$ is the sample mean of the differences.

##### Estimating $\sigma$

a $100(1-\alpha)\%$ CI for $\sigma^2$ is $\left(\frac{(n-1) S^2}{\chi_{\alpha / 2, n-1}^2}, \frac{(n-1) S^2}{\chi_{1-\alpha / 2, n-1}^2}\right)$

where $S^2$ is the sample variance and $\chi_{\alpha / 2, n-1}^2$ and $\chi_{1-\alpha / 2, n-1}^2$ are the critical values of the chi-square distribution with $n-1$ degrees of freedom.

##### Estimating $\sigma_1^2/ \sigma_2^2$

A $100(1-\alpha)\%$ CI for $\frac{\sigma_1^2}{\sigma_2^2}$ using F-statistics with $f_{1-\alpha/2}(n_1-1,n_2-1)=1/f_{\alpha/2}(n_1-1,n_2-1)$ is $$
\frac{s_1^2}{s_2^2} \frac{1}{f_{\alpha / 2}\left(n_1-1, n_2-1\right)}<\frac{\sigma_1^2}{\sigma_2^2}<\frac{s_1^2}{s_2^2} f_{\alpha / 2}\left(n_2-1, n_1-1\right)
$$ where $f_{\alpha / 2}\left(v_1, v_2\right)$ is an $F$-value with $v_1$ and $v_2$ degrees of freedom, leaving an area of $\alpha / 2$ to the right, and $f_{\alpha / 2}\left(v_2, v_1\right)$ is a similar $F$-value with $v_2$ and $v_1$ degrees of freedom.

# Hypothesis Testing

![](images/clipboard-2219722049.png)

-   z-test

Suppose $X_1, \cdots, X_n \xrightarrow[\sim]{\text { iid }} \mathcal{N}\left(\mu, \sigma^2\right)$, where $\mu$ is unknown and where $\sigma^2=\sigma_0^2$ is known.

Suppose we wish to test $H_0: \mu=\mu_0$ against $H_1: \mu>\mu_0$. Then we can use the test statistic $$
Z=\frac{\bar{X}-\mu_0}{\sigma_0 / \sqrt{n}}
$$

If $H_0$ is true then $Z \sim \mathcal{N}(0,1)$. Let $$
z_{\mathrm{obs}}=\frac{\bar{x}-\mu_0}{\sigma_0 / \sqrt{n}}
$$

A large value of $z_{\text {obs }}$ casts doubt on the validity of $H_0$ and indicates a departure from $H_0$ in the direction of $H_1$. So the $p$-value for testing $H_0$ against $H_1$ is $$
\begin{aligned}
p & =P\left(Z \geq Z_{\mathrm{obs}} \mid H_0\right) \\
& =P\left(\mathcal{N}(0,1) \geq Z_{\mathrm{obs}}\right) \\
& =1-\Phi\left(Z_{\mathrm{obs}}\right)
\end{aligned}
$$

The z-test of $H_0: \mu=\mu_0$ against the alternative $H_1: \mu<\mu_0$ is similar but this time a small, i.e. very negative, value of $Z_{\text {obs }}$ casts doubt on $H_0$. So the $p$-value is $$
\begin{aligned}
p & =P\left(Z \leq Z_{\mathrm{obs}} \mid H_0\right) \\
& =P\left(\mathcal{N}(0,1) \leq Z_{\mathrm{obs}}\right) \\
& =\Phi\left(Z_{\mathrm{obs}}\right)
\end{aligned}
$$

Finally, consider testing $H_0: \mu=\mu_0$ against the alternative $H_1: \mu \neq$ $\mu_0$. Let $z_0=\left|z_{\text {obs }}\right|$. A large value of $z_0$ indicates a departure from $H_0$, so the $p$-value is $$
\begin{aligned}
p & =P\left(|Z| \geq z_0 \mid H_0\right) \\
& =P\left(\mathcal{N}(0,1) \geq z_0\right)+P\left(\mathcal{N}(0,1) \leq-z_0\right) \\
& =2\left(1-\phi\left(z_0\right)\right)
\end{aligned}
$$

-   t-test

We can use the test statistic $$
T=\frac{\bar{X}-\mu_0}{S / \sqrt{n}}
$$

If $H_0$ is true then $T \sim t_{n-1}$. Let $t_{\mathrm{obs}}=t(x)=\frac{\bar{x}-\mu_0}{s / \sqrt{n}}$ and $t_0=\left|t_{\mathrm{obs}}\right|$. Similarly, we have $\square$ the $p$-value is $P\left(t_{n-1} \geq t_{\mathrm{obs}}\right)$ - the $p$-value is $P\left(t_{n-1} \leq t_{\text {obs }}\right)$ the $p$-value is $2 P\left(t_{n-1} \geq t_0\right)$

## Example of one-sample t-test

A marine biologist is studying a species of fish known to have an average length of 20 cm in ocean populations. A new population in a freshwater lake is being analyzed to determine if the environmental differences have altered the fish’s average length. The biologist measures the lengths of 10 randomly selected fish, yielding the following data:

22, 23, 21, 24, 22, 20, 25, 19, 23, 22

Assuming the data satisfy the assumption of normality, please address the following using a significance level of 0.1:

### a

-   null hypothesis: The mean length of fish is 20 cm ($H_0: \mu = 20$).

-   alternative hypothesis: The mean length of fish is not 20 cm ($H_1: \mu \ne 20$).

### b

Since the data is supposed to be normally distributed, the sampling distribution of the sample mean follows t-distribution. The t-test statistic is calculated as follows:

$$t = \frac{\bar x - \mu}{s / \sqrt n}$$

The t-test statistic is calculated as follows:

```{r}
data2_4 <- c(22, 23, 21, 24, 22, 20, 25, 19, 23, 22)

x_bar <- mean(data2_4)

s <- sd(data2_4)

t <- (x_bar - 20) / (s / sqrt(length(data2_4)))

t

```

The t-test statistic is 3.705882...

### c

Using pt() function, the p-value is calculated as follows:

```{r}
p_value <- 2 * pt(-t, df = 9)
p_value
```

The p-value is approximately 0.1. Since the p-value is less than 0.1, we reject the null hypothesis.

Therefore, there is sufficient evidence to conclude that the population mean is not equal to 20 which means the environmental differences have altered the fish’s average length.

### d

Using qt() function to find the critical value for a two-tailed test with 90% confidence level:

```{r}
t_critical <- qt(0.95, df = 9)
t_critical

```

The critical value for a two-tailed test with 90% confidence level is about 1.833113.

Since the t-test statistic 3.705882 is greater than the critical value 1.833113, which is in the critical region. Therefore, we reject the null hypothesis.

Also, we could use confidence interval to verify the result. The 90% confidence interval for the population mean is calculated as follows:

```{r}
ci_4 <- c(x_bar - t_critical * s / sqrt(10), x_bar + t_critical * s / sqrt(10))
ci_4

```

So the 90% confidence interval for the population mean is about (21.1, 23.2).

The confidence interval does not contain the hypothesized value 20. Therefore, we reject the null hypothesis.

Therefore, there is sufficient evidence to conclude that the population mean is not equal to 20 which means the environmental differences have altered the fish’s average length.

## Example of unpaired t-test

Suppose $\sigma_1^2$ and $\sigma_2^2$ are unknown but assumed equal. We want to test the null hypothesis $H_0: \mu_1 = \mu_2$ against the alternative hypothesis $H_1: \mu_1 \ne \mu_2$. The test statistic is given by

$$
t = \frac{\bar x_1 - \bar x_2}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
$$

where $s_p$ is the pooled sample standard deviation, given by

$$
s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}
$$

![](images/clipboard-3291557304.png)

## Example of one-sample Variance Test

$\chi^2$-test for variance. Suppose $X_1, \cdots, X_n$ are i.i.d. normal random variables with mean $\mu$ and variance $\sigma^2$. We want to test the null hypothesis $H_0: \sigma^2 = \sigma_0^2$ against the alternative hypothesis $H_1: \sigma^2 \ne \sigma_0^2$. The test statistic is given by

$$
\chi^2 = \frac{(n - 1)s^2}{\sigma^2}
$$

## Example of two-sample Variance Test

| $H_0$                                                      | Test Statistic               | $H_1$                                                                                        | Rejection Region                          |
|:-----------------|:-----------------|:-----------------|:-----------------|
| $\sigma_1^2=\sigma_2^2$                                    | $f=\frac{s_1^2}{s_2^2}$      | $\sigma_1^2<\sigma_2^2$                                                                      | $f<f_\alpha\left(\nu_1, \nu_2\right)$     |
|                                                            |                              | $\sigma_1^2>\sigma_2^2$                                                                      | $f>f_{1-\alpha}\left(\nu_1, \nu_2\right)$ |
|                                                            | $\sigma_1^2 \neq \sigma_2^2$ | $f<f_{\alpha / 2}\left(\nu_1, \nu_2\right)$ or $f>f_{1-\alpha / 2}\left(\nu_1, \nu_2\right)$ |                                           |
| $\nu_1=n_1-1$ and $\nu_2=n_2-1$ are two degree of freedom. |                              |                                                                                              |                                           |

$$
F=\frac{\frac{\left(n_1-1\right) S_1^2}{\sigma_1^2} /\left(n_1-1\right)}{\frac{\left(n_2-1\right) S_2^2}{\sigma_2^2} /\left(n_2-1\right)}=\frac{\sigma_2^2 S_1^2}{\sigma_1^2 S_2^2}
$$

If $\sigma_1^2=\sigma_2^2$, we have $$
F=\frac{S_1^2}{S_2^2} \sim F_{n_1-1, n_2-1}
$$

## ANOVA-- Analysis of Variance

-   one-way ANOVA

we need to test the null hypothesis that the group population means are all the same against the alternative that at least one group population mean differs from the others. That is,

$H_0: \mu_1=\mu_2=\cdots=\mu_k$ against $H_1: \text { at least one } \mu_i \text { differs from the others.}$

ANOVA Table

| Source |  DF |             Sum Sq |      Mean Sq |         F value |                        p value |
|:-----------|-----------:|-----------:|-----------:|-----------:|-----------:|
| Factor | m-1 | 11.84 (SS between) | 2.9587 (MSB) | 8.074 (MSB/MSW) | $5.38 \mathrm{e}-05$ (p-value) |
| Error  | n-m |  16.49 (SS Within) | 0.3664 (MSW) |                 |                                |
| Total  | n-1 |   28.33 (SS Total) |              |                 |                                |

Source means "the source of the variation in the data." the possible sources for a one-factor study are Factor, Residuals, and Total.

Factor means "the variability due to the factor of interest." In the drug example, the factor was the different drug. In the learning example on the previous page, the factor was the method of learning. Sometimes the row heading is labeled as Between.

Error (or Residuals) means "the variability within the groups" or "unexplained random error." Sometimes the row heading is labeled as Within.

Total means "the total variation in the data from the grand mean".

DF means "the degrees of freedom in the source."

Sum Sq means "the sum of squares due to the source."

Mean Sq means "the mean sum of squares due to the source."

F value means "the F-statistic."

P value means "the P-value."

SS(Total)=SS(Between)+SS(Within), where

SS(Between) is the sum of squares between the group means and the grand mean. As the name suggests, it quantifies the variability between the groups of interest.

SS(Within) is the sum of squares between the data and the group means. It quantifies the variability within the groups of interest.

SS(Total) is the sum of squares between the n data points and the grand mean. As the name suggests, it quantifies the total variability in the observed data.

-   two-way ANOVA

We can extend the idea of a one-way ANOVA, which tests the effects of one factor on a response variable, to a two-way ANOVA which tests the effects of two factors and their interaction on a response variable.

| Source                         | DF           | Sum Sq                                                                           | MSW              | F        |
|:--------------|:--------------|:--------------|:--------------|:--------------|
| Cells                          | $a b-1$      | $\sum_{i=1}^a \sum_{j=1}^b n\left(\bar{X}_{i j}-\bar{X}_{\ldots . .}\right)^2$   |                  |          |
| A                              | a-1          | bn $\sum_{i=1}^a\left(\bar{X}_{i . .}-\bar{X}_{\ldots .}\right)^2$               | SS(A)            | MS(Error |
| B                              | b-1          | an $\sum_{j=1}^b\left(\bar{X}_{. j .}-\bar{X}_{\ldots .}\right)^2$               | SS(B)            | MS(B)    |
| $\mathrm{A} \times \mathrm{B}$ | $(a-1)(b-1)$ | SS(Cells)-SS(A)-SS(B)                                                            | SS(AB)           | MS(Error |
|                                |              |                                                                                  | DF(A $\times$ B) | MS(Error |
| Error                          | $a b(n-1)$   | $\sum_{i=1}^a \sum_{j=1}^b \sum_{l=1}^n\left(X_{i j l}-\bar{X}_{i j} .\right)^2$ | SS(Error)        |          |
| Total                          | $a b n-1$    | $\sum_{i=1}^a \sum_{j=1}^b n\left(X_{i j l}-\bar{X}_{\ldots .}\right)^2$         |                  |          |

-   $F=\frac{\mathrm{MS}(\mathrm{A})}{\mathrm{MS}(\text { Error })}$, for $H_0$ : no effect of factor A on response variable,

-   $F=\frac{\mathrm{MS}(\mathrm{B})}{\mathrm{MS}(\text { Error) }}$, for $H_0$ : no effect of factor B on response variable,

-   $F=\frac{\mathrm{MS}(\mathrm{A} \times \mathrm{B})}{\mathrm{MS}(\text { Error })}$, for $H_0$ : no effect of interaction on response variable.

We reject any $H_0$ if $F \geq F_{\text {critical }}$; otherwise, we do not reject $H_0$.

### Example of two-way ANOVA

Two-way ANOVA. In this question, we will use the built-in R data set ToothGrowth to perform two-way ANOVA test. ToothGrowth includes information from a study on the effects of vitamin C on tooth growth in Guinea pigs. The response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC). Assuming the data satisfy the assumptions of normality and equal variance, please address the following using a significance level of 0.05

#### a

-   The effects of vitamin C on tooth growth in guinea pigs:

-   null hypothesis: $H_0$: mean tooth growth for all doses of vitamin C are equal

-   alternative hypothesis: $H_1$: at least one of the means of all doses of vitamin C is different from the others

-   The effects of delivery method on tooth growth in guinea pigs:

-   null hypothesis: $H_0$: mean tooth growth for the delivery method of orange juice and ascorbic acid are equal.

-   alternative hypothesis $H_1$: mean tooth growth for the delivery method of orange juice and ascorbic acid are different.

-   The interaction effects of the dose of vitamin C and delivery method on tooth growth in guinea pigs:

-   null hypothesis: $H_0:$ there is no interaction between the dose of vitamin C and delivery method on tooth growth in guinea pigs, meaning that the relationship between vitamin C and tooth growth is the same for both delivery methods (similarly, the relationship between delivery method and tooth growth is the same for all doses of vitamin C).

-   alternative hypothesis: $H_1$: there is an interaction between the dose vitamin C and delivery method on tooth growth in guinea pigs, meaning that the relationship between vitamin C and tooth growth is different for both delivery methods (similarly, the relationship between delivery method and tooth growth depends on the dose of vitamin C).

#### b

We can plot the relationship one by one using two plots

```{r}

library(ggplot2)
data(ToothGrowth)
head(ToothGrowth)

# potential effects of vitamin C on tooth growth.

ggplot(ToothGrowth, aes(x = factor(dose), y = len)) +
  geom_boxplot() +
  labs(x = "Dose (mg/day)", y = "Tooth Growth (len)", title = "Tooth Growth by Dose of vitamin C")


# potential effects of delivery method on tooth growth.

ggplot(ToothGrowth, aes(x = supp, y = len)) +
  geom_boxplot() +
  labs(x = "Delivery Method", y = "Tooth Growth (len)", title = "Tooth Growth by Delivery Method") 



```

or just one:

```{r}
library(ggplot2)

# potential effects of vitamin C and delivery method. 

# OJ represents orange juice and VC represents ascorbic acid.

ggplot(ToothGrowth, aes(x = factor(dose), y = len, fill = supp)) +
  geom_boxplot() +
  labs(x = "Dose (mg/day)", y = "Tooth Growth (len)", title = "Tooth Growth by Dose and Delivery Method") 

```

#### c

```{r}

# Perform two-way ANOVA

anova_result <- aov(len ~ supp * dose, data = ToothGrowth)

summary(anova_result)

```

Since all p-values are less than 0.05, we reject all null hypotheses. Therefore, there is sufficient evidence to conclude that the dose of vitamin C, delivery method, and their interaction have significant effects on tooth growth in guinea pigs.

#### d

```{r}
library(car)

qqPlot(anova_result$residuals, main = "QQ-plot of residuals")

```

## Non-parametric tests

### Application of testing the goodness of fit

Testing whether there is a "good fit" between the observed data and the assumed probability model amounts to testing:

#### Construction of test statistics with an example of 2 categories

Population is 60% female and 40% male. Then, if a sample of 100 students yields 53 females and 47 males, can we conclude that the sample is (random and) representative of the population? That is, how "good" do the data "fit" the assumed probability model of 60% female and 40% male?

Here, let $Y_1$ denote the number of females selected, $Y_1 \sim B(n,p_1)$ and let $Y_2$ denote males selected, $Y_2 = (n-Y_1)\sim B(n,p_2)=B(n,1-p_1)$.

for samples satisfying the general rule of thumb (the expected number of successes must be at least 5 and the expected number of failures must be at least 5), we can use the normal approximation to the binomial distribution. The test statistic is given by

$$
Z = \frac{Y_1 - np_1}{\sqrt{np_1(1-p_1)}}\sim N(0,1)
$$

which is at least approximately normally distributed.

and $$
Z^2 =Q_1= \frac{(Y_1 - np_1)^2}{np_1(1-p_1)}\sim \chi^2(1)
$$

which is an approximate chi-square distribution with one degree of freedom.

Now we can multiply $Q_1$ by 1 = $(1-p_1)+p_1$ to get

$$
Q_1 = \frac{(Y_1 - np_1)^2(1-p_1)}{np_1(1-p_1)} + \frac{(Y_1 - np_1)^2p_1}{np_1(1-p_1)}\sim \chi^2(1)
$$

Since $Y_1=n-Y_2$ and $p_1=1-p_2$, after simplifying, we have

$$
Q_1 = \frac{(Y_1-np_1)^2}{np_1} + \frac{(-(Y_2-np_2))^2}{np_2}\sim \chi^2(1)
$$

which is $Q_1=\sum_{i=1}^2 \frac{\left(Y_i-n p_i\right)^2}{n p_i}=\sum_{i=1}^2 \frac{(\text { Observed }- \text { Expected })^2}{\text { Expected }}\sim \chi^2(1)$

Hence, it is observed that if the observed counts are very different from the expected counts, then the test statistic will be large. So we reject the null hypothesis if $Q_1$ is large and how large is large is determined by the critical value of the chi-square distribution with one degree of freedom.

The statistics $Q_1$ is called the chi-square goodness of fit statistic.

Going back to the example,

-   $H_0$:$p_F = 0.6$

-   $H_1$:$p_F \ne 0.6$

we can calculate the test statistic using a significant level of $\alpha = 0.05$ ($\chi^2_{0.05,1}=3.84$)as follows:

$$
Q_1 = \frac{(53-60)^2}{60} + \frac{(47-40)^2}{40} = 2.04
$$

Since $Q_1=2.04<3.84$, we do not reject the null hypothesis. Therefore, we conclude that the sample is (random and) representative of the population.

**This can be extended to k categories**

#### Construction of test statistics with an example of k categories

For categories more than 2, i.e.

| Categories | 1       | 2       | $\cdots$ | $k-1$       | $k$                        |
|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|
| Observed   | $Y_1$   | $Y_2$   | $\cdots$ | $Y_{k-1}$   | $n-Y_1-Y_2-\cdots-Y_{k-1}$ |
| Expected   | $n p_1$ | $n p_2$ | $\cdots$ | $n p_{k-1}$ | $n p_k$                    |

Karl Pearson showed that the chi-square statistic $Q_{k-1}$ defined as: $$
Q_{k-1}=\sum_{i=1}^k \frac{\left(Y_i-n p_i\right)^2}{n p_i}
$$ follows approximately a chi-square random variable with $k-1$ degrees of freedom. Let's try it out on an example.

-   Example:

| Categories                    | Brown | Yellow | Orange | Green | Coffee | Total |
|:------------------------------|:------|:-------|:-------|:------|:-------|:------|
| Observed $y_i$                | 224   | 119    | 130    | 48    | 59     | 580   |
| Assumed $H_0\left(p_i\right)$ | 0.4   | 0.2    | 0.2    | 0.1   | 0.1    | 1.0   |
| Expected $n p_i$              | 232   | 116    | 116    | 58    | 58     | 580   |

$Q_4=\frac{(224-232)^2}{232}+\frac{(119-116)^2}{116}+\frac{(130-116)^2}{116}+\frac{(48-58)^2}{58}+\frac{(59-58)^2}{58}=3.784$

Because there are $k=5$ categories, we have to compare our chisquare statistic $Q_4$ to a chi-square distribution with $k-1=5-1=4$ degrees of freedom: $$
Q_4=3.784<\chi_{4,0.05}^2=9.488
$$ we fail to reject the null hypothesis.

### Application of testing for homogeneity

This is to look at a method for testing whether two or more multinomial distributions are equal.

-   Example:

Test the hypothesis that the acceptances of males and females are ditributed equally among the four schools,

| (Acceptances) | Bus       | Eng       | L Arts    | Sci       | (FIXED) Total |
|:--------------|:----------|:----------|:----------|:----------|:--------------|
| Male          | 240 (20%) | 480 (40%) | 120 (10%) | 360 (30%) | 1200          |
| Female        | 240 (30%) | 80 (10%)  | 320 (40%) | 160 (20%) | 800           |
| Total         | 480 (24%) | 560 (28%) | 440 (22%) | 520 (26%) | 2000          |

Here,

$H_0: p_{M B}=p_{F B}, p_{M E}=p_{F E}, p_{M L}=p_{F L}$, and $p_{M S}=p_{F S}$

$H_1: p_{M B} \neq p_{F B}$ or $p_{M E} \neq p_{F E}$ or $p_{M L} \neq p_{F L}$, or $p_{M S} \neq p_{F S}$

where:

-   $p_{M j}$ is the proportion of males accepted into school $j=B, E, L, S$.

-   $p_{F j}$ is the proportion of females accepted into school $j=B, E, L, S$.

In conducting such a hypothesis test, we're comparing the proportions of two multinomial distributions.

| #(Acc)            | Bus ( $j=1$ )                         | Eng ( $j=2$ )                         | L Arts ( $j=3$ )                      | Sci $(j=4)$                           | (FIXED) Total              |
|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|
| $\mathrm{M}(i=1)$ | $y_{11}\left(\hat{p}_{11}\right)$     | $y_{12}\left(\hat{p}_{12}\right)$     | $y_{13}\left(\hat{p}_{13}\right)$     | $y_{14}\left(\hat{p}_{14}\right)$     | $n_1=\sum_{j=1}^k y_{1 j}$ |
| F $(i=2)$         | $y_{21}\left(\hat{p}_{21}\right)$     | $y_{22}\left(\hat{p}_{22}\right)$     | $y_{23}\left(\hat{p}_{23}\right)$     | $y_{24}\left(\hat{p}_{24}\right)$     | $n_2=\sum_{j=1}^k y_{2 j}$ |
| Total             | $y_{11}+y_{21}\left(\hat{p}_1\right)$ | $y_{12}+y_{22}\left(\hat{p}_2\right)$ | $y_{13}+y_{23}\left(\hat{p}_3\right)$ | $y_{14}+y_{24}\left(\hat{p}_4\right)$ | $n_1+n_2$                  |

The chi-square test statistic for testing the equality of two multinomial distributions: $$
Q=\sum_{i=1}^2 \sum_{j=1}^k \frac{\left(y_{i j}-n_i \hat{p}_j\right)^2}{n_i \hat{p}_j}
$$ follows an approximate chi-square distribution with $k-1$ degrees of freedom. Reject the null hypothesis of equal proportions if $Q$ is large (since if male and female distributed nearly equally, the expected number of each should be $n_i\hat p_j$): $$
Q \geq \chi_{\alpha, k-1}^2
$$

(omit the derive of the above Q)

Generally,

$$
Q=\sum_{i=1}^h \sum_{j=1}^k \frac{\left(y_{i j}-n_i \hat{p}_j\right)^2}{n_i \hat{p}_j}\sim \chi^2_{(h-1)(k-1)}
$$

#### Further example

The head of a surgery department at a university medical center was concerned that surgical residents in training applied unnecessary blood transfusions at a different rate than the more experienced attending physicians. Therefore, he ordered a study of the 49 Attending Physicians and 71 Residents in Training with privileges at the hospital. For each of the 120 surgeons, the number of blood transfusions prescribed unnecessarily in a one-year period was recorded. Based on the number recorded, a surgeon was identified as either prescribing unnecessary blood transfusions Frequently, Occasionally, Rarely, or Never. Here’s a summary table (or "contingency table") of the resulting data:

| Physician | Frequent | Occasionally | Rarely | Never | Total |
|:----------|:---------|:-------------|:-------|:------|:------|
| Attending | 6.942    | 12.658       | 22.05  | 7.35  | 49    |
| Resident  | 10.058   | 18.342       | 31.95  | 10.65 | 71    |
| Total     | 17       | 31           | 54     | 18    | 120   |

Here,

$H_0: p_{R F}=p_{A F}, p_{R O}=p_{A O}, p_{R R}=p_{A R}$, and $p_{R N}=p_{A N}$

$H_1: p_{R F} \neq p_{A F}$ or $p_{R O} \neq p_{A O}$ or $p_{R R} \neq p_{A R}$, or $p_{R N} \neq p_{A N}$

We should also calculate the expected counts under the null hypothesis. The expected counts are calculated as follows:

| Physician | Frequent | Occasionally | Rarely | Never | Total |
|:----------|:---------|:-------------|:-------|:------|:------|
| Attending | 6.942    | 12.658       | 22.05  | 7.35  | 49    |
| Resident  | 10.058   | 18.342       | 31.95  | 10.65 | 71    |
| Total     | 17       | 31           | 54     | 18    | 120   |

where, for example, $6.942=\frac{17}{120} \times 49$ and $10.058=\frac{17}{120} \times 71$. Now that we have the observed and expected counts, calculating the chisquare statistic is a straightforward exercise: $$
Q=\frac{(2-6.942)^2}{6.942}+\cdots+\frac{(5-10.65)^2}{10.65}=31.88
$$

The chi-square test tells us to reject the null hypothesis, at the 0.05 level, if $Q$ is greater than a chi-square random variable with 3 degrees of freedom, that is, if $Q=31.88>7.815$, we reject the null hypothesis.

### Application of testing for independence

This is to look at whether two or more categorical variables are independent.

(previously,the the sampling scheme involves: Taking two random (and therefore independent) samples with n1 and n2 fixed in advance and observing into which of the k categories the first random samples fall, and observing into which of the k categories the second random samples fall. )

lets consider a different example to illustrate an alternative sampling scheme. Suppose 395 people are randomly selected, and are "cross-classified" into one of eight cells, depending into which age category they fall and whether or not they support legalizing marijuana:

(the sampling scheme involves: Taking one random sample of size n, with n fixed in advance, and then "cross-classifying" each subject into one and only one of the mutually exclusive and exhaustive $A_i\cap B_j$ cells.)

| Marijuana Support |             | Variable B (Age) |                 |               |               |         |
|:---------|:---------|:---------|:---------|:---------|:---------|:---------|
| Variable A        | OBSERVED    | $(18-24) B_1$    | (25-34) $B_1 2$ | (35-49) $B_3$ | $(50-64) B_4$ | Total   |
|                   | (YES) $A_1$ | 60               | 54              | 46            | 41            | 201     |
|                   | (NO) $A_2$  | 40               | 44              | 53            | 57            | 194     |
|                   | Total       | 100              | 98              | 99            | 98            | $n=395$ |

Here,

H0 : Variable A is independent of variable B, that is $P(A_i\cap B_j)=PA_i \times B_j$ for all i and j

H1: Variable A is not independent of variable B.

Generally,

Suppose we have $k$ (column) levels of Variable B indexed by the letter $j$, and $h$ (row) levels of Variable $A$ indexed by the letter $i$. Then, we can summarize the data and probability model in tabular format, as follows:

| Variable B |                             |                             |                             |                             |                        |     |
|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|
| Variable A |         $B_1(j=1)$          |         $B_2(j=2)$          |         $B_3(j=3)$          |         $B_4(j=4)$          |         Total          |     |
| $A_1(i=1)$ | $Y_{11}\left(p_{11}\right)$ | $Y_{12}\left(p_{12}\right)$ | $Y_{13}\left(p_{13}\right)$ | $Y_{14}\left(p_{14}\right)$ | $\left(p_{1 .}\right)$ |     |
| $A_2(i=2)$ | $Y_{21}\left(p_{21}\right)$ | $Y_{22}\left(p_{22}\right)$ | $Y_{23}\left(p_{23}\right)$ | $Y_{24}\left(p_{24}\right)$ | $\left(p_{2 .}\right)$ |     |
|   Total    |    $\left(p_{.1}\right)$    |    $\left(p_{.2}\right)$    |    $\left(p_{.3}\right)$    |    $\left(p_{.4}\right)$    |          $n$           |     |

where $p_{i j}=Y_{i j} / n, p_i .=\sum_{j=1}^k p_{i j}$, and $p_{. j}=\sum_{i=1}^h p_{i j}$

$Q=\sum_{j=1}^k \sum_{i=1}^h \frac{\left(y_{i j}-\frac{y_i \cdot y_{\cdot j}}{n}\right)^2}{\frac{y_i \cdot y_{\cdot j}}{n}}\sim \chi^2_{(h-1)(k-1)}$

#### Are chi-square statistic for homogeneity and the chi-square statistic for independence equivalent?

Although their chi-square statistics are equivalent, the two tests are not equivalent since their sampling experiment designs are different.

Here’s the table of expected counts:

| Bicycle Riding Interest |          | Variable B (Age) |        |        |        |       |
|:------------|:---------|:---------|:---------|:---------|:---------|:---------|
| Variable A              | EXPECTED | 18-24            | 25-34  | 35-49  | 50-64  | Total |
|                         | YES      | 50.886           | 49.868 | 50.377 | 49.868 | 201   |
|                         | NO       | 49.114           | 48.132 | 48.623 | 48.132 | 194   |
|                         | Total    | 100              | 98     | 99     | 98     | 395   |

$$
Q=\frac{(60-50.886)^2}{50.886}+\cdots+\frac{(57-48.132)^2}{48.132}=8.006
$$

The chi-square test tells us to reject the null hypothesis, at the 0.05 level, since $Q$ is greater than a chi-square random variable with 3 degrees of freedom, that is, $Q=8.006>7.815$.

#### Summary

Parametric tests make assumptions that aspects of the data follow some sort of theoretical probability distribution. Non-parametric tests or distribution free methods do not, and are used when the distributional assumptions for a parametric test are not met. While this is an advantage, it often comes at a cost of power (in the sense they are less likely to be able to detect a difference when a true difference exists).

Most non-parametric tests are just hypothesis tests; there is no estimation of a confidence interval.

Most non-parametric methods are based on ranking the values of a variable in ascending order and then calculating a test statistic based on the sums of these ranks.

Non-parametric tests include:

-   Two-sample independent t-test - Wilcoxon rank-sum test or Mann-Whitney U test

-   Paired t-test - Wilcoxon signed-rank test

-   One-way ANOVA - Kruskal-Wallace Test

-   Normality tests - Shapiro-Wilk test and Kolmogorov-Smirnov test

# Linear Regression

## Simple linear regression

### Brief introduction

Some people think simple methods is bad and like complicated methods , but actually simple is very good--SLR works very well in lots of situations.

SLR is used to answer:

is there a relationship between..

How strong the relationship is

which variable contribute to this relationship

How accurate could we predict the response variable

Is the relationship linear?

Is there a synergy among independent variables?

This is a model with two random variables, X and Y, where we are trying to predict Y from X. Here are the model’s assumptions:

-   The distribution of X is arbitrary, possibly is even non-random;

-   If X=x, then $Y = \beta_0+\beta_1x+\epsilon$ for some constants $\beta_0, \beta_1$ and some random noise variable $\epsilon$

-   $\epsilon$ has mean 0, a constant variance $\sigma^2$, and is uncorrelated with X and uncorrelated across observations Cov$(\epsilon_i, \epsilon_j)=0$ for $i \ne j$

Using Least Squares, we can estimate $\hat \beta_0$ and $\hat \beta_1$, which are unbiased estimates of $\beta_0$ and $\beta_1$.

-   Gaussian-Noise Simple Linear Regression Model

Now we further assume that the distribution of $\epsilon$ is normal, i.e. $\epsilon \sim N(0, \sigma^2)$, independent of X.

They tell us, exactly, the probability distribution for Y given X, and so will let us get exact distributions for predictions and for other inferential statistics.

### Maximum Likelihood Estimation (MLE)

#### Introduction to MLE

Likelihood is a fundamental concept in statistics that measures how well a particular set of parameters (e.g., the mean of a distribution) explains observed data. Think of it as a "score" that tells you which parameter values make your data most plausible.

Compared to probability, which answers: "What’s the chance of seeing this data if we assume specific parameters?" , likelihood answers: "Given this data, how plausible are these parameters?"

If the parameters are $b_0, b_1, s^2$ (reserving the Greek letters for their true values), then $Y \mid X=x \sim N\left(b_0+b_1 x, s^2\right)$, and $Y_i$ and $Y_j$ are independent given $X_i$ and $X_j$, so the overall likelihood is $$
\prod_{i=1}^n \frac{1}{\sqrt{2 \pi s^2}} e^{-\frac{\left(y_i-\left(b_0+b_i x_i\right)\right)^2}{2 s^2}}
$$

As usual, we work with the log-likelihood, which gives us the same information but replaces products with sums: $$
L\left(b_0, b_1, s^2\right)=-\frac{n}{2} \ln \left(2 \pi s^2\right)-\frac{1}{2 s^2} \sum_{i=1}^n\left(y_i-\left(b_0+b_1 x_i\right)\right)^2
$$

maximize it:

$\begin{aligned} \frac{\partial L}{\partial b_0} & =-\frac{1}{2 s^2} \sum_{i=1}^n 2\left(y_i-\left(b_0+b_1 x_i\right)\right)(-1) \\ \frac{\partial L}{\partial b_1} & =-\frac{1}{2 s^2} \sum_{i=1}^n 2\left(y_i-\left(b_0+b_1 x_i\right)\right)\left(-x_i\right)\end{aligned}$

#### Same result of MLE as least squares in linear regression

Notice that when we set these derivatives to zero, all the multiplicative constants - in particular, the prefactor of $1 / 2 s^2$ - go away. We are left with $$
\begin{aligned}
\sum_{i=1}^n\left(y_i-\left(\hat{\beta_0}+\hat{\beta_1} x_i\right)\right) & =0 \\
\sum_{i=1}^n\left(y_i-\left(\hat{\beta_0}+\hat{\beta_0} x_i\right)\right) x_i & =0
\end{aligned}
$$

These are, up to a factor of $1 / n$, exactly the equations we got from the method of least squares. That means that the least squares solution is the maximum likelihood estimate under the Gaussian noise model.

Maximum likelihood estimates of the regression curve coincide with least-squares estimates when the noise around the curve is additive, Gaussian, of constant variance, and both independent of $X$ and of other noise terms. If any of those assumptions fail, maximum likelihood and least squares estimates can diverge.

### Hypothesis testing for estimates with unknown $\sigma^2$

#### Residual sum of squares (RSS) and t-statistics construction in linear regression

It can be shown that (the proof is beyond the scope of this course) $$
\frac{R S S}{\sigma^2}=\frac{(n-2) \hat{\sigma}^2}{\sigma^2} \sim \chi_{n-2}^2
$$

This allows us to construct a $t$-value $$
t=\frac{\hat{\beta}-\beta}{s_{\hat{\beta}}} \sim t_{n-2}
$$

Under the normality assumption of the error terms, the estimator of the slope coefficient will itself be normally distributed with mean $\beta_i$ and variance $\operatorname{Var}\left[\beta_i\right]$ For $\hat{\beta_1}$, its mean is $\beta_1$ and its variance is $\sigma^2 / \sum\left(x_i-\bar{x}\right)^2$. When $\sigma^2$ is known, we know $$
\frac{\hat{\beta}_1-\beta_1}{\frac{\sigma}{\sqrt{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}}}
$$ follows standard normal distribution. However, in practice, $\sigma^2$ is often unknown. We then divide this standard normal distributed term by $$
\sqrt{\frac{(n-2) \hat{\sigma}^2}{(n-2) \sigma^2}}=\frac{\hat{\sigma}}{\sigma}
$$

Therefore, when we write $$
s_{\hat{\beta_1}}=\frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}}
$$ we construct a $t$-statistic for $\hat{\beta_1}$ with degrees of freedom $n-2$. This then allows us to construct a $100(1-\alpha) \%$ confidence interval for $\beta_1$ : $$
\hat{\beta_1} \pm t_{n-2, \alpha / 2} \times \boldsymbol{s}_{\hat{\beta_1}}
$$

We can also do similar calculation to get the $t$-statistic and confidence interval for $\beta_0$.

#### Hyphothesis Testing

$$
t =\hat {\beta}-\beta / s_{\hat{\beta_1}} \sim t_{n-2}
$$ $H_0: \beta_i =0$

$H_1: \beta_i \ne 0$

#### Codes of linear regression with CI

```{r}
lmodel <- lm(Petal.Length ~ Petal.Width, data=iris)
summary(lmodel)
confint(lmodel)
conf_interval <- predict(lmodel, data=iris, interval='confidence', level=0.95)
plot(iris$Petal.Width, iris$Petal.Length,
     xlab='Petal.Width', ylab='Petal.Length',
     main='Simple Linear Regression')
abline(lmodel, col='lightblue')
matlines(iris$Petal.Width, conf_interval[,2:3], col='blue', lty=2)

# Using ggplot2
library(ggplot2)
ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) +
  geom_point() +
  geom_smooth(method=stats::lm, se=T, level=0.95)

```

#### Linear Regression and ANOVA

``` r
fev_dat <- read.table('fev_dat.txt', header=T)
fev_dat_subset <- fev_dat[fev_dat$age >= 6 & fev_dat$age <= 10,]
ggplot(fev_dat_subset, aes(x=age, y=FEV)) +
  geom_point() +
  geom_smooth(method=stats::lm, se=T, level=0.95)
summary(aov(FEV ~ age, data=fev_dat_subset))
summary(lm(FEV ~ age, data=fev_dat_subset))
anova(lm(FEV ~ age, data=fev_dat_subset))
```

### $R^2$--the fraction of variability explained by the regression

$$
R^2 =1-\frac{SSR}{SSTO}
$$

## Multiple linear regression (MLR)

$y=X\beta+\epsilon$, where y is a n × 1 row vector, X is a n × (k + 1) matrix, and β is a (k + 1) × 1 column vector for all n observations.

### A potential problem in practice --multicollinearity

When multicollinearity exists, any of the following pitfalls can be exacerbated:

-   The estimated regression coefficient of any one variable depends on which other predictors are included in the model

-   The precision of the estimated regression coefficients decreases as more predictors are added to the model

-   The marginal contribution of any one predictor variable in reducing the error sum of squares depends on which other predictors are already in the model

-   Typothesis tests for βj = 0 may yield different conclusions depending on which predictors are in the model

#### Perfect multicollinearity

Perfect multicollinearity refers to a situation where the predictive variables have an exact linear relationship. When there is perfect collinearity, the design matrix X has less than full rank, and therefore the moment matrix X′X cannot be inverted. In this situation, the parameter estimates of the regression are not well-defined, as the system of equations has infinitely many solutions.

#### Imperfect multicollinearity

Imperfect multicollinearity refers to a situation where the predictive variables have a nearly exact linear relationship.

$R^2=r^2$ where r is the Pearson correlation coefficient.

### Adjusted R-squared

Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a model. It provides a more accurate measure of the model's explanatory power, penalizing for the addition of irrelevant predictors. This helps in comparing models with different numbers of predictors.

# Logistic Regression

This is a regression of categorical outcome.

In regression analysis with a categorical outcome, such as predicting a binary variable (yes or no), simple linear regression is not ideal. This is because:

-   The predicted values may fall outside the range of 0 to 1, which is not meaningful for probabilities.

-   Small changes in the predictors can lead to relatively small fluctuations in the predicted probabilities near the 0.5 mark (natural threshold), which is actually where decision-making is most critical.

So we need S-curve to satisfy above things.

```{r}
curve(1 / (1 + exp(-x)), from = -6, to = 6, xlab = "x", ylab = "f(x)",
main = "Logistic Function: f(x) = 1 / (1 + exp(-x))", col = "blue", lwd = 2)
```

An example of a not well-predicted logistic model (since stock is not easy to predict)

```{r}
library(ISLR2)
attach(Smarket)
glm.fits <- glm(
    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
    data = Smarket, family = binomial
  )
summary(glm.fits)

glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction)
(507 + 145) / 1250
mean(glm.pred == Direction)
```

## Odds

Odds are another way of quantifying the probability of an event, commonly used in gambling (and logistic regression).

For some event $E$, $$
\operatorname{odds}(E)=\frac{P(E)}{P\left(E^c\right)}=\frac{P(E)}{1-P(E)}
$$

The odds ratio (OR) is the ratio of the odds of an event occurring in one group to the odds of it occuring in another group. If the event in each of the groups are $p_1$ (first group) and $p_2$ (second group), then the odds ratio is: $$
\mathrm{OR}=\frac{p_1 /\left(1-p_1\right)}{p_2 /\left(1-p_2\right)}
$$

# Generalized Linear Models

Under the hood, we're still using a linear model $\left(\beta_0+\beta_1 x\right)$, but now it's embedded in a function that ensures valid probabilities. This is the essence of logistic regression - a generalized linear model (GLM) designed for binary outcomes.

Given predictor $X$ and d an outcom $Y$, a GLM is defined by three components: - A random component, that specifies a distribution for $Y \mid X$ - A systematic component, that relates a parameter $\eta$ to the predictor $X$ $$
\eta=\beta_0+\beta_1 X_1+\cdots+\beta_k X_k
$$ - A link function, that connects the random and systematic component

## Random Component

The random component specifies a distribution for the outcome variable (conditional on $X$ ). In the case of linear regression, we assume that $Y \mid X \sim \mathcal{N}\left(\mu, \sigma^2\right)$, for some mean $\mu$ and variance $\sigma^2$. In the case of logistic regression, we assume that $Y \mid X \sim \operatorname{Bern}(p)$ for some probability $p$.

In a generalized model, we are allowed to assume that $Y \mid X$ has a probability density function or probability mass function of the form $$
f(y ; \theta, \phi)=\exp \left(\frac{y \theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)
$$

Here $\theta, \phi$ are parameters, and $a, b, c$ are functions. Any density of the above form is called an exponential family density. The parameter $\theta$ is called the natural parameter, and the parameter $\phi$ the dispersion parameter.

## Exponential Family

Exponential families include many of the most common distributions. For example: - Exponential $$
f(y ; \lambda)=\lambda e^{-\lambda y}=\exp (-y \lambda+\ln \lambda)
$$ where $\theta=-\lambda, \phi=1, b(\theta)=\ln \lambda, a(\phi)=1$, and $c(y, \phi)=0$ - Poisson $$
f(y ; \lambda)=\frac{e^{-\lambda} \lambda^y}{y!}=\exp (y \ln \lambda-\lambda-\ln (y!))
$$ where $\theta=\ln \lambda, \phi=1, b(\theta)=e^\theta=\lambda, a(\phi)=1$, and $c(y, \phi)=-\lambda-\ln (y!)$

## Systematic Component and Link Component

The systematic component relates a parameter $\eta$ to the predictors $X$. In a GLM, this is always done via $$
\eta=X \beta=\beta_0+\beta_1 X_1+\cdots+\beta_k X_k
$$

We will denote the expectation of the distribution in random component as $\mu$, i.e., $\mathbb{E}[Y \mid X]=\mu$. It will be our goal to estimate $\mu$. Finally, the link component connects the random and systematic components, via a link function $g$. In particular, this link function provides a connection between $\mu$ and $\eta$, as in $$
g(\mu)=\eta \quad \text { or } \quad \mu=g^{-1}(\eta)
$$

## Example

### Gaussian-noise Linear Regression

-   Random Component: $Y \mid X \sim \mathcal{N}\left(\mu, \sigma^2\right)$ and $\mathbb{E}[Y \mid X]=\mu$
-   Systematic Component: $\eta=X \beta$
-   Link Component: $g(\mu)=\mu$, so that $\mu=\eta=X \beta$

### Bernoulli

Suppose that $Y \in\{0,1\}$, and we model the distribution of $Y \mid X$ as Bernoulli with success probability $p$. Then the probability mass function (not a density, since $Y$ is discrete) is $$
f(y)=p^y(1-p)^{1-y}
$$

We can rewrite to fit the exponential family form as $$
\begin{aligned}
f(y) & =\exp (y \log p+(1-y) \log (1-p)) \\
& =\exp (y \log (p /(1-p))+\log (1-p))
\end{aligned}
$$

$$
f(y ; \theta, \phi)=\exp \left(\frac{y \theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)
$$

Here we would identify $\theta=\log (p /(1-p))$ as the natural parameter. Note that the mean here is $\mu=p$, and using the inverse of the above relationship, we can directly write the mean $p$ as a function of $\theta$, as in $p=e^\theta /\left(1+e^\theta\right)$. Hence $b(\theta)=\log (1-p)=-\log \left(1+e^\theta\right)$. There is no dispersion parameter, so we can set $a(\phi)=1$. Also, $c(y, \phi)=0$.

# Link Function

$$
g(\mu)=\Phi^{-1}(\mu)
$$ where $\Phi$ is the standard normal CDF.

### Logistic

The three GLM criteria give us: - $y_i \sim \operatorname{Bern}\left(p_i\right)$ - $\eta=\beta_0+\beta_1 X_1+\cdots+\beta_k X_k$ - $\operatorname{logit}(p)=\log \frac{p}{1-p}=\eta$

From which we know, $$
p_i=\frac{\exp \left(\beta_0+\beta_1 x_{i 1}+\cdots+\beta_k X_{i k}\right)}{1+\exp \left(\beta_0+\beta_1 x_{i 1}+\cdots+\beta_k X_{i k}\right)}
$$

# Survival Analysis

Survival analysis is used to analyze data in which the time until the event is of interest. The response is often referred to as a failure time, survival time, or event time.

## Some definations

Hazard ratios; ratios of hazard functions between different groups (e.g., exposed vs. unexposed) while adjusting for confounders.

censoring - which occurs when the survival time is only partially known

-   Fixed type I censoring occurs when a study is designed to end after C years of follow-up. In this case, everyone who does not have an event observed during the course of the study is censored at C years.

-   In random type I censoring, the study is designed to end after C years, but censored subjects do not all have the same censoring time. This is the main type of right-censoring we will be concerned with.

-   In type II censoring, a study ends when there is a pre-specified number of events.

## Kaplan-Meier estimate

```{r}
library(ISLR2)
names(BrainCancer)
attach(BrainCancer)
table(status)

library(survival)
fit.surv <- survfit(Surv(time, status) ~ 1)
plot(fit.surv, xlab = "Months",
    ylab = "Estimated Probability of Survival")


```

## Cox-proportional harzards model

S(t) = P(T\>t)=1-F(t)

$h(t)=\lim _{\Delta t \rightarrow 0} \frac{P(t<T \leq t+\Delta t \mid T>t)}{\Delta t}$

```{r}
fit.all <- coxph(
Surv(time, status) ~ sex + diagnosis + loc + ki + gtv +
   stereo)
fit.all
modaldata <- data.frame(
     diagnosis = levels(diagnosis),
     sex = rep("Female", 4),
     loc = rep("Supratentorial", 4),
     ki = rep(mean(ki), 4),
     gtv = rep(mean(gtv), 4),
     stereo = rep("SRT", 4)
     )
survplots <- survfit(fit.all, newdata = modaldata)
plot(survplots, xlab = "Months",
    ylab = "Survival Probability", col = 2:5)
legend("bottomleft", levels(diagnosis), col = 2:5, lty = 1)
```

# Final review of R codes

## Calculation

```{r}
log(exp(1)) #  base `e` is the default (log(e) is not defined)
```

## Vectors -Lab 2

```{r}
x <- c(1,2,3,4)
class(x)
x %*%x # scalar ("inner") product (but default in R as an 1*1 matrix)
rep(c('a','b'),3)
rep(c(2, 4, 8), each = 3)
y <- seq(from = 1, to = 4, by =1)
class(y)
1:4 # c(1,2,3,4) and 1:4 are all the same in.R
seq(1,5, length.out=11)
# vac<-c((1,2,3),(3,4,5)) is wrong, but the below is true
vec1 <- c(1,2,3)
vec2 <- c(4,5,6)
vec3 <- c(vec1, vec2)
vec3[1] == vec1[1]
vec3[3:5];vec3[c(2,3)]
vec3[-1] # everything but the first element
vec3[-2*c(1,2)]
x <- -5:5
class(x)
str(x)
x <- c(-5:5)
str(x)
abs(-5:5)
exp(c(-5:5))
exp(-5:5)
x <- c(1, 2, 3, 4, 5,6)
y <- c(10,11)
result <- x + y
print(result)
# sample variance and population variance 
x <- c(1, 2, 3, 4, 5)
# x * y  # Element-wise multiplication: 10 40 90 160 250 360
c(1,2)*c(2,3)

# Calculate the sample variance using R's var() function
sample_variance <- var(x)

# Calculate the sample(var(x)) and population variance
n <- length(x)
sample_variance <-var(x)
population_variance <- sample_variance * (n - 1) / n
print(population_variance)

```

## Rmd knowledge

{r, echo = FALSE} Hidden Code

eval: run or not

message/warning

result

# Probability in R

```{r}
library(mosaic)
plotDist('norm',mean=10,sd=2)
plotDist('binom',size=10,prob=.2) # x-axis is the number of success from 1-10
plotDist("pois", lambda = 2)

sum(dpois(0:3, lambda = 2)) # the probability it is less than or equal to 3 of Poison(2)
ppois(3, lambda = 2) 
ppois(3, lambda = 2) == sum(dpois(0:3, lambda = 2)) # this inequivalance is because the floating point precision

#xpnorm(-1)  # P(Z ≤ -1) = 0.1586553 -package mosaic
#xpnorm(-1, lower.tail = FALSE)  # P(Z ≥ 1.5) = 0.0668072
pnorm(-1)
pnorm(-1,lower.tail = F)

pnorm(0)
qnorm(0)
qnorm(0.95)
qnorm(0.975)

```

# Data class

```{r}
df<-("R is the statistical analysis language")
strsplit(df, split = " ")
strsplit(df, split = "")
df<-"all16i5need6is4a9long8vacation"
?strsplit
strsplit(df,split = "[0-9]+")
paste("Good", "afternoon", "ladies", "and", "gentlemen")
paste0("Good", "afternoon", "ladies", "and", "gentlemen")
6 != 10
x <- -10:10
which(x>0)
x[which(x>0)]
vowels <- c('a','e','i','o','u')
which(is.element(letters, vowels))

# Note that logical elements are NOT in quotes.
z = c("TRUE", "FALSE", "TRUE", "FALSE")
class(z)
as.logical(z)

# TRUE = 1 and FALSE = 0. sum() and mean() work on logical vectors

# remember:
TRUE & TRUE
TRUE & FALSE
TRUE | FALSE


## Factor

y <- c('B','B','A','A','C')
z <- factor(y)
str(z)
as.numeric(z)
levels(z)

z <- factor(z,                       # vector of data levels to convert 
            levels=c('B','A','C'),   # Order of the levels
            labels=c("B Group", "A Group", "C Group")) # Pretty labels to use
z

### eg of use in the plot's x-axis name lable

iris$Species <- factor(iris$Species,
                       levels = c('versicolor','setosa','virginica'),
                       labels = c('Versicolor','Setosa','Virginica'))
#boxplot(Sepal.Length ~ Species, data=iris)

### another eg
#age_category <- ifelse(ages >= 18, "Adult", "Minor")
#age_factor <- factor(age_category, levels = c("Minor", "Adult"))
#age_factor

# transform a conituous numerical vector and transform it into a factor

x <- 1:10

# cut(x,breaks=3)
cut(x, breaks = c(0, 2.5, 5.0, 7.5, 10))
cut(x, breaks=3, labels=c('Low','Medium','High'))

# dates
mydates <- as.Date(c("2023-04-07", "2023-01-01"))
mydates

days <- mydates[1] - mydates[2]; days

```

The following symbols can be used with the `format()` function to print dates.

-   %d day as a number (0-31) 01-31
-   %a abbreviated weekday Mon
-   %A unabbreviated weekday Monday
-   %m month (00-12) 00-12
-   %b abbreviated month Jan
-   %B unabbreviated month January
-   %y 2-digit year 07
-   %Y 4-digit year 2007

```{r}
today <- Sys.Date()
format(today, format="%B %d %Y")

# or library(lubridate)
#x = c("2014-02-4 05:02:00","2016/09/24 14:02:00") 
#ymd_hms(x)

# POSIXct
```

```{r}
# Matrices

n=1:9
mat = matrix(n,nrow=3)
mat
y <- diag(n)

# use %*% as the product of matrices

## Eigenvalue and Eigenvector

A <- matrix(c(13, -4, 2, -4, 11, -2, 2, -2, 8), 3, 3, byrow=TRUE)
ev <- eigen(A)

(values <- ev$values)
(vectors <- ev$vectors)

## Data selection --row then column

mat[1, 1]
mat[1,]
mat[,1] 
class(mat[1, ]) # Note that the class of the returned object is no longer a matrix

# Data frames

Data_Frame <- data.frame(Tr =c("1","2","3"),
                         Pu =c(11,21,32),
                         Dur=c(22,222,1))
Data_Frame
summary(Data_Frame)

# List ---Can hold vectors, strings, matrices, models, list of other list, lists upon lists!

mylist <- list(letters=c("a","b","c"),
               numbers=1:3,matrix(1:25,ncol=5))
head(mylist)

# Can reference data using $ (if the elements are named), or using [], or [[]]

mylist[1] # list
mylist["letters"] # list
mylist[[1]] # vector
mylist$letters == mylist[["letters"]]
mylist[[3]][1:2,1:2]
class(mylist[[3]][1:2,1:2])
x = c(0, 2, 2, 3, 4); 2 %in% x

# eg of Matrices data frames
x <- c(5.1, 4.9, 5.6, 4.2, 4.8, 4.5, 5.3, 5.2)   # some toy data
results <- t.test(x, alternative='less', mu=5)   # do a t-test
str(results)    
results$p.value

```

Practice of Lab 5

```{r}
db_data <- list(
  drugs = list(
    general_information = data.frame(
      drugbank_id = c("DB001", "DB002", "DB003", "DB004", "DB005"),
      name = c("Aspirin", "Ibuprofen", "Paracetamol", "Insulin", "Morphine"),
      type = c("small molecule", "small molecule", "small molecule", "biotech", "small molecule"),
      created = as.Date(c("2020-01-01", "2020-02-01", "2020-03-01", "2020-04-01", "2020-05-01")),
      stringsAsFactors = FALSE
    ),
    drug_classification = data.frame(
      drugbank_id = c("DB001", "DB002", "DB003", "DB004", "DB005"),
      classification = c("Analgesic", "Anti-inflammatory", "Analgesic", "Hormone", "Analgesic"),
      stringsAsFactors = FALSE
    ),
    experimental_properties = data.frame(
      drugbank_id = c("DB001", "DB002", "DB003", "DB004", "DB005", "DB001", "DB002", "DB003", "DB004", "DB005"),
      kind = c("logP", "logP", "logP", "logP", "logP", "Molecular Weight", "Molecular Weight", "Molecular Weight", "Molecular Weight", "Molecular Weight"),
      value = c("1.2", "1.5", "0.8", "2.1", "1.8", "180.1", "206.3", "151.2", "5800.0", "281.5"),
      stringsAsFactors = FALSE
    )
  )
)

general_information <- db_data$drugs$general_information

print(general_information)
# 20. Number of drugs in the general_information dataframe
general_information <- db_data$drugs$general_information
nrow(general_information)

# 21. Filter drugs of type "biotech"
general_information[general_information$type == 'biotech',]

# 22. Sort by the created column and display the first 5 rows
general_information$created <- as.Date(general_information$created)
sorted_df <- general_information[order(general_information$created), ]
head(sorted_df, 5)

# 23. Subset with specific columns and display the first 5 rows
subset_df <- general_information[, c("drugbank_id", "name")]
head(subset_df, 5)

# 24. Merge dataframes and count rows
drug_classification <- db_data$drugs$drug_classification
merged_df <- merge(general_information, drug_classification, by = "drugbank_id")
nrow(merged_df)

# 25. Count unique experimental properties (kind)
experimental_properties <- db_data$drugs$experimental_properties
unique_kinds <- unique(experimental_properties$kind)
length(unique_kinds)

# 26. Filter for kind "logP" and count rows
logP_df <- experimental_properties[experimental_properties$kind == "logP", ]
nrow(logP_df)

# 27. Convert value column to numeric and calculate mean
logP_df$value <- as.numeric(logP_df$value)
mean(logP_df$value, na.rm = TRUE)

# 28. Calculate summary statistics for logP values
summary(logP_df$value)
sd(logP_df$value, na.rm = TRUE)

# 29. Create a histogram of molecular weight values
molecular_weight <- experimental_properties[experimental_properties$kind == "Molecular Weight", ]
molecular_weight$value <- as.numeric(molecular_weight$value)
# clean based on 3 sigma rule
molecular_weight_clean <- molecular_weight[
  abs(molecular_weight$value - mean(molecular_weight$value, na.rm = TRUE)) <= 3 * sd(molecular_weight$value, na.rm = TRUE),]
hist(molecular_weight_clean$value, main = "Histogram of Molecular Weight", xlab = "Molecular Weight", ylab = "Frequency", col = "lightblue",breaks = 20)

# 30. Filter for kind "Water Solubility" and count unique values
water_solubility_df <- experimental_properties[experimental_properties$kind == "Water Solubility", ]
length(unique(water_solubility_df$value))
```

# LAB 5 --data visualization

```{r, fig.align='center'}
library(ggplot2)
data(iris)
boxplot(iris$Sepal.Length ~ iris$Species)
hist(iris$Sepal.Length)
plot(Petal.Length ~ Sepal.Length, data=iris)
abline(lm(Petal.Length ~ Sepal.Length, data=iris), col="red")
data(mpg, package='ggplot2')
ggplot(data=mpg, aes(x=class)) +
  geom_bar()
table(mpg$class)
df <- as.data.frame(table(mpg$class))
df
# By default, the geom_bar() just counts the number of cases and displays how many observations were in each factor level. If we have a data frame that we have already summarized, geom_col will allow us to set the height of the bar by a y column.

ggplot(df, aes(Var1, Freq)) +
  geom_col()
ggplot(mpg,aes(x=hwy)) +geom_histogram(binwidth = 2)


# density instead of the count 

p1 <- ggplot(mpg, aes(x=hwy, y=after_stat(density))) + 
  geom_histogram(bins=8, fill="blue", alpha=0.5) +
  labs(title="Histogram of Highway MPG density")
p2 <- ggplot(mpg, aes(x=hwy)) + 
  geom_density(fill='red', alpha=0.5) +
  labs(title="Density Plot of Highway MPG")
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
#Scatterplots
ggplot(data=iris, aes(x=Sepal.Length, y=Petal.Length,color=Species))+
  geom_point()+# Anything set inside an aes() command will be of the form attribute=Column_Name and will change based on the data. Anything set outside an aes() command will be in the form attribute=value and will be fixed.
  geom_smooth(method="lm")


ggplot(data=iris, aes(x=Sepal.Length, y=Petal.Length)) +
  geom_point(aes(color=Species,shape=Species))+
  geom_smooth(method="lm")

ggplot(iris, aes(x = Sepal.Length)) +
  geom_density(aes(color = Species, fill = Species), alpha = 0.5) +
  labs(title = "Density plot") +
  labs(x = "Sepal Length", y = "Density")

ggplot(iris, aes(x = Sepal.Length)) +
  geom_density(aes(fill = Species,color=Species), alpha = 0.5) +
  labs(title = "Density plot") +
  labs(x = "Sepal Length", y = "Density") +
  labs(fill = "Revise the name",color="Revise the name")  # fill is the area,color is the line or dot

mtcars$cyl <- factor(mtcars$cyl) 

ggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_point() +
  labs(title='Weight vs Miles per Gallon') +
  labs(x="Weight in tons (2000 lbs)", y="Miles per Gallon (US)" ) +
  labs(color="Cylinders") + # color is dot or line
  scale_color_manual(values=c('blue', 'darkmagenta', 'aquamarine')) # diy color
rainbow(3);rainbow(6)



library(colorspace)   # these two packages have some decent 
library(grDevices)    # color palettes functions.
library(MDplot) #Ramachandran plots is often used to show the sampling of the 𝜙/𝜓protein backbone dihedral angles in order to assign propensities of secondary structure elements to the protein of interest
exp_data <- load_ramachandran(system.file( "extdata/ramachandran_example_GROMACS.txt.gz",package = "MDplot" ), mdEngine = "GROMACS")
exp_data <- data.frame(exp_data)
colnames(exp_data) <- c('phi', 'psi')
head(exp_data)

#ggplot(exp_data, aes(x=phi, y=psi) ) + geom_bin_2d(bins=100)
ggplot(exp_data, aes(x=phi, y=psi) ) + geom_hex(bins=100)

ggplot(exp_data, aes(x=phi, y=psi) ) + geom_hex(bins=100) +
  scale_fill_continuous(type = "viridis") # scale_fill_gradient(low = "blue", high = "red")
ggplot(exp_data, aes(x=phi, y=psi) ) + geom_hex(bins=100) +
  scale_fill_gradient2(low = "blue", mid='purple', high = "red", midpoint=10) 
#ggplot(exp_data, aes(x=phi, y=psi) ) + geom_hex(bins=100) +
  #scale_fill_gradientn(colours = terrain.colors(5))

ggplot(exp_data, aes(x=phi, y=psi) ) + geom_density_2d_filled()

# Adjusting axes

ggplot(mpg, aes(x=class, y=hwy)) + 
  geom_boxplot() +
  scale_y_continuous(breaks = seq(10, 45, by=5))  #---diy scale in y-axis 

# ggplot(mpg, aes(x=class, y=hwy)) + geom_boxplot() +scale_y_continuous(breaks = seq(10, 45, by=5), minor_breaks = NULL)

# Zooming in/out--Danger!  This removes the data points first!
#ggplot(trees, aes(x=Girth, y=Volume)) + 
  #geom_point() +
  #geom_smooth(method='lm') +
  #xlim( 8, 19 ) + ylim(0, 60)





```

-   Example (https://github.com/jcoliver/learn-r/blob/gh-pages/data/mine-microbe-class-data.csv)

```{r}

mine.data <- data.frame(
  Site = c(1, 2, 3, 1, 2, 3, 2, 3),
  Depth = c(0.5, 0.5, 0.5, 3.5, 3.5, 3.5, 25, 25),
  Sample.name = c('1-S', '2-S', '3-S', '1-M', '2-M', '3-M', '2-B', '3-B'),
  Actinobacteria = c(373871, 332856, 326695, 409809, 319778, 445572, 128251, 96304),
  Cytophagia = c(8052, 28561, 10468, 4481, 15885, 7302, 4732, 5566),
  Flavobacteriia = c(0, 0, 0, 0, 5230, 6218, 5917, 6353),
  Sphingobacteriia = c(0, 10013, 4918, 0, 8274, 8284, 0, 0),
  Nitrospira = c(0, 0, 0, 0, 0, 0, 4609, 0),
  Planctomycetia = c(4553, 10008, 0, 0, 0, 0, 56836, 67380),
  Alphaproteobacteria = c(143534, 70575, 105890, 110746, 52504, 45000, 133851, 95580),
  Betaproteobacteria = c(124454, 170161, 187673, 87245, 146073, 91711, 90204, 85707),
  Deltaproteobacteria = c(0, 0, 0, 0, 0, 0, 4260, 0),
  Gammaproteobacteria = c(8426, 9005, 12935, 7025, 110825, 69452, 31956, 165572)
)
```

3.  Check the structure of the data frame.

```{r}
str(mine.data)
```

The data frame has 8 rows (“obs.”) and 13 columns (“variables”). The first three columns have information about the observation (Site, Depth, Sample.name), and the remaining columns have the abundance for each of 10 classes of bacteria.

We ultimately want a heatmap where the different sites are shown along the x-axis, the classes of bacteria are shown along the y-axis, and the shading of the cell reflects the abundance. This latter value, the abundance of a bacteria at specific depths at different site, is really just a third dimension. However, instead of creating a 3-dimensional plot that can be difficult to visualize, we instead use shading for our “z-axis”. To this end, we need our data formatted so we have a column corresponding to each of these three dimensions:

-   X: Sample identity
-   Y: Bacterial class
-   Z: Abundance

The challenge is that our data are not formatted like this. While the Sample.name column corresponds to what we would like for our x-axis, we do not have columns that correspond to what is needed for the y- and z-axes. All the data are in our data frame, but we need to take a table that looks like this:

Sample.name Actinobacteria Cytophagia 1-S 373871 8052 2-S 332856 28561 ... ... ...

And transform it to one with a column for bacterial class and a column for abundance, like this:

Sample.name Class Abundance 1-S Actinobacteria 373871 1-S Cytophagia 8052 2-S Actinobacteria 332856 ... ... ...

In the later labs, you will learn convenient package and function to achieve this. But for now, we just use some basic function to do so.

4.  Remove the first two columns.

```{r}
mine.data <- mine.data[,-c(1,2)]
mine.data
```

5.  Use "colnames()" to get a vector of class names and assign it to the object named by "class.name"

```{r}
class.name <- colnames(mine.data)[-1]
class.name




```

6.  For Sample.name 1-S, use the as.numeric() function to convert its corresponding row to a vector. You should first exclude the 1-S in that row. Store this vector using an object named "abundance".

```{r}
abundance <- as.numeric(mine.data[mine.data$Sample.name == '1-S', -1])
abundance

```

7.  Combine the `Sample.name` 1-S, `class.name` vector, and `abundance` vector into a 10-row, 3-column (Sample.name , Class, Abundance) data.frame as shown in the table above.

```{r}
df1s <- data.frame(Sample.name = '1-S',
                   Class=class.name,
                   Abundance=abundance)
df1s
```

8.  Repeat question 6 and 7 for all Sample.name, and `rbind()` the results into a data frame named "mine.table".

```{r}
df2s <- data.frame(Sample.name = '2-S',
                   Class=class.name,
                   Abundance=as.numeric(mine.data[mine.data$Sample.name == '2-S', -1]))
df3s <- data.frame(Sample.name = '3-S',
                   Class=class.name,
                   Abundance=as.numeric(mine.data[mine.data$Sample.name == '3-S', -1]))
df1m <- data.frame(Sample.name = '1-M',
                   Class=class.name,
                   Abundance=as.numeric(mine.data[mine.data$Sample.name == '1-M', -1]))
df2m <- data.frame(Sample.name = '2-M',
                   Class=class.name,
                   Abundance=as.numeric(mine.data[mine.data$Sample.name == '2-M', -1]))
df3m <- data.frame(Sample.name = '3-M',
                   Class=class.name,
                   Abundance=as.numeric(mine.data[mine.data$Sample.name == '3-M', -1]))
df2b <- data.frame(Sample.name = '2-B',
                   Class=class.name,
                   Abundance=as.numeric(mine.data[mine.data$Sample.name == '2-B', -1]))
df3b <- data.frame(Sample.name = '3-B',
                   Class=class.name,
                   Abundance=as.numeric(mine.data[mine.data$Sample.name == '3-B', -1]))
mine.table <- rbind(df1s, df2s, df3s, df1m, df2m, df3m, df2b, df3b)
mine.table
```

This can also be realized by

``` r
exp.long <- pivot_longer(data = exp.data, 
                         cols = -c(subject, treatment),
                         names_to = "gene", 
                         values_to = "expression")
```

better look of lables

``` r

exp.heatmap + theme(axis.title.y = element_blank(), # Remove the y-axis title
                    axis.text.x = element_text(angle = 45, vjust = 0.5)) # Rotate the x-axis labels
```

Now the data are ready - on to the plot!

9.  To plot a heatmap, we are going to use the `ggplot2` package. Install and library `ggplot2`.

For this plot, we are going to first create the heatmap object with the `ggplot` function, then print the plot. We create the object by assigning the output of the `ggplot` call to the variable `mine.heatmap`, then entering the name of this object to print it to the screen.

-   Plot heatmap with x=Sample.name and y=Class. Fill the heatmap using Abundance value.
-   Choose right `geom_` function to perform heatmap

```{r}
mine.heatmap <- ggplot(data = mine.table, mapping = aes(x = Sample.name,
                                                        y = Class,
                                                        fill = Abundance)) + 
  geom_tile() # use geom_tile to draw the heatmap
mine.heatmap
```

11. First let’s deal with a scaling issue. Most values are dark blue, but this is partly caused by the distribution of values - there are a few really high values in the Abundance column. We can transform the data for display by taking the square root of abundance and plotting that. To do this, we need to add another column to our data frame and update our call to ggplot to reference this new column, `Sqrt.abundance`.

```{r}
mine.table$Sqrt.abundance <- sqrt(mine.table$Abundance)
mine.heatmap <- ggplot(data = mine.table, mapping = aes(x = Sample.name,
                                                        y = Class,
                                                        fill = Sqrt.abundance)) +
  geom_tile() +
  xlab(label = "Sample")
mine.heatmap
```

12. Change the shading so low values are light and high values are dark. Use white (#FFFFFF) for the low values and a dark blue (#012345) for high values. You can also pick whatever color you like.

``` r
mine.heatmap <- mine.heatmap + scale_fill_gradient(low="#FFFFFF", high="#012345")
mine.heatmap
```

13. Add a title "Microbe Class Abundance" to this plot.

``` r
mine.heatmap <- mine.heatmap + ggtitle(label = "Microbe Class Abundance")
mine.heatmap
```

14. Reverse the order of the y-axis, so ‘Actinobacteria’ is at the top. Use `rev()` function to get the reverse of the `levels` of the Class. Use the obtained character vector as the `limits` of `scale_y_discrete()` function.

```{r}
mine.heatmap <- mine.heatmap + scale_y_discrete(limits=rev(levels(as.factor(mine.table$Class))))
mine.heatmap
```

15. Use "geom_text()" to add abundance values on top of the heatmap. Maybe the text is too big to display on top of the heatmap? what can we do? Consider removing decimals and resizing the text.

```{r}
mine.heatmap + geom_text(aes(label = round(Sqrt.abundance, 0)), size=2)
```

16. Congratulations on completing the above task, but you may have noticed that the x-axis Sample.name are not in the correct order. In fact, a good idea is to visualize different categories of data, we will learn this function next lab. But if you have some time, you may have a try by looking the help information of `facet_grid`.

This approach, called “faceting” requires one additional layer to the plotting command called facet_grid. With `facet_grid` we indicate which column contains the categories we want to use for the plot. But before we do that, we need add `Depth` back to the data frame. For Sample.name %in% c("1-S", "2-S", "3-S"), add Depth=0.5 in a new column named `Depth`; For Sample.name ends with M, add Depth=3.5; and 25 for those end with B.

```{r}
mine.table$Depth <- NULL
mine.table[mine.table$Sample.name %in% c("1-S", "2-S", "3-S"), 'Depth'] = 0.5
mine.table[mine.table$Sample.name %in% c("1-M", "2-M", "3-M"), 'Depth'] = 3.5
mine.table[mine.table$Sample.name %in% c("2-B", "3-B"), 'Depth'] = 25
mine.table
```

17. Faceting the plot by `Depth`, set `scales` and `space` as `free` and add `x` to `switch`.

```{r}
mine.table$Sqrt.abundance <- sqrt(mine.table$Abundance)
mine.heatmap <- ggplot(data = mine.table, mapping = aes(x = Sample.name,
                                                        y = Class,
                                                        fill = Sqrt.abundance)) +
  geom_tile() +
  facet_grid(~Depth, switch = 'x', scales='free', space='free') + 
  xlab(label = "Sample") + 
  scale_fill_gradient(low="#FFFFFF", high="#012345") + 
  ggtitle(label = "Microbe Class Abundance") + 
  scale_y_discrete(limits=rev(levels(as.factor(mine.table$Class)))) + 
  geom_text(aes(label = round(Sqrt.abundance, 0)), size=2)
mine.heatmap
```

```{r}
library(palmerpenguins)
ggplot(penguins, aes(x=flipper_length_mm, y=body_mass_g)) + geom_point(aes(color=bill_depth_mm, shape=species), na.rm=T) + geom_smooth(na.rm=T, se=F) + scale_color_gradient2(low='yellow', mid='green', high='blue', midpoint = 17) + labs(x='Flipper length (millimeters)', y='Body mass (grams)', color='Bill depth (millimeters)') + theme_bw()
```

-   Faceting (make many panels of graphics where each panel represents the same relationship between variables, but something changes between each pane)

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +
  geom_point() +
  facet_grid(.~Species) #or facet_grid(Species~.)--Categorical variables of species will be vertical
```

-   Another example

```{r}
library(reshape)
data(tips, package='reshape')
head(tips, 3)
ggplot(tips, aes(x = total_bill, y = tip / total_bill)) +
  geom_point() +
  facet_grid( smoker ~ day )
# 'free_y' means the scale of different panels are adjusted by themselves
ggplot(tips, aes(x = total_bill, y = tip / total_bill)) +
  geom_point() +
  facet_wrap( ~ day, scales='free_y')
# log scales ---a wrapper of  scale_y_continuous() function , trans_new() function
# ggplot(ACS, aes(x=Age, y=Income)) + geom_point() +
# scale_y_log10(breaks=c(1, 10, 100),
#            minor=c(1:10,
#                 seq(10, 100, by=10 ),
#                seq(100, 1000, by=100))) +
#  ylab('Income (1000s of dollars)')



```

-   Multi-plot

```{r}
p1 <- ggplot(ChickWeight, aes(x=Time, y=weight, colour=Diet, group=Chick)) +
    geom_line() +
    ggtitle("Growth curve for individual chicks")
# Second plot
p2 <- ggplot(ChickWeight, aes(x=Time, y=weight, colour=Diet)) +
    geom_point(alpha=.3) +
    geom_smooth(alpha=.2, linewidth=1) +
    ggtitle("Fitted growth curve per diet")
# Third plot 
p3 <- ggplot(subset(ChickWeight, Time==21), aes(x=weight, colour=Diet)) +
    geom_density() +
    ggtitle("Final weight, by diet")


# to realize:
# plot1 plot2 plot2
# plot1 plot2 plot2
# plot1 plot3 plot3

my.layout = cbind( c(1,1,1), c(2,2,3), c(2,2,3) ) # each c represents a column in a matrix and 1,2,3 represents p1,p2,p3
library(Rmisc)
Rmisc::multiplot( p1, p2, p3, layout=my.layout)



# OR library(ggpubr) https://rpkgs.datanovia.com/ggpubr/). 

library(ggpubr)
# Box plot (bp)
bxp <- ggboxplot(ToothGrowth, x = "dose", y = "len",
                 color = "dose", palette = "jco")
# Dot plot (dp)
dp <- ggdotplot(ToothGrowth, x = "dose", y = "len",
                 color = "dose", palette = "jco", binwidth = 1)
mtcars$name <- rownames(mtcars)
mtcars$cyl <- as.factor(mtcars$cyl)
bp <- ggbarplot(mtcars, x = "name", y = "mpg",
          fill = "cyl",               # change fill color by cyl
          color = "white",            # Set bar border colors to white
          palette = "jco",            # jco journal color palett. see ?ggpar
          sort.val = "asc",           # Sort the value in ascending order
          sort.by.groups = TRUE,      # Sort inside each group
          x.text.angle = 90           # Rotate vertically x axis texts
          ) + font("x.text", size = 8)
# Scatter plots (sp)
sp <- ggscatter(mtcars, x = "wt", y = "mpg",
                add = "reg.line",               # Add regression line
                conf.int = TRUE,                # Add confidence interval
                color = "cyl", palette = "jco", # Color by groups "cyl"
                shape = "cyl"                   # Change point shape by groups "cyl"
                ) + 
  stat_cor(aes(color = cyl), label.x = 3)       # Add correlation coefficient

ggarrange(bxp, dp, bp + rremove("x.text"),
          labels = c("A", "B", "C"),
          ncol = 2, nrow = 2)
# Themes
# Rmisc::multiplot( p1 + theme_bw(),          # Black and white
#                   p1 + theme_minimal(),   
#                   p1 + theme_dark(),        
#                   p1 + theme_light(),
#                   cols=2 )

#ggsave('p1.png', width=6, height=3, dpi=350)


```

![](images/clipboard-2348163751.png)

-   data manipulation

```{r}
library(dplyr)
library(tidyverse)
# apply
# Summarize each column by calculating the mean.
apply(iris[,-5],        # what object do we want to apply the function to
      MARGIN=1,    # rows = 1, columns = 2, (same order as [rows, cols]
      FUN=mean     # what function do we want to apply     
     ) %>% head(10)


average <- apply( 
  iris[,-5],        # what object do we want to apply the function to
  MARGIN=2,    # rows = 1, columns = 2, (same order as [rows, cols]
  FUN=mean     # what function do we want to apply
)
iris <- rbind(iris[,-5], average)
iris %>% head(3)

# There are several variants of the apply() function, and the most frequently used ones are lapply() and sapply(). These two functions apply a given function to each element of a list or vector and returns a corresponding list or vector of results.

#lapply
x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))
x
lapply(x, quantile, probs = 1:3/4) # list 
sapply(x, quantile, probs = 1:3/4) # matrix

```

A tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they don t change variable names or types, and don t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.

```{r}
data <- data.frame(a = 1:3, b = letters[1:3], c = Sys.Date() - 1:3)
data

as_tibble(data)

```

-   %\>%

For example, if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %\>%, this sequence of operations becomes x %\>% f() %\>% g() %\>% h().

-   select

```r

library(dplyr)

# Correct usage of select() within a pipeline
starwars %>% select(-ends_with('color'))

```

-   filter

```{r}
library(dplyr)

# Filter rows where species is "Droid" and mass is greater than or equal to 100
filtered_data <- starwars %>% filter(species == "Droid", mass < 100)
print(filtered_data)

```

-   slice

filter rows based on row number:

```{r}
starwars %>% slice(2:4)
```

-   arrange

```{r}
starwars %>% arrange(desc(name)) #The default sorting is in ascending order

starwars %>% arrange(desc(height)) %>% head(3)
dd <- data.frame(
  Trt = factor(c("High", "Med", "High", "Low"),        
               levels = c("Low", "Med", "High")), # level
  y = c(8, 3, 9, 9),      
  z = c(1, 1, 1, 2)) 
dd %>% arrange(Trt, desc(y))
```

-   mutate

```r
# select using the old columns
starwars$bmi = starwars$mass / ((starwars$height / 100) ^ 2)
starwars %>% select(name, bmi) %>% head(3)

# mutate avoids all the starwars$

starwars$bmi <- NULL
starwars %>% 
  mutate(bmi = mass / ((height / 100) ^ 2)) %>%  
  select(name, bmi) %>% head(3)

# mutate_at() and mutate_if() allow us to apply a function to a particular column and save the output.

subset <- starwars %>% 
  mutate(square_height = (height / 100) ^ 2,
         bmi = mass / square_height) %>%
  select(name, square_height, bmi)
subset %>% head(3)

subset %>% mutate_if(is.numeric, round, digits=0) # here, is.numeric is the condition

subset %>% mutate_at(2:3, round, digits=0) %>% head() # column 2 3

# Apply the transformation to columns 2 and 3 for rows 1 to 3
result <- subset %>% 
  mutate_at(2:3, ~ifelse(row_number() %in% 1:3, round(., digits = 0), .))

subset %>% mutate(avg.example = select(., square_height:bmi) %>% rowMeans())


```

-   summarise

This function is used to create a summary table. It reduces the data frame to a single row containing summary statistics.

```r
starwars %>% summarise(mean.height=mean(height, na.rm=T), sd.height=sd(height, na.rm=T))

# apply the same statistic to each column
starwars %>% select(height:mass) %>% summarise_all(list(min=min, max=max), na.rm=T)
starwars %>% summarise_if(is.numeric, list(min=min, max=max), na.rm = T)
```

-   group_by

```r

library(dplyr)
library(palmerpenguins)
table(penguins$sex, penguins$species)
penguins %>% 
  filter(!is.na(sex)) %>%
  group_by(sex, species) %>%           
  summarise(n = n(), 
            mean.flipper = mean(flipper_length_mm),
            sd.flipper = sd(flipper_length_mm),
            .groups='keep') %>%
  head(3)
  
```

-   the comparison between whether to use %\>% or not

``` r
# %>% 
penguins %>% 
  filter(!is.na(sex)) %>%
  group_by(sex, species) %>%           
  mutate(Sum.Sq.Cells = (flipper_length_mm - mean(flipper_length_mm))^2)  %>%  
  select(sex, species, flipper_length_mm, Sum.Sq.Cells) %>% head()

# not use %>% 
head(
  select(mutate(group_by(filter(penguins, !is.na(sex)), sex, species),
                Sum.Sq.Cells = (flipper_length_mm - mean(flipper_length_mm))^2),
       sex, species, flipper_length_mm, Sum.Sq.Cells))
```

``` r
library(nycflights13)
str(nycflights13::flights)
# the order of group_by and summarize matters
flights %>% 
  group_by(carrier) %>% 
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>% 
  arrange(desc(avg_dep_delay))
```

13. Find the flight with the longest departure delay among flights from the same origin and destination (use `filter()`). Relocate the origin, destination, and departure delay to the first three columns and sort by origin and dest.

``` r
flights %>% 
  filter(!is.na(dep_delay)) %>% 
  group_by(origin, dest) %>% 
  filter(dep_delay == max(dep_delay)) %>% 
  relocate(origin, dest, dep_delay) %>% 
  arrange(origin, dest)
```

14. Find the flight with the longest departure delay among flights from the same origin and destination (use `top_n()` or `slice_max()`). Relocate the origin, destination, and departure delay to the first three columns and sort by origin and dest.

``` r
flights %>% 
  filter(!is.na(dep_delay)) %>% 
  group_by(origin, dest) %>% 
  top_n(1, dep_delay) %>%  # or using slice_max(dep_delay) %>% 
  relocate(origin, dest, dep_delay) %>% 
  arrange(origin, dest)
```

15. How do departure delays vary at different times of the day? Summarize the averaged departure delays by hours and create an new column named as `dep_delay_level` which `cut()` the averaged departure delays into three levels (low, median, and high).

``` r
flights %>% 
  group_by(hour) %>% 
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  mutate(dep_delay_level = cut(avg_dep_delay, breaks=3, c('low', 'median', 'high')))
```

16. How do departure delays vary at different times of the day? Illustrate your answer with a `geom_smooth()` plot.

``` r
flights %>% 
  group_by(hour) %>% 
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  ggplot(aes(x = hour, y = avg_dep_delay)) + geom_smooth()
```

``` r
# ways to deleter the blanks
students %>%
  rename(
    student_id = `Student ID`,
    full_name = `Full Name`
  ) %>% head(3)
```

``` r


read_csv(
  "# A comment I want to skip
  x,y,z
  1,2,3",
  comment = "#"
)
```

```{r}
library(tidyr)
grade.book <- rbind(
  data.frame(name='Alison',  HW.1=8, HW.2=5, HW.3=8, HW.4=4),
  data.frame(name='Brandon', HW.1=5, HW.2=3, HW.3=6, HW.4=9),
  data.frame(name='Charles', HW.1=9, HW.2=7, HW.3=9, HW.4=10))
grade.book
tidy.scores <- grade.book %>%
  pivot_longer(
    cols = starts_with("HW"),
    names_to = "Homework",
    values_to = "Score"
  )
tidy.scores
tidy.scores %>% pivot_wider(names_from=Homework, values_from=Score)

# table joins

Fish.Data <- tibble(
  Lake_ID = c('A','A','B','B','C','C'), 
  Fish.Weight=rnorm(6, mean=260, sd=25) ) # make up some data
Fish.Data
Lake.Data <- tibble(
  Lake_ID = c('B','C','D'),   
  Lake_Name = c('Lake Elaine', 'Mormon Lake', 'Lake Mary'),   
  pH=c(6.5, 6.3, 6.1),
  area = c(40, 210, 240),
  avg_depth = c(8, 10, 38))
Lake.Data
full_join(Fish.Data, Lake.Data)
left_join(Fish.Data, Lake.Data)
inner_join(Fish.Data, Lake.Data)
```

``` r
mutate(
    week = parse_number(week)
  )
```

how many data points are in the data set

``` r
gender_year <- Survey %>% 
  filter(!is.na(Year)) %>% 
  group_by(Sex, Year) %>% 
  count() %>% 
  rename(nu=n)
gender_year
gender_year %>% pivot_wider(names_from = Year, values_from = nu)
```

``` r
who2 %>% 
  head(3)
who2 <- who2 %>% 
  pivot_longer(
    cols = !(country:year), 
    names_to = c("diagnosis", "gender", "age"), 
    names_sep = "_",
    values_to = "count"
  ) %>% 
  filter(!is.na(count))
who2
```

``` r
left_join(feb14_VX, airports, by=c('dest'='faa'))
```
```r

library(psych)
drug_prop <- drug_prop %>% 
  filter(class == 'Carboxylic acids and derivatives') 
drug_prop %>% 
  select(logP, logS, water_solubility) %>% 
  pairs.panels()
```

- for loop

```r
F <- rep(0, 10)        
F[1] <- 0             
F[2] <- 1              
cat('F = ', F, '\n') 

for( n in 3:10 ){
  F[n] <- F[n-1] + F[n-2]
  cat('F = ', F, '\n')    
}
```

 - bootstrap estimate of a sampling distribution
 
```{r}
library(dplyr)
library(ggplot2)
SampDist <- data.frame()

for (i in 1:1000){
  SampDist <- trees %>% 
    slice_sample(n=30, replace =TRUE) %>% 
    dplyr::summarise(xbar=mean(Height)) %>% 
    rbind(SampDist)
}
ggplot(SampDist,aes(x=xbar)) +
  geom_histogram()

```

## Ellipses

```{r}
# a function that draws the regression line and confidence interval
# notice it doesn't return anything, all it does is draw a plot
show.lm <- function(m, interval.type='confidence', fill.col='light grey', ...){
  x <- m$model[,2]       # extract the predictor variable
  y <- m$model[,1]       # extract the response
  pred <- predict(m, interval=interval.type)
  plot(x, y, ...)
  polygon( c(x,rev(x)),                         # draw the ribbon defined
           c(pred[,'lwr'], rev(pred[,'upr'])),  # by lwr and upr - polygon
           col='light grey')                    # fills in the region defined by            
  lines(x, pred[, 'fit'])                       # a set of vertices, need to reverse            
  points(x, y)                                  # the uppers to make a nice figure
} 
```

# copy of x

```{r}
k <- 3
example.func <- function(x){
  x <- sort(x)
  if (k > 1){
    print(x)
  }
}
x <- c(3,1,5,4,2)
example.func(x)
x # x is changed inside the function but not outsied the function
```

# LAB 10
## Part 1

1. Load the `bloodpress.txt`

```{r}
bloodpress <- read.table("bloodpress.txt", header=T)
bloodpress
```

2. Use `pairs.panels()` function from `psych` pacakge to draw scatterplots, histograms, and calculate correlations between variables.
```{r}
library(psych)
pairs.panels(bloodpress[, -1])
```

3. Fit a simple linear regression model of BP vs Stress. Is Stress significant?

```{r}
model.1 <- lm(BP ~ Stress, data=bloodpress)
summary(model.1)
```

4. Fit a simple linear regression model of BP vs Weight.

```{r}
model.2 <- lm(BP ~ Weight, data=bloodpress)
summary(model.2)
```

5. Fit a simple linear regression model of BP vs BSA.

```{r}
model.3 <- lm(BP ~ BSA, data=bloodpress)
summary(model.3)
```

6. Fit a multiple linear regression model of BP vs Weight + BSA. Is BSA still significant? Why?

```{r}
model.4 <- lm(BP ~ Weight + BSA, data=bloodpress)
summary(model.4)
```

8. Predict BP for Weight=92 and BSA=2 for the two simple linear regression models and the multiple linear regression model, by hand and by `predict()` function.

```{r}
2.20531 + 1.20093 * 92
predict(model.2,
        newdata=data.frame(Weight=92))
45.183 + 34.443 * 2
predict(model.3,
        newdata=data.frame(BSA=2))
5.6534 + 1.0387 * 92 + 5.8313 * 2
predict(model.4,
        newdata=data.frame(Weight=92, BSA=2))
```

7. Fit a multiple linear regression model of BP vs Age + Weight. Set argument `x` and `y` as TRUE. Save the output of `lm()` as `model.5`. How do we interpret each estimated coefficients?

```{r}
model.5 <- lm(BP ~ Age + Weight, data=bloodpress, x=TRUE, y=TRUE)
summary(model.5)
```

8. Use the `plot_ly` function in the `plotly` package to create a 3D scatterplot of the data with the fitted plane for a multiple linear regression model of BP vs Age + Weight.

```{r}
library(plotly)
plot_ly(x=bloodpress$Age, y=bloodpress$Weight, z=bloodpress$BP, type='scatter3d', mode='markers', color=bloodpress$BP)
```

9. Extract the matrix `x` and `y` of `model.5` and assign it to a new object `X` and `y`. Remember, if you save the output of `lm()` as an object, this object contains many elements. After we set `x=TRUE` and `y=TRUE` in question 8, we can find `x` and `y` in this list.

```{r}
X <- model.5$x
y <- model.5$y

```

10. Calculate $X^{T}X$, $X^{T}y$, $(X^{T}X)^{-1}$, and $(X^{T}X)^{-1}X^{T}y$. Use `t()` for transpose, `%*%` for matrix multiplication, and `solve()` for inverse of matrix. For the last one, is your result same as the estimated values you obtained in question 7? --Of course!

```{r}
t(X) %*% X
t(X) %*% y
solve(t(X) %*% X)
solve(t(X) %*% X) %*% (t(X) %*% y)
```
11. Use the anova function to display the ANOVA table with sequential (type I) sums of squares for the `model.5`.

$SS_{\text{Variable}} = \sum_{i=1}^n (\hat{y}_{\text{Variable}, i} - \bar{y})^2 $

$\hat{y}_{\text{Variable}, i}$ is the model including only varible i.

$ F = \frac{\text{Mean Square for the Variable (MS_Variable)}}{\text{Mean Square for Residuals (MS_Residuals)}} $

If (F $\approx$ 1) : It indicates that the sizes of MS_Variable and MS_Residuals are approximately the same, suggesting that the explanatory power of the independent variable for the dependent variable is comparable to the random error, and the null hypothesis ((H_0)) cannot be rejected. 

If (F $\gg$ 1) : It indicates that MS_Variable is significantly greater than MS_Residuals, suggesting that the independent variable has a significant influence on the dependent variable, and the null hypothesis ((H_0)) can be rejected.

```{r}
anova(model.5)
# remark
sum((model.5$y-mean(model.5$y))^2) == 243.266+311.910+4.824
```
$SS_{\text{Total}} = SS_{\text{Age}} + SS_{\text{Weight}} + SS_{\text{Residuals}} $

12. Use the `residuals` element in fitted model or `residuals()` function to extract the fitted residuals. Calculate the sum of square of these residual values. Extract the `df.residual` element in fitted model and use the above elements to calculate the MSE. Is your result same as the `anova()` output?
```{r}
sum((model.5$residuals)^2)/model.5$df.residual
```

13. Fit a multiple linear regression model of BP vs Age + Weight + Pulse. Save the output of `lm()` as `model.6`.

```{r}
model.6 <- lm(BP ~ Age + Weight + Pulse, data=bloodpress)
summary(model.6)
```

14. Use `anova()` function to obtain the ANOVA table for `model.6`. We may consider `model.6` as full model, and `model.5` as reduced model in this question. Based on the obtained ANOVA table and the output of question 11, calculate the F-statistic for testing the reduced model by hand. You may use the Residuals Sum sq and the corresponding Residuals Df from both tables. Then, calculate the p-value using $pf()$ function, don't forget about the `lower.tail`.

```{r}
anova(model.6)
Fstat <- (4.824-4.328)/(17-16) / (4.328/16)
Fstat
pf(Fstat, 1, 16, lower.tail = F)
```

15. Use `anova()` function to do the F-test on `model.5` and `model.6`. Compare the output with your answers of question 14. What is the conclustion of the F-test?

$\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 $

$\text{Sum of Sq} = \text{RSS(Model 1)} - \text{RSS(Model 2)} $ (It represents the variation in the interpretation of the dependent variable by the newly added variable Pulse.)

$$ F = \frac{\text{Sum of Sq} / \text{Df}}{\text{RSS(Model 2)} / \text{Res.Df(Model 2)}} \\F = \frac{0.49557}{0.270525} \approx 1.8319$$

```{r}
anova(model.5, model.6)
```

16. Plot the qqPlot for residuals of `model.5`. What is the x-axis and y-axis of the qqPlot? What can we say about the qqPlot?

```{r}
library(car)
qqPlot(model.5$residuals)
```

17. Plot the residual vs fitted plot of `model.5`. You may extract `fitted.values` from `model.5` and use it as `x` in `plot()`.

```{r}
plot(x=model.5$fitted.values, y=model.5$residuals)
```

18. Directly use `plot()` function on `model.5`.

```{r}
plot(model.5)
```

## Part 2

19. Load the `hospital_infct.txt` data and select observations with Stay <= 14.

```{r}
infectionrisk <- read.table("/Users/luyu/Desktop/NOTEsAPH101/hospital_infct.txt", header=T)
infectionrisk <- infectionrisk[infectionrisk$Stay<=14,]
infectionrisk
```

20. Create new dummy/indicator columns (`i1`, `i2`, `i3`, `i4`) for regions using `ifelse()` function. For example, i1 = 1 when Region = 1 and i1 = 0 when Region is not equal to 1; i2 = 1 when Region = 2 and i2 = 0 when Region is not equal to 2; ... 

```{r}
infectionrisk$i1 <- ifelse(infectionrisk$Region == 1, 1, 0)
infectionrisk$i2 <- ifelse(infectionrisk$Region == 2, 1, 0)
infectionrisk$i3 <- ifelse(infectionrisk$Region == 3, 1, 0)
infectionrisk$i4 <- ifelse(infectionrisk$Region == 4, 1, 0)
```

21. Fit a multiple linear regression model of InfctRsk on Stay + Xray + i2 + i3 + i4.

```{r}
model.7 <- lm(InfctRsk ~ Stay + Xray + i2 + i3 + i4, data=infectionrisk)
summary(model.7)
```


23. Can we include i1 + i2 + i3 + i4 in this multiple linear regression? Why?

No. In the context of using dummy variables for categorical data in regression analysis, it's essential to designate a reference category. This reference category is represented by a coefficient of zero, while the coefficients for the other categories are interpreted as deviations from this reference point.

```{r}
model.8 <- lm(InfctRsk ~ Stay + Xray + i1 + i2 + i3 + i4, data=infectionrisk)
summary(model.8)
```

24. Conduct an F-test (use `anova()` function) to see if at least one of i2, i3, and i4 are useful. 
```{r}
model.9 <- lm(InfctRsk ~ Stay + Xray, data=infectionrisk)
anova(model.7, model.9)
```



# Confusion Matrix


\small
| Total population $=\mathrm{P}+\mathrm{N}$ | Predicted Positive (PP) | Predicted Negative (PN) | Informedness, bookmaker informedness (BM) $=\mathrm{TPR}+\mathrm{TNR}-1$ | Prevalence threshold (PT) $=\frac{\sqrt{\mathrm{TPR} \times \mathrm{FPR}}-\mathrm{FPR}}{\mathrm{TPR}-\mathrm{FPR}}$ |
| :--- | :--- | :--- | :--- | :--- |
| Positive (P) ${ }^{[\text {a }]}$ | True positive (TP), hit ${ }^{[b]}$ | False negative (FN), miss, underestimation | True positive rate (TPR), recall, sensitivity (SEN), probability of detection, hit rate, power $=\frac{\mathrm{TP}}{\mathrm{P}}=1-\mathrm{FNR}$ | False negative rate (FNR), miss rate type ll error ${ }^{[c]}$ $=\frac{\mathrm{FN}}{\mathrm{P}}=1-\mathrm{TPR}$ |
| Negative ( N ) ${ }^{[d]}$ | False positive (FP), false alarm, overestimation | True negative (TN), correct rejection ${ }^{[\text {e }]}$ | False positive rate (FPR), probability of false alarm, fall-out type I error ${ }^{[7]}$ $=\frac{\mathrm{FP}}{\mathrm{~N}}=1-\mathrm{TNR}$ | True negative rate (TNR), specificity (SPC), selectivity $=\frac{\mathrm{TN}}{\mathrm{N}}=1-\mathrm{FPR}$ |

## example


### Lung Cancer Classification (https://www.kaggle.com/datasets/mysarahmadbhat/lung-cancer?select=survey+lung+cancer.csv)

The effectiveness of cancer prediction system helps the people to know their cancer risk with low cost and it also helps the people to take the appropriate decision based on their cancer risk status. The data is collected from the website online lung cancer prediction system.

Total no. of attributes: 16
No. of instances: 284

Attribute information:

Gender: M(male), F(female)
Age: Age of the patient
Smoking: YES=2, NO=1.
Yellow fingers: YES=2, NO=1.
Anxiety: YES=2, NO=1.
Peer_pressure: YES=2, NO=1.
Chronic Disease: YES=2, NO=1.
Fatigue: YES=2, NO=1.
Allergy: YES=2, NO=1.
Wheezing: YES=2, NO=1.
Alcohol: YES=2, NO=1.
Coughing: YES=2, NO=1.
Shortness of Breath: YES=2, NO=1.
Swallowing Difficulty: YES=2, NO=1.
Chest pain: YES=2, NO=1.
Lung Cancer: YES, NO.

Goal: It is your job to classify Lung Cancer using other variables

#### Example Code

```{r}
#Load the dataset 
library(readr)
data = read_csv('/Users/luyu/Desktop/survey_lung_cancer.csv', show_col_types = FALSE)
data$LUNG_CANCER <- ifelse(data$LUNG_CANCER=="YES", 1, 0)
summary(data)
```

```r
library(ggplot2)
ggplot(data, aes(x = factor(SMOKING), fill = factor(LUNG_CANCER))) +
geom_bar(position = "fill") +
scale_fill_manual(values = c("0" = "lightblue", "1" = "salmon")) +
theme_minimal()
```

```{r}
### Data SAMPLING ####
library(caret)
set.seed(101)
split = createDataPartition(data$LUNG_CANCER, p = 0.80, list = FALSE)
train_data = data[split,]
test_data = data[-split,]
nrow(train_data)
nrow(test_data)
```

```{r}
#error metrics -- Confusion Matrix
err_metric=function(CM)
{
  TN =CM[1,1]
  TP =CM[2,2]
  FP =CM[1,2]
  FN =CM[2,1]
  precision =(TP)/(TP+FP)
  recall_score =(TP)/(TP+FN)
  f1_score=2*((precision*recall_score)/(precision+recall_score))
  accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
  False_positive_rate =(FP)/(FP+TN)
  False_negative_rate =(FN)/(FN+TP)
  print(paste("Precision value of the model: ",round(precision,2)))
  print(paste("Accuracy of the model: ",round(accuracy_model,2)))
  print(paste("Recall value of the model: ",round(recall_score,2)))
  print(paste("False Positive rate of the model: ",round(False_positive_rate,2)))
  print(paste("False Negative rate of the model: ",round(False_negative_rate,2)))
  print(paste("F1 score of the model: ",round(f1_score,2)))
}
```

```{r}
# Logistic regression
logit_m =glm(formula = LUNG_CANCER ~ ., data = train_data, family = 'binomial')
summary(logit_m)
```

```{r}
# Logistic regression
logit_m2 =glm(formula = LUNG_CANCER ~ ANXIETY+PEER_PRESSURE+`CHRONIC DISEASE`+FATIGUE+ALLERGY+`ALCOHOL CONSUMING`+COUGHING+`SWALLOWING DIFFICULTY`, data = train_data, family = 'binomial')
summary(logit_m2)
```

```{r}
library(dplyr)
logit_P_prob = predict(logit_m, newdata = select(test_data, -LUNG_CANCER), type = 'response')
logit_P_prob[1:3]
logit_P <- ifelse(logit_P_prob > 0.5, 1, 0) # Probability check
logit_P[1:3]
```
```{r}
CM = table(test_data$LUNG_CANCER, logit_P)
print(CM)
```
```{r}
err_metric(CM)
```
```{r}
#ROC-curve using pROC library
library(pROC)
roc_score=roc(test_data$LUNG_CANCER, logit_P_prob) #AUC score
plot(roc_score, main = "ROC curve -- Logistic Regression")
```
